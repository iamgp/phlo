# Part 11: Observability and Monitoringâ€”Knowing Your Pipeline

You've built a data lakehouse with validation and governance. But what happens at 3am when something breaks? This post covers observability: monitoring, alerting, and troubleshooting.

## The Observability Problem

Without proper monitoring, failures hide:

```
Tuesday 3am:
  â€¢ DLT fails to fetch from Nightscout API
  â€¢ Data stops flowing
  â€¢ Nobody notices
  
Wednesday 9am:
  â€¢ Users report: "Dashboard shows stale data"
  â€¢ Investigation: "Last update was 30 hours ago"
  â€¢ Impact: 500+ people using outdated metrics
  â€¢ Root cause: API timeout at 3:14am, log buried in Dagster logs
```

Observability solves this with:

```
[Monitoring] â†’ Understand what's happening
[Alerting]   â†’ Get notified of problems
[Tracing]    â†’ Find root causes quickly
[Dashboards] â†’ Visualize pipeline health
```

## Three Pillars of Observability

### 1. Metrics (Numbers)

Track quantitative data:

```
â€¢ Pipeline runtime: 5.2 seconds
â€¢ Data quality: 99.7% valid rows
â€¢ Row throughput: 12,500 rows/minute
â€¢ Freshness: 2 hours since last update
â€¢ API latency: 150ms average
```

### 2. Logs (Events)

Track what happened and when:

```
[2024-10-15 10:35:42] âœ“ dlt_glucose_entries started
[2024-10-15 10:35:45] âœ“ Fetched 487 rows from API
[2024-10-15 10:35:47] âœ“ Pandera validation: 487/487 rows valid
[2024-10-15 10:35:49] âš  2 rows with invalid device type (logged)
[2024-10-15 10:35:51] âœ“ Merged to Iceberg (487 inserts, 2 updates)
[2024-10-15 10:35:53] âœ“ dlt_glucose_entries succeeded
```

### 3. Traces (Flows)

Track execution paths:

```
Request: dlt_glucose_entries asset materialization
â”œâ”€ Fetch from API (45ms)
â”‚  â”œâ”€ Auth (5ms)
â”‚  â””â”€ Network (40ms)
â”œâ”€ Pandera validation (12ms)
â”‚  â”œâ”€ Type checking (8ms)
â”‚  â””â”€ Constraint checking (4ms)
â”œâ”€ Merge to Iceberg (80ms)
â”‚  â”œâ”€ Read current snapshot (20ms)
â”‚  â”œâ”€ Merge operation (40ms)
â”‚  â””â”€ Write metadata (20ms)
â””â”€ Asset check (15ms)

Total: 152ms
```

## Cascade's Observability Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Layer           â”‚
â”‚  (Dagster, dbt, DLT)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ (emits events)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Collection Layer            â”‚
â”‚  â€¢ Dagster Logs              â”‚
â”‚  â€¢ Application Metrics       â”‚
â”‚  â€¢ System Metrics (Prometheus)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage Layer               â”‚
â”‚  â€¢ Loki (logs)               â”‚
â”‚  â€¢ Prometheus (metrics)      â”‚
â”‚  â€¢ Jaeger (traces)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analysis & Visualization    â”‚
â”‚  â€¢ Grafana (dashboards)      â”‚
â”‚  â€¢ Alertmanager (alerts)     â”‚
â”‚  â€¢ Superset (data quality)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Metrics: What to Track

### Asset-Level Metrics

```python
# cascade/defs/monitoring/metrics.py
from dagster import op, Out, DynamicOut, DynamicOutput, resource
from prometheus_client import Counter, Histogram, Gauge
import time


# Define metrics
asset_execution_time = Histogram(
    name="asset_execution_seconds",
    documentation="Asset execution time in seconds",
    labelnames=["asset_name", "status"],
)

asset_rows_processed = Counter(
    name="asset_rows_processed_total",
    documentation="Total rows processed",
    labelnames=["asset_name", "operation"],
)

asset_row_count_gauge = Gauge(
    name="asset_row_count",
    documentation="Current row count in asset",
    labelnames=["asset_name", "schema"],
)

data_freshness_seconds = Gauge(
    name="asset_freshness_seconds",
    documentation="Seconds since last update",
    labelnames=["asset_name"],
)

validation_pass_rate = Gauge(
    name="validation_pass_rate",
    documentation="Percentage of rows passing validation",
    labelnames=["asset_name", "check_name"],
)


@op
def ingest_with_metrics(context) -> int:
    """Ingest glucose data with metrics."""
    
    start_time = time.time()
    
    try:
        # Fetch data
        data = fetch_from_api()
        row_count = len(data)
        
        # Log metrics
        asset_rows_processed.labels(
            asset_name="dlt_glucose_entries",
            operation="fetch",
        ).inc(row_count)
        
        # Validate
        valid_rows = validate(data)
        pass_rate = (len(valid_rows) / row_count) * 100
        
        validation_pass_rate.labels(
            asset_name="dlt_glucose_entries",
            check_name="schema",
        ).set(pass_rate)
        
        # Execution time
        elapsed = time.time() - start_time
        asset_execution_time.labels(
            asset_name="dlt_glucose_entries",
            status="success",
        ).observe(elapsed)
        
        context.log.info(
            f"âœ“ Ingestion: {row_count} rows in {elapsed:.2f}s "
            f"({pass_rate:.1f}% valid)"
        )
        
        return row_count
        
    except Exception as e:
        elapsed = time.time() - start_time
        asset_execution_time.labels(
            asset_name="dlt_glucose_entries",
            status="failure",
        ).observe(elapsed)
        raise
```

### System-Level Metrics

```python
# cascade/monitoring/system_metrics.py
from prometheus_client import start_http_server, Gauge, Counter
import psutil
import docker

disk_usage_percent = Gauge(
    name="disk_usage_percent",
    documentation="Disk usage percentage",
    labelnames=["mount_point"],
)

memory_usage_percent = Gauge(
    name="memory_usage_percent",
    documentation="Memory usage percentage",
)

container_health = Gauge(
    name="container_health",
    documentation="Container status (1=healthy, 0=unhealthy)",
    labelnames=["container_name"],
)

storage_lake_size_bytes = Gauge(
    name="storage_lake_size_bytes",
    documentation="MinIO lake storage size",
)


def collect_system_metrics():
    """Collect infrastructure metrics."""
    
    # Disk usage
    disk = psutil.disk_usage("/")
    disk_usage_percent.labels(mount_point="/").set(disk.percent)
    
    # Memory usage
    memory = psutil.virtual_memory()
    memory_usage_percent.set(memory.percent)
    
    # Container health
    docker_client = docker.from_env()
    for container in docker_client.containers.list():
        status = container.status
        health = 1 if status == "running" else 0
        container_health.labels(container_name=container.name).set(health)
    
    # Lake storage size
    minio_client = get_minio_client()
    size = get_bucket_size(minio_client, "lake")
    storage_lake_size_bytes.set(size)
```

## Logs: Structured Logging

Use structured logs for easy searching:

```python
# cascade/defs/ingestion/dlt_assets.py
import structlog

logger = structlog.get_logger()


@asset
def dlt_glucose_entries(context) -> None:
    """Ingest glucose entries with structured logging."""
    
    logger.info(
        "asset_started",
        asset_name="dlt_glucose_entries",
        timestamp=datetime.utcnow().isoformat(),
    )
    
    try:
        # Fetch API
        logger.info(
            "api_fetch_started",
            endpoint="https://api.nightscout.info/api/v1/entries",
            timeout_seconds=30,
        )
        
        response = fetch_from_api()
        
        logger.info(
            "api_fetch_success",
            rows_returned=len(response),
            response_time_ms=response.elapsed.total_seconds() * 1000,
        )
        
        # Validate
        logger.info(
            "validation_started",
            validator="pandera",
            schema="glucose_entries_v1",
        )
        
        validated = validate(response)
        invalid_count = len(response) - len(validated)
        
        logger.info(
            "validation_complete",
            total_rows=len(response),
            valid_rows=len(validated),
            invalid_rows=invalid_count,
            pass_rate=100.0 * len(validated) / len(response),
        )
        
        # Merge
        logger.info(
            "merge_started",
            table="raw.glucose_entries",
            rows_to_merge=len(validated),
            unique_key="_id",
        )
        
        result = merge_to_iceberg(validated)
        
        logger.info(
            "merge_complete",
            table="raw.glucose_entries",
            inserts=result["inserts"],
            updates=result["updates"],
            merge_time_ms=result["duration_ms"],
        )
        
        # Success
        logger.info(
            "asset_succeeded",
            asset_name="dlt_glucose_entries",
            total_time_ms=get_total_elapsed(),
        )
        
    except Exception as e:
        logger.exception(
            "asset_failed",
            asset_name="dlt_glucose_entries",
            error_type=type(e).__name__,
            error_message=str(e),
        )
        raise
```

In Grafana, search logs:

```
{job="dagster"} | json | asset_name="dlt_glucose_entries" | status="success"

Last 24 hours:
â”œâ”€ 10/15 10:35 âœ“ Succeeded in 152ms
â”œâ”€ 10/15 10:30 âœ“ Succeeded in 145ms
â”œâ”€ 10/15 10:25 âœ“ Succeeded in 168ms
â”œâ”€ 10/15 10:20 âš  Succeeded in 1,240ms (slow)
â””â”€ 10/15 10:15 âœ“ Succeeded in 156ms
```

## Alerting: Detecting Problems

### Freshness Alerts

```yaml
# monitoring/prometheus_rules.yaml
groups:
  - name: data_quality
    rules:
      # Alert if data is stale (>2 hours old)
      - alert: DatasetFreshness
        expr: |
          (time() - asset_last_update_timestamp) / 3600 > 2
        for: 5m
        annotations:
          summary: "Dataset {{ $labels.asset_name }} is stale"
          description: "{{ $labels.asset_name }} not updated for {{ $value }}h"
          
      # Alert if validation fails
      - alert: ValidationFailure
        expr: validation_pass_rate < 95
        for: 1m
        annotations:
          summary: "Data validation failed for {{ $labels.asset_name }}"
          description: "Pass rate: {{ $value }}%"
          
      # Alert on high error rate
      - alert: AssetErrorRate
        expr: |
          (
            rate(asset_execution_failures_total[5m])
            /
            rate(asset_executions_total[5m])
          ) > 0.1
        for: 5m
        annotations:
          summary: "Asset {{ $labels.asset_name }} has high error rate"
          description: "Error rate: {{ humanizePercentage $value }}"
```

### Sending Alerts

```python
# monitoring/alerting.py
from slack_sdk import WebClient
from alertmanager_api_client import AlertmanagerClient
import os


slack_client = WebClient(token=os.environ["SLACK_BOT_TOKEN"])
alertmanager = AlertmanagerClient(url="http://alertmanager:9093")


def send_slack_alert(
    alert_name: str,
    severity: str,
    message: str,
    context: dict,
):
    """Send alert to Slack."""
    
    color_map = {
        "critical": "#FF0000",
        "warning": "#FF9900",
        "info": "#0099FF",
    }
    
    slack_client.chat_postMessage(
        channel="#data-alerts",
        blocks=[
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"{'ğŸ”´' if severity == 'critical' else 'âš ï¸'} {alert_name}",
                },
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{message}*\n\n{json.dumps(context, indent=2)}",
                },
            },
            {
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {"type": "plain_text", "text": "View Dashboard"},
                        "url": f"http://grafana:3000/dashboard/{alert_name}",
                    },
                ],
            },
        ],
    )


# Example: Hook from Dagster
def on_asset_failure(context, event):
    """Called when asset fails."""
    send_slack_alert(
        alert_name=event.asset_key.path[-1],
        severity="critical",
        message=f"Asset failed: {event.asset_key}",
        context={
            "Run ID": event.run_id,
            "Error": event.step_key,
            "Time": event.timestamp,
        },
    )
```

## Dashboards: Visualizing Health

### Main Operations Dashboard

```
Cascade Data Pipeline - Operations Dashboard

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pipeline Status â”‚  Data Freshness â”‚  Quality Score  â”‚
â”‚      âœ“ HEALTHY  â”‚      2.3 hours  â”‚    99.74%       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Asset Execution Times (last 24 hours)
â”œâ”€ dlt_glucose_entries:    150ms avg âœ“
â”œâ”€ stg_glucose_entries:    45ms avg âœ“
â”œâ”€ fct_glucose_readings:   320ms avg âœ“
â”œâ”€ mrt_glucose_readings:   85ms avg âœ“
â””â”€ publish_to_postgres:   1,240ms avg âš 

Data Quality Checks (pass rate)
â”œâ”€ glucose_range_check:         100% âœ“
â”œâ”€ glucose_freshness_check:     100% âœ“
â”œâ”€ no_duplicates:               100% âœ“
â”œâ”€ statistical_bounds_check:    99.9% âœ“
â””â”€ validation_pass_rate:        99.74% âœ“

Active Alerts
â”œâ”€ âš  publish_to_postgres running slow (1240ms vs 500ms avg)
â””â”€ â„¹ API latency slightly elevated (180ms vs 150ms avg)

Resource Utilization
â”œâ”€ Disk: 45% used (180 GB / 400 GB)
â”œâ”€ Memory: 62% used (10 GB / 16 GB)
â””â”€ MinIO lake bucket: 280 GB
```

### Asset Health Dashboard

```
Asset: fct_glucose_readings

Status:        âœ“ HEALTHY
Last Update:   2024-10-15 10:35:42 UTC (2.3 hours ago)
Owner:         data-platform-team
Layer:         Gold (Marts)
Row Count:     487,239

Execution Metrics (24 hours)
â”œâ”€ Total runs: 288
â”œâ”€ Success: 285 (98.96%)
â”œâ”€ Failures: 3 (1.04%)
â”œâ”€ Avg time: 320ms
â”œâ”€ P95 time: 580ms
â”œâ”€ P99 time: 950ms

Quality Checks
â”œâ”€ glucose_range_check:         âœ“ 487,239/487,239 valid
â”œâ”€ glucose_freshness_check:     âœ“ Latest: 2.3h ago
â”œâ”€ no_duplicates:               âœ“ 0 duplicates
â””â”€ statistical_bounds_check:    âš  2 outliers detected

Data Distribution
â”œâ”€ Mean: 150 mg/dL
â”œâ”€ Std Dev: 45 mg/dL
â”œâ”€ Min: 22 mg/dL
â”œâ”€ Max: 598 mg/dL
â””â”€ Nulls: 0 (0%)

Downstream Usage
â”œâ”€ mrt_glucose_readings (Gold) â†’ 100K reads/day
â”œâ”€ Superset Dashboard (Glucose Monitoring) â†’ 450 views/day
â””â”€ Alert: Low Glucose Detection â†’ 12 alerts/day avg
```

## Tracing: Deep Debugging

Use distributed tracing to understand slow operations:

```python
# cascade/monitoring/tracing.py
from jaeger_client import Config
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from contextlib import contextmanager


jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

tracer = trace.get_tracer(__name__)


@contextmanager
def trace_operation(operation_name: str, attributes: dict = None):
    """Context manager for tracing operations."""
    with tracer.start_as_current_span(operation_name) as span:
        if attributes:
            for key, value in attributes.items():
                span.set_attribute(key, value)
        yield span


# Usage in code
@asset
def dlt_glucose_entries(context):
    """Ingest with tracing."""
    
    with trace_operation("dlt_glucose_entries") as span:
        # Fetch
        with trace_operation("fetch_from_api") as fetch_span:
            fetch_span.set_attribute("endpoint", "nightscout_api")
            data = fetch_api()
            fetch_span.set_attribute("rows_fetched", len(data))
        
        # Validate
        with trace_operation("pandera_validation") as val_span:
            val_span.set_attribute("schema", "glucose_entries_v1")
            valid = validate(data)
            val_span.set_attribute("rows_valid", len(valid))
        
        # Merge
        with trace_operation("iceberg_merge") as merge_span:
            merge_span.set_attribute("table", "raw.glucose_entries")
            result = merge_to_iceberg(valid)
            merge_span.set_attribute("inserts", result["inserts"])
            merge_span.set_attribute("updates", result["updates"])
```

In Jaeger UI, you see:

```
Trace: dlt_glucose_entries
Duration: 152ms

â”œâ”€ dlt_glucose_entries [0ms - 152ms] (main)
â”‚  â”œâ”€ fetch_from_api [0ms - 45ms]
â”‚  â”‚  â””â”€ http.request GET /api/v1/entries [5ms - 40ms]
â”‚  â”œâ”€ pandera_validation [50ms - 62ms]
â”‚  â”‚  â”œâ”€ type_checking [50ms - 55ms]
â”‚  â”‚  â””â”€ constraint_checking [55ms - 60ms]
â”‚  â””â”€ iceberg_merge [65ms - 152ms]
â”‚     â”œâ”€ read_snapshot [65ms - 85ms]
â”‚     â”œâ”€ merge_operation [85ms - 125ms]
â”‚     â””â”€ write_metadata [125ms - 152ms]
```

Click on any span to see:
- Start time and duration
- Attributes (table name, row count, etc.)
- Logs within that span
- Errors or exceptions

## Monitoring as Code

```python
# cascade/monitoring/observability_assets.py
from dagster import asset, schedule


@asset(group_name="monitoring")
def freshness_dashboard(context):
    """Generate freshness dashboard."""
    queries = {
        "dlt_glucose_entries": """
            (time() - asset_last_update_timestamp{'asset'='dlt_glucose_entries'}) / 3600
        """,
        "fct_glucose_readings": """
            (time() - asset_last_update_timestamp{'asset'='fct_glucose_readings'}) / 3600
        """,
    }
    
    dashboard = create_grafana_dashboard(
        name="Data Freshness",
        panels=[
            create_gauge_panel(
                title=f"{asset} Age (hours)",
                query=query,
                thresholds={"warning": 2, "critical": 4},
            )
            for asset, query in queries.items()
        ],
    )
    
    context.log.info(f"âœ“ Created freshness dashboard: {dashboard.url}")


@asset(group_name="monitoring")
def sla_tracker(context):
    """Track SLA compliance."""
    slas = {
        "dlt_glucose_entries": {
            "freshness": "2 hours",
            "availability": "99.9%",
        },
        "fct_glucose_readings": {
            "freshness": "1 hour",
            "availability": "99.95%",
        },
    }
    
    for asset_name, sla in slas.items():
        record_sla_metric(asset_name, sla)
        context.log.info(f"âœ“ Updated SLA for {asset_name}")


@schedule(
    name="observability_updates",
    cron_schedule="* * * * *",  # Every minute
)
def update_observability():
    """Update monitoring dashboards and alerts."""
    return {}
```

## Summary

Cascade's observability stack provides:

**Metrics**: Track what's happening (execution time, throughput, quality)  
**Logs**: Understand why (structured logs, searchable)  
**Traces**: Debug how (distributed tracing of slow ops)  
**Dashboards**: Visualize health (asset, system, quality)  
**Alerts**: Get notified (Slack, PagerDuty, email)  

Combined, you have:
- **Visibility**: Know your pipeline state at any time
- **Reliability**: Detect failures before users do
- **Speed**: Find root causes in minutes, not hours
- **Confidence**: Deploy with safety nets in place

**Next**: [Part 12: Production Deployment and Scaling](12-production-deployment.md)

See you there!
