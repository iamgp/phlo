# Cascade Installable Package Plan

## Executive Summary

Transform Cascade from a monolithic repository into an installable Python package where users can:
1. Install Cascade via `pip install cascade` or `uv add cascade`
2. Create minimal project directories with only workflow files
3. Use the Cascade CLI to scaffold new workflows in their own projects
4. Run Dagster with their custom workflows without modifying core Cascade code

---

## Current State Analysis

### What Works Well ✅

1. **Package structure**: Already uses `src/cascade/` layout
2. **Build configuration**: Modern `pyproject.toml` with hatchling
3. **Entry point**: CLI already defined (`cascade` command)
4. **Decorator pattern**: `@cascade_ingestion` provides clean API
5. **Templates**: Existing template system for scaffolding
6. **Auto-discovery**: Workflows auto-register via decorators

### What Needs Changing ⚠️

1. **Project assumptions**: CLI expects to run inside Cascade repo
2. **Workflow location**: Currently in `src/cascade/defs/ingestion/`
3. **User modifications**: Users edit core package files
4. **Service coupling**: Docker services bundled with package
5. **dbt transforms**: Located in package, should be user-managed
6. **Example workflows**: Mixed with framework code

---

## Architecture Overview

### Three-Layer Model

```
┌─────────────────────────────────────────────────┐
│  CASCADE PACKAGE (pip install cascade)         │
│  - Core framework & decorators                  │
│  - CLI tools                                    │
│  - Dagster resources                           │
│  - Schema conversion utilities                 │
│  - Templates                                    │
└─────────────────────────────────────────────────┘
                      ▼
┌─────────────────────────────────────────────────┐
│  USER PROJECT (created by user)                 │
│  - workflows/                                   │
│    ├── ingestion/                              │
│    ├── schemas/                                │
│    └── quality/                                │
│  - transforms/dbt/                             │
│  - pyproject.toml (minimal)                    │
│  - .env (user secrets)                         │
│  - cascade_config.py (optional overrides)      │
└─────────────────────────────────────────────────┘
                      ▼
┌─────────────────────────────────────────────────┐
│  RUNTIME (dagster/docker)                       │
│  - Loads user workflows via plugin discovery   │
│  - Merges with Cascade framework                │
│  - Runs orchestrated pipelines                 │
└─────────────────────────────────────────────────┘
```

---

## Detailed Implementation Plan

### Phase 1: Package Separation

#### 1.1 Core Framework Code (What Gets Installed)

**Include in package:**
```
cascade/
├── __init__.py
├── config.py                    # Configuration management
├── exceptions.py                # Error handling
├── cli/                         # CLI commands
│   ├── main.py
│   ├── init.py                  # NEW: Initialize user project
│   ├── create_workflow.py       # MODIFIED: Work in user projects
│   └── validate.py
├── framework/                   # NEW: Rename from 'defs'
│   ├── decorators.py           # Core decorators
│   ├── resources/              # Dagster resources
│   ├── validation/             # Validation framework
│   └── discovery.py            # NEW: Plugin discovery system
├── iceberg/                     # Iceberg utilities
├── ingestion/                   # Ingestion framework
│   ├── decorator.py
│   └── dlt_helpers.py
├── schemas/                     # NEW: Only base/abstract schemas
│   └── converter.py            # Schema conversion utils
└── templates/                   # Workflow templates
    ├── project/                # NEW: Project initialization
    │   ├── pyproject.toml
    │   ├── .env.example
    │   ├── workspace.yaml
    │   └── cascade_config.py
    ├── ingestion/
    │   ├── rest_api.py
    │   ├── database.py
    │   └── file.py
    ├── schemas/
    │   └── example_schema.py
    └── tests/
        └── test_ingestion.py
```

**Package data declaration in `pyproject.toml`:**
```toml
[tool.hatch.build.targets.wheel]
packages = ["src/cascade"]

[tool.hatch.build.targets.wheel.shared-data]
"templates" = "cascade/templates"
```

#### 1.2 User Project Structure (What Users Create)

**Generated by `cascade init`:**
```
my-data-project/
├── pyproject.toml              # Minimal: just cascade dependency
├── .env                        # User secrets
├── .env.example
├── workflows/                  # User workflow definitions
│   ├── __init__.py
│   ├── ingestion/
│   │   ├── __init__.py
│   │   └── weather/           # Example domain
│   │       ├── __init__.py
│   │       └── observations.py
│   └── schemas/
│       ├── __init__.py
│       └── weather.py
├── transforms/                 # dbt project
│   └── dbt/
│       ├── dbt_project.yml
│       └── models/
│           ├── bronze/
│           ├── silver/
│           └── gold/
├── tests/
│   └── test_weather.py
├── workspace.yaml              # Dagster workspace config
└── cascade_config.py           # Optional: override defaults
```

**Minimal `pyproject.toml` for user project:**
```toml
[project]
name = "my-data-project"
version = "0.1.0"
dependencies = [
    "cascade",
]

[tool.cascade]
workflows_path = "workflows"
dbt_project_dir = "transforms/dbt"
```

#### 1.3 Examples Repository (Separate)

**Move to `cascade-examples` repo:**
- All current example workflows (nightscout, github, etc.)
- Full reference implementations
- Tutorial projects
- Integration tests

---

### Phase 2: Plugin Discovery System

#### 2.1 Workflow Discovery Mechanism

**Current approach (in-package):**
```python
# In src/cascade/defs/ingestion/__init__.py
from cascade.defs.ingestion import nightscout  # noqa: F401
from cascade.defs.ingestion import github      # noqa: F401
```

**New approach (external plugins):**
```python
# In cascade/framework/discovery.py
import importlib
import sys
from pathlib import Path
from typing import List
from dagster import Definitions

def discover_user_workflows(workflows_path: Path) -> Definitions:
    """
    Discover and load user workflow modules from external directory.

    Uses Python's import system to dynamically load user workflow modules
    and collect their Dagster definitions.
    """
    # Add user workflows to Python path
    sys.path.insert(0, str(workflows_path.parent))

    # Discover all Python modules in workflows/
    workflow_modules = []
    for py_file in workflows_path.rglob("*.py"):
        if py_file.name.startswith("_"):
            continue

        # Convert file path to module name
        relative_path = py_file.relative_to(workflows_path.parent)
        module_name = str(relative_path.with_suffix("")).replace("/", ".")

        # Import module (triggers decorator registration)
        module = importlib.import_module(module_name)
        workflow_modules.append(module)

    # Collect all registered assets/jobs/schedules
    return merge_workflow_definitions(workflow_modules)
```

#### 2.2 Dagster Definitions Loading

**New definitions entry point:**
```python
# In cascade/framework/definitions.py
from dagster import Definitions
from cascade.config import CascadeConfig
from cascade.framework.discovery import discover_user_workflows
from cascade.framework.resources import build_resources

def build_definitions() -> Definitions:
    """
    Build Dagster definitions by merging:
    1. Core Cascade framework resources
    2. User-defined workflows from external directory
    """
    config = CascadeConfig()

    # Load user workflows from configured path
    user_defs = discover_user_workflows(
        workflows_path=config.workflows_path
    )

    # Merge with core resources
    return Definitions.merge(
        Definitions(resources=build_resources(config)),
        user_defs,
    )

# Export for Dagster to load
defs = build_definitions()
```

#### 2.3 Workspace Configuration

**User's `workspace.yaml`:**
```yaml
load_from:
  - python_module:
      module_name: cascade.framework.definitions
      location_name: cascade

      # Pass config to definitions
      working_directory: .

      # Optional: specify workflows path
      environment:
        WORKFLOWS_PATH: ./workflows
```

---

### Phase 3: CLI Enhancements

#### 3.1 New Command: `cascade init`

**Purpose:** Initialize a new Cascade project

**Usage:**
```bash
# Interactive
$ cascade init my-data-project

# Non-interactive
$ cascade init my-data-project --template basic

# Initialize in current directory
$ cascade init . --force
```

**Implementation:**
```python
# In cascade/cli/init.py
@click.command()
@click.argument("project_name")
@click.option("--template", default="basic",
              type=click.Choice(["basic", "advanced", "minimal"]))
@click.option("--force", is_flag=True, help="Initialize in non-empty directory")
def init(project_name: str, template: str, force: bool):
    """
    Initialize a new Cascade data project.

    Creates project structure with:
    - Minimal pyproject.toml with cascade dependency
    - workflows/ directory for workflow definitions
    - transforms/dbt/ for dbt transformations
    - Example workflow files
    - Configuration files (.env, workspace.yaml)
    """
    # Implementation similar to create_workflow but for whole project
    pass
```

#### 3.2 Modified Command: `cascade create-workflow`

**Changes needed:**
1. Remove assumption about being in Cascade repo
2. Find user project root (look for `pyproject.toml` with cascade dependency)
3. Create files in `workflows/` instead of `src/cascade/defs/`
4. Update import paths in generated templates

**Updated template generation:**
```python
def _generate_asset_template(config: dict[str, Any]) -> str:
    """Generate asset for USER PROJECT (not in cascade package)."""

    return f'''"""
{domain.title()} {asset_name.title()} Ingestion
"""

from dlt.sources.rest_api import rest_api
from cascade.ingestion import cascade_ingestion
from workflows.schemas.{domain} import {schema_class}  # User's schema


@cascade_ingestion(
    table_name="{table_name}",
    unique_key="{unique_key}",
    validation_schema={schema_class},
    group="{domain}",
    cron="{cron}",
    freshness_hours=({warn_hours}, {fail_hours}),
)
def {asset_name}(partition_date: str):
    """Ingest {asset_name} data."""
    # Implementation...
'''
```

#### 3.3 New Command: `cascade dev`

**Purpose:** Run Dagster development server with user workflows

**Usage:**
```bash
# Start development server
$ cascade dev

# With specific host/port
$ cascade dev --host 0.0.0.0 --port 3000

# With auto-reload on file changes
$ cascade dev --reload
```

**Implementation:**
```python
@click.command()
@click.option("--host", default="127.0.0.1")
@click.option("--port", default=3000, type=int)
@click.option("--reload/--no-reload", default=True)
def dev(host: str, port: int, reload: bool):
    """
    Start Dagster development server with your workflows.

    Automatically discovers workflows in ./workflows/ and serves
    the Dagster UI.
    """
    from dagster.cli import ui

    # Set environment for discovery
    os.environ["CASCADE_WORKFLOWS_PATH"] = "./workflows"

    # Launch Dagster UI
    ui.ui_command(
        host=host,
        port=port,
        module_name="cascade.framework.definitions",
    )
```

---

### Phase 4: Configuration System

#### 4.1 Configuration Priority

**Priority order (highest to lowest):**
1. Environment variables (`.env`)
2. User's `cascade_config.py`
3. Package defaults

#### 4.2 Configuration Schema

**Update `cascade/config.py`:**
```python
from pathlib import Path
from pydantic_settings import BaseSettings, SettingsConfigDict

class CascadeConfig(BaseSettings):
    """Cascade configuration with user project support."""

    # Project paths
    project_root: Path = Path.cwd()
    workflows_path: Path = Path("workflows")
    dbt_project_dir: Path = Path("transforms/dbt")

    # Iceberg configuration
    iceberg_catalog: str = "default"
    iceberg_warehouse: str = "s3://cascade-warehouse"

    # Nessie configuration
    nessie_uri: str = "http://localhost:19120/api/v1"
    nessie_ref: str = "main"

    # Database configuration
    postgres_host: str = "localhost"
    postgres_port: int = 5432
    postgres_db: str = "cascade"
    postgres_user: str = "cascade"
    postgres_password: str = ""

    # Trino configuration
    trino_host: str = "localhost"
    trino_port: int = 8080
    trino_catalog: str = "iceberg"

    model_config = SettingsConfigDict(
        env_prefix="CASCADE_",
        env_file=".env",
        env_file_encoding="utf-8",
        extra="allow",  # Allow user extensions
    )

    @classmethod
    def from_user_config(cls, config_file: Path | None = None):
        """Load config with user overrides from cascade_config.py"""
        if config_file and config_file.exists():
            # Import user config module
            import importlib.util
            spec = importlib.util.spec_from_file_location("user_config", config_file)
            user_config = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(user_config)

            # Merge with defaults
            if hasattr(user_config, "CASCADE_CONFIG"):
                return cls(**user_config.CASCADE_CONFIG)

        return cls()
```

#### 4.3 User Configuration File

**User's `cascade_config.py` (optional):**
```python
"""
Custom Cascade configuration for my-data-project.

Overrides default settings from the Cascade package.
"""

CASCADE_CONFIG = {
    # Override warehouse location
    "iceberg_warehouse": "s3://my-company-datalake/prod",

    # Override Nessie endpoint
    "nessie_uri": "https://nessie.mycompany.com/api/v1",

    # Custom workflow path
    "workflows_path": "custom_workflows",

    # Any additional custom settings
    "my_custom_setting": "value",
}
```

---

### Phase 5: Docker & Services

#### 5.1 Separate Docker Compose

**Move to separate `cascade-services` repo or keep as optional:**

**Option A: Separate Repository**
```
cascade-services/
├── docker-compose.yml
├── services/
│   ├── dagster/
│   ├── nessie/
│   ├── postgres/
│   ├── trino/
│   └── minio/
└── README.md
```

Users install services separately:
```bash
# Clone services repo
git clone https://github.com/cascade/cascade-services
cd cascade-services

# Start infrastructure
docker-compose up -d
```

**Option B: Package Optional Services**

Include `cascade-services` as optional extra:
```toml
[project.optional-dependencies]
services = [
    "docker-compose>=1.29",
]
```

Users can then:
```bash
# Install with services
pip install cascade[services]

# Start services via CLI
cascade services start
cascade services stop
cascade services status
```

#### 5.2 Service Management CLI

**New commands:**
```python
@click.group()
def services():
    """Manage Cascade infrastructure services."""
    pass

@services.command()
def start():
    """Start all Cascade services (Dagster, Nessie, Trino, etc.)"""
    pass

@services.command()
def stop():
    """Stop all Cascade services."""
    pass

@services.command()
def status():
    """Show status of all services."""
    pass
```

---

### Phase 6: dbt Integration

#### 6.1 User-Managed dbt Projects

**User's `transforms/dbt/dbt_project.yml`:**
```yaml
name: 'my_data_project'
version: '1.0.0'

profile: 'cascade'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]

models:
  my_data_project:
    # User-defined models
    bronze:
      +materialized: view
    silver:
      +materialized: table
    gold:
      +materialized: table
```

#### 6.2 dbt Profile Management

**Cascade manages dbt profile automatically:**

```python
# In cascade/framework/resources/dbt.py
from dagster_dbt import DbtCliResource

def build_dbt_resource(config: CascadeConfig) -> DbtCliResource:
    """Build dbt resource with auto-configured profile."""

    # Generate profile from Cascade config
    profile = {
        "my_data_project": {
            "target": "dev",
            "outputs": {
                "dev": {
                    "type": "trino",
                    "host": config.trino_host,
                    "port": config.trino_port,
                    "catalog": config.trino_catalog,
                    "schema": "default",
                }
            }
        }
    }

    return DbtCliResource(
        project_dir=str(config.dbt_project_dir),
        profiles_dir=None,  # Use auto-generated
        profile=profile,
    )
```

---

### Phase 7: Testing Strategy

#### 7.1 Framework Tests (Cascade Package)

**Tests to include in package:**
```
tests/
├── unit/
│   ├── test_decorators.py
│   ├── test_discovery.py
│   ├── test_config.py
│   └── test_schema_converter.py
├── integration/
│   ├── test_iceberg_integration.py
│   ├── test_dagster_integration.py
│   └── test_dlt_integration.py
└── e2e/
    └── test_full_workflow.py
```

#### 7.2 User Project Tests

**Generated by `cascade init`:**
```python
# tests/test_workflows.py
"""Tests for user workflows."""

import pytest
from workflows.ingestion.weather.observations import observations

def test_observations_asset_exists():
    """Test that observations asset is properly defined."""
    assert observations.op is not None
    assert observations.op.name == "dlt_weather_observations"
```

#### 7.3 Testing Utilities

**Export testing helpers:**
```python
# cascade/testing/helpers.py
from dagster import materialize
from cascade.framework.resources import build_test_resources

def test_workflow(asset, test_data=None):
    """Helper to test workflows in isolation."""

    result = materialize(
        [asset],
        resources=build_test_resources(),
    )

    return result.success
```

---

### Phase 8: Documentation Updates

#### 8.1 Quick Start (New User Experience)

**New documentation flow:**

```markdown
# Quick Start

## Installation

```bash
# Install Cascade
pip install cascade

# Or with uv (recommended)
uv add cascade
```

## Create Your First Project

```bash
# Initialize new project
cascade init my-data-project
cd my-data-project

# Start services (optional, or use existing infrastructure)
cascade services start

# Create your first workflow
cascade create-workflow --type ingestion --domain weather

# Run Dagster UI
cascade dev
```

## Your First Workflow

Edit `workflows/ingestion/weather/weather.py`:

```python
from cascade.ingestion import cascade_ingestion
from dlt.sources.rest_api import rest_api
from workflows.schemas.weather import RawWeatherObservations

@cascade_ingestion(
    table_name="weather_observations",
    unique_key="id",
    validation_schema=RawWeatherObservations,
    group="weather",
    cron="0 * * * *",  # Hourly
)
def weather_observations(partition_date: str):
    source = rest_api({
        "client": {
            "base_url": "https://api.weather.gov",
        },
        "resources": [{
            "name": "observations",
            "endpoint": {
                "path": f"stations/KORD/observations",
                "params": {"date": partition_date},
            },
        }],
    })
    return source
```

Navigate to http://localhost:3000 and materialize your asset!
```

#### 8.2 Migration Guide

**For existing Cascade users:**

```markdown
# Migration Guide: Cascade v0.x to v1.0

## Overview

Cascade 1.0 is now an installable package. Your workflows move from
`src/cascade/defs/` to a separate user project.

## Migration Steps

### 1. Install Cascade Package

```bash
pip install cascade>=1.0.0
```

### 2. Create New Project Structure

```bash
cascade init my-cascade-project --from-existing
```

### 3. Move Your Workflows

**Before (v0.x):**
```
cascade/
└── src/cascade/defs/ingestion/
    └── myworkflow/
        └── myasset.py
```

**After (v1.0):**
```
my-cascade-project/
└── workflows/ingestion/
    └── myworkflow/
        └── myasset.py
```

### 4. Update Imports

**Before:**
```python
from cascade.defs.ingestion.myworkflow.myasset import myasset
from cascade.schemas.myschema import MySchema
```

**After:**
```python
from workflows.ingestion.myworkflow.myasset import myasset
from workflows.schemas.myschema import MySchema
```

### 5. Update Dagster Workspace

**Before:**
```yaml
load_from:
  - python_module:
      module_name: cascade.definitions
```

**After:**
```yaml
load_from:
  - python_module:
      module_name: cascade.framework.definitions
```

### 6. Move Environment Variables

Copy your `.env` file to your new project root.

### 7. Test Migration

```bash
cd my-cascade-project
cascade dev
```

## Breaking Changes

- Workflows must be in external `workflows/` directory
- Import paths changed from `cascade.defs.*` to `workflows.*`
- Services are now optional (not bundled with package)
- Configuration moved to `cascade_config.py` (optional)
```

---

### Phase 9: Publishing & Distribution

#### 9.1 Package Publishing

**Build and publish to PyPI:**
```bash
# Build package
uv build

# Publish to PyPI
uv publish

# Install from PyPI
pip install cascade
```

**Version strategy:**
- v1.0.0: First installable package release
- Semantic versioning for updates
- Use `uv-dynamic-versioning` for auto-versioning from git tags

#### 9.2 Installation Methods

**Via pip:**
```bash
pip install cascade
```

**Via uv (recommended):**
```bash
uv add cascade
```

**With optional dependencies:**
```bash
# With services management
pip install cascade[services]

# With all extras (dev tools, testing, services)
pip install cascade[all]
```

**From source (development):**
```bash
git clone https://github.com/cascade/cascade
cd cascade
uv sync
uv pip install -e .
```

---

### Phase 10: Backwards Compatibility

#### 10.1 Support Both Modes Temporarily

**Detection in definitions.py:**
```python
def build_definitions() -> Definitions:
    """Build definitions supporting both legacy and new project structures."""

    config = CascadeConfig()

    # Check if running in legacy mode (workflows in package)
    legacy_workflows = Path(__file__).parent / "defs" / "ingestion"
    if legacy_workflows.exists():
        logger.warning(
            "Running in LEGACY mode. Workflows found in package directory. "
            "Consider migrating to external project structure. "
            "See migration guide: https://cascade.dev/docs/migration"
        )
        return build_legacy_definitions()

    # New mode: discover user workflows
    return build_user_definitions(config)
```

#### 10.2 Deprecation Timeline

**Version strategy:**
- v1.0.0: Support both modes, warn on legacy usage
- v1.1.0: Legacy mode deprecated with louder warnings
- v2.0.0: Remove legacy mode entirely

---

## Implementation Checklist

### Phase 1: Package Separation ⬜
- [ ] Rename `cascade/defs/` to `cascade/framework/`
- [ ] Move example workflows to separate repo
- [ ] Create base/abstract schemas only in package
- [ ] Declare package data in pyproject.toml
- [ ] Remove `src/pyproject.toml` duplicate

### Phase 2: Plugin Discovery ⬜
- [ ] Implement `discover_user_workflows()` function
- [ ] Create new `cascade/framework/definitions.py`
- [ ] Add module import system for external workflows
- [ ] Test workflow registration from external directory

### Phase 3: CLI Enhancements ⬜
- [ ] Implement `cascade init` command
- [ ] Update `cascade create-workflow` for user projects
- [ ] Create `cascade dev` command
- [ ] Add `cascade services` command group
- [ ] Update all templates for user project structure

### Phase 4: Configuration ⬜
- [ ] Add `workflows_path` to CascadeConfig
- [ ] Implement `from_user_config()` method
- [ ] Add environment variable support
- [ ] Create example `cascade_config.py` template

### Phase 5: Docker & Services ⬜
- [ ] Extract services to separate compose file
- [ ] Create optional `[services]` dependency group
- [ ] Implement service management CLI
- [ ] Update documentation for service setup

### Phase 6: dbt Integration ⬜
- [ ] Auto-generate dbt profiles from config
- [ ] Create dbt project template
- [ ] Update DbtCliResource configuration
- [ ] Test dbt integration with user projects

### Phase 7: Testing ⬜
- [ ] Separate framework tests from examples
- [ ] Create testing utilities for users
- [ ] Add integration tests for discovery system
- [ ] Test full end-to-end workflow

### Phase 8: Documentation ⬜
- [ ] Write new Quick Start guide
- [ ] Create migration guide
- [ ] Update all existing documentation
- [ ] Add API reference docs
- [ ] Create video tutorials

### Phase 9: Publishing ⬜
- [ ] Set up PyPI publishing workflow
- [ ] Configure version management
- [ ] Create release process documentation
- [ ] Set up CI/CD for releases

### Phase 10: Backwards Compatibility ⬜
- [ ] Implement dual-mode support
- [ ] Add migration warnings
- [ ] Create automated migration tool
- [ ] Plan deprecation timeline

---

## Success Metrics

### Developer Experience
- ✅ New user can start in < 5 minutes
- ✅ No need to clone/fork repository
- ✅ Clear separation of framework vs. user code
- ✅ Familiar Python package installation

### Technical Quality
- ✅ Clean plugin architecture
- ✅ No core package modifications needed
- ✅ Proper dependency management
- ✅ Comprehensive testing

### Community Growth
- ✅ Lower barrier to entry
- ✅ Easier to share workflows
- ✅ Better for team collaboration
- ✅ Simpler CI/CD integration

---

## Risks & Mitigations

### Risk: Breaking Changes for Current Users
**Mitigation:**
- Support legacy mode in v1.0
- Provide automated migration tool
- Clear migration documentation
- Gradual deprecation timeline

### Risk: Plugin Discovery Complexity
**Mitigation:**
- Use standard Python import system
- Extensive testing
- Clear error messages
- Fallback mechanisms

### Risk: Service Coupling
**Mitigation:**
- Make services optional
- Support external infrastructure
- Clear service setup documentation
- Provide docker-compose as reference

### Risk: Configuration Complexity
**Mitigation:**
- Sensible defaults
- Clear configuration hierarchy
- Validation with helpful errors
- Example configurations

---

## Timeline Estimate

**Assuming 1 full-time developer:**

- Phase 1-2 (Core refactoring): 2 weeks
- Phase 3 (CLI): 1 week
- Phase 4 (Config): 3 days
- Phase 5 (Services): 1 week
- Phase 6 (dbt): 3 days
- Phase 7 (Testing): 1 week
- Phase 8 (Documentation): 1 week
- Phase 9-10 (Publishing & Compat): 1 week

**Total: ~8 weeks** for full implementation

**MVP (Phases 1-4, 7):** ~4 weeks

---

## Next Steps

1. **Review this plan** with team/stakeholders
2. **Validate assumptions** with potential users
3. **Create POC** for plugin discovery system
4. **Test migration** with one example workflow
5. **Iterate** based on feedback
6. **Execute phases** in order
7. **Beta test** with early adopters
8. **Release v1.0.0** to PyPI

---

## Questions to Resolve

1. **Naming:** Keep `cascade` or rename for clarity?
2. **Services:** Separate repo or optional in package?
3. **Examples:** Separate repo or included as templates?
4. **Versioning:** Start at 1.0.0 or 0.1.0?
5. **License:** Keep current or change for distribution?
6. **Support:** What level of backwards compatibility?
7. **Hub service:** Include in package or separate?
8. **API service:** Include in package or separate?

---

## Appendix: Example User Workflow

**Full example of user experience:**

```bash
# Install Cascade
$ pip install cascade

# Create project
$ cascade init my-weather-pipeline
Creating new Cascade project: my-weather-pipeline
✓ Created directory structure
✓ Generated pyproject.toml
✓ Created example workflow
✓ Initialized dbt project

# Navigate to project
$ cd my-weather-pipeline

# Create workflow
$ cascade create-workflow --type ingestion --domain weather
✓ Created workflows/ingestion/weather/observations.py
✓ Created workflows/schemas/weather.py
✓ Created tests/test_weather.py

# Edit workflow (user edits files)
$ vim workflows/ingestion/weather/observations.py

# Test locally
$ pytest tests/test_weather.py
✓ All tests passed

# Run Dagster UI
$ cascade dev
Starting Dagster webserver on http://localhost:3000
✓ Discovered 1 workflow
✓ Loaded 1 asset
✓ Ready for materialization

# Materialize asset via UI or CLI
$ cascade materialize weather_observations --partition 2024-01-15
✓ Materialization succeeded
  Rows: 1,234
  Duration: 2.3s
```

**User's final project:**
```
my-weather-pipeline/
├── pyproject.toml          # Just: dependencies = ["cascade"]
├── workflows/
│   ├── ingestion/
│   │   └── weather/
│   │       └── observations.py  # ~50 lines
│   └── schemas/
│       └── weather.py           # ~30 lines
├── transforms/dbt/
│   └── models/
│       └── weather_hourly.sql   # ~20 lines
└── tests/
    └── test_weather.py          # ~40 lines

Total user code: ~140 lines
Framework code: 0 lines (installed via pip)
```

---

**This plan transforms Cascade from a repository into a professional, distributable data platform framework while maintaining its power and flexibility.**
