{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Phlo Documentation","text":"<p>Welcome to Phlo - a modern data lakehouse platform combining Apache Iceberg, Project Nessie, Trino, dbt, and Dagster.</p>"},{"location":"#what-is-phlo","title":"What is Phlo?","text":"<p>Phlo is a decorator-driven data lakehouse framework that reduces boilerplate by 74% while providing:</p> <ul> <li>Write-Audit-Publish pattern with Git-like branching</li> <li>Type-safe data quality with automatic validation</li> <li>Production-ready patterns out of the box</li> <li>Schema-first development with Pandera</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install and start\ngit clone https://github.com/iamgp/phlo.git\ncd phlo\nphlo services init\n# Update secrets in .phlo/.env.local and port defaults in phlo.yaml (env:)\nphlo services start\n\n# Materialize example pipeline\nphlo materialize dlt_glucose_entries\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>New to Phlo? Start with the Installation Guide then Core Concepts</li> <li>Build pipelines: Follow the Developer Guide</li> <li>Production deployment: Check the Operations Guide</li> <li>Troubleshoot issues: See Troubleshooting</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Essential guides for new users:</p> <ul> <li>Installation Guide - Complete installation instructions</li> <li>Quickstart Guide - Get running in 10 minutes</li> <li>Core Concepts - Understand Phlo's architecture and patterns</li> </ul>"},{"location":"#guides","title":"Guides","text":"<p>In-depth tutorials and how-tos:</p> <ul> <li>Developer Guide - Master decorators and workflow development</li> <li>Service Packages - Installable infrastructure services</li> <li>Plugin Development - Build custom plugins to extend Phlo</li> <li>Workflow Development - Build complete data pipelines</li> <li>Data Modeling - Bronze/Silver/Gold architecture</li> <li>dbt Development - SQL transformations</li> <li>Dagster Assets - Orchestration patterns</li> <li>GitHub Workflow - Git branching and CI/CD</li> </ul>"},{"location":"#setup","title":"Setup","text":"<p>Configure additional services:</p> <ul> <li>OpenMetadata - Data catalog and governance</li> <li>PostgREST - REST API from PostgreSQL</li> <li>Hasura - GraphQL API</li> <li>Observability - Monitoring with Grafana</li> </ul>"},{"location":"#reference","title":"Reference","text":"<p>Technical documentation:</p> <ul> <li>CLI Reference - Complete command-line interface guide</li> <li>Configuration Reference - Environment variables and settings</li> <li>Architecture - System design and components</li> <li>API Reference - REST and GraphQL APIs</li> <li>DuckDB Queries - Ad-hoc analysis</li> <li>Common Errors - Error messages explained</li> </ul>"},{"location":"#operations","title":"Operations","text":"<p>Production operations and maintenance:</p> <ul> <li>Operations Guide - Daily operations, backups, scaling, security</li> <li>Troubleshooting - Debug common issues</li> <li>Best Practices - Production patterns</li> <li>Testing Guide - Testing strategies</li> </ul>"},{"location":"#blog","title":"Blog","text":"<p>Tutorial series and deep dives:</p> <ul> <li>See blog/ for the complete 13-part article series</li> </ul>"},{"location":"#learning-paths","title":"Learning Paths","text":""},{"location":"#path-1-complete-beginner-to-first-pipeline","title":"Path 1: Complete Beginner to First Pipeline","text":"<pre><code>1. getting-started/installation.md        (Install Phlo)\n2. getting-started/core-concepts.md       (Understand architecture)\n3. getting-started/quickstart.md          (Run first pipeline)\n4. guides/developer-guide.md              (Build custom workflows)\n5. operations/troubleshooting.md          (Fix issues)\n</code></pre> <p>Outcome: Working data pipeline with custom ingestion and quality checks</p>"},{"location":"#path-2-developer-to-production-expert","title":"Path 2: Developer to Production Expert","text":"<pre><code>1. getting-started/core-concepts.md       (Understand patterns)\n2. guides/developer-guide.md              (Master decorators)\n3. reference/cli-reference.md             (Learn CLI tools)\n4. guides/dbt-development.md              (SQL transformations)\n5. operations/operations-guide.md         (Production operations)\n6. setup/observability.md                 (Monitoring)\n</code></pre> <p>Outcome: Production-ready pipelines with monitoring and automation</p>"},{"location":"#path-3-quick-setup-to-running-system","title":"Path 3: Quick Setup to Running System","text":"<pre><code>1. getting-started/installation.md        (Install)\n2. getting-started/quickstart.md          (Start services)\n3. reference/configuration-reference.md   (Configure)\n4. operations/troubleshooting.md          (Debug)\n</code></pre> <p>Outcome: Running Phlo instance ready for development</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ol> <li>Search this documentation - Use your editor's search</li> <li>Check troubleshooting - operations/troubleshooting.md</li> <li>Review common errors - reference/common-errors.md</li> <li>Official documentation:</li> <li>Dagster</li> <li>dbt</li> <li>Trino</li> <li>Iceberg</li> <li>Nessie</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>Phlo is open source. Contributions welcome!</p> <ul> <li>Report bugs via GitHub Issues</li> <li>Submit improvements via Pull Requests</li> <li>See guides/github-workflow.md for workflow</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#decorator-driven-development","title":"Decorator-Driven Development","text":"<p>Reduce boilerplate by 74% with <code>@phlo_ingestion</code> and <code>@phlo_quality</code> decorators.</p>"},{"location":"#write-audit-publish-pattern","title":"Write-Audit-Publish Pattern","text":"<p>Automated branch lifecycle with quality gates and auto-promotion to production.</p>"},{"location":"#schema-first-development","title":"Schema-First Development","text":"<p>Pandera schemas auto-generate Iceberg schemas and enforce validation.</p>"},{"location":"#production-ready","title":"Production-Ready","text":"<p>Built-in monitoring, alerting, backups, and disaster recovery patterns.</p> <p>Version: 2.0 | Last Updated: 2025-12-06</p>"},{"location":"architecture/decisions/0001-quality-check-contract/","title":"1. Standardize quality check naming and metadata contract","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0001-quality-check-contract/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0001-quality-check-contract/#context","title":"Context","text":"<p>Phlo produces quality signals from multiple sources (Pandera validation, dbt tests, and custom checks). Observability surfaces (Dagster UI, GraphQL consumers, and Observatory) need a consistent way to:</p> <ul> <li>Identify checks across runs and assets.</li> <li>Render results without source-specific branching.</li> <li>Provide enough debugging context (partition, counts, repro SQL, sample).</li> </ul> <p>Without a contract, each producer would emit bespoke metadata keys and naming, forcing consumers to special-case every check source and making new check sources expensive to add.</p>"},{"location":"architecture/decisions/0001-quality-check-contract/#decision","title":"Decision","text":"<p>Adopt a small, shared contract for all asset checks:</p> <ul> <li>Naming</li> <li>Pandera contract check name is <code>pandera_contract</code>.</li> <li>dbt checks are named deterministically as <code>dbt__&lt;test_type&gt;__&lt;target&gt;</code> (sanitized for Dagster).</li> <li>Required metadata keys</li> <li><code>source</code> (<code>pandera|dbt|phlo</code>)</li> <li><code>partition_key</code> (when applicable)</li> <li><code>failed_count</code></li> <li><code>total_count</code> (when available)</li> <li><code>query_or_sql</code> (when applicable)</li> <li><code>sample</code> (&lt;= 20 items, when available)</li> <li>Optional: <code>repro_sql</code> (safe SQL snippet for investigation)</li> </ul>"},{"location":"architecture/decisions/0001-quality-check-contract/#consequences","title":"Consequences","text":"<ul> <li>Consumers can render quality results consistently and add features (filtering, drilldown) without   needing per-source logic.</li> <li>Check producers must follow the contract; deviations are treated as bugs.</li> <li>Some metadata is necessarily best-effort (e.g., <code>total_count</code> or <code>repro_sql</code>) depending on source.</li> </ul>"},{"location":"architecture/decisions/0002-pandas-datetime-coercion-scope/","title":"2. Limit datetime coercion during Pandera validation","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0002-pandas-datetime-coercion-scope/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0002-pandas-datetime-coercion-scope/#context","title":"Context","text":"<p>Ingestion validation uses Pandera schemas with <code>coerce=True</code> defaults. Naively coercing many columns to datetime can:</p> <ul> <li>Mask upstream data issues by converting broad inputs unexpectedly.</li> <li>Add unnecessary CPU overhead on large datasets.</li> <li>Create surprising behavior when non-datetime columns are coerced (or attempted).</li> </ul> <p>We need validation to be strict enough to catch real schema issues while remaining performant and predictable.</p>"},{"location":"architecture/decisions/0002-pandas-datetime-coercion-scope/#decision","title":"Decision","text":"<p>When validating with Pandera, only attempt datetime coercion for columns that are:</p> <ul> <li>Declared as datetime in the Pandera schema, and</li> <li>Present in the incoming dataframe, and</li> <li>Currently stored as object/string types that Pandas can reasonably parse.</li> </ul> <p>All other columns are left untouched and Pandera validation remains the source of truth.</p>"},{"location":"architecture/decisions/0002-pandas-datetime-coercion-scope/#consequences","title":"Consequences","text":"<ul> <li>Validation is more predictable and less \u201cmagical\u201d.</li> <li>Performance improves on wide tables where only a small number of datetime columns exist.</li> <li>Some dirty-but-coercible values may still pass; this remains a schema design choice (strictness   should live in the schema, not hidden coercion logic).</li> </ul>"},{"location":"architecture/decisions/0003-quality-severity-policy/","title":"3. Define a severity policy for quality checks (blocking vs warn)","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0003-quality-severity-policy/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0003-quality-severity-policy/#context","title":"Context","text":"<p>Not all quality failures should have the same impact:</p> <ul> <li>Contract/schema violations are typically correctness failures and should block downstream work.</li> <li>Some tests (e.g., anomaly/drift) are informative and should be warn-only.</li> </ul> <p>We need a uniform policy so check producers and consumers agree on semantics, and so automation can rely on consistent status meaning.</p>"},{"location":"architecture/decisions/0003-quality-severity-policy/#decision","title":"Decision","text":"<p>Adopt a shared severity mapping:</p> <ul> <li>Pandera contract failures are ERROR (blocking).</li> <li>dbt tests are ERROR for <code>not_null</code>, <code>unique</code>, and <code>relationships</code>; other test types default to   WARN.</li> <li>dbt tags can override:</li> <li><code>tag:blocking</code> forces ERROR</li> <li><code>tag:warn</code> or <code>tag:anomaly</code> forces WARN</li> </ul>"},{"location":"architecture/decisions/0003-quality-severity-policy/#consequences","title":"Consequences","text":"<ul> <li>Users get sensible defaults without needing to annotate every test.</li> <li>Teams can still override per-test behavior via tags.</li> <li>Consumers (Observatory, sensors) can treat WARN and ERROR differently without custom heuristics.</li> </ul>"},{"location":"architecture/decisions/0004-partition-scoped-checks-and-samples/","title":"4. Make checks partition-aware and include failure sampling","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0004-partition-scoped-checks-and-samples/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0004-partition-scoped-checks-and-samples/#context","title":"Context","text":"<p>Running checks against full tables can be:</p> <ul> <li>Expensive (especially on Iceberg tables).</li> <li>Misleading for partitioned pipelines (where correctness is evaluated per partition/run).</li> </ul> <p>When checks fail, users also need actionable debugging context: a small sample and a query/SQL link to reproduce the failure in Trino.</p>"},{"location":"architecture/decisions/0004-partition-scoped-checks-and-samples/#decision","title":"Decision","text":"<p>Standardize two behaviors across check sources:</p> <ol> <li>Partition scoping</li> <li>If the run has a partition key, checks should scope to that partition by default.</li> <li>When unpartitioned, checks may use a rolling window (configurable) unless explicitly full-table.</li> <li>Default partition column is <code>_phlo_partition_date</code> unless overridden.</li> <li>Actionable failure output</li> <li>Include a small failure sample (&lt;= 20 items) and a reproducible SQL snippet when possible.</li> </ol>"},{"location":"architecture/decisions/0004-partition-scoped-checks-and-samples/#consequences","title":"Consequences","text":"<ul> <li>Checks become safe-by-default for partitioned assets (no accidental full-table scans).</li> <li>Observatory can show consistent failure drilldowns across check sources.</li> <li>Some sources (e.g., dbt) may not always provide row-level samples; the contract allows best-effort.</li> </ul>"},{"location":"architecture/decisions/0005-dbt-translator-metadata/","title":"5. Store dbt compiled SQL in Dagster asset metadata (not descriptions)","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0005-dbt-translator-metadata/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0005-dbt-translator-metadata/#context","title":"Context","text":"<p>Dagster asset descriptions are intended to be human-facing summaries, but compiled SQL can be large. Embedding compiled SQL in descriptions:</p> <ul> <li>Bloats UI payloads and makes descriptions noisy.</li> <li>Increases risk of exceeding size limits or reducing readability.</li> <li>Makes it harder for consumers to selectively fetch/use the SQL.</li> </ul> <p>We still want compiled SQL available for tooling (lineage, debugging, \u201copen in SQL\u201d workflows).</p>"},{"location":"architecture/decisions/0005-dbt-translator-metadata/#decision","title":"Decision","text":"<p>Use a custom <code>DagsterDbtTranslator</code> to:</p> <ul> <li>Improve group naming heuristics for dbt models.</li> <li>Attach compiled SQL to asset metadata (e.g., <code>phlo/compiled_sql</code> and related fields), with   truncation controls and source tracking.</li> </ul>"},{"location":"architecture/decisions/0005-dbt-translator-metadata/#consequences","title":"Consequences","text":"<ul> <li>UIs and consumers can access compiled SQL when needed without polluting descriptions.</li> <li>Metadata size is still bounded; truncation is explicit and discoverable.</li> <li>Translator becomes the central place to evolve dbt\u2192Dagster mapping semantics.</li> </ul>"},{"location":"architecture/decisions/0006-public-api-and-structured-logging/","title":"6. Maintain explicit public exports and avoid print() in library code","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0006-public-api-and-structured-logging/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0006-public-api-and-structured-logging/#context","title":"Context","text":"<p>Phlo is a library + CLI. For good developer ergonomics and type checking:</p> <ul> <li>The public API should be explicit (<code>__all__</code>) and stable.</li> <li>Import-time side effects should be minimized (lazy imports where appropriate).</li> </ul> <p>Separately, <code>print()</code> in library modules creates noisy output and is hard to route/structure compared to standard logging.</p>"},{"location":"architecture/decisions/0006-public-api-and-structured-logging/#decision","title":"Decision","text":"<ul> <li>Use explicit <code>__all__</code> exports for public modules and resources, with typed/lazy imports where   necessary.</li> <li>Replace non-CLI <code>print()</code> usage with structured logging (<code>phlo.logging.get_logger(__name__)</code>   and Dagster context logs where applicable).</li> </ul>"},{"location":"architecture/decisions/0006-public-api-and-structured-logging/#consequences","title":"Consequences","text":"<ul> <li>basedpyright has a clearer view of the public surface area.</li> <li>Users get consistent logs that can be routed/filtered across environments.</li> <li>Some internal refactors are required to avoid import cycles while keeping exports explicit.</li> </ul>"},{"location":"architecture/decisions/0007-cli-services-architecture/","title":"7. Refactor <code>phlo services</code> into testable, composable units","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0007-cli-services-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0007-cli-services-architecture/#context","title":"Context","text":"<p>The <code>phlo services</code> CLI interacts with Docker Compose and local project state. A monolithic implementation makes it hard to:</p> <ul> <li>Test behavior without invoking Docker.</li> <li>Provide consistent error handling and argument validation.</li> <li>Extend commands safely as the service catalog grows.</li> </ul>"},{"location":"architecture/decisions/0007-cli-services-architecture/#decision","title":"Decision","text":"<p>Refactor services CLI logic into focused modules (command building, container selection, service selection, command runner), and keep the Click/Typer command layer thin.</p>"},{"location":"architecture/decisions/0007-cli-services-architecture/#consequences","title":"Consequences","text":"<ul> <li>Core logic becomes unit-testable and easier to reason about.</li> <li>Error messages are more consistent and actionable.</li> <li>Some internal module boundaries are introduced and must be maintained to avoid regressions.</li> </ul>"},{"location":"architecture/decisions/0008-scaffolds-without-placeholders/","title":"8. Scaffold generators must emit working code (no TODO placeholders by default)","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0008-scaffolds-without-placeholders/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0008-scaffolds-without-placeholders/#context","title":"Context","text":"<p>Early development can tolerate breaking changes, but generated scaffolds should still be immediately actionable. Emitting TODO-heavy templates:</p> <ul> <li>Produces \u201cslop\u201d that users must manually clean up.</li> <li>Leads to inconsistent patterns across projects.</li> <li>Makes it harder to scale beyond a single developer.</li> </ul>"},{"location":"architecture/decisions/0008-scaffolds-without-placeholders/#decision","title":"Decision","text":"<p>All <code>phlo</code> scaffolding commands should:</p> <ul> <li>Require explicit inputs for values that cannot be reasonably inferred.</li> <li>Generate minimal working code/config without TODO placeholders.</li> <li>Provide flags (e.g., <code>--field name:type</code>) to ensure generated artifacts are valid and typed.</li> </ul>"},{"location":"architecture/decisions/0008-scaffolds-without-placeholders/#consequences","title":"Consequences","text":"<ul> <li>Scaffolds are more reliable and reduce follow-up manual work.</li> <li>Users may need to supply more inputs up front (a deliberate tradeoff).</li> <li>Template logic must remain strict to prevent regressions into placeholder output.</li> </ul>"},{"location":"architecture/decisions/0009-publishing-yaml-scaffold/","title":"9. Scaffold <code>publishing.yaml</code> from dbt manifest with an idempotent merge strategy","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0009-publishing-yaml-scaffold/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0009-publishing-yaml-scaffold/#context","title":"Context","text":"<p>Publishing configuration should reflect project reality (dbt models, dependencies, and metadata), but it is also hand-edited. We need a CLI that can:</p> <ul> <li>Bootstrap a valid <code>publishing.yaml</code>.</li> <li>Update it as the project evolves without clobbering manual edits.</li> <li>Restrict scope (e.g., only marts models) and support dry runs.</li> </ul>"},{"location":"architecture/decisions/0009-publishing-yaml-scaffold/#decision","title":"Decision","text":"<p>Implement <code>phlo publishing scaffold</code> that:</p> <ul> <li>Reads dbt <code>manifest.json</code> directly for accurate model metadata.</li> <li>Generates/updates <code>publishing.yaml</code> using an idempotent merge strategy that preserves existing   fields and only adds missing stanzas/values.</li> <li>Supports <code>--select</code>, <code>--dry-run</code>, and output path controls.</li> </ul>"},{"location":"architecture/decisions/0009-publishing-yaml-scaffold/#consequences","title":"Consequences","text":"<ul> <li>Publishing config stays in sync with dbt with low manual effort.</li> <li>The scaffold must be careful about merge semantics; \u201cpreserve edits\u201d becomes a contract.</li> <li>The CLI depends on dbt artifacts being present (manifest generation becomes part of workflow).</li> </ul>"},{"location":"architecture/decisions/0010-emit-dagster-asset-checks/","title":"10. Emit Pandera and dbt results as Dagster asset checks","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0010-emit-dagster-asset-checks/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0010-emit-dagster-asset-checks/#context","title":"Context","text":"<p>Dagster asset checks are the canonical mechanism to surface data quality in the platform. We want Pandera and dbt results to be visible as asset checks so that:</p> <ul> <li>Dagster UI, sensors, and GraphQL can treat quality signals uniformly.</li> <li>Observatory can build a Quality Center on top of Dagster checks, not bespoke sources.</li> </ul>"},{"location":"architecture/decisions/0010-emit-dagster-asset-checks/#decision","title":"Decision","text":"<ul> <li>In ingestion, emit a partition-scoped <code>pandera_contract</code> asset check based on the Pandera schema   and staged parquet data; fail the run by default on contract failures.</li> <li>In dbt transforms, parse <code>run_results.json</code> + <code>manifest.json</code> after dbt execution and emit one   Dagster asset check per dbt test, attached to the tested asset key, following the shared naming,   metadata, and severity policies.</li> </ul>"},{"location":"architecture/decisions/0010-emit-dagster-asset-checks/#consequences","title":"Consequences","text":"<ul> <li>Quality becomes first-class in Dagster across ingestion and transform layers.</li> <li>Check emission adds overhead (artifact parsing, metadata shaping) but remains bounded and testable.</li> <li>Check producers must maintain correct asset key mapping; the dbt translator becomes critical.</li> </ul>"},{"location":"architecture/decisions/0011-observatory-quality-from-dagster/","title":"11. Observatory Quality Center sources from Dagster GraphQL with caching and drilldown","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0011-observatory-quality-from-dagster/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0011-observatory-quality-from-dagster/#context","title":"Context","text":"<p>Observatory needs a unified view of quality across assets with:</p> <ul> <li>A dashboard summary (counts, recent activity).</li> <li>Filtering (by layer/status/type).</li> <li>Drilldown (metadata, sample, history) and links to investigate in SQL.</li> </ul> <p>Dagster is the source of truth for asset checks, but GraphQL queries can be expensive if repeatedly issued from the UI.</p>"},{"location":"architecture/decisions/0011-observatory-quality-from-dagster/#decision","title":"Decision","text":"<ul> <li>Implement server-side aggregation of Dagster asset checks via Dagster GraphQL:</li> <li>Fetch check definitions per asset.</li> <li>Fetch recent check executions per asset and compute \u201clatest status\u201d per check.</li> <li>Provide a consolidated API for overview, failing checks, recent executions, and per-check history.</li> <li>Use a short-lived in-memory cache to avoid hammering Dagster on rapid UI refresh.</li> <li>Provide \u201cOpen in SQL\u201d deep links by reusing check metadata (<code>repro_sql</code> / <code>query_or_sql</code>) and by   enabling <code>?sql=</code> and <code>?tab=</code> URL parameters in the Data Explorer.</li> </ul>"},{"location":"architecture/decisions/0011-observatory-quality-from-dagster/#consequences","title":"Consequences","text":"<ul> <li>Observatory stays Dagster-check-native and does not need to understand Pandera/dbt internals.</li> <li>Caching improves perceived performance but introduces brief staleness (acceptable for monitoring).</li> <li>Deep linking depends on stable URL and query-handling semantics in the Data Explorer.</li> </ul>"},{"location":"architecture/decisions/0012-observatory-data-explorer-branch-aware-routing/","title":"12. Make Observatory Data Explorer branch-aware via path-based routing","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0012-observatory-data-explorer-branch-aware-routing/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0012-observatory-data-explorer-branch-aware-routing/#context","title":"Context","text":"<p>Observatory\u2019s Data Explorer must support browsing non-main branches (e.g., feature/dev branches) for Iceberg/Nessie-backed data. Hardcoding <code>branch=main</code> breaks workflows and makes shareable URLs misleading.</p> <p>This decision is tracked and implemented by bead <code>phlo-nwk.1.2</code> (closed).</p>"},{"location":"architecture/decisions/0012-observatory-data-explorer-branch-aware-routing/#decision","title":"Decision","text":"<ul> <li>Represent the selected branch in the Data Explorer URL via a path segment (not UI-only state),   enabling stable, shareable links.</li> <li>Treat the branch as an end-to-end parameter:</li> <li>UI reads/writes the branch from the route.</li> <li>Server endpoints accept an explicit branch and pass it through to the Trino/Nessie query layer.</li> <li>Default to <code>main</code> only when no branch is present in the route (back/forward compatible behavior   for existing links).</li> </ul>"},{"location":"architecture/decisions/0012-observatory-data-explorer-branch-aware-routing/#consequences","title":"Consequences","text":"<ul> <li>Branch selection becomes durable (reload-safe) and shareable.</li> <li>Server APIs become explicit about branch context, reducing accidental cross-branch reads.</li> <li>Route structure becomes part of the public UX contract and must remain stable.</li> </ul>"},{"location":"architecture/decisions/0013-cli-generate-pandera-from-dlt-inference/","title":"13. Generate Pandera schemas from DLT inference via CLI sample runs","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0013-cli-generate-pandera-from-dlt-inference/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0013-cli-generate-pandera-from-dlt-inference/#context","title":"Context","text":"<p>Phlo users often start ingestion before a stable schema is known. DLT can infer schema from sampled source data, but drift can occur if Pandera validation schemas aren\u2019t kept in sync with the actual extracted columns and types.</p> <p>We want a first-class, non-Dagster mechanism to:</p> <ul> <li>Run a small DLT sample (without materializing Dagster assets).</li> <li>Extract DLT-inferred column types/nullability.</li> <li>Generate Pandera <code>DataFrameModel</code> schemas (using <code>PhloSchema</code>) into the user\u2019s lakehouse code.</li> </ul> <p>This decision is tracked and implemented by bead <code>phlo-nwk.3.5</code> (closed).</p>"},{"location":"architecture/decisions/0013-cli-generate-pandera-from-dlt-inference/#decision","title":"Decision","text":"<ul> <li>Add <code>phlo schema generate</code>:</li> <li>Accepts a Python reference to either:<ul> <li>a callable returning a DLT source/resource (or iterable of records), or</li> <li>a Dagster <code>AssetsDefinition</code> created via <code>@phlo_ingestion</code>, from which we extract the wrapped   source-building function.</li> </ul> </li> <li>Executes a bounded \u201csample run\u201d to a local filesystem destination and reads DLT\u2019s inferred     schema from the resulting pipeline schema.</li> <li>Emits a <code>PhloSchema</code> class for the selected DLT table/resource.</li> <li>Default behavior is non-destructive:</li> <li>Prints generated code (dry-run).</li> <li>Refuses to overwrite existing schema files unless explicitly requested.</li> </ul>"},{"location":"architecture/decisions/0013-cli-generate-pandera-from-dlt-inference/#consequences","title":"Consequences","text":"<ul> <li>Users can bootstrap schemas early, without needing Iceberg tables to exist.</li> <li>Schema generation becomes reproducible and can be re-run to prevent drift.</li> <li>This introduces a small amount of DLT coupling in the CLI surface area that must be maintained.</li> </ul>"},{"location":"architecture/decisions/0014-observatory-ui-redesign-with-shadcn-lyra-preset/","title":"14. Redesign Observatory UI with shadcn Lyra preset (fixed design system)","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0014-observatory-ui-redesign-with-shadcn-lyra-preset/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0014-observatory-ui-redesign-with-shadcn-lyra-preset/#context","title":"Context","text":"<p>The Observatory UI currently uses ad-hoc Tailwind styling and bespoke components, making it hard to iterate quickly and keep the experience cohesive as the product grows.</p> <p>Shadcn/ui now supports generating a consistent component + styling baseline via presets, including the Lyra style and modern CSS-variable theming. We want a full redesign of the Observatory UI using that baseline.</p> <p>This decision is tracked by bead <code>phlo-bms</code> (closed).</p>"},{"location":"architecture/decisions/0014-observatory-ui-redesign-with-shadcn-lyra-preset/#decision","title":"Decision","text":"<ul> <li>Use shadcn/ui as the Observatory UI component system and adopt the Lyra preset baseline.</li> <li>Generate the baseline in a throwaway workspace via:</li> <li><code>npx shadcn@latest create --preset \"https://ui.shadcn.com/init?base=base&amp;style=lyra&amp;baseColor=neutral&amp;theme=amber&amp;iconLibrary=lucide&amp;font=jetbrains-mono&amp;menuAccent=subtle&amp;menuColor=default&amp;radius=default&amp;template=start\" --template start</code></li> <li>Copy the relevant artifacts into <code>packages/phlo-observatory/src/phlo_observatory/</code> (CSS variables, <code>components.json</code>,   utility helpers, and shadcn component primitives) and use them as the source of truth going   forward.</li> <li>Ship a fixed design system:</li> <li>No in-app controls for changing palettes, radii, or density.</li> </ul>"},{"location":"architecture/decisions/0014-observatory-ui-redesign-with-shadcn-lyra-preset/#consequences","title":"Consequences","text":"<ul> <li>Observatory screens share consistent layout, typography, and interaction primitives.</li> <li>UI development accelerates (standard components + predictable tokens), and future work (command   palette, tables, settings) builds on the same foundation.</li> <li>The frontend gains a small set of additional dependencies (shadcn + utilities + font package).</li> </ul>"},{"location":"architecture/decisions/0015-observatory-table-engine-on-tanstack/","title":"15. Standardize Observatory tables on TanStack Table (+ virtualization)","text":"<p>Date: 2025-12-14</p>"},{"location":"architecture/decisions/0015-observatory-table-engine-on-tanstack/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0015-observatory-table-engine-on-tanstack/#context","title":"Context","text":"<p>Observatory contains multiple \u201cdata grid\u201d style surfaces (Data Preview, Query Results, and various column/value tables) that need consistent support for:</p> <ul> <li>Column resizing and pinning (especially for key columns).</li> <li>Sorting and filtering.</li> <li>Keyboard navigation and accessibility.</li> <li>Responsive rendering with very large result sets (50k+ rows).</li> </ul> <p>Ad-hoc table implementations (or one-off wrappers) increase maintenance cost and make it difficult to add features without regressions across pages.</p> <p>This decision is tracked by bead <code>phlo-4su</code> (closed).</p>"},{"location":"architecture/decisions/0015-observatory-table-engine-on-tanstack/#decision","title":"Decision","text":"<ul> <li>Adopt TanStack Table (<code>@tanstack/react-table</code>) as the single table engine for Observatory UI.</li> <li>Use TanStack Virtual (<code>@tanstack/react-virtual</code>) for row virtualization where the UI renders   client-side rows.</li> <li>Keep server-side pagination for truly large result sets; virtualization optimizes rendering, not   query execution.</li> <li>Implement a shared table wrapper component used by Data Preview and Query Results, parameterized   by:</li> <li>Column definitions (including type-aware formatting).</li> <li>Row data and a stable row id getter.</li> <li>Feature toggles (sorting, filtering, pinning, resizing).</li> <li>Use shadcn/ui primitives for styling and interactions (menus, inputs, badges) and preserve   existing cell renderers (timestamps, nulls, links).</li> </ul>"},{"location":"architecture/decisions/0015-observatory-table-engine-on-tanstack/#consequences","title":"Consequences","text":"<ul> <li>Table behavior is consistent across pages, enabling faster iteration and fewer regressions.</li> <li>Future features (inline contributing rows, table browser improvements) can reuse the shared table   component rather than reimplementing grid behavior.</li> <li>The frontend gains additional dependencies (<code>@tanstack/react-table</code>, <code>@tanstack/react-virtual</code>).</li> </ul>"},{"location":"architecture/decisions/0016-observatory-settings-and-query-guardrails/","title":"16. Add Observatory settings and server-side query guardrails","text":"<p>Date: 2025-12-16</p>"},{"location":"architecture/decisions/0016-observatory-settings-and-query-guardrails/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0016-observatory-settings-and-query-guardrails/#context","title":"Context","text":"<p>Observatory needs to talk to multiple backends (Dagster GraphQL, Trino, Nessie) and to provide a consistent data exploration experience across environments. Hardcoding these connection details (or keeping them scattered across the UI) makes it hard to switch environments, set sensible defaults, and keep behavior consistent across pages.</p> <p>In addition, Observatory includes a SQL runner. Without enforced guardrails, it is too easy to run DDL/DML, multi-statement input, or unbounded queries that are unsafe for shared environments.</p> <p>This decision is tracked by beads <code>phlo-npw</code> and <code>phlo-8os</code> (closed).</p>"},{"location":"architecture/decisions/0016-observatory-settings-and-query-guardrails/#decision","title":"Decision","text":"<ul> <li>Add a typed Settings model for Observatory covering:</li> <li>Connection URLs (Dagster GraphQL, Trino, Nessie).</li> <li>Default context (catalog/schema/branch).</li> <li>Query defaults (limit/timeout).</li> <li>Safety (read-only / SELECT-only mode).</li> <li>UI preferences (e.g., theme/density/date format).</li> <li>Persist settings in <code>localStorage</code> with environment-provided defaults used as the initial values.</li> <li>Route all Observatory backend calls through the settings-aware clients so the configured endpoints   are respected consistently.</li> <li>Enforce server-side query validation and safety limits for the SQL runner:</li> <li>Allowlist read-only statements (e.g., <code>SELECT</code>, <code>SHOW</code>, <code>DESCRIBE</code>, <code>EXPLAIN</code>).</li> <li>Reject DDL/DML and multi-statement input.</li> <li>Apply default and maximum <code>LIMIT</code> behavior, plus a statement timeout.</li> <li>Return structured error information so the UI can clearly distinguish blocked vs failed vs     timed-out queries.</li> </ul>"},{"location":"architecture/decisions/0016-observatory-settings-and-query-guardrails/#consequences","title":"Consequences","text":"<ul> <li>Users can switch environments and preferences without code changes.</li> <li>Query execution becomes safer by default; dangerous statements are blocked with clear errors.</li> <li>A future migration path to server-side, multi-user settings storage remains available when   authentication/identity is introduced.</li> </ul>"},{"location":"architecture/decisions/0017-observatory-contributing-rows-inline-pagination-and-sampling/","title":"17. Add inline contributing rows with pagination and deterministic sampling","text":"<p>Date: 2025-12-16</p>"},{"location":"architecture/decisions/0017-observatory-contributing-rows-inline-pagination-and-sampling/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0017-observatory-contributing-rows-inline-pagination-and-sampling/#context","title":"Context","text":"<p>Observatory currently renders \"contributing rows\" as SQL only: from a selected downstream (often gold/aggregate) row, the UI can generate a deterministic query to fetch upstream rows, but it requires the user to switch into the SQL runner to see results.</p> <p>For transformed tables with a stable row key (<code>_phlo_row_id</code>), contributing rows are typically small and can be fetched directly. For aggregate rows, the contributing set can be very large, so the product needs a clear, safe strategy for bounded exploration (pagination and deterministic sampling) with an auditable SQL trail.</p> <p>This decision is tracked by beads <code>phlo-rml</code> and <code>phlo-504</code> (closed).</p>"},{"location":"architecture/decisions/0017-observatory-contributing-rows-inline-pagination-and-sampling/#decision","title":"Decision","text":"<ul> <li>Add a server endpoint that returns contributing rows for a (downstream row, upstream table) pair.</li> <li>Support two modes (auto-selected):</li> <li>Entity mode: if <code>_phlo_row_id</code> exists downstream and upstream, fetch matching upstream rows.</li> <li>Aggregate mode: derive safe predicates from partition/date and mapped dimensions, then return a     deterministic pseudo-random ordering with pagination.</li> <li>Enforce guardrails on the endpoint:</li> <li>Hard caps for <code>pageSize</code> and overall server timeouts for Trino calls.</li> <li>Deterministic ordering for sampling/pagination so results are repeatable.</li> <li>Return the effective SQL used for audit/debug.</li> <li>Extend the Row Journey UI to render contributing rows inline in a drawer/panel:</li> <li>Paginated table view (using the shared TanStack table wrapper).</li> <li>Clear communication of mode (entity vs aggregate) and page size caps.</li> <li>Retain \"open in SQL\" as a fallback/debug action.</li> </ul>"},{"location":"architecture/decisions/0017-observatory-contributing-rows-inline-pagination-and-sampling/#consequences","title":"Consequences","text":"<ul> <li>Users can inspect contributing rows inline without context switching to the SQL runner.</li> <li>Aggregate contributing rows are bounded and repeatable via deterministic sampling and paging,   avoiding accidental expensive queries.</li> <li>The API and UI patterns become a reusable building block for future \"Quality Center \u2192 source   rows\" workflows.</li> </ul>"},{"location":"architecture/decisions/0018-observatory-command-palette/","title":"18. Observatory command palette global search","text":"<p>Date: 2025-12-17</p>"},{"location":"architecture/decisions/0018-observatory-command-palette/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0018-observatory-command-palette/#context","title":"Context","text":"<p>Observatory needs a quick way to navigate across different entity types: Dagster assets, Iceberg tables, table columns, and recent pipeline runs. Users should be able to find and jump to these items without manually navigating through the sidebar hierarchy.</p> <p>A basic <code>CommandPalette.tsx</code> component already exists with \u2318K/Ctrl+K support and limited asset search. This decision extends it to support the full searchable entity index, caching, and additional actions.</p> <p>This decision is tracked by bead <code>phlo-8aw</code>.</p>"},{"location":"architecture/decisions/0018-observatory-command-palette/#decision","title":"Decision","text":"<ul> <li>Preload and cache the search index at application init using existing server functions:</li> <li>Assets from <code>getAssets</code> (Dagster GraphQL).</li> <li>Tables and columns from <code>getTables</code> and <code>getTableSchema</code> (Iceberg via Trino).</li> <li>Use the existing <code>cmdk</code> library which already handles fuzzy search and keyboard navigation.</li> <li>Add debouncing (300ms) for column schema lookups to avoid overwhelming Trino on initial load.</li> <li>Extend the command palette with new search groups and actions:</li> </ul> Group Source Action Assets <code>getAssets</code> Open Asset page, Focus in Graph Tables <code>getTables</code> Open in Data Explorer Columns <code>getTableSchema</code> (cached) Open table, copy column name SQL Templates derived from Tables Copy <code>SELECT * FROM ...</code> to clipboard <ul> <li>Store the search index in React state at the <code>__root.tsx</code> level, refreshing on route changes to   the Data Explorer (to pick up new tables).</li> <li>\"Insert SQL template\" copies a <code>SELECT * FROM \"catalog\".\"schema\".\"table\" LIMIT 100</code> string to   the clipboard and shows a toast notification.</li> </ul>"},{"location":"architecture/decisions/0018-observatory-command-palette/#consequences","title":"Consequences","text":"<ul> <li>Users can quickly navigate to any asset, table, or column via \u2318K.</li> <li>The search index is cached and preloaded, so lookups are instant after initial load.</li> <li>Adding more entity types later (e.g., saved queries, bookmarks) follows the same pattern.</li> <li>Column schema lookups may add initial load latency; we mitigate this with progressive loading   (tables first, columns after).</li> </ul>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/","title":"19. Observatory metadata caching","text":"<p>Date: 2025-12-17</p>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#context","title":"Context","text":"<p>Observatory server functions (<code>iceberg.server.ts</code>, <code>dagster.server.ts</code>, <code>search.server.ts</code>) query Trino and Dagster GraphQL directly on every request. This causes:</p> <ol> <li>Latency: Each <code>getTables</code> call executes 6+ <code>SHOW TABLES</code> queries against Trino (one per schema).</li> <li>Load: Repeated navigation across the same tables re-queries metadata every time.</li> <li>No observability: No visibility into cache performance or query frequency.</li> </ol> <p>Observatory has no caching layer. The TanStack Start server functions (<code>createServerFn</code>) run in the Node.js SSR context and query upstream services directly on every request.</p> <p>Related beads:</p> <ul> <li><code>phlo-b1w</code>: Observatory: Metadata caching (this decision)</li> <li><code>phlo-13a</code>: Observatory: Table browser improvements (depends on caching)</li> <li><code>phlo-8aw</code>: Observatory: Command palette (uses search index)</li> </ul>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#decision","title":"Decision","text":"<p>Implement a server-side metadata cache in Observatory with TTL-based expiration and optional event-based invalidation.</p>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#cache-architecture","title":"Cache Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Observatory SSR                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 getTables()  \u2502    \u2502 getAssets()  \u2502    \u2502getSearchIdx()\u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                   \u2502                   \u2502          \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                             \u25bc                              \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502                   \u2502  MetadataCache  \u2502                      \u2502\n\u2502                   \u2502  (in-memory)    \u2502                      \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                            \u2502 miss                          \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502         \u25bc                  \u25bc                  \u25bc            \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502    \u2502  Trino  \u2502       \u2502  Dagster \u2502       \u2502  Nessie \u2502       \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502  GraphQL \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#implementation","title":"Implementation","text":"<ol> <li>Cache module (<code>server/cache.ts</code>):</li> <li>In-memory Map with TTL per entry</li> <li>Key generation from function name + normalized args</li> <li><code>get</code>, <code>set</code>, <code>invalidate</code>, <code>invalidatePattern</code> methods</li> <li> <p><code>getStats</code> for hit/miss metrics</p> </li> <li> <p>Cache wrapper (<code>withCache</code> HOF):</p> </li> </ol> <p><code>typescript    export function withCache&lt;T&gt;(      fn: () =&gt; Promise&lt;T&gt;,      key: string,      ttlMs: number = DEFAULT_TTL_MS,    ): Promise&lt;T&gt;;</code></p> <ol> <li> <p>TTL configuration (environment variables with sensible defaults):    | Data Type | TTL | Rationale |    |-----------|-----|-----------|    | Table list | 5 min | Changes infrequently, okay to be slightly stale |    | Table schema | 10 min | Schema changes are rare |    | Asset list | 2 min | Assets may be added during development |    | Search index | 5 min | Composite of above, balanced freshness |</p> </li> <li> <p>Cache key structure:</p> </li> </ol> <p><code>iceberg:tables:{catalog}:{branch}    iceberg:schema:{catalog}:{schema}:{table}    dagster:assets:{dagsterUrl}    search:index:{dagsterUrl}:{trinoUrl}</code></p> <ol> <li>Logging:</li> <li>Log cache hits/misses at debug level</li> <li>Expose <code>/api/observatory/cache/stats</code> endpoint for monitoring</li> <li>Track: total hits, total misses, hit rate, entries by prefix</li> </ol>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#event-based-invalidation-phase-2","title":"Event-Based Invalidation (Phase 2)","text":"<p>Defer Dagster event-based invalidation to a follow-up. The TTL approach provides immediate value and the 2-5 minute staleness is acceptable for metadata. Event-based invalidation adds complexity (webhook setup, Dagster sensor configuration) for marginal benefit.</p> <p>If needed later:</p> <ul> <li>Dagster sensor emits webhook on asset materialization</li> <li>Webhook handler calls <code>cache.invalidatePattern('iceberg:*')</code> for schema-affecting materializations</li> </ul>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#fallback-behavior","title":"Fallback Behavior","text":"<p>On cache miss or cache error:</p> <ol> <li>Execute the underlying query directly</li> <li>Log the cache miss/error</li> <li>Attempt to populate cache with result</li> <li>Return result to caller (never fail due to cache issues)</li> </ol>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0019-observatory-metadata-caching/#positive","title":"Positive","text":"<ul> <li>Repeated table browser navigation is instant after first load</li> <li>Command palette search index loads from cache on subsequent opens</li> <li>Reduced load on Trino and Dagster during active Observatory sessions</li> <li>Cache stats provide visibility into query patterns</li> </ul>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#negative","title":"Negative","text":"<ul> <li>Metadata can be stale for up to TTL duration (mitigated by reasonable TTLs)</li> <li>In-memory cache is process-local (acceptable for single-instance Observatory)</li> <li>Additional code complexity in server functions</li> </ul>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#risks","title":"Risks","text":"<ul> <li>Memory growth if cache is unbounded: mitigate with max entry count and LRU eviction</li> <li>Cache key collisions: mitigate with structured key format and tests</li> </ul>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Create <code>server/cache.ts</code> with <code>MetadataCache</code> class</li> <li>Add <code>withCache</code> wrapper function</li> <li>Integrate into <code>getTables</code>, <code>getTableSchema</code>, <code>getAssets</code>, <code>getSearchIndex</code></li> <li>Add cache stats endpoint</li> <li>Add debug logging for cache operations</li> <li>Write tests for cache behavior (TTL, invalidation, fallback)</li> </ol>"},{"location":"architecture/decisions/0019-observatory-metadata-caching/#beads","title":"Beads","text":"<ul> <li>phlo-b1w: Observatory: Metadata caching (in progress)</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/","title":"20. Observatory table browser improvements","text":"<p>Date: 2025-12-17</p>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#context","title":"Context","text":"<p>The current Table Browser sidebar in Observatory provides basic functionality for browsing Iceberg tables organized by medallion layer (bronze/silver/gold/publish). However, as the number of tables grows, several UX issues emerge:</p> <ol> <li>No virtualization: All table rows are rendered in the DOM, causing performance degradation    with 100+ tables.</li> <li>Basic search: Only filters on table name; users cannot search by schema or layer.</li> <li>Limited visual feedback: Selected table state is not visually highlighted; loading states    for individual tables are not shown.</li> <li>No keyboard navigation: Users must use mouse to navigate the table tree.</li> <li>No schema grouping option: Tables are grouped only by layer, not by schema.</li> </ol> <p>The implementation currently lives in two places:</p> <ul> <li><code>TableBrowser.tsx</code>: Original component with internal data fetching (deprecated)</li> <li><code>$branchName.tsx</code>: Layout route with <code>TableBrowserCached</code> inline component</li> </ul> <p>This ADR proposes a unified, improved Table Browser component.</p> <p>Related beads:</p> <ul> <li><code>phlo-13a</code>: Observatory: Table browser improvements (this decision)</li> <li><code>phlo-b1w</code>: Observatory: Metadata caching (completed - provides server-side caching)</li> <li><code>phlo-4su</code>: Observatory: TanStack Table migration (completed - provides virtualization patterns)</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#decision","title":"Decision","text":"<p>Implement the following improvements to the Table Browser:</p>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#1-virtualized-list-rendering","title":"1. Virtualized List Rendering","text":"<p>Use <code>@tanstack/react-virtual</code> (already installed) to virtualize the table list. Only visible table items are rendered in the DOM.</p> <pre><code>const parentRef = useRef&lt;HTMLDivElement&gt;(null);\nconst rowVirtualizer = useVirtualizer({\n  count: filteredTables.length,\n  getScrollElement: () =&gt; parentRef.current,\n  estimateSize: () =&gt; 32, // px per row\n  overscan: 10,\n});\n</code></pre>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#2-enhanced-search-with-fuzzy-matching","title":"2. Enhanced Search with Fuzzy Matching","text":"<p>Extend search to match on:</p> <ul> <li>Table name (primary)</li> <li>Schema name</li> <li>Layer name</li> <li>Combined (e.g., \"bronze events\" matches <code>bronze.dlt_user_events</code>)</li> </ul> <p>Use a simple scoring function rather than a library to avoid bundle bloat:</p> <pre><code>function matchScore(table: IcebergTable, query: string): number {\n  const q = query.toLowerCase();\n  const name = table.name.toLowerCase();\n  const schema = table.schema.toLowerCase();\n  const layer = table.layer.toLowerCase();\n\n  if (name === q) return 100; // Exact name match\n  if (name.startsWith(q)) return 80; // Prefix match\n  if (name.includes(q)) return 60; // Substring match\n  if (schema.includes(q)) return 40; // Schema match\n  if (layer.includes(q)) return 20; // Layer match\n  return 0;\n}\n</code></pre>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#3-keyboard-navigation","title":"3. Keyboard Navigation","text":"<p>Add keyboard support:</p> <ul> <li><code>\u2191</code>/<code>\u2193</code>: Navigate between tables</li> <li><code>Enter</code>: Select focused table</li> <li><code>Escape</code>: Clear search and focus search input</li> <li><code>Arrow Right</code>/<code>Left</code>: Expand/collapse layer groups</li> </ul> <p>Implement using React state for <code>focusedIndex</code> and <code>onKeyDown</code> handler on the container.</p>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#4-list-density-from-existing-settings","title":"4. List Density (from existing Settings)","text":"<p>Use the existing <code>settings.ui.density</code> preference (<code>'comfortable' | 'compact'</code>) already available in Observatory Settings (ADR-0016). The Table Browser reads this setting and adjusts row heights:</p> Density Row Height Source Compact 28px <code>settings.ui.density === 'compact'</code> Comfortable 36px <code>settings.ui.density === 'comfortable'</code> (default) <pre><code>const { settings } = useObservatorySettings();\nconst rowHeight = settings.ui.density === \"compact\" ? 28 : 36;\n\nconst rowVirtualizer = useVirtualizer({\n  count: filteredTables.length,\n  getScrollElement: () =&gt; parentRef.current,\n  estimateSize: () =&gt; rowHeight,\n  overscan: 10,\n});\n</code></pre> <p>Note: No new density UI is added to the Table Browser - users change density in Settings.</p>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#6-visual-improvements","title":"6. Visual Improvements","text":"<ul> <li>Add highlight for selected table that persists during navigation</li> <li>Show loading skeleton when tables are loading</li> <li>Add empty state with clear messaging for:</li> <li>No tables in catalog</li> <li>No tables matching search</li> <li>Connection error (with retry button)</li> <li>Add table count badge in header</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#7-consolidate-components","title":"7. Consolidate Components","text":"<p>Remove the duplicated <code>TableBrowser.tsx</code> component and keep only the cached version in the layout route. Extract the improved component to <code>components/data/TableBrowserVirtualized.tsx</code> for reusability.</p>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#file-changes","title":"File Changes","text":"File Change <code>components/data/TableBrowserVirtualized.tsx</code> [NEW] Virtualized table browser with all improvements <code>routes/data/$branchName.tsx</code> Use new <code>TableBrowserVirtualized</code> component <code>components/data/TableBrowser.tsx</code> [DELETE] Remove deprecated component <code>hooks/useTableBrowserKeyboard.ts</code> [NEW] Keyboard navigation hook"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#positive","title":"Positive","text":"<ul> <li>Smooth scrolling with 500+ tables (virtualization)</li> <li>Faster search with fuzzy matching across name/schema/layer</li> <li>Keyboard-accessible navigation</li> <li>Consistent selected table highlight</li> <li>Clear empty/error states improve debuggability</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#negative","title":"Negative","text":"<ul> <li>Slightly more complex component (keyboard state, virtualizer refs)</li> <li>Schema grouping toggle adds UI complexity</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#risks","title":"Risks","text":"<ul> <li>Virtualization edge cases with dynamic height items: mitigate with fixed row height</li> <li>Keyboard navigation conflicts with global shortcuts: mitigate by only handling when component   is focused</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#automated-tests","title":"Automated Tests","text":"<ul> <li>Unit tests for <code>matchScore</code> function</li> <li>Component tests for keyboard navigation</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#manual-verification","title":"Manual Verification","text":"<ul> <li>Verify virtualization works with 100+ tables (scroll performance)</li> <li>Test search: \"bronze\", \"events\", \"gold fct\", schema name</li> <li>Test keyboard: arrow keys, Enter to select, Escape to clear</li> <li>Verify selected table stays highlighted after navigation</li> </ul>"},{"location":"architecture/decisions/0020-observatory-table-browser-improvements/#beads","title":"Beads","text":"<ul> <li>phlo-13a: Observatory: Table browser improvements (complete)</li> </ul>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/","title":"21. Observatory stage diff view","text":"<p>Date: 2025-12-18</p>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#context","title":"Context","text":"<p>The Data Journey feature in Observatory shows how rows flow through the pipeline from bronze \u2192 silver \u2192 gold. Currently, users can see the transformation SQL and contributing rows at each stage, but there's no visual way to understand what changed between stages.</p> <p>This makes it harder to:</p> <ol> <li>Debug data quality issues (why did a value change?)</li> <li>Understand schema evolution (what columns were added/removed?)</li> <li>Trace aggregation effects (how did 150 rows become 1?)</li> </ol> <p>Related beads:</p> <ul> <li><code>phlo-1p9</code>: Add visual diff view between pipeline stages (this decision)</li> <li><code>phlo-ecv</code>: Show lineage confidence and transform type in UI (completed - provides confidence scoring)</li> <li><code>phlo-ruz</code>: Data Journey UX Improvements (blocked on this)</li> </ul>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#decision","title":"Decision","text":"<p>Implement a Stage Diff component that compares data between adjacent pipeline stages:</p>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#1-stagediff-component","title":"1. StageDiff Component","text":"<p>Create <code>components/data/StageDiff.tsx</code> with three diff views:</p>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#column-diff","title":"Column Diff","text":"<p>Shows schema changes between stages:</p> <ul> <li>Added columns: Columns in downstream but not upstream (green)</li> <li>Removed columns: Columns in upstream but not downstream (red)</li> <li>Renamed columns: Detected via SQL column mappings (yellow)</li> <li>Transformed columns: Columns with function applied (blue)</li> </ul> <pre><code>&lt;ColumnDiff\n  upstreamColumns={[\"created_at\", \"user_id\", \"payload\"]}\n  downstreamColumns={[\"event_date\", \"user_id\", \"total_events\"]}\n  mappings={columnMappings}\n/&gt;\n</code></pre>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#value-diff-11-transforms-only","title":"Value Diff (1:1 transforms only)","text":"<p>For non-aggregate transforms, show before/after values:</p> Column Upstream Downstream Change event_date 2024-01-01T10:30:00Z 2024-01-01 DATE() user_id 123 123 unchanged"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#aggregate-explanation","title":"Aggregate Explanation","text":"<p>For GROUP BY transforms, show aggregation summary:</p> <pre><code>Grouped by: [activity_date]\nAggregations: COUNT(*) \u2192 total_events, SUM(score) \u2192 total_score\nSource rows: ~150 rows \u2192 1 row\n</code></pre>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#2-server-side-diff-computation","title":"2. Server-Side Diff Computation","text":"<p>Add <code>getStageDiff</code> server function in <code>contributing.server.ts</code>:</p> <pre><code>export interface StageDiffResult {\n  transformType: TransformType;\n  confidence: number;\n  columns: {\n    added: string[];\n    removed: string[];\n    renamed: Array&lt;{ from: string; to: string; transformation?: string }&gt;;\n    unchanged: string[];\n  };\n  aggregation?: {\n    groupBy: string[];\n    aggregates: Array&lt;{ expression: string; alias: string }&gt;;\n    estimatedSourceRows: number;\n  };\n  sampleValues?: Array&lt;{\n    column: string;\n    upstream: Primitive;\n    downstream: Primitive;\n    transformation?: string;\n  }&gt;;\n}\n</code></pre>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#3-integration-with-rowjourney","title":"3. Integration with RowJourney","text":"<p>Add a \"Compare with upstream\" button in <code>NodeDetailPanel</code> that opens the diff view:</p> <pre><code>&lt;Button onClick={() =&gt; setDiffOpen(true)}&gt;\n  Compare with upstream\n&lt;/Button&gt;\n&lt;StageDiff\n  open={diffOpen}\n  upstreamAssetKey={selectedUpstream}\n  downstreamAssetKey={assetKey}\n  rowData={rowData}\n  onClose={() =&gt; setDiffOpen(false)}\n/&gt;\n</code></pre>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#4-file-changes","title":"4. File Changes","text":"File Change <code>components/data/StageDiff.tsx</code> [NEW] Main diff component with ColumnDiff, ValueDiff, AggregateExplanation <code>server/diff.server.ts</code> [NEW] Server function for computing diffs <code>components/data/RowJourney.tsx</code> Add StageDiff integration to NodeDetailPanel <code>utils/sqlParser.ts</code> Minor: expose additional helpers if needed"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#positive","title":"Positive","text":"<ul> <li>Users understand schema evolution at a glance</li> <li>Debugging data issues is faster (see what transformation caused change)</li> <li>Aggregation effects are clearly communicated</li> <li>Integrates with existing confidence scoring from <code>phlo-ecv</code></li> </ul>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#negative","title":"Negative","text":"<ul> <li>Additional server queries for diff computation (mitigate with caching)</li> <li>More complex NodeDetailPanel component</li> </ul>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#risks","title":"Risks","text":"<ul> <li>SQL parsing limitations may miss some transformations (show \"unknown\" gracefully)</li> <li>Value comparison requires fetching sample data from both stages (performance concern)</li> </ul>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#automated-tests","title":"Automated Tests","text":"<ol> <li> <p>Unit tests for diff computation (<code>server/diff.server.test.ts</code>):    <code>bash    cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm test -- diff.server.test</code></p> </li> <li> <p>Test column diff: added, removed, renamed, unchanged</p> </li> <li>Test aggregate detection from SQL</li> <li>Test transform type classification integration</li> </ol>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#manual-verification","title":"Manual Verification","text":"<ol> <li>Start Observatory in dev mode:</li> </ol> <p><code>bash    cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm run dev -- --port 3001</code></p> <ol> <li>Navigate to Data Explorer:</li> <li>Open http://localhost:3001/data/main</li> <li>Select a gold table (e.g., <code>fct_daily_github_metrics</code>)</li> <li> <p>Click a row to open the Row Journey view</p> </li> <li> <p>Verify Stage Diff:</p> </li> <li>Click on an upstream node (e.g., <code>stg_github_events</code>)</li> <li>Click \"Compare with upstream\" button</li> <li>Verify column diff shows:<ul> <li><code>event_date</code> as renamed from <code>created_at</code> (via DATE())</li> <li><code>total_events</code> as added (aggregate)</li> </ul> </li> <li>Verify aggregate explanation shows GROUP BY columns</li> </ol>"},{"location":"architecture/decisions/0021-observatory-stage-diff-view/#beads","title":"Beads","text":"<ul> <li>phlo-1p9: Add visual diff view between pipeline stages (complete)</li> </ul>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/","title":"22. Ingestion validation hardening &amp; cleanup","text":"<p>Date: 2025-12-18</p>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#context","title":"Context","text":"<p>We have identified technical debt and potential data quality risks in our data ingestion pipelines:</p> <ol> <li>Loose Validation: <code>validate_with_pandera</code> currently catches schema validation errors and continues with a warning. This hides data quality issues and prevents pipelines from \"failing fast\" when bad data is ingested.</li> <li>Deprecated Code: <code>add_phlo_timestamp</code> is deprecated in favor of <code>inject_metadata_columns</code>, but still exists in the codebase.</li> <li>Lack of Control: Users cannot configure whether a pipeline should fail or warn on validation errors.</li> </ol> <p>Related beads:</p> <ul> <li><code>phlo-nwk.4.3</code>: Make ingestion validation failures actionable (configurable strictness)</li> <li><code>phlo-nwk.4.4</code>: Remove deprecated <code>add_phlo_timestamp</code> path</li> </ul>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#decision","title":"Decision","text":"<p>We will harden the ingestion process by introducing strict validation controls and removing deprecated code.</p>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#1-configurable-strict-validation","title":"1. Configurable Strict Validation","text":"<p>We will add a <code>strict_validation</code> parameter to the <code>@phlo_ingestion</code> decorator and update the underlying validation logic.</p> <ul> <li>By Default (Strict): <code>strict_validation=True</code>. Pipelines will FAIL if Pandera validation fails. This ensures data quality is enforced by default.</li> <li>Opt-out (Warn): Users can set <code>strict_validation=False</code> to preserve the old behavior (log warning and continue), useful for legacy pipelines or \"best effort\" ingestion.</li> </ul>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#changes-in-decoratorpy","title":"Changes in <code>decorator.py</code>","text":"<pre><code>def phlo_ingestion(\n    # ...\n    strict_validation: bool = True,  # Default to True for safety\n    # ...\n)\n</code></pre> <p>The decorator wrapper will use this flag to decide whether to raise a <code>dagster.Failure</code> or just yield a failed <code>AssetCheckResult</code>.</p>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#changes-in-dlt_helperspy","title":"Changes in <code>dlt_helpers.py</code>","text":"<p>Update <code>validate_with_pandera</code> to accept a <code>strict</code> flag:</p> <pre><code>def validate_with_pandera(\n    # ...\n    strict: bool = False,\n) -&gt; bool:\n    # ...\n    # If strict=True and validation fails -&gt; Raise Exception\n    # If strict=False and validation fails -&gt; Return False (log warning)\n</code></pre>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#2-remove-deprecated-code","title":"2. Remove Deprecated Code","text":"<p>Remove <code>add_phlo_timestamp</code> from <code>phlo_dlt.phlo_ingestion.dlt_helpers</code>. All ingestion paths should now use <code>inject_metadata_columns</code> which handles <code>_phlo_row_id</code> generation and other metadata consistently.</p>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#positive","title":"Positive","text":"<ul> <li>Better Data Quality: Pipelines fail by default on invalid data, preventing \"silent failures\".</li> <li>Cleaner Codebase: Removed deprecated function reduces confusion.</li> <li>User Control: Explicit control over validation strictness.</li> </ul>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#negative","title":"Negative","text":"<ul> <li>Potential Breakage: Existing pipelines that rely on the implicit \"warn-only\" behavior will now fail if they have data quality issues (unless updated to <code>strict_validation=False</code>). This is an intentional breaking change to enforce quality.</li> </ul>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#risks","title":"Risks","text":"<ul> <li>Users upgrading <code>phlo</code> might experience sudden pipeline failures if their data was already failing validation silently. We should document this in the changelog.</li> </ul>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#automated-tests","title":"Automated Tests","text":"<ol> <li> <p>Strict Validation Tests:</p> <ul> <li>Verify <code>phlo_ingestion(..., strict_validation=True)</code> raises Failure on invalid data.</li> <li>Verify <code>phlo_ingestion(..., strict_validation=False)</code> does NOT raise Failure but reports failed check.</li> <li>Verify <code>validate_with_pandera(..., strict=True)</code> raises Exception.</li> <li>Verify <code>phlo_ingestion(..., strict_validation=True)</code> raises Failure on invalid data.</li> <li>Verify <code>phlo_ingestion(..., strict_validation=False)</code> does NOT raise Failure but reports failed check.</li> <li>Verify <code>validate_with_pandera(..., strict=True)</code> raises Exception.</li> </ul> </li> <li> <p>Regression Tests:</p> <ul> <li>Ensure existing tests for <code>validate_with_pandera</code> pass (they verify datetime coercion logic).</li> </ul> </li> </ol>"},{"location":"architecture/decisions/0022-ingestion-validation-hardening/#beads","title":"Beads","text":"<ul> <li>phlo-nwk.4.3: Make ingestion validation failures actionable (configurable strictness) (complete)</li> <li>phlo-nwk.4.4: Remove deprecated add_phlo_timestamp path (complete)</li> </ul>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/","title":"23. Observatory saved queries and bookmarks","text":"<p>Date: 2025-12-18</p>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#context","title":"Context","text":"<p>Observatory currently stores user settings (connections, preferences) in localStorage, but lacks persistence for:</p> <ol> <li>Saved Queries: Users cannot save SQL queries for reuse. Each time they navigate away, the query is lost.</li> <li>Saved Views: The current table/filters/sort state is not persisted or shareable.</li> <li>Bookmarks: No way to bookmark frequently accessed tables or queries.</li> <li>Shareable URLs: URLs don't fully encode navigation state, making it hard to share specific views.</li> </ol> <p>This limits productivity for power users who frequently run the same queries or access the same tables.</p> <p>Related beads:</p> <ul> <li><code>phlo-5rf</code>: Observatory: Bookmarks + shareable URLs</li> <li><code>phlo-pyg</code>: Observatory: Saved queries &amp; views</li> </ul>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#decision","title":"Decision","text":"<p>We will implement server-side persistence for saved queries and bookmarks, with shareable URLs that encode navigation state.</p>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#1-server-side-storage","title":"1. Server-Side Storage","text":"<p>Use localStorage for MVP (consistent with existing <code>observatorySettings</code>). Add versioned schema with types:</p> <pre><code>// lib/savedQueries.ts\ninterface SavedQuery {\n  id: string; // ULID\n  name: string;\n  query: string;\n  description?: string;\n  tags?: string[];\n  branch?: string; // Optional default branch\n  createdAt: string; // ISO timestamp\n  updatedAt: string;\n}\n\ninterface SavedView {\n  id: string; // ULID\n  name: string;\n  tableRef: {\n    // Full table reference\n    catalog: string;\n    schema: string;\n    table: string;\n  };\n  columns?: string[]; // Selected columns\n  filters?: string; // WHERE clause fragment\n  sortBy?: { column: string; direction: \"asc\" | \"desc\" };\n  createdAt: string;\n  updatedAt: string;\n}\n\ninterface Bookmark {\n  id: string;\n  type: \"table\" | \"query\" | \"view\";\n  targetId: string; // Table path or SavedQuery/SavedView ID\n  label?: string; // Optional display name\n  createdAt: string;\n}\n</code></pre>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#2-shareable-urls","title":"2. Shareable URLs","text":"<p>Encode navigation state in URL search params:</p> <pre><code>/data/main/gold/fct_metrics?cols=date,value&amp;sort=date:desc&amp;q=value&gt;100\n</code></pre> <p>URL encoding:</p> Param Description Example <code>cols</code> Selected columns (comma-sep) <code>cols=id,name,value</code> <code>sort</code> Sort column:direction <code>sort=created_at:desc</code> <code>q</code> Filter expression <code>q=status='active'</code> <code>limit</code> Row limit <code>limit=50</code> <code>offset</code> Pagination offset <code>offset=100</code> <p>For saved queries, use a short ID in the path:</p> <pre><code>/data/query/$queryId\n</code></pre>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#3-new-files","title":"3. New Files","text":"File Purpose <code>lib/savedQueries.ts</code> Schema + localStorage CRUD for queries/views/bookmarks <code>hooks/useSavedQueries.ts</code> React hook for query persistence <code>hooks/useBookmarks.ts</code> React hook for bookmarks <code>components/data/SavedQueriesPanel.tsx</code> UI panel listing saved queries <code>components/data/BookmarkButton.tsx</code> Bookmark toggle button <code>routes/data/query.$queryId.tsx</code> Route for loading saved query"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#4-ui-components","title":"4. UI Components","text":"<p>SavedQueriesPanel: Collapsible sidebar section showing saved queries grouped by tags. Actions: Run, Edit, Delete, Copy SQL.</p> <p>BookmarkButton: Star icon in table header and query editor. Click to bookmark/unbookmark.</p> <p>QueryEditor enhancements:</p> <ul> <li>\"Save Query\" button opens dialog to name/tag the query</li> <li>Dropdown to load saved queries</li> </ul>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#5-url-state-sync","title":"5. URL State Sync","text":"<p>Use TanStack Router's <code>searchParams</code> to sync state:</p> <pre><code>// In route component\nconst { cols, sort, q } = Route.useSearch();\n\n// Update URL without navigation\nnavigate({ search: { cols: newCols, sort, q } });\n</code></pre>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#positive","title":"Positive","text":"<ul> <li>Users can save frequently-used queries and access them quickly</li> <li>URLs are fully shareable - copy URL to share exact view with teammates</li> <li>Bookmarks provide quick access to important tables/queries</li> <li>Consistent with existing localStorage pattern (later upgradeable to server-side)</li> </ul>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#negative","title":"Negative","text":"<ul> <li>localStorage has size limits (~5MB) - sufficient for typical usage</li> <li>No cross-device sync until server-side storage is added</li> <li>URL complexity increases</li> </ul>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#risks","title":"Risks","text":"<ul> <li>URL encoding edge cases (special characters in filters) - mitigate with proper encoding</li> <li>localStorage data loss on browser clear - document in UI</li> </ul>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#automated-tests","title":"Automated Tests","text":"<ol> <li>Unit tests for <code>savedQueries.ts</code>:</li> <li>Test CRUD operations (create, read, update, delete)</li> <li>Test schema validation</li> <li> <p>Test localStorage serialization/deserialization</p> </li> <li> <p>URL encoding tests:</p> </li> <li>Test encoding/decoding of complex filter expressions</li> <li>Test handling of special characters</li> </ol>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#manual-verification","title":"Manual Verification","text":"<ol> <li>Saved Queries Flow:</li> <li>Open Query Editor, write a query, click \"Save Query\"</li> <li>Enter name and tags, confirm save</li> <li>Refresh page, verify query appears in Saved Queries panel</li> <li> <p>Click saved query to load it, run to verify it executes</p> </li> <li> <p>Bookmarks Flow:</p> </li> <li>Navigate to a table, click bookmark star</li> <li>Verify bookmark appears in sidebar/bookmarks list</li> <li> <p>Click bookmark to navigate back to table</p> </li> <li> <p>Shareable URLs Flow:</p> </li> <li>Select columns, apply sort, add filter</li> <li>Copy URL from browser</li> <li>Open in new tab/incognito, verify same state loads</li> </ol>"},{"location":"architecture/decisions/0023-observatory-saved-queries-and-bookmarks/#beads","title":"Beads","text":"<ul> <li>phlo-5rf: Observatory: Bookmarks + shareable URLs (complete)</li> <li>phlo-pyg: Observatory: Saved queries &amp; views (complete)</li> </ul>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/","title":"24. GitHub Example: phlo_quality Reconciliation Checks","text":"<p>Date: 2025-12-19</p>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#beads","title":"Beads","text":"<ul> <li><code>phlo-2md</code>: Docs/examples: Add phlo_quality reconciliation checks (GitHub)</li> </ul>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#context","title":"Context","text":"<p>Users need working examples demonstrating how to use <code>phlo_quality</code> for warehouse-native reconciliation and aggregate consistency checks. These checks ensure data integrity across pipeline stages:</p> <ul> <li>Row count parity: Verify staging tables match expected row counts in fact tables (per partition)</li> <li>Aggregate consistency: Verify computed aggregates match source data sums</li> <li>Freshness validation: Ensure data is current for each partition</li> </ul> <p>The GitHub example project already has basic quality checks (null checks, range checks, uniqueness) but lacks reconciliation checks that validate data consistency across tables.</p>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#decision","title":"Decision","text":"<p>Add two partition-aware reconciliation checks to the GitHub example:</p>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#1-reconciliationcheck-row-count-parity","title":"1. ReconciliationCheck - Row Count Parity","text":"<p>A new <code>ReconciliationCheck</code> class that compares row counts between staging and fact tables for a given partition:</p> <pre><code>@phlo_quality(\n    table=\"gold.fct_daily_github_metrics\",\n    checks=[\n        ReconciliationCheck(\n            source_table=\"silver.stg_github_events\",\n            partition_column=\"_phlo_partition_date\",\n            check_type=\"rowcount_parity\",\n            tolerance=0.0,  # Exact match\n        ),\n    ],\n    partition_aware=True,\n)\ndef daily_metrics_reconciliation():\n    \"\"\"Verify fct_daily_github_metrics row count matches source events.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#2-aggregateconsistencycheck-sum-verification","title":"2. AggregateConsistencyCheck - Sum Verification","text":"<p>A check that validates computed aggregates match their source:</p> <pre><code>@phlo_quality(\n    table=\"gold.fct_daily_github_metrics\",\n    checks=[\n        AggregateConsistencyCheck(\n            source_table=\"silver.stg_github_events\",\n            aggregate_column=\"total_events\",\n            source_expression=\"COUNT(*)\",\n            partition_column=\"_phlo_partition_date\",\n            group_by=[\"activity_date\"],\n            tolerance=0.0,\n        ),\n    ],\n    partition_aware=True,\n)\ndef daily_metrics_aggregate_consistency():\n    \"\"\"Verify total_events count matches source row count.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#implementation","title":"Implementation","text":""},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#new-files","title":"New Files","text":""},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#new-packagesphlo-qualitysrcphlo_qualityreconciliationpy","title":"[NEW] <code>packages/phlo-quality/src/phlo_quality/reconciliation.py</code>","text":"<p>Contains:</p> <ul> <li><code>ReconciliationCheck</code>: Compare row counts or other metrics across tables</li> <li><code>AggregateConsistencyCheck</code>: Verify aggregates match source computations</li> </ul>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#modify-phlo-examplesgithubworkflowsqualitygithubpy","title":"[MODIFY] <code>phlo-examples/github/workflows/quality/github.py</code>","text":"<p>Add reconciliation check examples using the new check classes.</p>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#modify-phlo-examplesgithubworkflowsquality__init__py","title":"[MODIFY] <code>phlo-examples/github/workflows/quality/__init__.py</code>","text":"<p>Optional: keep module exports for organization; no registration step is required.</p>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#automated-tests","title":"Automated Tests","text":"<pre><code># Run quality module tests\npytest tests/test_quality*.py -v\n\n# Run strict validation tests (related)\npytest packages/phlo-dlt/tests/test_strict_validation.py -v\n</code></pre>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#manual-verification","title":"Manual Verification","text":"<ol> <li>Materialize the GitHub example pipeline</li> <li>Check Dagster UI for asset checks appearing on <code>fct_daily_github_metrics</code></li> <li>Verify the check metadata includes partition key and row count comparison</li> </ol>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#consequences","title":"Consequences","text":"<ul> <li>GitHub example becomes a comprehensive reference for reconciliation patterns</li> <li>Users have copy-paste examples for common warehouse validation scenarios</li> <li>New check classes can be reused across other projects</li> </ul>"},{"location":"architecture/decisions/0024-github-quality-reconciliation-checks/#documentation","title":"Documentation","text":"<p>Add a brief debugging workflow section to the GitHub example README showing:</p> <ol> <li>How to identify a failing reconciliation check in Dagster</li> <li>How to use the SQL query in the check metadata to debug</li> <li>Common causes (late-arriving data, transform logic bugs)</li> </ol>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/","title":"25. Observatory responsive layout","text":"<p>Date: 2025-12-19</p>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#context","title":"Context","text":"<p>Observatory currently uses a fixed desktop layout with shadcn's <code>SidebarProvider</code> which already includes mobile infrastructure (Sheet overlay, <code>useIsMobile()</code> hook). However, the app is not fully optimized for smaller screens:</p> <ol> <li>Tablet breakpoints: The md (768px) breakpoint jumps directly from mobile to desktop without tablet-optimized layouts.</li> <li>Touch targets: Some interactive elements (table cells, buttons) are too small for comfortable touch interaction.</li> <li>Content overflow: Data tables and code blocks can overflow horizontally on narrow viewports.</li> <li>Header density: The header wastes space on mobile with redundant labels.</li> </ol> <p>Related bead: <code>phlo-wq1</code></p>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#decision","title":"Decision","text":"<p>We will enhance the existing responsive infrastructure with targeted improvements for tablets (md-lg) and mobile (sm) breakpoints.</p>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#1-breakpoint-strategy","title":"1. Breakpoint Strategy","text":"<p>Use Tailwind's default breakpoints consistently:</p> Breakpoint Width Layout <code>sm</code> &lt; 640px Mobile: Sheet sidebar, compact header <code>md</code> 640-768px Tablet: Icon-collapsed sidebar by default <code>lg</code> &gt; 1024px Desktop: Full sidebar"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#2-changes-by-component","title":"2. Changes by Component","text":""},{"location":"architecture/decisions/0025-observatory-responsive-layout/#root-layout-__roottsx","title":"Root Layout (<code>__root.tsx</code>)","text":"<ul> <li>Reduce header padding on mobile: <code>px-4 md:px-4</code> \u2192 <code>px-2 sm:px-4</code></li> <li>Hide \"Search\" label on mobile, show only icon + \u2318K shortcut</li> <li>Make sidebar default to collapsed state on tablet (md)</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#appsidebar-appsidebartsx","title":"AppSidebar (<code>AppSidebar.tsx</code>)","text":"<ul> <li>Already uses <code>collapsible=\"icon\"</code> - no changes needed</li> <li>Sidebar header already compact</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#data-tables-datapreviewtsx-queryresultstsx","title":"Data Tables (<code>DataPreview.tsx</code>, <code>QueryResults.tsx</code>)","text":"<ul> <li>Add horizontal scroll wrapper with <code>-webkit-overflow-scrolling: touch</code></li> <li>Increase touch targets: min cell height 44px on mobile</li> <li>Use <code>whitespace-nowrap</code> on table headers to prevent wrapping</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#table-browser-tablebrowsertsx","title":"Table Browser (<code>TableBrowser.tsx</code>)","text":"<ul> <li>Already virtualized - ensure touch scrolling works</li> <li>Increase row height on mobile: <code>h-8</code> \u2192 <code>h-10 sm:h-8</code></li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#query-editor-queryeditortsx","title":"Query Editor (<code>QueryEditor.tsx</code>)","text":"<ul> <li>Make editor height responsive: <code>min-h-[200px] md:min-h-[300px]</code></li> <li>Stack action buttons vertically on mobile</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#3-css-additions","title":"3. CSS Additions","text":"<p>Add to <code>styles.css</code>:</p> <pre><code>/* Touch-friendly scrolling */\n@layer base {\n  .touch-scroll {\n    -webkit-overflow-scrolling: touch;\n    overscroll-behavior: contain;\n  }\n}\n\n/* Mobile-first table density */\n@media (max-width: 640px) {\n  :root {\n    --table-cell-py: 0.625rem; /* 10px - larger touch targets */\n  }\n}\n</code></pre>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#4-modified-files","title":"4. Modified Files","text":"File Changes <code>routes/__root.tsx</code> Responsive header, tablet sidebar default <code>components/data/DataPreview.tsx</code> Scroll wrapper, responsive cell sizing <code>components/data/QueryResults.tsx</code> Scroll wrapper, touch targets <code>components/data/QueryEditor.tsx</code> Responsive height, button stacking <code>components/data/TableBrowser.tsx</code> Responsive row height <code>styles.css</code> Touch scroll utilities, mobile density"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0025-observatory-responsive-layout/#positive","title":"Positive","text":"<ul> <li>Observatory usable on tablets and phones</li> <li>Touch-friendly controls improve accessibility</li> <li>Minimal changes to existing architecture (leverages shadcn mobile infrastructure)</li> <li>No breaking changes to desktop experience</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#negative","title":"Negative","text":"<ul> <li>Slightly increased CSS complexity</li> <li>Some features may be harder to use on very small screens (&lt; 375px)</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#risks","title":"Risks","text":"<ul> <li>Testing on real devices needed (simulator may not catch touch issues)</li> </ul>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0025-observatory-responsive-layout/#browser-testing","title":"Browser Testing","text":"<ol> <li>Open Observatory at http://localhost:3001</li> <li>Use browser DevTools (F12) \u2192 Toggle device toolbar (Ctrl+Shift+M)</li> <li>Test at these viewports:</li> <li>iPhone SE (375\u00d7667)</li> <li>iPad Mini (768\u00d71024)</li> <li>Desktop (1920\u00d71080)</li> </ol>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#manual-verification-checklist","title":"Manual Verification Checklist","text":"<ol> <li>Mobile (375px):</li> <li>[ ] Sidebar opens as sheet overlay when hamburger tapped</li> <li>[ ] Header shows search icon only (no \"Search\" text)</li> <li>[ ] Data tables scroll horizontally without page scroll</li> <li> <p>[ ] Table rows are tappable (44px+ height)</p> </li> <li> <p>Tablet (768px):</p> </li> <li>[ ] Sidebar starts collapsed (icon-only)</li> <li>[ ] Can expand sidebar with trigger button</li> <li> <p>[ ] Query editor has reasonable height</p> </li> <li> <p>Desktop (1024px+):</p> </li> <li>[ ] Full sidebar visible</li> <li>[ ] No visual regressions from current layout</li> </ol>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#typescriptlint","title":"TypeScript/Lint","text":"<pre><code>cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm run lint &amp;&amp; npm run typecheck\n</code></pre>"},{"location":"architecture/decisions/0025-observatory-responsive-layout/#beads","title":"Beads","text":"<ul> <li>phlo-wq1: Observatory: Responsive/mobile layout (complete)</li> </ul>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/","title":"26. Observatory authentication and real-time updates","text":"<p>Date: 2025-12-20</p>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#context","title":"Context","text":"<p>Observatory needs two production-readiness features:</p> <ol> <li> <p>Authentication (phlo-h2c): Protect Observatory endpoints with environment-level authentication. Currently all server functions are unauthenticated.</p> </li> <li> <p>Real-time Updates (phlo-cil): Show live status updates for materializations and quality checks without manual page refresh.</p> </li> </ol> <p>Both are from epic <code>phlo-6mz: Observatory Production Readiness</code>.</p>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#decision","title":"Decision","text":""},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#1-authentication","title":"1. Authentication","text":"<p>Use a simple token-based approach for environment-level protection.</p>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#environment-configuration","title":"Environment Configuration","text":"<pre><code># .phlo/.env.local\nOBSERVATORY_AUTH_ENABLED=true\nOBSERVATORY_AUTH_TOKEN=your-secret-token-here\n</code></pre>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#auth-middleware","title":"Auth Middleware","text":"<p>Create <code>auth.server.ts</code> with a middleware function that all server functions can use:</p> <pre><code>export function requireAuth(\n  request: Request,\n): void | { error: string; status: 401 } {\n  if (process.env.OBSERVATORY_AUTH_ENABLED !== \"true\") return;\n\n  const token = request.headers.get(\"X-Observatory-Token\");\n  if (token !== process.env.OBSERVATORY_AUTH_TOKEN) {\n    return { error: \"Unauthorized\", status: 401 };\n  }\n}\n</code></pre>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#client-token-storage","title":"Client Token Storage","text":"<p>Add auth token to settings schema and include in all server function calls:</p> <pre><code>// observatorySettings.ts additions\nexport const observatorySettingsSchema = z.object({\n  // ... existing fields\n  auth: z\n    .object({\n      token: z.string().optional(),\n    })\n    .optional(),\n});\n</code></pre>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#2-real-time-updates","title":"2. Real-time Updates","text":"<p>Use polling with configurable intervals. TanStack Query's <code>refetchInterval</code> provides this natively.</p>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#settings-extension","title":"Settings Extension","text":"<pre><code>// Add to observatorySettings schema\nrealtime: z.object({\n  enabled: z.boolean(),\n  intervalMs: z.number().int().min(1000).max(60000), // 1s - 60s\n}).optional();\n</code></pre> <p>Default: <code>{ enabled: true, intervalMs: 5000 }</code></p>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#polling-hook","title":"Polling Hook","text":"<p>Create <code>useRealtimePolling.ts</code>:</p> <pre><code>export function useRealtimePolling&lt;T&gt;(\n  queryFn: () =&gt; Promise&lt;T&gt;,\n  queryKey: string[],\n  options?: { enabled?: boolean },\n) {\n  const { settings } = useObservatorySettings();\n  const intervalMs = settings.realtime?.intervalMs ?? 5000;\n  const enabled = options?.enabled ?? settings.realtime?.enabled ?? true;\n\n  return useQuery({\n    queryKey,\n    queryFn,\n    refetchInterval: enabled ? intervalMs : false,\n    refetchIntervalInBackground: false,\n  });\n}\n</code></pre>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#use-cases","title":"Use Cases","text":"<ol> <li>Dashboard health metrics (<code>routes/index.tsx</code>) - show live health status</li> <li>Quality checks (<code>routes/quality/index.tsx</code>) - live check results</li> <li>Asset materialization status - show in-progress indicators</li> </ol>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#3-modified-files","title":"3. Modified Files","text":"File Changes <code>server/auth.server.ts</code> [NEW] Auth middleware <code>lib/observatorySettings.ts</code> Add auth and realtime settings <code>hooks/useRealtimePolling.ts</code> [NEW] Polling hook wrapper <code>routes/index.tsx</code> Add polling for health metrics <code>routes/quality/index.tsx</code> Add polling for quality checks <code>routes/settings.tsx</code> Add auth token and polling interval UI <code>server/*.server.ts</code> Add optional auth check to handlers"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#positive","title":"Positive","text":"<ul> <li>Observatory protected in production environments</li> <li>Live updates improve observability experience</li> <li>Minimal changes - uses existing patterns</li> <li>Backward compatible (auth disabled by default)</li> </ul>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#negative","title":"Negative","text":"<ul> <li>Token must be configured manually per environment</li> <li>Polling adds load to backend services</li> <li>No SSE/WebSocket support (future enhancement)</li> </ul>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#risks","title":"Risks","text":"<ul> <li>Token in localStorage is visible in browser devtools (acceptable for internal tool)</li> </ul>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#auth-testing","title":"Auth Testing","text":"<ol> <li>Set <code>OBSERVATORY_AUTH_ENABLED=true</code> and <code>OBSERVATORY_AUTH_TOKEN=test123</code></li> <li>Open Observatory without token - should show auth error</li> <li>Add token to Settings - should load normally</li> <li>Test each server function returns 401 without valid token</li> </ol>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#polling-testing","title":"Polling Testing","text":"<ol> <li>Open Dashboard and Quality pages</li> <li>Verify \"last updated\" timestamps change automatically</li> <li>Adjust polling interval in Settings - verify change takes effect</li> <li>Disable polling - verify no automatic updates</li> </ol>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#typescriptlint","title":"TypeScript/Lint","text":"<pre><code>cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm run check &amp;&amp; npm test\n</code></pre>"},{"location":"architecture/decisions/0026-observatory-auth-and-realtime/#beads","title":"Beads","text":"<ul> <li>phlo-h2c: Observatory: Authentication (environment-level) (complete)</li> <li>phlo-cil: Observatory: Real-time updates (polling/SSE) (complete)</li> </ul>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/","title":"27. Observatory performance monitoring and budgets","text":"<p>Date: 2025-12-20</p>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#context","title":"Context","text":"<p>Observatory needs performance observability to ensure good user experience and diagnose issues. Currently there is no:</p> <ul> <li>Structured logging for server function calls</li> <li>API response time tracking</li> <li>Query execution timing</li> <li>Error tracking and reporting</li> </ul> <p>This implements <code>phlo-954: Observatory: Performance monitoring and budgets</code>.</p>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#decision","title":"Decision","text":""},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#1-use-pino-for-structured-logging","title":"1. Use Pino for Structured Logging","text":"<p>Pino is the standard Node.js structured logging library:</p> <ul> <li>5-10x faster than Winston</li> <li>JSON-first output by default</li> <li>First-class TypeScript support</li> <li>Child loggers for adding context</li> </ul>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#installation","title":"Installation","text":"<pre><code>cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm install pino\n</code></pre>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#logger-setup","title":"Logger Setup","text":"<p>Create <code>server/logger.server.ts</code>:</p> <pre><code>import pino from \"pino\";\n\nexport const logger = pino({\n  level: process.env.LOG_LEVEL ?? \"info\",\n  formatters: {\n    level: (label) =&gt; ({ level: label }),\n  },\n});\n\n// Create child logger with function context\nexport function fnLogger(fn: string, meta?: Record&lt;string, unknown&gt;) {\n  return logger.child({ fn, ...meta });\n}\n\n// Timing wrapper for async operations\nexport async function withTiming&lt;T&gt;(\n  fn: string,\n  operation: () =&gt; Promise&lt;T&gt;,\n  meta?: Record&lt;string, unknown&gt;,\n): Promise&lt;T&gt; {\n  const log = fnLogger(fn, meta);\n  const start = performance.now();\n\n  try {\n    const result = await operation();\n    const durationMs = Math.round(performance.now() - start);\n    log.info({ durationMs }, \"completed\");\n    return result;\n  } catch (error) {\n    const durationMs = Math.round(performance.now() - start);\n    log.error({ durationMs, err: error }, \"failed\");\n    throw error;\n  }\n}\n</code></pre>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#example-usage","title":"Example Usage","text":"<pre><code>// In trino.server.ts\nimport { withTiming } from \"./logger.server\";\n\nexport const previewData = createServerFn()\n  .middleware([authMiddleware])\n  .handler(async ({ data }) =&gt; {\n    return withTiming(\n      \"previewData\",\n      () =&gt; {\n        // existing logic\n      },\n      { table: data.table, branch: data.branch },\n    );\n  });\n</code></pre>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#2-server-function-instrumentation","title":"2. Server Function Instrumentation","text":"<p>Wrap key server functions with timing:</p> Module Functions Why <code>trino.server.ts</code> <code>previewData</code>, <code>executeQuery</code>, <code>profileColumn</code> Query performance <code>dagster.server.ts</code> <code>getHealthMetrics</code>, <code>listAssets</code>, <code>getAssetDetails</code> Dagster API latency <code>iceberg.server.ts</code> <code>listIcebergTables</code>, <code>getTableSchema</code> Metadata loading <code>nessie.server.ts</code> <code>getBranches</code>, <code>getBranchCommits</code> Branch operations <code>quality.server.ts</code> <code>getQualityChecks</code> Check results loading"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#3-performance-budgets","title":"3. Performance Budgets","text":"<p>Log warnings when operations exceed acceptable thresholds:</p> Operation Budget Rationale Data preview (100 rows) 2000ms User expects quick preview Query execution 30000ms Configurable timeout Table list 1000ms Sidebar should load fast Asset list 2000ms Hub page load Health metrics 1500ms Dashboard responsiveness <pre><code>export function withTimingBudget&lt;T&gt;(\n  fn: string,\n  budgetMs: number,\n  operation: () =&gt; Promise&lt;T&gt;,\n  meta?: Record&lt;string, unknown&gt;,\n): Promise&lt;T&gt; {\n  const log = fnLogger(fn, meta);\n  const start = performance.now();\n\n  return operation().then((result) =&gt; {\n    const durationMs = Math.round(performance.now() - start);\n    if (durationMs &gt; budgetMs) {\n      log.warn({ durationMs, budgetMs }, \"exceeded budget\");\n    } else {\n      log.info({ durationMs }, \"completed\");\n    }\n    return result;\n  });\n}\n</code></pre>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#4-log-output-examples","title":"4. Log Output Examples","text":"<pre><code>{\"level\":\"info\",\"time\":1734740000000,\"fn\":\"previewData\",\"table\":\"gold.fct_events\",\"durationMs\":234,\"msg\":\"completed\"}\n{\"level\":\"warn\",\"time\":1734740001000,\"fn\":\"listAssets\",\"durationMs\":2500,\"budgetMs\":2000,\"msg\":\"exceeded budget\"}\n{\"level\":\"error\",\"time\":1734740002000,\"fn\":\"executeQuery\",\"durationMs\":5234,\"err\":\"Trino connection refused\",\"msg\":\"failed\"}\n</code></pre>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#5-implementation-summary","title":"5. Implementation Summary","text":"File Changes <code>package.json</code> Add <code>pino</code> dependency <code>server/logger.server.ts</code> [NEW] Pino logger setup with timing helpers <code>server/trino.server.ts</code> Wrap key functions with <code>withTiming</code> <code>server/dagster.server.ts</code> Wrap key functions with <code>withTiming</code> <code>server/iceberg.server.ts</code> Wrap key functions with <code>withTiming</code> <code>server/nessie.server.ts</code> Wrap key functions with <code>withTiming</code> <code>server/quality.server.ts</code> Wrap key functions with <code>withTiming</code>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#positive","title":"Positive","text":"<ul> <li>Industry-standard logging library (Pino)</li> <li>Structured JSON logs enable aggregation and querying</li> <li>Timing data helps identify slow queries</li> <li>Child loggers add context without repetition</li> <li>Very low overhead</li> </ul>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#negative","title":"Negative","text":"<ul> <li>Adds one dependency (pino)</li> <li>Log volume increases</li> <li>No persistent metrics storage (logs only)</li> </ul>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Add <code>pino-pretty</code> for dev-friendly output</li> <li>Aggregated metrics endpoint for Observatory UI</li> <li>Integration with external monitoring (Datadog, etc.)</li> </ul>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#automated-tests","title":"Automated Tests","text":"<ol> <li>Existing tests still pass:    <code>bash    cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm test</code></li> </ol>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#manual-verification","title":"Manual Verification","text":"<ol> <li>Check structured logs appear:</li> <li>Start Observatory: <code>cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm run dev -- --port 3001</code></li> <li>Open browser to http://localhost:3001</li> <li>Navigate to Data Explorer, select a table</li> <li> <p>Check terminal output for JSON logs with <code>fn</code>, <code>durationMs</code> fields</p> </li> <li> <p>TypeScript/Lint checks:    <code>bash    cd packages/phlo-observatory/src/phlo_observatory &amp;&amp; npm run check</code></p> </li> </ol>"},{"location":"architecture/decisions/0027-observatory-performance-monitoring/#beads","title":"Beads","text":"<ul> <li>phlo-954: Observatory: Performance monitoring and budgets (in_progress)</li> </ul>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/","title":"28. Unified Logging and Observability","text":"<p>Date: 2025-12-21</p>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#context","title":"Context","text":"<p>Phlo consists of multiple services:</p> <ul> <li>Python - Dagster definitions, DLT ingestion, dbt transforms</li> <li>TypeScript - Observatory UI and server functions</li> <li>Infrastructure - Trino, Nessie, Postgres (Docker containers)</li> </ul> <p>Currently:</p> <ul> <li>Python uses Dagster's <code>context.log.*()</code> with structured <code>extra={}</code> fields</li> <li>TypeScript (Observatory) now uses Pino for structured JSON logging (ADR-0027)</li> <li>Infrastructure logs are native container output</li> <li>All logs are collected by Grafana Alloy to Loki with 30-day retention</li> </ul> <p>Goal: Enable Observatory to display logs from all services, correlated to assets, runs, and quality checks.</p>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#decision","title":"Decision","text":""},{"location":"architecture/decisions/0028-unified-logging-and-observability/#1-correlation-fields-standard","title":"1. Correlation Fields Standard","text":"<p>All services must include these fields in logs for correlation:</p> Field Description Example <code>run_id</code> Dagster run ID <code>\"abc123-def...\"</code> <code>asset_key</code> Dagster asset key <code>\"dlt_user_events\"</code> <code>job_name</code> Dagster job name <code>\"github_pipeline\"</code> <code>partition_key</code> Partition date <code>\"2025-12-20\"</code> <code>check_name</code> Quality check name <code>\"pandera/contract\"</code>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#2-python-logging-dagster","title":"2. Python Logging (Dagster)","text":"<p>Continue using <code>context.log.*()</code> with consistent correlation fields:</p> <pre><code># In asset/op code\ncontext.log.info(\n    \"Processing partition\",\n    extra={\n        \"asset_key\": context.asset_key.to_string(),\n        \"partition_key\": context.partition_key,\n        \"run_id\": context.run_id,\n    }\n)\n</code></pre>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#3-typescript-logging-observatory","title":"3. TypeScript Logging (Observatory)","text":"<p>Use <code>withTiming</code> wrapper (ADR-0027):</p> <pre><code>return withTiming(\n  \"queryTrino\",\n  async () =&gt; {\n    // logic\n  },\n  { catalog, run_id },\n);\n</code></pre>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#4-loki-integration-in-observatory","title":"4. Loki Integration in Observatory","text":"<p>Create <code>loki.server.ts</code> to query logs:</p> <pre><code>export async function queryLogs({\n  runId?: string,\n  assetKey?: string,\n  job?: string,\n  level?: 'info' | 'warn' | 'error',\n  start: Date,\n  end: Date,\n}): Promise&lt;LogEntry[]&gt; {\n  const query = buildLogQuery({ runId, assetKey, job, level })\n  return await lokiClient.queryRange(query, start, end)\n}\n</code></pre>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#5-log-viewer-ui","title":"5. Log Viewer UI","text":"<p>Add \"Logs\" tab to asset detail view showing:</p> <ul> <li>Logs filtered by <code>asset_key</code></li> <li>Time-scoped to materialization window</li> <li>Level filtering (info/warn/error)</li> <li>Link to Grafana for advanced queries</li> </ul>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#6-architecture","title":"6. Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Dagster       \u2502    \u2502   Observatory   \u2502    \u2502  Infrastructure \u2502\n\u2502   (Python)      \u2502    \u2502  (TypeScript)   \u2502    \u2502  (Trino, etc)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 JSON logs            \u2502 Pino                 \u2502 native\n         \u25bc                      \u25bc                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Grafana Alloy                             \u2502\n\u2502          (Docker log collection + JSON parsing)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502      Loki       \u2502\n                   \u2502  (Log storage)  \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502 LogQL queries\n                            \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502   Observatory   \u2502\n                   \u2502   (Log viewer)  \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#7-implementation-phases","title":"7. Implementation Phases","text":"Phase Bead Description 1 phlo-954 \u2705 Observatory Pino logging (complete) 2 phlo-27f \u2705 Loki querying server functions 3 phlo-rti \u2705 Log Viewer UI 4 phlo-6jk \u2705 Python correlation field standardization"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0028-unified-logging-and-observability/#positive","title":"Positive","text":"<ul> <li>Unified log viewing in Observatory</li> <li>Logs correlated to assets, runs, quality checks</li> <li>No external SaaS dependency (uses existing Loki/Grafana)</li> <li>Debug failures by viewing related logs inline</li> </ul>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#negative","title":"Negative","text":"<ul> <li>Adds Loki querying complexity to Observatory</li> <li>Log volume increases with structured fields</li> <li>Requires discipline to include correlation fields</li> </ul>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#trade-offs","title":"Trade-offs","text":"<ul> <li>Observatory UI vs Grafana for log viewing</li> <li>Observatory: integrated, asset-aware</li> <li>Grafana: advanced queries, dashboards</li> <li>Decision: Basic viewing in Observatory, link to Grafana for power users</li> </ul>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0028-unified-logging-and-observability/#automated-tests","title":"Automated Tests","text":"<ul> <li>Unit tests for <code>loki.server.ts</code></li> <li>Integration test: emit log, query via Loki API</li> </ul>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#manual-verification","title":"Manual Verification","text":"<ol> <li>Materialize an asset</li> <li>View asset in Observatory</li> <li>Click \"Logs\" tab</li> <li>See related logs with timestamps and levels</li> </ol>"},{"location":"architecture/decisions/0028-unified-logging-and-observability/#beads","title":"Beads","text":"<ul> <li>phlo-8d2: Epic: Unified Logging and Observability</li> <li>phlo-954: Observatory structured logging \u2705 (closed)</li> <li>phlo-27f: Loki log querying \u2705 (closed)</li> <li>phlo-rti: Log Viewer UI \u2705 (closed)</li> <li>phlo-6jk: Python correlation standardization \u2705 (closed)</li> </ul>"},{"location":"architecture/decisions/0029-cli-services-enhancements/","title":"29. CLI Services Enhancements: Restart Command and Profile Flag Fix","text":"<p>Date: 2025-12-21</p>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#beads","title":"Beads","text":"<ul> <li><code>phlo-6zp</code>: CLI: Add 'phlo services restart' command to simplify stop/start cycle</li> <li><code>phlo-8z5</code>: CLI: '--profile' flag should only affect profile services, not restart all running services</li> </ul>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#context","title":"Context","text":"<p>Two related CLI improvements are needed for the <code>phlo services</code> command:</p> <ol> <li> <p>Missing restart command: Users need a convenient way to restart services without manually running <code>stop</code> followed by <code>start</code>. This is particularly useful during development when configuration changes require a full service restart.</p> </li> <li> <p>Profile flag behavior bug: When running <code>phlo services start --profile observability</code>, the current implementation restarts all services (including core services like postgres, dagster, etc.) instead of just starting the observability profile services. The expected behavior is that <code>--profile</code> should only affect the profile-specific services without restarting already-running core services.</p> </li> </ol>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#decision","title":"Decision","text":""},{"location":"architecture/decisions/0029-cli-services-enhancements/#1-add-restart-command","title":"1. Add <code>restart</code> Command","text":"<p>Add a new <code>restart</code> subcommand that combines <code>stop</code> and <code>start</code> in a single operation:</p> <pre><code>phlo services restart                          # Restart all services\nphlo services restart --profile observability  # Restart profile services\nphlo services restart --service postgres       # Restart specific service\nphlo services restart --build                  # Rebuild before starting\n</code></pre> <p>The command will support all options common to both <code>stop</code> and <code>start</code>:</p> <ul> <li><code>--profile</code>: Target specific profile services</li> <li><code>--service</code>: Target specific service(s)</li> <li><code>--build</code>: Rebuild images before starting</li> <li><code>--dev</code>: Development mode mount</li> </ul>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#2-fix-profile-flag-behavior","title":"2. Fix <code>--profile</code> Flag Behavior","text":"<p>When <code>--profile</code> is specified without <code>--service</code>, the <code>start</code> command should only bring up the profile services, not restart all services. The fix involves:</p> <ol> <li>When <code>--profile</code> is specified, extract the list of services belonging to that profile</li> <li>Pass those services explicitly to <code>docker compose up</code> to only affect those containers</li> </ol>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#proposed-changes","title":"Proposed Changes","text":""},{"location":"architecture/decisions/0029-cli-services-enhancements/#core-cli","title":"Core CLI","text":""},{"location":"architecture/decisions/0029-cli-services-enhancements/#modifyfileusersgarethpricedeveloperphlosrcphlocliservicespy","title":"[MODIFY][services.py](file:///Users/garethprice/Developer/phlo/src/phlo/cli/services.py)","text":"<ol> <li>Add new <code>restart</code> command (~50-70 lines):</li> <li>Accept <code>--profile</code>, <code>--service</code>, <code>--build</code>, <code>--dev</code> options</li> <li>Execute <code>stop</code> logic followed by <code>start</code> logic</li> <li> <p>Share common code with existing commands</p> </li> <li> <p>Fix <code>start</code> command <code>--profile</code> behavior:</p> </li> <li>Add helper function <code>get_profile_services(profile_names)</code> to resolve profile names to service names</li> <li>When <code>--profile</code> is specified without <code>--service</code>, automatically populate <code>services_list</code> with profile services</li> <li> <p>Use <code>docker compose up &lt;services&gt;</code> instead of <code>docker compose up</code> to target specific services</p> </li> <li> <p>Fix <code>stop</code> command <code>--profile</code> behavior (same pattern):</p> </li> <li>When <code>--profile</code> is specified without <code>--service</code>, target only profile services</li> </ol>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#tests","title":"Tests","text":""},{"location":"architecture/decisions/0029-cli-services-enhancements/#modifyfileusersgarethpricedeveloperphloteststest_cli_servicespy","title":"[MODIFY][test_cli_services.py](file:///Users/garethprice/Developer/phlo/tests/test_cli_services.py)","text":"<p>Add tests for:</p> <ul> <li><code>get_profile_services()</code> helper function</li> <li>Verify profile flag produces correct service list</li> </ul>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#verification-plan","title":"Verification Plan","text":""},{"location":"architecture/decisions/0029-cli-services-enhancements/#automated-tests","title":"Automated Tests","text":"<pre><code># Run the CLI services tests\npytest tests/test_cli_services.py -v\n\n# Run full test suite to ensure no regressions\npytest tests/ -v --ignore=tests/integration\n</code></pre>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#manual-verification","title":"Manual Verification","text":"<ol> <li>Restart command:</li> </ol> <p><code>bash    cd phlo-examples/github    phlo services start    phlo services restart --service postgres  # Should restart only postgres    phlo services status                       # Verify postgres restarted</code></p> <ol> <li>Profile flag fix:    <code>bash    phlo services start                        # Start core services    phlo services start --profile observability # Should only start prometheus/grafana/loki/alloy    docker ps                                   # Verify core services weren't restarted</code></li> </ol>"},{"location":"architecture/decisions/0029-cli-services-enhancements/#consequences","title":"Consequences","text":"<ul> <li>Users get a convenient single command for restarting services</li> <li>Profile flag behaves as expected, only affecting profile-specific services</li> <li>No breaking changes to existing commands</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/","title":"ADR 0030: Unified Plugin System with Registry","text":""},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#status","title":"Status","text":"<p>Accepted - Implemented in epic <code>phlo-930</code></p>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#context","title":"Context","text":"<p>Phlo's plugin system originally supported three Python-based plugin types (source connectors, quality checks, transformations). Docker-based services (postgres, dagster, minio, nessie, trino) were managed separately via <code>service.yaml</code> files in <code>services/core/</code>.</p> <p>This created:</p> <ul> <li>Inconsistent extension models: Python plugins vs YAML-defined services</li> <li>No discoverability: Users couldn't easily find available plugins</li> <li>Duplicated definitions: Service configs existed in both <code>services/core/</code> and required manual maintenance</li> <li>No centralized registry: Plugin metadata scattered across packages</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#decision","title":"Decision","text":"<p>Implement a unified plugin system that:</p> <ol> <li>Adds <code>ServicePlugin</code> type - Docker-based services distributed as Python packages</li> <li>Creates a plugin registry - JSON catalog of available plugins (hosted + bundled fallback)</li> <li>Enhances CLI - <code>phlo plugin search/install/update</code> commands</li> <li>Packages core services - phlo-postgres, phlo-dagster, phlo-minio, phlo-nessie, phlo-trino</li> <li>Bundles core plugins - phlo-core-plugins with quality checks and REST API source</li> <li>Integrates Observatory - Plugin management UI in the Hub</li> </ol>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#implementation","title":"Implementation","text":""},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#phase-1-plugin-type-extension","title":"Phase 1: Plugin Type Extension","text":"<ul> <li><code>ServicePlugin</code> base class in <code>src/phlo/plugins/base.py</code></li> <li><code>phlo.plugins.services</code> entry point group</li> <li><code>PluginRegistry</code> extended with service methods</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#phase-2-plugin-registry","title":"Phase 2: Plugin Registry","text":"<ul> <li><code>registry/plugins.json</code> - Authoritative plugin catalog</li> <li><code>registry/schema/v1.json</code> - JSON Schema validation</li> <li><code>src/phlo/plugins/registry_client.py</code> - Fetch/cache/search logic</li> <li><code>src/phlo/plugins/registry_data.json</code> - Bundled fallback for offline use</li> <li>CLI commands: <code>search</code>, <code>install</code>, <code>update</code> with <code>--json</code> output</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#phase-3-core-plugins-package","title":"Phase 3: Core Plugins Package","text":"<ul> <li><code>packages/phlo-core-plugins/</code> containing:</li> <li>Quality checks: null_check, uniqueness_check, freshness_check, schema_check</li> <li>Sources: rest_api</li> <li>Added as dependency in main <code>pyproject.toml</code></li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#phase-4-service-plugin-packages","title":"Phase 4: Service Plugin Packages","text":"<ul> <li><code>packages/phlo-postgres/</code></li> <li><code>packages/phlo-dagster/</code> (includes dagster-daemon)</li> <li><code>packages/phlo-minio/</code> (includes minio-setup)</li> <li><code>packages/phlo-nessie/</code></li> <li><code>packages/phlo-trino/</code></li> </ul> <p>Each package bundles its <code>service.yaml</code> and uses <code>importlib.resources</code> to load definitions.</p>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#phase-5-cleanup","title":"Phase 5: Cleanup","text":"<ul> <li>Removed <code>services/core/</code> directory (12 files, 402 lines)</li> <li><code>ServiceDiscovery</code> now loads services exclusively from plugins</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#phase-6-observatory-integration","title":"Phase 6: Observatory Integration","text":"<ul> <li><code>packages/phlo-observatory/src/phlo_observatory/src/server/plugins.server.ts</code> - Server functions</li> <li><code>packages/phlo-observatory/src/phlo_observatory/src/routes/hub/plugins.tsx</code> - Plugin management page</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#positive","title":"Positive","text":"<ul> <li>Unified extension model: Services and data plugins share the same discovery mechanism</li> <li>Discoverability: Registry enables <code>phlo plugin search</code> for finding plugins</li> <li>Distribution: Services installable via pip like any other plugin</li> <li>Reduced maintenance: Single source of truth for service definitions</li> <li>Offline support: Bundled registry fallback when remote unavailable</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#negative","title":"Negative","text":"<ul> <li>More packages to maintain: 6 new packages under <code>packages/</code></li> <li>Build complexity: Workspace management with uv</li> </ul>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#verification","title":"Verification","text":"<pre><code># Tests pass\nuv run pytest tests/test_plugin_system.py tests/test_plugin_registry.py tests/test_cli_plugin.py -v\n\n# CLI works\nuv run phlo plugin list --json\nuv run phlo plugin search postgres\n\n# Service discovery works without services/core/\nuv run pytest tests/test_services_discovery.py -v\n</code></pre>"},{"location":"architecture/decisions/0030-unified-plugin-system-with-registry/#related","title":"Related","text":"<ul> <li>Epic: <code>phlo-930</code></li> <li>Beads: <code>phlo-930.1</code> through <code>phlo-930.15</code></li> </ul>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/","title":"ADR 0031: Observatory as Core with Plugin DX Improvements","text":""},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#status","title":"Status","text":"<p>Accepted - Implementation in progress</p>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#context","title":"Context","text":"<p>The current Phlo architecture has Observatory as an optional plugin package (<code>phlo-observatory</code>), installed separately via <code>pip install phlo-observatory</code>. This creates several issues:</p>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#problems-with-observatory-as-plugin","title":"Problems with Observatory as Plugin","text":"<ol> <li>Awkward DX - Observatory is \"the face of Phlo\" but users must install it separately</li> <li>No phlo access - As a Node.js TanStack Start app, Observatory cannot access phlo internals (installed plugins, configs, service status)</li> <li>Mismatch with purpose - Plugins should be swappable alternatives (dagster vs airflow); Observatory has no alternative</li> <li>Version coupling - Observatory features depend on phlo-api endpoints that don't exist yet</li> </ol>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#additional-dx-issues","title":"Additional DX Issues","text":"<ol> <li>No user overrides - Users cannot customize installed service ports/environment in <code>phlo.yaml</code></li> <li>No enable/disable - Cannot easily disable default services</li> <li>Naming inconsistency - Some code still references \"Cascade\" instead of \"Phlo\"</li> <li>Dual discovery systems - Confusing separation between <code>plugins/discovery.py</code> and <code>services/discovery.py</code></li> <li>Deprecated package - <code>phlo-fastapi</code> is superseded by postgrest and the new phlo-api</li> </ol>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#decision","title":"Decision","text":""},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#1-make-observatory-a-core-service","title":"1. Make Observatory a Core Service","text":"<p>Observatory ships with <code>pip install phlo</code>, not as a separate plugin.</p> <pre><code># Current\npip install phlo phlo-observatory\n\n# New\npip install phlo  # Includes Observatory\n</code></pre>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#2-create-phlo-api-backend","title":"2. Create phlo-api Backend","text":"<p>A new core Python FastAPI service that exposes phlo internals:</p> <ul> <li><code>GET /api/plugins</code> - List installed plugins</li> <li><code>GET /api/services</code> - List services and status</li> <li><code>GET /api/config</code> - Read phlo.yaml configuration</li> <li><code>POST /api/services/{name}/restart</code> - Restart a service (future)</li> </ul> <p>Observatory calls phlo-api to display plugin status, configs, etc.</p>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#3-add-user-service-overrides-in-phloyaml","title":"3. Add User Service Overrides in phlo.yaml","text":"<pre><code>services:\n  observatory:\n    port: 8080\n    environment:\n      CUSTOM_VAR: \"value\"\n  superset:\n    enabled: false\n  custom-api:\n    type: inline\n    image: my-registry/api:latest\n</code></pre>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#4-add-defaults-extra","title":"4. Add <code>[defaults]</code> Extra","text":"<pre><code>[project.optional-dependencies]\ndefaults = [\"phlo-dagster\", \"phlo-postgres\", \"phlo-trino\", \"phlo-nessie\", \"phlo-minio\"]\n</code></pre> <p>Enables: <code>pip install phlo[defaults]</code></p>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#5-consolidate-discovery","title":"5. Consolidate Discovery","text":"<p>Merge plugin and service discovery into single module:</p> <pre><code>src/phlo/discovery/\n\u251c\u2500\u2500 plugins.py   # Entry point discovery\n\u251c\u2500\u2500 services.py  # Service loading (core + plugins)\n\u2514\u2500\u2500 registry.py  # Remote registry\n</code></pre>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#6-remove-phlo-fastapi","title":"6. Remove phlo-fastapi","text":"<p>Package is superseded by:</p> <ul> <li>postgrest for auto-generated REST APIs from Postgres</li> <li>phlo-api for phlo-specific internal APIs</li> </ul>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#7-naming-cleanup","title":"7. Naming Cleanup","text":"<p>Replace all \"Cascade\" references with \"Phlo\".</p>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#implementation","title":"Implementation","text":"<p>See implementation-roadmap.md for detailed phases.</p>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#core-vs-packages-model","title":"Core vs Packages Model","text":"<pre><code>CORE (bundled with pip install phlo):\n\u251c\u2500\u2500 phlo library (CLI, config, framework glue)\n\u2514\u2500\u2500 plugin discovery + service orchestration\n\nPACKAGES (swappable, installed separately):\n\u251c\u2500\u2500 phlo-observatory\n\u251c\u2500\u2500 phlo-api\n\u251c\u2500\u2500 phlo-dagster / phlo-airflow\n\u251c\u2500\u2500 phlo-postgres / phlo-mysql\n\u251c\u2500\u2500 phlo-trino / phlo-duckdb\n\u251c\u2500\u2500 phlo-nessie\n\u251c\u2500\u2500 phlo-minio\n\u251c\u2500\u2500 phlo-superset / phlo-metabase\n\u251c\u2500\u2500 phlo-grafana\n\u2514\u2500\u2500 etc.\n</code></pre>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#file-changes","title":"File Changes","text":"Action Path Create <code>packages/phlo-observatory/src/phlo_observatory/service.yaml</code> Create <code>packages/phlo-api/</code> Update <code>src/phlo/services/discovery.py</code> - Plugin-only service discovery Update <code>src/phlo/services/composer.py</code> - Apply user overrides Update <code>src/phlo/config_schema.py</code> - Add <code>ServiceOverride</code> model Update <code>pyproject.toml</code> - Workspace packages for services Delete <code>src/phlo/core_services/</code>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#positive","title":"Positive","text":"<ul> <li>Better DX - Observatory available immediately after <code>pip install phlo</code></li> <li>Observatory empowered - Can show plugins, configs, service status via phlo-api</li> <li>User customization - Override service configs in phlo.yaml</li> <li>Cleaner architecture - Single discovery flow, consistent naming</li> <li>Focused packages - Each package represents a swappable component</li> </ul>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#negative","title":"Negative","text":"<ul> <li>Larger core install - phlo now includes Observatory assets</li> <li>Build complexity - CI must build Observatory's Node.js app</li> <li>Migration - Existing users with <code>phlo-observatory</code> installed need migration path</li> </ul>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#migration","title":"Migration","text":"<p>For users who already have <code>phlo-observatory</code> installed:</p> <pre><code>pip uninstall phlo-observatory  # Now bundled with core\npip install --upgrade phlo      # Gets new version with Observatory\n</code></pre>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#verification","title":"Verification","text":"<pre><code># Observatory starts without separate install\npip install phlo\nphlo services start\ncurl http://localhost:3001  # Observatory responds\n\n# phlo-api accessible\ncurl http://localhost:4000/api/plugins\n\n# Service overrides work\ncat &gt;&gt; phlo.yaml &lt;&lt; EOF\nservices:\n  observatory:\n    port: 8080\nEOF\nphlo services start\ncurl http://localhost:8080  # Observatory on custom port\n\n# Disable service works\ncat &gt;&gt; phlo.yaml &lt;&lt; EOF\nservices:\n  superset:\n    enabled: false\nEOF\nphlo services start\ndocker ps | grep superset  # Should not appear\n</code></pre>"},{"location":"architecture/decisions/0031-observatory-as-core-and-dx-improvements/#related","title":"Related","text":"<ul> <li>Goals: plugin-dx.md</li> <li>Roadmap: implementation-roadmap.md</li> <li>Prior: ADR 0030 - Unified Plugin System</li> </ul>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/","title":"ADR 0032: Eliminate Docker Images - Port Observatory Server to Python","text":""},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#context","title":"Context","text":"<p>Observatory uses TanStack Start with Nitro and has 17+ server function modules that communicate with Trino, Nessie, Dagster, Loki, etc. These cannot be compiled to static assets, which means we cannot simply bundle Observatory as static files served by phlo-api.</p>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#current-server-modules-130kb-total","title":"Current Server Modules (~130KB total)","text":"Module Lines Purpose dagster.server.ts 739 GraphQL queries to Dagster API trino.server.ts 644 SQL execution via Trino HTTP API services.server.ts ~500 Service health checks contributing.server.ts ~400 Row-level lineage queries nessie.server.ts 450 Git-like branch operations iceberg.server.ts 419 Table metadata via Trino quality.server.ts ~350 Data quality checks lineage.server.ts ~280 Column/asset lineage loki.server.ts ~300 Log queries diff.server.ts ~250 Branch diff computation graph.server.ts ~270 Lineage graph building Others ~500 Auth, cache, search, settings, plugins"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#decision","title":"Decision","text":"<p>Port all server functions to phlo-api (Python/FastAPI) and convert Observatory to a pure client-side SPA.</p>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#target-architecture","title":"Target Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 phlo-api                    \u2502\n\u2502  \u251c\u2500\u2500 /api/trino/*      \u2192 Trino queries     \u2502\n\u2502  \u251c\u2500\u2500 /api/dagster/*    \u2192 Dagster GraphQL   \u2502\n\u2502  \u251c\u2500\u2500 /api/nessie/*     \u2192 Branch ops        \u2502\n\u2502  \u251c\u2500\u2500 /api/iceberg/*    \u2192 Table metadata    \u2502\n\u2502  \u251c\u2500\u2500 /api/quality/*    \u2192 Data quality      \u2502\n\u2502  \u251c\u2500\u2500 /api/loki/*       \u2192 Logs              \u2502\n\u2502  \u2514\u2500\u2500 /*                \u2192 Observatory SPA   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       Native uvicorn subprocess\n</code></pre>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#implementation","title":"Implementation","text":""},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#phase-1-core-data-access","title":"Phase 1: Core Data Access","text":"<p>New Python routers in <code>packages/phlo-api/src/phlo_api/observatory_api/</code>:</p> <ul> <li><code>trino.py</code> - Query execution, preview, column profiling</li> <li><code>iceberg.py</code> - Table listing, schema, metadata</li> <li><code>dagster.py</code> - Health metrics, assets, runs</li> <li><code>nessie.py</code> - Branches, commits, merge operations</li> </ul>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#phase-2-secondary-features","title":"Phase 2: Secondary Features","text":"<ul> <li><code>quality.py</code> - Data quality check results</li> <li><code>loki.py</code> - Log queries</li> <li><code>lineage.py</code> - Column/asset lineage</li> <li><code>search.py</code> - Global search</li> </ul>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#phase-3-observatory-client-refactor","title":"Phase 3: Observatory Client Refactor","text":"<p>Replace <code>createServerFn()</code> calls with <code>fetch('/api/...')</code> and remove Nitro dependencies.</p>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#phase-4-distribution","title":"Phase 4: Distribution","text":"<p>Publish <code>phlo-api</code> and <code>phlo-observatory</code> as packages; Observatory consumes phlo-api endpoints.</p>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#positive","title":"Positive","text":"<ul> <li>No Docker images needed for core services</li> <li>No Node.js runtime dependency for users</li> <li>Install <code>phlo</code> + <code>phlo-api</code> + <code>phlo-observatory</code> for the full UI stack</li> <li>Python-native backend (easier to extend)</li> </ul>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#negative","title":"Negative","text":"<ul> <li>~2-3 days implementation effort</li> <li>Temporary feature parity gap during migration</li> <li>Some TypeScript type safety lost in API calls</li> </ul>"},{"location":"architecture/decisions/0032-eliminate-docker-images-port-observatory-to-python/#related","title":"Related","text":"<ul> <li>Prior: ADR 0031 - Observatory as Core</li> <li>Supersedes Docker image publishing workflow</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/","title":"ADR 0033: Hook-Based Capability Plugins","text":""},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#context","title":"Context","text":"<p>Phlo already supports a unified plugin system with entry-point discovery, a registry, and base classes for sources, quality checks, transformations, services, Dagster extensions, and CLI commands. Service lifecycle hooks exist in <code>service.yaml</code>, but there is no first-class, typed hook system for data-lake capabilities such as semantic layers, observability, telemetry, lineage, publishing, or marts.</p> <p>This leaves open questions:</p> <ul> <li>Plugins cannot reliably coordinate or observe pipeline events without importing   each other directly.</li> <li>Capability packages (e.g., metadata, telemetry, semantic models) lack a clean,   stable contract for interop.</li> <li>Service hooks are separate from data workflow events, which fragments the   extension model.</li> </ul> <p>We need a best-practice, scalable hook system that preserves the current plugin architecture while enabling new capability plugins to compose cleanly.</p>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#decision","title":"Decision","text":"<p>Introduce a Hook Bus with typed events and optional hook protocols to enable capability plugins (semantic layer, observability, telemetry, lineage, publishing, and similar) to integrate without direct dependencies.</p> <p>Key decisions:</p> <ol> <li>Hook Bus in core: add a lightweight dispatcher responsible for emitting and    routing events to registered hook providers.</li> <li>Typed event contexts: define a small, stable set of event models (dataclasses    or Pydantic) covering pipeline, quality, publishing, and service lifecycle.</li> <li>Synchronous by default: hook dispatch is synchronous in-process for ordering    and determinism. Hooks should be fast; heavy work must delegate to background    systems. Async/queued dispatch is a future extension for high-volume telemetry.</li> <li>Ordering + priority: hooks register with an integer priority (lower runs first).    Stable ordering is maintained by <code>(priority, plugin_name, hook_name)</code>.</li> <li>Failure policy: hooks declare <code>failure_policy</code> (ignore/log/raise). Default is    <code>log</code> and continues the pipeline; <code>raise</code> can abort the originating flow.</li> <li>Event filtering: hook registration includes optional filters (event type,    asset keys, tags) so hooks only receive relevant events.</li> <li>Optional hook protocols: plugins may implement hook protocols without    changing their primary plugin type.</li> <li>Hook-only plugins: add a dedicated entry-point group for plugins whose    only responsibility is handling hooks.</li> <li>Unified lifecycle: map existing <code>service.yaml</code> hooks onto Hook Bus events so    service lifecycle hooks are part of the same system.</li> <li>Schema evolution: events carry a <code>version</code> field and follow additive-only    changes; deprecations are documented and removed only on major versions.</li> </ol>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#implementation","title":"Implementation","text":""},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#phase-1-core-hook-system","title":"Phase 1: Core Hook System","text":"<ul> <li>Add <code>src/phlo/hooks/</code> with:</li> <li><code>events.py</code> defining typed event contexts:<ul> <li><code>ServiceLifecycleEvent</code> (pre/post start/stop)</li> <li><code>IngestionEvent</code> (start/end, asset key, table name, run metadata)</li> <li><code>TransformEvent</code> (start/end, tool name, model/asset metadata)</li> <li><code>PublishEvent</code> (start/end, marts/publish metadata)</li> <li><code>QualityResultEvent</code> (check results, severity, asset metadata)</li> <li><code>LineageEvent</code> (edges, asset metadata)</li> <li><code>TelemetryEvent</code> (metric/log/span payloads with tags)</li> </ul> </li> <li><code>bus.py</code> implementing <code>HookBus.emit(event)</code> with ordering, filtering, and     failure policy handling.</li> <li>Add <code>src/phlo/plugins/hooks.py</code> with:</li> <li><code>HookRegistration</code> dataclass (hook_name, handler, priority, filters,     failure_policy).</li> <li><code>HookProvider</code> Protocol defining <code>get_hooks()</code> (declarative registration).</li> <li><code>HookHandler</code> Protocol defining <code>handle_event()</code> (single-dispatch convenience).</li> <li><code>HookPlugin</code> ABC for hook-only plugins (inherits <code>Plugin</code>).</li> <li>Extend <code>src/phlo/discovery/plugins.py</code> with a new entry-point group:</li> <li><code>phlo.plugins.hooks</code></li> <li>Extend <code>src/phlo/discovery/registry.py</code> to register hook plugins and list them.</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#phase-2-emit-events-from-core-pipelines","title":"Phase 2: Emit Events from Core Pipelines","text":"<ul> <li>In <code>phlo-dlt</code> ingestion decorator, emit <code>IngestionEvent</code> start/end.</li> <li>In <code>phlo-quality</code>, emit <code>QualityResultEvent</code> after check execution.</li> <li>In <code>phlo-dbt</code> publishing flow, emit <code>PublishEvent</code> for marts publishing.</li> <li>In <code>phlo-dbt</code> or transform orchestration, emit <code>TransformEvent</code>.</li> <li>In service management (<code>src/phlo/cli/services.py</code>), translate <code>service.yaml</code>   hooks into <code>ServiceLifecycleEvent</code> and also emit events from start/stop flows.</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#phase-3-update-capability-packages","title":"Phase 3: Update Capability Packages","text":"<ul> <li><code>phlo-openmetadata</code> subscribes to:</li> <li><code>LineageEvent</code> for lineage publishing</li> <li><code>QualityResultEvent</code> for test results</li> <li><code>PublishEvent</code> to track published marts</li> <li><code>phlo-metrics</code> and <code>phlo-alerting</code> subscribe to:</li> <li><code>TelemetryEvent</code> for metrics/logs</li> <li><code>QualityResultEvent</code> for alerting</li> <li>Add a semantic layer capability contract:</li> <li>Protocol like <code>SemanticLayerProvider</code> with <code>list_models()</code> and <code>get_model()</code></li> <li>Hook subscription to <code>PublishEvent</code> for model refresh</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#phase-4-registry-dx","title":"Phase 4: Registry + DX","text":"<ul> <li>Update <code>registry/plugins.json</code> and <code>src/phlo/plugins/registry_data.json</code> to   include hook plugin types.</li> <li>Extend <code>phlo plugin create</code> scaffolds to include hook providers and config   examples.</li> <li>Update <code>docs/guides/plugin-development.md</code> to document hook events and   capability protocols.</li> <li>Add <code>phlo-testing</code> helpers (mock bus, event fixtures) so plugin authors can   test hook handlers in isolation.</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#positive","title":"Positive","text":"<ul> <li>Composable capabilities: telemetry, metadata, and semantic layers can   integrate without direct imports.</li> <li>Stable contracts: protocols + typed events reduce brittle coupling.</li> <li>Unified model: service hooks and data workflow hooks share the same bus.</li> <li>Scalability: new capabilities can be added without touching core pipelines.</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#negative","title":"Negative","text":"<ul> <li>More surface area: additional plugin type and hook definitions to maintain.</li> <li>Event discipline required: event schemas must remain stable to avoid churn.</li> <li>Blocking risk: synchronous hooks can slow hot paths if misused.</li> </ul>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#verification","title":"Verification","text":"<pre><code># Hook discovery and registry\nphlo plugin list --type hooks\n\n# Hook dispatch during ingestion\nphlo test tests/test_ingestion.py\n\n# Hook dispatch during quality checks\nphlo test tests/test_quality_decorator.py\n\n# Service lifecycle hooks map to Hook Bus\nphlo services start\nphlo services stop\n</code></pre>"},{"location":"architecture/decisions/0033-hook-based-capability-plugins/#related","title":"Related","text":"<ul> <li>Prior: ADR 0030 - Unified Plugin System</li> <li>Prior: ADR 0031 - Observatory as Core with Plugin DX Improvements</li> </ul>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/","title":"ADR 0034: Migrate to TY Type Checker","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#context","title":"Context","text":"<p>Phlo currently uses basedpyright for Python type checking, configured in <code>pyproject.toml</code> under <code>[tool.basedpyright]</code>. The CI pipeline runs <code>basedpyright src/phlo</code> on every PR.</p> <p>Astral released ty, a new Python type checker written in Rust, into public beta on December 16, 2025. Key advantages:</p> <ul> <li>10x-100x faster than mypy and Pyright (2.19s vs 45.66s for home-assistant)</li> <li>Incremental analysis with fine-grained updates (up to 80x faster for editor changes)</li> <li>Rich contextual diagnostics inspired by Rust's compiler error messages</li> <li>First-class Language Server with Go-to-Definition, Symbol Rename, Auto-Complete, inlay hints</li> <li>Gradual adoption support with redeclarations and partially-typed code handling</li> <li>Unified Astral ecosystem alongside Ruff and uv</li> </ul> <p>Since we already use Ruff for linting and uv for package management, adopting ty completes our Astral-based Python toolchain. <code>ty</code> is already in our dev dependencies (<code>pyproject.toml</code> line 120).</p>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#current-type-checking-configuration","title":"Current Type Checking Configuration","text":"<pre><code>[tool.basedpyright]\ninclude = [\"src/phlo\"]\nexclude = [\"src/phlo/testing\"]\nstubPath = \"typings\"\npythonVersion = \"3.11\"\ntypeCheckingMode = \"standard\"\n# Many rules disabled for pandas/dagster compatibility\nreportUnknownMemberType = false\nreportUnknownArgumentType = false\n# ... (12 rules disabled)\n</code></pre>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#decision","title":"Decision","text":"<p>Replace basedpyright with ty as the primary Python type checker for the Phlo codebase.</p> <p>Key decisions:</p> <ol> <li>Remove basedpyright from dev dependencies; keep ty (already present).</li> <li>Add <code>[tool.ty]</code> configuration to <code>pyproject.toml</code> matching current scope.</li> <li>Update CI to run <code>uv run ty check</code> instead of <code>uv run basedpyright</code>.</li> <li>Configure rule equivalents for currently disabled basedpyright rules.</li> <li>Run in parallel initially (if needed) to validate parity before full cutover.</li> <li>Remove <code>[tool.basedpyright]</code> section after successful migration.</li> </ol>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#implementation","title":"Implementation","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#phase-1-add-ty-configuration","title":"Phase 1: Add TY Configuration","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#modifyfileusersgarethpricedeveloperphlopyprojecttoml","title":"[MODIFY][pyproject.toml](file:///Users/garethprice/Developer/phlo/pyproject.toml)","text":"<p>Add <code>[tool.ty]</code> section matching current type checking scope:</p> <pre><code>[tool.ty]\ninclude = [\"src/phlo\"]\nexclude = [\"src/phlo/testing\"]\npython-version = \"3.11\"\n\n[tool.ty.rules]\n# Disable noisy rules for pandas/dagster compatibility (equivalent to basedpyright settings)\n# ty uses rule names like \"possibly-unbound-attribute\", \"unknown-argument\", etc.\n# We will start with default rules and refine based on initial run results\n</code></pre>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#phase-2-update-ci-pipeline","title":"Phase 2: Update CI Pipeline","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#modifyfileusersgarethpricedeveloperphlogithubworkflowsciyml","title":"[MODIFY][ci.yml](file:///Users/garethprice/Developer/phlo/.github/workflows/ci.yml)","text":"<p>Replace the type-check job:</p> <pre><code>   type-check:\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v4\n\n       - name: Install uv\n         uses: astral-sh/setup-uv@v7\n         with:\n           version: \"latest\"\n\n       - name: Set up Python\n         run: uv python install 3.11\n\n       - name: Install dependencies\n         run: uv sync --dev\n\n-      - name: Type check with basedpyright\n-        run: uv run basedpyright src/phlo\n+      - name: Type check with ty\n+        run: uv run ty check src/phlo\n</code></pre>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#phase-3-update-dependencies","title":"Phase 3: Update Dependencies","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#modifyfileusersgarethpricedeveloperphlopyprojecttoml_1","title":"[MODIFY][pyproject.toml](file:///Users/garethprice/Developer/phlo/pyproject.toml)","text":"<p>Remove basedpyright from dev dependencies:</p> <pre><code> [dependency-groups]\n dev = [\n     \"pytest&gt;=8.4.2\",\n     \"sqlfluff&gt;=3.5.0\",\n     ...\n     \"ruff\",\n-    \"basedpyright\",\n     \"ty\",\n ]\n</code></pre>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#phase-4-cleanup","title":"Phase 4: Cleanup","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#delete-toolbasedpyright-section-from-pyprojecttoml","title":"[DELETE] <code>[tool.basedpyright]</code> section from pyproject.toml","text":"<p>Remove the entire basedpyright configuration block (lines 160-182).</p>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#positive","title":"Positive","text":"<ul> <li>Faster CI: Type checking will complete in seconds instead of minutes.</li> <li>Faster development: Incremental editor checks will be near-instant.</li> <li>Unified toolchain: Ruff, uv, and ty form a cohesive Astral ecosystem.</li> <li>Better diagnostics: Rich, contextual error messages improve DX.</li> <li>Active development: ty is actively maintained by Astral with planned stable release in 2026.</li> </ul>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#negative","title":"Negative","text":"<ul> <li>Beta software: ty is still in beta; edge cases may exist.</li> <li>Rule mapping: Some basedpyright rules may not have exact ty equivalents.</li> <li>Configuration learning curve: Different configuration syntax from pyright.</li> </ul>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Mitigation Missing type issues not caught by ty Run both checkers in CI during transition Beta instability Pin ty version; update carefully Editor integration gaps ty has VS Code, PyCharm, Neovim support"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#verification","title":"Verification","text":"<p>[!IMPORTANT] Please suggest any additional manual verification steps you'd like me to perform.</p>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#automated-tests","title":"Automated Tests","text":"<pre><code># Run ty locally to verify configuration\nuv run ty check src/phlo\n\n# Compare output with basedpyright (optional during transition)\nuv run basedpyright src/phlo\n</code></pre>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#ci-validation","title":"CI Validation","text":"<p>After merging, verify the <code>type-check</code> job passes in GitHub Actions.</p>"},{"location":"architecture/decisions/0034-migrate-to-ty-typechecker/#related","title":"Related","text":"<ul> <li>ty documentation: https://docs.astral.sh/ty/</li> <li>Astral announcement: https://astral.sh/blog/ty</li> <li>Prior: ADR 0006 - Public API and Structured Logging</li> </ul>"},{"location":"architecture/decisions/0035-package-integration-tests/","title":"ADR 0035: Package-Level Integration Tests","text":""},{"location":"architecture/decisions/0035-package-integration-tests/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0035-package-integration-tests/#context","title":"Context","text":"<p>Phlo packages are split across service plugins and library integrations. The recent plugin refactor left integration test coverage uneven: some packages had no integration tests, while others were tested only when specific services happened to be running. This made it difficult to verify that all packages work after refactors, and caused integration runs to skip large portions of the codebase without a clear signal.</p> <p>We need consistent package-level integration tests that:</p> <ul> <li>Exercise each package at least once during integration runs.</li> <li>Validate service-backed packages via health endpoints when services are running.</li> <li>Allow local runs to skip cleanly when a dependency is not available.</li> </ul>"},{"location":"architecture/decisions/0035-package-integration-tests/#decision","title":"Decision","text":"<p>Add package-level integration tests across all packages, with the following rules:</p> <ol> <li>Service packages (those with <code>service.yaml</code>) validate a health endpoint when the service is    reachable. If the service is not reachable, the test skips with a clear message.</li> <li>Library packages add lightweight integration smoke tests that validate imports and    basic initialization paths without external services.</li> <li>Environment overrides follow a consistent pattern:</li> <li>Prefer <code>&lt;SERVICE&gt;_URL</code> when set.</li> <li>Otherwise, use <code>&lt;SERVICE&gt;_HOST</code> (default <code>localhost</code>) and <code>&lt;SERVICE&gt;_PORT</code> (service default).</li> <li>Integration tests remain inside each package to keep ownership aligned with plugin scope.</li> </ol>"},{"location":"architecture/decisions/0035-package-integration-tests/#implementation","title":"Implementation","text":"<ul> <li>Add <code>pytest.mark.integration</code> tests under <code>packages/&lt;package&gt;/tests/</code> for each package.</li> <li>Service health checks use lightweight HTTP probes (or TCP connect for PostgreSQL).</li> <li>Library packages use minimal smoke tests (e.g., import and constructor validation).</li> </ul> <p>This standardizes integration coverage without imposing a full-stack requirement on every local run.</p>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/","title":"ADR 0036: Iceberg Maintenance Observability","text":""},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#context","title":"Context","text":"<p>Iceberg maintenance jobs live in <code>packages/phlo-iceberg/src/phlo_iceberg/maintenance.py</code> and run snapshot expiration, orphan file cleanup, and table statistics collection on a schedule. Today they emit human-readable log lines and return summary dicts, but they do not provide:</p> <ul> <li>Structured log fields for correlation (run, job, namespace, table, ref).</li> <li>Consistent metrics for durations, counts, and errors.</li> <li>Alerting when maintenance jobs fail.</li> <li>A surfaced maintenance status view in Observatory.</li> </ul> <p>The release plan calls for end-to-end observability of maintenance operations across logging, metrics, alerting, and UI.</p>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#decision","title":"Decision","text":"<p>Implement first-class observability for Iceberg maintenance operations, aligned with ADR 0028 (structured logging) and the hook-based telemetry pipeline.</p> <ol> <li>Structured logging: emit start/end logs for each maintenance operation and per-table actions    with <code>extra</code> fields: <code>maintenance_op</code>, <code>namespace</code>, <code>table_name</code>, <code>ref</code>, <code>dry_run</code>, <code>run_id</code>,    <code>job_name</code>.</li> <li>Telemetry events: use <code>TelemetryEventEmitter</code> to emit <code>telemetry.metric</code> and    <code>telemetry.log</code> events with tags: <code>maintenance=true</code>, <code>operation</code>, <code>namespace</code>, <code>ref</code>, and    optional <code>table</code> for log-level detail.</li> <li>Prometheus metrics: aggregate telemetry into Prometheus counters/gauges/histograms using    <code>phlo-metrics</code>. Prometheus labels stay low-cardinality (operation, namespace, ref, status),    while table-level detail remains in logs/telemetry events.</li> <li>Alerting: on maintenance job failure or non-zero error counts, send alerts via    <code>phlo-alerting</code> with run/job context.</li> <li>Observatory status: expose last maintenance run status, duration, and key counters in the    Observatory UI from metrics/telemetry data.</li> </ol>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#implementation","title":"Implementation","text":"<ul> <li>Add maintenance telemetry and structured logging to <code>phlo_iceberg.maintenance</code> for   <code>expire_table_snapshots</code>, <code>cleanup_orphan_files</code>, and <code>collect_table_stats</code>.</li> <li>Extend <code>phlo-metrics</code> to aggregate maintenance telemetry into Prometheus-compatible metrics.</li> <li>Trigger alerts through <code>phlo-alerting</code> when maintenance errors occur.</li> <li>Surface maintenance status in Observatory using the metrics API.</li> </ul>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#positive","title":"Positive","text":"<ul> <li>Clear visibility into maintenance health, duration, and impact.</li> <li>Faster detection of failed or risky maintenance runs.</li> <li>Consistent observability pattern across Phlo operations.</li> </ul>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#negative","title":"Negative","text":"<ul> <li>Additional log volume and telemetry storage.</li> <li>Metrics aggregation work required in <code>phlo-metrics</code>.</li> </ul>"},{"location":"architecture/decisions/0036-iceberg-maintenance-observability/#verification","title":"Verification","text":"<ul> <li>Run a maintenance job and confirm structured logs include maintenance fields.</li> <li>Verify Prometheus exposes maintenance metrics with low-cardinality labels.</li> <li>Trigger a failure and confirm an alert is sent and shown in Observatory.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/","title":"ADR 0037: Advanced Reconciliation Checks","text":""},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#context","title":"Context","text":"<p>Phlo currently ships reconciliation checks that compare row counts and aggregates between source and target tables. These catch bulk loss or aggregation drift, but they do not detect silent row-level mismatches when counts and aggregates still align (for example, incorrect values for a subset of rows). Users can build checksum comparisons with ad-hoc SQL, but this is repetitive and hard to standardize across pipelines.</p> <p>We want a first-class, warehouse-native reconciliation check that detects row-level drift while remaining deterministic, partition-aware, and cost-controlled.</p>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#decision","title":"Decision","text":"<p>Add a checksum-based reconciliation check to <code>phlo_quality.reconciliation</code> that compares row-level hashes between a source and target table using Trino, and add three complementary reconciliation capabilities that were prioritized alongside checksums: key-level anti-join parity, absolute-difference tolerances, and multi-aggregate parity.</p>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#check-definition","title":"Check definition","text":"<p>Introduce <code>ChecksumReconciliationCheck</code> with the following behavior:</p> <ul> <li>Required inputs:</li> <li><code>source_table</code>: fully qualified source table name.</li> <li><code>key_columns</code>: primary key or composite key used to align rows across tables.</li> <li>Optional inputs:</li> <li><code>columns</code>: columns to hash. If omitted, hash all non-metadata columns from the target table.</li> <li><code>partition_column</code>: used to filter both source and target to the same partition.</li> <li><code>tolerance</code>: allowed fraction of mismatches (default 0.0).</li> <li><code>sample</code>: optional sampling fraction for large tables (default none).</li> <li><code>limit</code>: optional cap on rows hashed for cost control.</li> <li><code>hash_algorithm</code>: <code>xxhash64</code> by default, with <code>md5</code> supported when configured.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#hashing-semantics","title":"Hashing semantics","text":"<ul> <li>Hashes are computed over a stable, ordered list of columns.</li> <li>Each value is normalized before hashing:</li> <li><code>NULL</code> becomes a sentinel value (<code>'__NULL__'</code>).</li> <li>Timestamps are normalized to UTC ISO8601 strings.</li> <li>Floats are rounded to a configurable precision (default 6) before casting.</li> <li>The hash input is the concatenation of normalized values with a delimiter.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#comparison-logic","title":"Comparison logic","text":"<ul> <li>The check computes a <code>(key_columns, row_hash)</code> dataset for both source and target.</li> <li>It joins by <code>key_columns</code> and reports:</li> <li>missing keys (in target but not in source, and vice versa),</li> <li>hash mismatches for shared keys,</li> <li>total compared rows.</li> <li>The check passes when <code>(mismatches / total_compared) &lt;= tolerance</code>.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#result-metadata","title":"Result metadata","text":"<p>Return a <code>QualityCheckResult</code> with:</p> <ul> <li><code>metric_name</code>: <code>checksum_reconciliation_check</code></li> <li><code>metric_value</code>: counts for <code>missing_in_target</code>, <code>missing_in_source</code>, <code>hash_mismatches</code>,   <code>total_compared</code>, and <code>mismatch_pct</code>.</li> <li><code>metadata</code>: query strings, key/column list, normalization settings, and up to 10 sample mismatches.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#key-level-anti-join-parity","title":"Key-level anti-join parity","text":"<p>Introduce <code>KeyParityCheck</code> to validate that keys align between source and target tables.</p> <ul> <li>Required inputs:</li> <li><code>source_table</code></li> <li><code>key_columns</code></li> <li>Optional inputs:</li> <li><code>partition_column</code></li> <li><code>where_clause</code> (source filter)</li> <li><code>tolerance</code> (fraction of missing keys allowed, default 0.0)</li> <li>Behavior:</li> <li>Compare distinct keys between source and target.</li> <li>Report missing keys on each side.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#absolute-difference-tolerances","title":"Absolute-difference tolerances","text":"<p>Extend reconciliation checks that compare row counts or aggregates to accept an absolute tolerance in addition to percentage tolerance.</p> <ul> <li>New optional field: <code>absolute_tolerance</code> (default <code>None</code>).</li> <li>A check passes when either percent tolerance or absolute tolerance is satisfied.</li> <li>Applies to <code>ReconciliationCheck</code> and <code>AggregateConsistencyCheck</code>, and is reused by   <code>ChecksumReconciliationCheck</code> where applicable (e.g., mismatches count).</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#multi-aggregate-parity","title":"Multi-aggregate parity","text":"<p>Introduce <code>MultiAggregateConsistencyCheck</code> to compare multiple aggregates in a single query.</p> <ul> <li>Required inputs:</li> <li><code>source_table</code></li> <li><code>aggregates</code>: list of <code>{name, expression, target_column}</code> mappings.</li> <li>Optional inputs:</li> <li><code>partition_column</code></li> <li><code>group_by</code></li> <li><code>tolerance</code> / <code>absolute_tolerance</code></li> <li>Behavior:</li> <li>Compute all aggregates in one source query.</li> <li>Compare each aggregate column in the target with the corresponding source value(s).</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#implementation","title":"Implementation","text":"<ul> <li>Add <code>ChecksumReconciliationCheck</code> to   <code>packages/phlo-quality/src/phlo_quality/reconciliation.py</code> and export from   <code>packages/phlo-quality/src/phlo_quality/__init__.py</code>.</li> <li>Add <code>KeyParityCheck</code> and <code>MultiAggregateConsistencyCheck</code> to the same module and exports.</li> <li>Extend <code>ReconciliationCheck</code> and <code>AggregateConsistencyCheck</code> to support <code>absolute_tolerance</code>.</li> <li>Build source/target queries using Trino SQL with deterministic normalization.</li> <li>Extend tests in <code>packages/phlo-quality/tests/test_quality_reconciliation.py</code> to cover   matching hashes, mismatches, missing keys, tolerance, absolute tolerance, and partition filtering.</li> <li>Add short documentation to the developer guide alongside other reconciliation checks.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#positive","title":"Positive","text":"<ul> <li>Detects silent row-level drift that count/aggregate checks miss.</li> <li>Standardizes checksum reconciliation without user-specific SQL.</li> <li>Adds key parity and multi-aggregate primitives for common lakehouse checks.</li> <li>Supports absolute tolerances for small tables and low-volume partitions.</li> <li>Leverages existing Trino integration and partition-aware patterns.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#negative","title":"Negative","text":"<ul> <li>More expensive than count/aggregate checks on large tables.</li> <li>Requires careful normalization to avoid false positives.</li> <li>May be sensitive to nondeterministic columns unless explicitly excluded.</li> <li>Additional API surface increases maintenance burden.</li> </ul>"},{"location":"architecture/decisions/0037-advanced-reconciliation-checks/#verification","title":"Verification","text":"<ul> <li>Run checksum reconciliation on a known-good dataset and verify zero mismatches.</li> <li>Introduce controlled row-level modifications and confirm mismatches are detected.</li> <li>Validate tolerance (percent + absolute), sampling, and partition filters with unit tests and   integration tests.</li> <li>Validate key parity and multi-aggregate checks against known mismatches.</li> </ul>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/","title":"ADR 0038: Golden Path E2E Workflow Test","text":""},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#context","title":"Context","text":"<p>We need a repeatable, end-to-end verification of the default Phlo experience: project scaffolding, service orchestration, ingestion, transformation, publishing, and optional service wiring. Earlier investigations introduced a Trino-specific diagnostic test to pinpoint startup issues, but that approach duplicates responsibility and fragments the system-level workflow story.</p> <p>The E2E test should: - exercise the CLI as users do, - run against a fresh environment (including Docker services), - validate the resulting artifacts and data flows, - provide actionable logs for each step, and - support both development installs and PyPI installs.</p>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#decision","title":"Decision","text":"<p>Adopt a single golden-path E2E workflow test (pytest-based) that: - initializes a project into a temp directory, - validates scaffolded files, - uses the CLI to create and run an ingestion workflow against JSONPlaceholder, - runs dbt transforms, publishes marts to Postgres, and verifies row counts, - starts core services with Docker Compose and waits for readiness, and - incrementally adds optional services (e.g., observability stack) and asserts auto-wiring.</p> <p>Remove the standalone Trino startup diagnostic test in favor of the integrated E2E flow.</p>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#implementation","title":"Implementation","text":"<ul> <li>Use <code>tests/test_golden_path_e2e.py</code> as the single golden-path E2E test.</li> <li>Write per-step logs to a temporary directory for debugging.</li> <li>Support <code>PHLO_E2E_MODE=dev|pypi</code> to select local source vs PyPI install paths.</li> <li>Gate execution behind <code>PHLO_E2E=1</code> to avoid accidental runs.</li> <li>Remove <code>tests/test_trino_startup_diagnostics.py</code>.</li> </ul>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#positive","title":"Positive","text":"<ul> <li>End-to-end confidence in the default user workflow.</li> <li>CLI-driven verification matches real usage patterns.</li> <li>Per-step logs make failures actionable without pytest output noise.</li> <li>One canonical test reduces duplication and drift.</li> </ul>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#negative","title":"Negative","text":"<ul> <li>Longer runtime than unit or targeted integration tests.</li> <li>Requires Docker and external network access for data ingestion.</li> </ul>"},{"location":"architecture/decisions/0038-golden-path-e2e-workflow-test/#verification","title":"Verification","text":"<ul> <li>Run <code>PHLO_E2E=1 PHLO_E2E_MODE=dev uv run pytest tests/test_golden_path_e2e.py -m integration -s</code>.</li> <li>Inspect step logs in the generated <code>e2e-logs</code> directory when failures occur.</li> </ul>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/","title":"ADR 0039: dbt Project Under Workflows","text":""},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#context","title":"Context","text":"<p>Phlo currently keeps dbt projects in <code>transforms/dbt</code>, while Python workflows live under <code>workflows/</code>. This split adds mental overhead in early-stage projects and makes it easy to lose context when switching between ingestion/quality assets and transformations. We also want to align with the repo convention that project-specific assets live under <code>workflows/</code>.</p> <p>Today, the Dagster services mount <code>workflows/</code> read-only. That prevents dbt from writing build artifacts (e.g., <code>target/</code> and <code>dbt_packages/</code>) if we move the dbt project under <code>workflows/</code>. We do not need backward compatibility for the old location.</p>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#decision","title":"Decision","text":"<p>Place the dbt project at <code>workflows/transforms/dbt</code> and make the <code>workflows/</code> volume writable in services so dbt can write artifacts in-place. Update defaults and documentation to treat this as the only supported layout.</p>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#implementation","title":"Implementation","text":"<ul> <li>Change configuration defaults to <code>workflows/transforms/dbt</code> and derive manifest/catalog paths   from <code>dbt_project_dir</code>.</li> <li>Update dbt discovery to search <code>workflows/transforms/dbt</code>, and ensure hooks respect   <code>DBT_PROJECT_DIR</code>.</li> <li>Update Dagster service mounts to make <code>workflows/</code> writable and point <code>DBT_PROJECT_DIR</code> at the   new location.</li> <li>Update scaffolded project structure and docs to match the new layout.</li> <li>Document migration steps for existing projects (manual move of dbt project).</li> </ul>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#positive","title":"Positive","text":"<ul> <li>dbt models live next to ingestion and quality workflows, reducing project sprawl.</li> <li>Single, consistent source of truth for project assets.</li> <li>Fewer path mismatches between config, docs, and runtime.</li> </ul>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#negative","title":"Negative","text":"<ul> <li>Breaking change for existing projects using <code>transforms/dbt</code>.</li> <li><code>workflows/</code> becomes writable in services, which increases the risk of accidental edits.</li> </ul>"},{"location":"architecture/decisions/0039-dbt-project-under-workflows/#verification","title":"Verification","text":"<ul> <li>Scaffold a new project and confirm dbt artifacts are generated under   <code>workflows/transforms/dbt/target</code>.</li> <li>Start services and run <code>dbt compile</code> via Dagster hooks without path errors.</li> </ul>"},{"location":"architecture/decisions/0040-centralized-logging-layer/","title":"ADR 0040: Centralized Logging Layer and Log Routing","text":""},{"location":"architecture/decisions/0040-centralized-logging-layer/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/decisions/0040-centralized-logging-layer/#context","title":"Context","text":"<p>Phlo logs are currently emitted via a mix of stdlib logging, Dagster <code>context.log</code>, and ad-hoc telemetry events. Observatory uses Pino in TypeScript. We already ship logs to Loki via Alloy, but there is no unified logging API or middle layer for routing logs to additional sinks via plugins. This makes correlation inconsistent and forces each package to invent its own logging setup.</p> <p>ADR-0028 established correlation field standards, but it did not define a central logging layer or a routing mechanism for pluggable sinks.</p>"},{"location":"architecture/decisions/0040-centralized-logging-layer/#decision","title":"Decision","text":"<p>Adopt a centralized logging layer for Python packages with a routing handler that forwards structured log events to the hook bus. Use <code>structlog</code> on top of stdlib <code>logging</code> as the backend to preserve compatibility with Dagster and third-party libraries.</p> <p>Key elements:</p> <ul> <li>Introduce <code>phlo.logging</code> API with <code>setup_logging()</code> and <code>get_logger()</code>.</li> <li>Define a normalized <code>LogEvent</code> (<code>event_type=\"log.record\"</code>) and emit it via a   <code>LogRouterHandler</code> into the HookBus.</li> <li>Keep JSON to stdout as the default sink for Alloy/Loki ingestion.</li> <li>Support optional file logging via <code>phlo_log_file_template</code> (default   <code>.phlo/logs/{YMD}.log</code>, empty disables).</li> <li>Allow hook plugins to subscribe to <code>log.record</code> for external sinks without   changing core logging code.</li> <li>Align log schema fields with ADR-0028 correlation keys.</li> </ul>"},{"location":"architecture/decisions/0040-centralized-logging-layer/#implementation","title":"Implementation","text":"<ul> <li>Add <code>LogEvent</code> to <code>phlo.hooks.events</code> and <code>LogRouterHandler</code> in   <code>phlo.logging.router</code>.</li> <li>Create <code>phlo.logging</code> module with <code>setup_logging()</code>, <code>get_logger()</code>, and   context binding helpers.</li> <li>Update CLI/service entrypoints to call <code>setup_logging()</code> early.</li> <li>Migrate core packages to use <code>get_logger()</code> and standard fields.</li> <li>Document hook-based sinks for <code>log.record</code>.</li> </ul>"},{"location":"architecture/decisions/0040-centralized-logging-layer/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/0040-centralized-logging-layer/#positive","title":"Positive","text":"<ul> <li>Consistent logging API across packages.</li> <li>Logs always flow through a middle layer, enabling multiple sinks.</li> <li>Compatible with existing Loki/Alloy setup and Dagster logging.</li> <li>External sinks can be added via plugins without changing core code.</li> </ul>"},{"location":"architecture/decisions/0040-centralized-logging-layer/#negative","title":"Negative","text":"<ul> <li>Adds a new dependency (<code>structlog</code>) and more configuration surface area.</li> <li>Requires incremental migration of existing package logging calls.</li> </ul>"},{"location":"architecture/decisions/0040-centralized-logging-layer/#verification","title":"Verification","text":"<ul> <li>Unit tests verifying <code>LogRecord</code> -&gt; <code>LogEvent</code> conversion.</li> <li>Integration test that logs appear in Loki and (optionally) a stub sink.</li> <li>Manual test in Dagster run to confirm correlation fields are attached.</li> </ul>"},{"location":"architecture/goals/implementation-roadmap/","title":"Plugin DX Implementation Roadmap","text":"<p>This document outlines what changes are needed to achieve the DX goals.</p>"},{"location":"architecture/goals/implementation-roadmap/#status-as-of-2025-12-24","title":"Status (as of 2025-12-24)","text":"<ul> <li>Phase 0: COMPLETED - Package-first services, core is glue-only</li> <li>Phase 1: COMPLETED - Unified discovery flow</li> <li>Phase 2: COMPLETED - Naming cleanup</li> <li>Phase 3: COMPLETED - User service overrides</li> <li>Phase 4: COMPLETED - Enable/disable services</li> <li>Phase 5: COMPLETED - Inline custom services (with test coverage)</li> <li>Phase 6: COMPLETED - Enhanced CLI discoverability (runtime status, ports, disabled services)</li> <li>Phase 7: COMPLETED - Documentation updated, phlo-fastapi removed from registry</li> </ul>"},{"location":"architecture/goals/implementation-roadmap/#key-architectural-changes","title":"Key Architectural Changes","text":"<ol> <li>Observatory as package - Remains a service plugin (not core)</li> <li>phlo-api backend - Service plugin exposing phlo internals</li> <li>Unified discovery - Single flow for plugins and services</li> <li>User service overrides - Enable <code>phlo.yaml</code> customization</li> <li>Remove phlo-fastapi - Superseded by postgrest and phlo-api</li> </ol>"},{"location":"architecture/goals/implementation-roadmap/#phase-0-package-first-services","title":"Phase 0: Package-First Services","text":"<p>The most significant change is keeping Observatory and phlo-api as packages while phlo core stays glue-only.</p>"},{"location":"architecture/goals/implementation-roadmap/#current-state","title":"Current State","text":"<pre><code>pip install phlo                  # Core only\npip install phlo-observatory      # Separate package\n</code></pre> <p>Observatory is a Python package wrapper around a TanStack Start app, registered via entry points.</p>"},{"location":"architecture/goals/implementation-roadmap/#target-state","title":"Target State","text":"<pre><code>pip install phlo                  # Core glue only\npip install phlo-observatory      # UI package\npip install phlo-api              # Backend for Observatory\n</code></pre> <p>Observatory and phlo-api ship as packages, discovered via service plugins.</p>"},{"location":"architecture/goals/implementation-roadmap/#changes-required","title":"Changes Required","text":""},{"location":"architecture/goals/implementation-roadmap/#1-keep-observatory-as-a-service-package","title":"1. Keep Observatory as a Service Package","text":"<p>Maintain Observatory in <code>packages/phlo-observatory</code> with its own <code>service.yaml</code> and plugin entry point:</p> <pre><code>packages/phlo-observatory/\n\u2514\u2500\u2500 src/phlo_observatory/service.yaml\n</code></pre>"},{"location":"architecture/goals/implementation-roadmap/#2-update-service-discovery","title":"2. Update Service Discovery","text":"<p>Modify <code>src/phlo/services/discovery.py</code> to load only package services from plugins.</p>"},{"location":"architecture/goals/implementation-roadmap/#3-create-phlo-api-backend-package","title":"3. Create phlo-api Backend Package","text":"<p>Implement the FastAPI service in <code>packages/phlo-api/src/phlo_api/</code> and expose service metadata via the service plugin.</p>"},{"location":"architecture/goals/implementation-roadmap/#4-keep-plugin-wrapper-for-observatory","title":"4. Keep Plugin Wrapper for Observatory","text":"<p>Observatory remains a plugin package with a <code>service.yaml</code> and entry point registration.</p>"},{"location":"architecture/goals/implementation-roadmap/#5-optional-defaults-extra","title":"5. Optional Defaults Extra","text":"<pre><code>[project.optional-dependencies]\ndefaults = [\n    \"phlo-dagster\",\n    \"phlo-postgres\",\n    \"phlo-trino\",\n    \"phlo-nessie\",\n    \"phlo-minio\",\n]\n</code></pre>"},{"location":"architecture/goals/implementation-roadmap/#6-remove-phlo-fastapi-package","title":"6. Remove phlo-fastapi Package","text":"<p>Delete <code>packages/phlo-fastapi/</code> - superseded by:</p> <ul> <li>postgrest for auto-generated REST APIs</li> <li>phlo-api for phlo-specific internal APIs</li> </ul> <p>Effort: ~2-3 days</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-1-unified-discovery-flow","title":"Phase 1: Unified Discovery Flow","text":""},{"location":"architecture/goals/implementation-roadmap/#current-problem","title":"Current Problem","text":"<p>Two separate discovery systems:</p> <ul> <li><code>src/phlo/plugins/discovery.py</code> - discovers plugins via entry points</li> <li><code>src/phlo/services/discovery.py</code> - discovers services from plugins</li> </ul> <p>Confusing naming and duplicated logic.</p>"},{"location":"architecture/goals/implementation-roadmap/#solution","title":"Solution","text":"<p>Consolidate into a single flow:</p> <pre><code>src/phlo/discovery/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 plugins.py       # Entry point discovery for all plugin types\n\u251c\u2500\u2500 services.py      # ServiceDefinition loading (from core + plugins)\n\u2514\u2500\u2500 registry.py      # Remote registry search\n</code></pre> <p>Key changes:</p> <ul> <li>Rename <code>plugins/discovery.py</code> \u2192 <code>discovery/plugins.py</code></li> <li>Rename <code>services/discovery.py</code> \u2192 <code>discovery/services.py</code></li> <li>Single import: <code>from phlo.discovery import discover_plugins, discover_services</code></li> </ul> <p>Effort: ~1 day</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-2-naming-cleanup","title":"Phase 2: Naming Cleanup","text":""},{"location":"architecture/goals/implementation-roadmap/#cascade-phlo","title":"\"Cascade\" \u2192 \"Phlo\"","text":"<p>Several files still reference \"Cascade\":</p> <ul> <li><code>src/phlo/cli/plugin.py</code> docstring: \"CLI commands for managing Cascade plugins\"</li> <li>Possibly other locations</li> </ul> <p>Action: Global search/replace and verify.</p> <p>Effort: ~0.5 days</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-3-user-service-overrides","title":"Phase 3: User Service Overrides","text":""},{"location":"architecture/goals/implementation-roadmap/#current-behavior","title":"Current Behavior","text":"<p>Users cannot customize installed service configs via <code>phlo.yaml</code>.</p>"},{"location":"architecture/goals/implementation-roadmap/#proposed-solution","title":"Proposed Solution","text":"<pre><code># phlo.yaml\nname: my-lakehouse\n\nservices:\n  dagster:\n    ports:\n      - \"3005:3000\"\n    environment:\n      CUSTOM_FLAG: \"enabled\"\n    enabled: false # Disable\n\n  observatory:\n    port: 8080\n</code></pre>"},{"location":"architecture/goals/implementation-roadmap/#implementation-changes","title":"Implementation Changes","text":"<p>1. Update <code>config_schema.py</code></p> <p>Add <code>ServiceOverride</code> model:</p> <pre><code>class ServiceOverride(BaseModel):\n    enabled: bool = True\n    ports: list[str] | None = None\n    environment: dict[str, str] | None = None\n    volumes: list[str] | None = None\n    depends_on: list[str] | None = None\n    command: list[str] | str | None = None\n</code></pre> <p>2. Update <code>composer.py</code></p> <p>Apply user overrides in <code>_build_service_config()</code>:</p> <pre><code>def _apply_user_overrides(self, config, overrides):\n    if overrides.ports:\n        config[\"ports\"] = overrides.ports\n    if overrides.environment:\n        config.setdefault(\"environment\", {})\n        config[\"environment\"].update(overrides.environment)\n    # etc.\n</code></pre> <p>3. Update <code>services.py</code> CLI</p> <p>Load and pass overrides to composer.</p> <p>Effort: ~2 days</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-4-enabledisable-services","title":"Phase 4: Enable/Disable Services","text":""},{"location":"architecture/goals/implementation-roadmap/#solution_1","title":"Solution","text":"<pre><code>services:\n  superset:\n    enabled: false\n</code></pre>"},{"location":"architecture/goals/implementation-roadmap/#implementation","title":"Implementation","text":"<p>In <code>ServiceDiscovery.get_default_services()</code>:</p> <pre><code>def get_default_services(self, disabled_services=None):\n    disabled = set(disabled_services or [])\n    return [\n        s for s in self._services.values()\n        if s.default and s.name not in disabled\n    ]\n</code></pre> <p>Effort: ~0.5 days</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-5-inline-custom-services","title":"Phase 5: Inline Custom Services","text":""},{"location":"architecture/goals/implementation-roadmap/#solution_2","title":"Solution","text":"<pre><code>services:\n  custom-api:\n    type: inline\n    image: my-registry/api:latest\n    ports:\n      - \"4000:4000\"\n    depends_on:\n      - trino\n</code></pre>"},{"location":"architecture/goals/implementation-roadmap/#implementation_1","title":"Implementation","text":"<p>Add <code>ServiceDefinition.from_inline()</code> and detect in CLI.</p> <p>Effort: ~1 day</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-6-enhanced-discoverability","title":"Phase 6: Enhanced Discoverability","text":""},{"location":"architecture/goals/implementation-roadmap/#proposed-cli-output","title":"Proposed CLI Output","text":"<pre><code>$ phlo services list\n\nPackage Services (installed):\n  \u2713 observatory      Running    :3001   Phlo UI\n  \u2713 phlo-api         Running    :4000   Phlo API backend\n  \u2713 dagster          Running    :3000   Orchestration\n  \u2713 postgres         Running    :5432   Database\n  \u2717 superset         Disabled           (disabled in phlo.yaml)\n\nCustom Services (phlo.yaml):\n  \u2713 custom-api       Running    :4000   (inline)\n</code></pre> <p>Effort: ~1 day</p>"},{"location":"architecture/goals/implementation-roadmap/#phase-7-documentation-cleanup","title":"Phase 7: Documentation &amp; Cleanup","text":"<ol> <li>Update <code>docs/guides/service-packages.md</code></li> <li>Update README examples</li> <li>Add examples to <code>phlo-examples/github/phlo.yaml</code></li> <li>Remove <code>packages/phlo-fastapi/</code></li> <li>Build/release strategy for Observatory (Docker image in CI)</li> </ol> <p>Effort: ~1 day</p>"},{"location":"architecture/goals/implementation-roadmap/#summary","title":"Summary","text":"Phase Description Effort 0 Package-first Observatory + phlo-api 2-3 days 1 Unified discovery flow 1 day 2 Naming cleanup (Cascade \u2192 Phlo) 0.5 days 3 User service overrides 2 days 4 Enable/disable services 0.5 days 5 Inline custom services 1 day 6 Enhanced discoverability 1 day 7 Documentation &amp; cleanup 1 day Total ~9-10 days"},{"location":"architecture/goals/implementation-roadmap/#registry-clarification","title":"Registry Clarification","text":"<p>The two registries serve different purposes:</p> Source Purpose <code>registry/plugins.json</code> Complete offerings - all available plugins for <code>phlo plugin search</code> Entry points (installed packages) What's actually installed - for service discovery <p>This is correct and intentional. The JSON registry enables discoverability of uninstalled plugins.</p>"},{"location":"architecture/goals/implementation-roadmap/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/goals/implementation-roadmap/#unit-tests","title":"Unit Tests","text":"<ul> <li><code>test_service_discovery_plugins_only()</code> - Service discovery via plugins only</li> <li><code>test_service_override_merge()</code> - Verify config merging</li> <li><code>test_enabled_false_excludes_service()</code> - Verify disable logic</li> <li><code>test_inline_service_creation()</code> - Verify inline parsing</li> </ul>"},{"location":"architecture/goals/implementation-roadmap/#integration-tests","title":"Integration Tests","text":"<ul> <li>Start services with override in phlo.yaml</li> <li>Verify disabled service doesn't start</li> <li>Verify inline service starts correctly</li> <li>Verify Observatory can reach phlo-api</li> </ul>"},{"location":"architecture/goals/implementation-roadmap/#e2e-tests","title":"E2E Tests","text":"<ul> <li>Full workflow: install packages \u2192 override \u2192 start \u2192 verify</li> <li>Observatory shows installed plugins via phlo-api</li> </ul>"},{"location":"architecture/goals/plugin-dx/","title":"Plugin System DX Goals","text":"<p>Vision: A user should be able to compose their perfect data lakehouse by installing Python packages, with zero infrastructure configuration for common use cases and simple overrides for advanced cases.</p>"},{"location":"architecture/goals/plugin-dx/#architecture-core-vs-packages","title":"Architecture: Core vs Packages","text":"<p>Phlo has a clear separation between core (bundled, non-swappable) and packages (installable, swappable):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     PHLO CORE (Glue)                        \u2502\n\u2502  - CLI (phlo services, phlo materialize, etc.)              \u2502\n\u2502  - Decorators (@phlo_ingestion, @phlo_quality)              \u2502\n\u2502  - Config system (phlo.yaml parsing)                        \u2502\n\u2502  - Plugin registry &amp; discovery                              \u2502\n\u2502  - Service composer (docker-compose generation)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502              \u2502              \u2502\n            \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OBSERVATORY  \u2502  \u2502  phlo-api    \u2502  \u2502  PACKAGES (plugins)  \u2502\n\u2502  (Core App)   \u2502  \u2502  (Core API)  \u2502  \u2502                      \u2502\n\u2502               \u2502  \u2502              \u2502  \u2502  phlo-dagster  \u2500\u2500\u2510   \u2502\n\u2502  THE phlo UI  \u2502  \u2502  Exposes     \u2502  \u2502  phlo-airflow \u2500\u2500\u2534\u2500 swap\n\u2502  Bundled      \u2502  \u2502  internals   \u2502  \u2502                      \u2502\n\u2502               \u2502  \u2502  to Obs.     \u2502  \u2502  phlo-postgres \u2500\u2510    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  phlo-mysql \u2500\u2500\u2500\u2500\u2534\u2500 swap\n                                     \u2502                      \u2502\n                                     \u2502  phlo-trino \u2500\u2500\u2500\u2500\u2510    \u2502\n                                     \u2502  phlo-duckdb \u2500\u2500\u2500\u2534\u2500 swap\n                                     \u2502                      \u2502\n                                     \u2502  phlo-minio         \u2502\n                                     \u2502  phlo-nessie        \u2502\n                                     \u2502  phlo-superset      \u2502\n                                     \u2502  phlo-grafana       \u2502\n                                     \u2502  etc.               \u2502\n                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#whats-core-bundled-with-pip-install-phlo","title":"What's Core (Bundled with <code>pip install phlo</code>)","text":"Component Purpose phlo library CLI, decorators, config, plugin system Observatory THE phlo UI \u2014 lineage, data explorer, quality dashboard phlo-api Python backend exposing phlo internals to Observatory <p>Observatory is unique:</p> <ul> <li>It's literally \"the face of phlo\"</li> <li>There's no alternative (unlike Dagster vs Airflow)</li> <li>It should evolve with phlo core</li> <li>Every phlo user gets it</li> </ul>"},{"location":"architecture/goals/plugin-dx/#whats-a-package-swappable","title":"What's a Package (Swappable)","text":"<p>Everything that's a commodity or has alternatives:</p> Package Category Alternatives phlo-dagster Orchestrator phlo-airflow, phlo-prefect phlo-postgres Database phlo-mysql phlo-trino Query Engine phlo-duckdb phlo-nessie Catalog (future: other Iceberg catalogs) phlo-minio Storage phlo-s3 (direct AWS) phlo-superset BI phlo-metabase phlo-grafana Observability - phlo-loki Logs -"},{"location":"architecture/goals/plugin-dx/#the-goal-best-in-class-dx","title":"The Goal: Best-in-Class DX","text":""},{"location":"architecture/goals/plugin-dx/#1-zero-config-by-default","title":"1. Zero-Config by Default","text":"<p>User Story: \"I installed phlo with defaults and it just worked.\"</p> <pre><code># Option A: Explicit package selection\npip install phlo phlo-dagster phlo-postgres phlo-trino phlo-nessie phlo-minio\ncd my-project\nphlo init .\nphlo services start\n\n# Option B: Install with curated defaults\npip install phlo[defaults]  # Includes dagster, postgres, trino, nessie, minio\nphlo init .\nphlo services start\n\n# Either way, Observatory is at :3001 automatically (it's core)\n</code></pre> <p>The plugin system should:</p> <ul> <li>Auto-discover installed packages via entry points</li> <li>Auto-configure connections between services (Observatory connects to Trino, Grafana scrapes Prometheus, etc.)</li> <li>Start with sensible defaults that work for 80% of users</li> <li>Require zero <code>phlo.yaml</code> configuration for basic usage</li> </ul>"},{"location":"architecture/goals/plugin-dx/#2-declarative-composition","title":"2. Declarative Composition","text":"<p>User Story: \"I want to swap Superset for Metabase, and override Observatory's port.\"</p> <pre><code># phlo.yaml - only write what you want to change\nname: my-lakehouse\n\nservices:\n  # Disable an installed package service\n  superset:\n    enabled: false\n\n  # Override settings for core Observatory\n  observatory:\n    port: 8080\n    environment:\n      CUSTOM_FEATURE: \"enabled\"\n\n  # Add a custom service not from a package\n  custom-api:\n    type: inline\n    image: my-registry/custom-api:latest\n    ports:\n      - \"4000:4000\"\n    depends_on:\n      - trino\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#3-the-pre-connected-promise","title":"3. The \"Pre-Connected\" Promise","text":"<p>User Story: \"Services talk to each other without me configuring endpoints.\"</p> <p>When you install packages, they should auto-wire:</p> <ul> <li>Observatory \u2192 Dagster (for assets/lineage)</li> <li>Observatory \u2192 Trino (for queries)</li> <li>Observatory \u2192 Nessie (for branches)</li> <li>Grafana \u2192 Prometheus (for metrics)</li> </ul> <p>This is achieved through:</p> <ol> <li>Standard environment variable conventions (TRINO_URL, NESSIE_URL, etc.)</li> <li>Compose generation that wires services together</li> <li>phlo-api backend that exposes phlo internals to Observatory</li> </ol>"},{"location":"architecture/goals/plugin-dx/#4-first-class-local-development","title":"4. First-Class Local Development","text":"<p>User Story: \"I'm developing a custom service and want it to hot-reload.\"</p> <pre><code># Dev mode mounts source for live changes\nphlo services init --dev --phlo-source /path/to/phlo\nphlo services start\n\n# My custom service code changes are immediately reflected\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#5-simple-extension","title":"5. Simple Extension","text":"<p>User Story: \"I want to create a custom service package for my team.\"</p> <pre><code>phlo plugin create my-cache --type service\n# Creates boilerplate with:\n# - service.yaml (service definition)\n# - Dockerfile\n# - README.md\n# - pyproject.toml with entry points\n</code></pre> <p>Then install it:</p> <pre><code>cd my-cache &amp;&amp; pip install -e .\nphlo services start  # my-cache is now available\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#current-state-analysis","title":"Current State Analysis","text":""},{"location":"architecture/goals/plugin-dx/#what-works-today","title":"What Works Today","text":"<ol> <li>Entry point discovery - Packages register via <code>phlo.plugins.services</code> entry points</li> <li>Service plugin structure - Each package has a <code>service.yaml</code> bundled and a <code>plugin.py</code></li> <li>Compose generation - <code>phlo services init</code> generates docker-compose from discovered services</li> <li>Dev mode - <code>--dev</code> flag applies overrides from <code>service.yaml</code>'s <code>dev</code> section</li> <li>Profile grouping - Services can be grouped into profiles (e.g., <code>observability</code>)</li> </ol>"},{"location":"architecture/goals/plugin-dx/#whats-missing","title":"What's Missing","text":"Gap Impact Proposed Solution Observatory as plugin Awkward DX, can't control phlo Move to core, add phlo-api backend No user overrides Users can't customize installed service configs Allow <code>services:</code> overrides in <code>phlo.yaml</code> No enable/disable Can't easily disable installed services Add <code>enabled: true/false</code> per service Limited env var configuration Only <code>.phlo/.env(.local)</code> files, no per-service env in yaml Support <code>environment:</code> in service overrides No port overrides Must edit bundled service.yaml or use env vars Support <code>ports:</code> in service overrides No custom inline services Can't easily add non-packaged services Support inline service definitions in <code>phlo.yaml</code>"},{"location":"architecture/goals/plugin-dx/#proposed-dx-model","title":"Proposed DX Model","text":""},{"location":"architecture/goals/plugin-dx/#tier-1-zero-config-most-users","title":"Tier 1: Zero-Config (Most Users)","text":"<pre><code>pip install phlo[defaults]\nphlo init my-lakehouse\ncd my-lakehouse\nphlo services start\n# \u2192 All default services running\n# \u2192 Observatory at :3001 (core, always there)\n</code></pre> <p>The generated <code>phlo.yaml</code> is minimal:</p> <pre><code>name: my-lakehouse\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#tier-2-simple-overrides-power-users","title":"Tier 2: Simple Overrides (Power Users)","text":"<pre><code># phlo.yaml\nname: my-lakehouse\n\nservices:\n  observatory:\n    port: 8080\n    environment:\n      DEBUG: \"true\"\n\n  superset:\n    enabled: false # Don't want BI layer\n</code></pre> <p>This merges with the package's <code>service.yaml</code> at compose generation time.</p>"},{"location":"architecture/goals/plugin-dx/#tier-3-custom-services-advanced-users","title":"Tier 3: Custom Services (Advanced Users)","text":"<pre><code># phlo.yaml\nname: my-lakehouse\n\nservices:\n  # Override core Observatory\n  observatory:\n    port: 8080\n\n  # Define a custom service inline\n  custom-api:\n    type: inline\n    image: my-registry/api:latest\n    ports:\n      - \"4000:4000\"\n    environment:\n      TRINO_URL: http://trino:8080\n    depends_on:\n      - trino\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#tier-4-create-publish-packages-contributors","title":"Tier 4: Create &amp; Publish Packages (Contributors)","text":"<pre><code># Create package scaffolding\nphlo plugin create my-service --type service\n\n# Develop locally\ncd my-service\npip install -e .\nphlo services start  # Service is discovered and running\n\n# Publish for others\n# 1. Push to PyPI\n# 2. Optionally register with phlo registry for `phlo plugin install my-service`\n</code></pre>"},{"location":"architecture/goals/plugin-dx/#implementation-principles","title":"Implementation Principles","text":""},{"location":"architecture/goals/plugin-dx/#1-configuration-layering","title":"1. Configuration Layering","text":"<pre><code>Base Layer:        Package's service.yaml (bundled in pip package)\n     \u2193\nUser Override:     phlo.yaml services section\n     \u2193\nEnvironment:       .phlo/.env + .phlo/.env.local + process environment\n     \u2193\nRuntime:           CLI flags (--dev, --profile, etc.)\n</code></pre> <p>Each layer can override values from the previous layer.</p>"},{"location":"architecture/goals/plugin-dx/#2-explicit-is-better-than-implicit","title":"2. Explicit is Better Than Implicit","text":"<ul> <li>A service is started because it's <code>default: true</code> OR explicitly listed</li> <li>Users can see exactly what's happening with <code>phlo services list</code></li> <li>Overrides are visible in <code>phlo.yaml</code>, not hidden in env vars</li> </ul>"},{"location":"architecture/goals/plugin-dx/#3-convention-over-configuration","title":"3. Convention Over Configuration","text":"<ul> <li>Standard environment variables: <code>TRINO_URL</code>, <code>NESSIE_URL</code>, <code>DAGSTER_URL</code>, etc.</li> <li>Standard ports: Each service has a well-known default port</li> <li>Standard container naming: <code>{project}-{service}-1</code></li> </ul>"},{"location":"architecture/goals/plugin-dx/#4-fail-fast-and-clear","title":"4. Fail Fast and Clear","text":"<ul> <li>Validate phlo.yaml against a schema before starting services</li> <li>Clear error messages when services fail to start</li> <li><code>phlo services status</code> shows exactly what's running/failed</li> </ul>"},{"location":"architecture/goals/plugin-dx/#comparison-with-best-in-class-tools","title":"Comparison with Best-in-Class Tools","text":""},{"location":"architecture/goals/plugin-dx/#docker-compose-dx","title":"Docker Compose DX","text":"<pre><code># docker-compose.override.yml merges with docker-compose.yml\nservices:\n  web:\n    environment:\n      DEBUG: \"true\"\n</code></pre> <p>What we adopt: Override files that merge with defaults.</p>"},{"location":"architecture/goals/plugin-dx/#terraform-modules-dx","title":"Terraform Modules DX","text":"<pre><code>module \"database\" {\n  source = \"terraform-aws-modules/rds/aws\"\n\n  # Override defaults\n  instance_class = \"db.t3.large\"\n}\n</code></pre> <p>What we adopt: Packages as \"modules\" with overridable variables.</p>"},{"location":"architecture/goals/plugin-dx/#helm-charts-dx","title":"Helm Charts DX","text":"<pre><code># values.yaml overrides chart defaults\ngrafana:\n  enabled: false\n\nprometheus:\n  resources:\n    requests:\n      memory: \"512Mi\"\n</code></pre> <p>What we adopt: <code>enabled: true/false</code> pattern, deep merge of config.</p>"},{"location":"architecture/goals/plugin-dx/#success-metrics","title":"Success Metrics","text":"<ol> <li>Time to first lakehouse: &lt; 5 minutes from <code>pip install</code> to running services</li> <li>Lines of config for common case: 2 lines (name + description)</li> <li>Discoverability: Users can find available services via <code>phlo plugin search</code></li> <li>Override success rate: 90% of customizations achievable via <code>phlo.yaml</code></li> <li>Error clarity: Every error message includes a suggested fix</li> </ol>"},{"location":"architecture/goals/plugin-dx/#next-steps","title":"Next Steps","text":"<p>See implementation-roadmap.md for the technical plan to achieve these goals.</p>"},{"location":"architecture/specs/0001-alpha3-release-plan/","title":"Phlo 0.1.0-alpha.3 Release Plan","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#overview","title":"Overview","text":"<p>This document outlines the work required to release version 0.1.0-alpha.3, following the 0.1.0-alpha.2 release on December 26, 2025.</p> <p>Target Version: 0.1.0-alpha.3 Timeline: No fixed deadline; quality over speed Package Versioning: Fully independent (each package manages its own version)</p>"},{"location":"architecture/specs/0001-alpha3-release-plan/#1-release-infrastructure","title":"1. Release Infrastructure","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#11-fix-release-please-prerelease-versioning","title":"1.1 Fix release-please Prerelease Versioning","text":"<p>Priority: Critical (blocking release) Status: Needs investigation and fix</p> <p>The release-please workflow is not correctly incrementing alpha versions (e.g., alpha.2 -&gt; alpha.3). The current configuration uses:</p> <pre><code>release-type: python\nprerelease: true\nprerelease-type: alpha\n</code></pre> <p>Tasks:</p> <ul> <li>[ ] Investigate release-please v4 prerelease behavior with Python release type</li> <li>[ ] Consider switching to manifest-based configuration (<code>.release-please-manifest.json</code>) for finer control</li> <li>[ ] Test the fix locally before merging</li> <li>[ ] Document the working configuration</li> </ul> <p>Files:</p> <ul> <li><code>.github/workflows/release-please.yml</code></li> <li>Potentially add: <code>.release-please-manifest.json</code>, <code>release-please-config.json</code></li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#12-selective-package-publishing","title":"1.2 Selective Package Publishing","text":"<p>Priority: High Status: Needs implementation</p> <p>The current <code>publish.yml</code> only builds the main <code>phlo</code> package. With 23+ workspace packages that version independently, we need selective publishing.</p> <p>Tasks:</p> <ul> <li>[ ] Modify publish workflow to detect which packages have changes</li> <li>[ ] Build and publish only changed packages</li> <li>[ ] Handle package dependency ordering (if phlo-core changes, dependents may need rebuild)</li> <li>[ ] Add dry-run mode for testing</li> </ul> <p>Files:</p> <ul> <li><code>.github/workflows/publish.yml</code></li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#2-testing-and-quality","title":"2. Testing and Quality","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#21-fix-test-warnings","title":"2.1 Fix Test Warnings","text":"<p>Priority: High Status: Ready to implement</p> <p>Task A: Remove pytest asyncio config warning</p> <ul> <li>[ ] Remove or update <code>asyncio_default_fixture_loop_scope</code> in <code>pyproject.toml</code></li> </ul> <p>Task B: Suppress pyiceberg Pydantic deprecation warnings</p> <ul> <li>[ ] Add pytest warning filter for pyiceberg's Pydantic V2.12 deprecations</li> <li>[ ] Document that this is an upstream issue to track</li> </ul> <p>Files:</p> <ul> <li><code>pyproject.toml</code> (pytest.ini_options section)</li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#22-auto-configure-testing","title":"2.2 Auto-Configure Testing","text":"<p>Priority: High Status: Needs testing</p> <p>The auto-configure feature (#119) needs validation across all core services.</p> <p>Tasks:</p> <ul> <li>[ ] Manual smoke test: Fresh install, run through quickstart</li> <li>[ ] Add automated integration tests for service configuration</li> <li>[ ] Docker Compose full-stack test</li> <li>[ ] Document any discovered edge cases</li> </ul> <p>Services to test:</p> <ul> <li>Dagster</li> <li>Postgres</li> <li>MinIO</li> <li>Nessie</li> <li>Trino</li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#3-observability","title":"3. Observability","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#31-iceberg-table-maintenance-monitoring","title":"3.1 Iceberg Table Maintenance Monitoring","text":"<p>Priority: Medium Status: Needs implementation</p> <p>Add comprehensive observability for Iceberg table maintenance operations.</p> <p>Tasks:</p> <ul> <li>[ ] Add structured logging for maintenance operations (start/end/duration)</li> <li>[ ] Export Prometheus metrics for maintenance operations</li> <li>[ ] Surface maintenance status in Observatory UI</li> <li>[ ] Add error alerting for failed maintenance</li> </ul> <p>Files:</p> <ul> <li><code>packages/phlo-iceberg/</code></li> <li><code>packages/phlo-metrics/</code></li> <li><code>packages/phlo-observatory/</code></li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#4-documentation","title":"4. Documentation","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#41-full-documentation-audit","title":"4.1 Full Documentation Audit","text":"<p>Priority: High Status: Needs review</p> <p>The README and other docs reference outdated patterns (basedpyright instead of ty, removed examples directory, etc.).</p> <p>Tasks:</p> <ul> <li>[ ] Update README.md: Replace <code>basedpyright src/</code> with <code>ty check src/phlo</code></li> <li>[ ] Remove references to examples/ directory (moved to separate repo)</li> <li>[ ] Verify all doc links are valid</li> <li>[ ] Update developer guide with current tooling</li> <li>[ ] Review and update CLI reference for any changed commands</li> <li>[ ] Update configuration reference for new features (auto-configure, hooks, security)</li> </ul> <p>Files to audit:</p> <ul> <li><code>README.md</code></li> <li><code>docs/getting-started/installation.md</code></li> <li><code>docs/getting-started/quickstart.md</code></li> <li><code>docs/guides/developer-guide.md</code></li> <li><code>docs/reference/cli-reference.md</code></li> <li><code>docs/reference/configuration-reference.md</code></li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#42-blog-series-rewrite","title":"4.2 Blog Series Rewrite","text":"<p>Priority: Medium Status: Needs full rewrite</p> <p>The 13-part blog series references deprecated patterns (DuckLake, old contracts module, etc.).</p> <p>Tasks:</p> <ul> <li>[ ] Audit all blog posts for outdated content</li> <li>[ ] Rewrite posts to match current architecture</li> <li>[ ] Update code examples to use current APIs</li> <li>[ ] Consider consolidating posts if content overlaps</li> </ul> <p>Files:</p> <ul> <li><code>docs/blog/*.md</code></li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#43-testing-guide-documentation","title":"4.3 Testing Guide Documentation","text":"<p>Priority: Medium Status: Needs creation</p> <p>The phlo-testing package references a <code>docs/TESTING_GUIDE.md</code> that needs to be created.</p> <p>Tasks:</p> <ul> <li>[ ] Create <code>docs/TESTING_GUIDE.md</code> with comprehensive testing patterns</li> <li>[ ] Reference phlo-testing package in main documentation</li> <li>[ ] Add examples for common testing scenarios</li> <li>[ ] Document local test mode usage</li> </ul> <p>Files:</p> <ul> <li>Create: <code>docs/TESTING_GUIDE.md</code></li> <li>Update: <code>docs/guides/developer-guide.md</code> (add testing section)</li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#44-security-documentation","title":"4.4 Security Documentation","text":"<p>Priority: Low Status: Needs creation</p> <p>The enterprise security configuration feature (#117) should be documented as experimental.</p> <p>Tasks:</p> <ul> <li>[ ] Create security configuration documentation</li> <li>[ ] Mark features as experimental with appropriate warnings</li> <li>[ ] Document available security options</li> </ul> <p>Files:</p> <ul> <li>Create: <code>docs/reference/security-configuration.md</code></li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#5-observatory","title":"5. Observatory","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#51-observatory-installation-experience","title":"5.1 Observatory Installation Experience","text":"<p>Priority: High Status: Needs verification</p> <p>Users should be able to start Observatory with:</p> <ol> <li><code>phlo observatory start</code> - dedicated command</li> <li><code>phlo services start</code> - included in services stack</li> </ol> <p>Tasks:</p> <ul> <li>[ ] Verify <code>phlo observatory start</code> command works</li> <li>[ ] Verify Observatory is included in <code>phlo services start</code></li> <li>[ ] Document the installation experience</li> <li>[ ] Test with fresh install</li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#6-quality-checks","title":"6. Quality Checks","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#61-reconciliation-checks-review","title":"6.1 Reconciliation Checks Review","text":"<p>Priority: Low Status: Needs evaluation</p> <p>Review the reconciliation checks in phlo-quality for edge cases or missing check types.</p> <p>Tasks:</p> <ul> <li>[ ] Audit existing check types</li> <li>[ ] Identify any missing common use cases</li> <li>[ ] Document available check types</li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#work-item-summary","title":"Work Item Summary","text":""},{"location":"architecture/specs/0001-alpha3-release-plan/#critical-blocking-release","title":"Critical (Blocking Release)","text":"<ol> <li>Fix release-please prerelease versioning</li> </ol>"},{"location":"architecture/specs/0001-alpha3-release-plan/#high-priority","title":"High Priority","text":"<ol> <li>Fix pytest warnings (asyncio config, pyiceberg suppression)</li> <li>Test auto-configure across all services</li> <li>Full documentation audit</li> <li>Observatory installation verification</li> <li>Selective package publishing</li> </ol>"},{"location":"architecture/specs/0001-alpha3-release-plan/#medium-priority","title":"Medium Priority","text":"<ol> <li>Iceberg maintenance monitoring (logging, metrics, UI)</li> <li>Blog series rewrite</li> <li>Testing guide documentation</li> </ol>"},{"location":"architecture/specs/0001-alpha3-release-plan/#low-priority","title":"Low Priority","text":"<ol> <li>Security documentation (mark as experimental)</li> <li>Reconciliation checks review</li> </ol>"},{"location":"architecture/specs/0001-alpha3-release-plan/#verification-checklist","title":"Verification Checklist","text":"<p>Before tagging alpha.3:</p> <ul> <li>[ ] All unit tests passing (583+ tests)</li> <li>[ ] No pytest warnings in output</li> <li>[ ] <code>ty check src/phlo</code> passes</li> <li>[ ] <code>ruff check src/</code> passes</li> <li>[ ] Fresh install smoke test succeeds</li> <li>[ ] <code>phlo services start</code> brings up all services</li> <li>[ ] <code>phlo observatory start</code> opens Observatory UI</li> <li>[ ] Documentation links are valid</li> <li>[ ] Release-please creates correct version bump</li> <li>[ ] PyPI publish workflow succeeds (dry run)</li> </ul>"},{"location":"architecture/specs/0001-alpha3-release-plan/#post-release","title":"Post-Release","text":"<p>After alpha.3 is released:</p> <ol> <li>Monitor PyPI download metrics</li> <li>Track any user-reported issues</li> <li>Begin planning for beta.1 (feature freeze criteria)</li> </ol>"},{"location":"architecture/specs/0002-centralized-logging-layer/","title":"Centralized Logging Layer Spec","text":""},{"location":"architecture/specs/0002-centralized-logging-layer/#overview","title":"Overview","text":"<p>Phlo currently emits logs in multiple ways (stdlib logging, Dagster <code>context.log</code>, ad-hoc telemetry events, and Pino in Observatory). This spec defines a centralized logging layer for Python packages that standardizes log shape, routes every log through a middle layer, and supports pluggable observability sinks (Loki/Alloy by default, with optional hook-based sinks).</p> <p>The design keeps logs visible even when optional packages are not installed and ensures packages can import either a setup function or a logger instance.</p>"},{"location":"architecture/specs/0002-centralized-logging-layer/#goals","title":"Goals","text":"<ul> <li>Provide a single, consistent logging API across Python packages.</li> <li>Standardize log schema and correlation fields across services.</li> <li>Route every log through a middle layer that can fan out to multiple sinks.</li> <li>Keep default behavior simple: JSON to stdout for Alloy/Loki collection.</li> <li>Enable optional sinks via hook plugins/packages.</li> <li>Keep compatibility with Dagster <code>context.log</code> and stdlib <code>logging</code>.</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#non-goals","title":"Non-Goals","text":"<ul> <li>Replacing the existing Loki/Grafana stack or its UI.</li> <li>Full distributed tracing (future work; allow trace/span fields).</li> <li>Rewriting Observatory logging (align fields only).</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#current-state","title":"Current State","text":"<ul> <li>Python packages use stdlib <code>logging.getLogger()</code> or Dagster <code>context.log</code>.</li> <li><code>src/phlo/logging.py</code> provides Dagster helpers but is not widely used.</li> <li><code>phlo-metrics</code> records hook-based telemetry events to JSONL.</li> <li><code>phlo-alerting</code> reacts to telemetry events.</li> <li>Observatory (TypeScript) uses Pino and logs to stdout.</li> </ul> <p>Gaps: no centralized configuration, inconsistent fields, no shared routing layer, no unified integration for external observability tools via plugins.</p>"},{"location":"architecture/specs/0002-centralized-logging-layer/#proposed-architecture","title":"Proposed Architecture","text":""},{"location":"architecture/specs/0002-centralized-logging-layer/#1-core-logging-api-phlo-core","title":"1. Core Logging API (phlo core)","text":"<p>Create a core logging module that packages can import:</p> <ul> <li><code>setup_logging(settings: LoggingSettings | None = None) -&gt; None</code></li> <li><code>get_logger(name: str | None = None) -&gt; BoundLogger</code></li> <li><code>bind_context(**fields) -&gt; None</code> / <code>clear_context() -&gt; None</code></li> <li><code>dagster_logger(context) -&gt; BoundLogger</code> (binds correlation fields)</li> </ul> <p>Example usage in packages:</p> <pre><code>from phlo.logging import setup_logging, get_logger\n\nsetup_logging()\nlogger = get_logger(__name__)\nlogger.info(\"Service started\", port=8080)\n</code></pre>"},{"location":"architecture/specs/0002-centralized-logging-layer/#2-standard-log-schema","title":"2. Standard Log Schema","text":"<p>Every log entry must include these core fields:</p> <ul> <li><code>timestamp</code> (ISO-8601 UTC)</li> <li><code>level</code> (debug/info/warning/error/critical)</li> <li><code>message</code></li> <li><code>logger</code> (module/logger name)</li> <li><code>service</code> (phlo component name, e.g., <code>phlo-alerting</code>)</li> </ul> <p>Correlation fields (from ADR-0028):</p> <ul> <li><code>run_id</code></li> <li><code>asset_key</code></li> <li><code>job_name</code></li> <li><code>partition_key</code></li> <li><code>check_name</code></li> </ul> <p>Optional fields:</p> <ul> <li><code>event</code> (short machine-readable name)</li> <li><code>tags</code> (string map)</li> <li><code>exception</code> (serialized traceback)</li> <li><code>trace_id</code> / <code>span_id</code> (future tracing)</li> <li><code>process</code> / <code>thread</code> / <code>module</code> / <code>function</code> / <code>line</code></li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#3-middle-layer-log-router","title":"3. Middle Layer (Log Router)","text":"<p>Introduce a log router handler that receives every log record and forwards a normalized <code>LogEvent</code> to the hook bus:</p> <pre><code>stdlib logging / structlog\n          \u2502\n          \u25bc\n  Phlo Log Router (logging.Handler)\n          \u2502\n          \u25bc\n      HookBus\n          \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc     \u25bc          \u25bc\n stdout  metrics   custom\n (JSON)  plugin    plugins\n</code></pre> <p>Implementation details:</p> <ul> <li>Add <code>LogEvent</code> to <code>phlo.hooks.events</code> with <code>event_type=\"log.record\"</code>.</li> <li>Add a <code>LogRouterHandler</code> that transforms <code>LogRecord</code> -&gt; <code>LogEvent</code>.</li> <li>Use HookBus to dispatch to installed sink plugins.</li> <li>Keep stdout JSON as the default sink for Alloy/Loki.</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#4-logging-backend-choice","title":"4. Logging Backend Choice","text":"<p>Use <code>structlog</code> on top of stdlib <code>logging</code>:</p> <ul> <li>Keeps compatibility with Dagster and third-party libraries.</li> <li>Supports structured JSON output with processors.</li> <li>Allows a single handler pipeline for routing.</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#5-hook-based-log-sinks","title":"5. Hook-Based Log Sinks","text":"<p>Allow optional packages to register HookBus handlers for <code>log.record</code> events. These sinks can forward logs to external tools (APMs, alerting systems, or custom storage) without changing core logging code.</p>"},{"location":"architecture/specs/0002-centralized-logging-layer/#6-configuration","title":"6. Configuration","text":"<p>Add logging settings to <code>phlo.config.settings</code>:</p> <ul> <li><code>phlo_log_level</code> (default: INFO)</li> <li><code>phlo_log_format</code> (json | console)</li> <li><code>phlo_log_router_enabled</code> (default: true)</li> <li><code>phlo_log_service_name</code> (default: phlo)</li> <li><code>phlo_log_file_template</code> (default: .phlo/logs/{YMD}.log, empty disables)</li> </ul> <p>Supported template placeholders (UTC):</p> <ul> <li><code>{YMD}</code> / <code>{YM}</code> / <code>{Y}</code> / <code>{YYYY}</code></li> <li><code>{M}</code> / <code>{MM}</code> / <code>{D}</code> / <code>{DD}</code></li> <li><code>{H}</code> / <code>{HM}</code> / <code>{HMS}</code></li> <li><code>{DATE}</code> (YYYY-MM-DD) / <code>{TIMESTAMP}</code> (YYYYMMDDHHMMSS)</li> </ul> <p>Allow overrides via env vars and <code>.phlo/.env(.local)</code>.</p>"},{"location":"architecture/specs/0002-centralized-logging-layer/#7-dagster-integration","title":"7. Dagster Integration","text":"<ul> <li>Provide <code>dagster_logger(context)</code> that binds correlation fields from context.</li> <li>Ensure Dagster <code>context.log</code> output is still captured by the log router.</li> <li>Replace ad-hoc <code>context.log</code> usage with <code>dagster_logger</code> where possible.</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#8-typescript-alignment-observatory","title":"8. TypeScript Alignment (Observatory)","text":"<ul> <li>Keep Pino, but align fields to the same schema (<code>service</code>, <code>event</code>,   correlation keys) so logs stay consistent across languages.</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#rollout-plan","title":"Rollout Plan","text":"<ol> <li>Implement core <code>phlo.logging</code> API + <code>LogRouterHandler</code> + <code>LogEvent</code>.</li> <li>Wire <code>setup_logging()</code> into CLI entrypoints and services.</li> <li>Update packages to use <code>get_logger()</code> or <code>setup_logging()</code>.</li> <li>Update docs and examples.</li> </ol>"},{"location":"architecture/specs/0002-centralized-logging-layer/#migration-compatibility","title":"Migration &amp; Compatibility","text":"<ul> <li>Continue to honor stdlib <code>logging</code> usage; logs will still flow through router.</li> <li>Existing telemetry log events remain unchanged; they are separate from   <code>log.record</code> but can be correlated via shared fields.</li> <li>Gradually migrate package code to <code>phlo.logging</code> helpers.</li> </ul>"},{"location":"architecture/specs/0002-centralized-logging-layer/#open-questions","title":"Open Questions","text":"<ul> <li>Should <code>log.record</code> be emitted for every log level, or only INFO+ by default?</li> <li>Do we want to require JSON logs in all environments or allow console output for dev?</li> <li>Should file logging default to on in all environments, or only in local dev?</li> </ul>"},{"location":"blog/","title":"Phlo Blog Series: Modern Data Engineering","text":"<p>A comprehensive, hands-on guide to building a production-ready data lakehouse using Phlo.</p> <p>This 12-part series walks through:</p> <ul> <li>Fundamental concepts of data lakehouses</li> <li>Setting up Phlo step-by-step</li> <li>Understanding Apache Iceberg and Project Nessie</li> <li>Data ingestion patterns</li> <li>SQL transformations with dbt</li> <li>Orchestration with Dagster</li> <li>Data quality and testing</li> <li>Real-world end-to-end example</li> <li>Metadata and governance</li> <li>Observability and monitoring</li> <li>Production deployment</li> </ul> <p>Each post includes:</p> <ul> <li>Clear explanations of concepts</li> <li>Code examples you can run</li> <li>Diagrams and visualizations</li> <li>Hands-on exercises</li> <li>References to actual Phlo code</li> </ul>"},{"location":"blog/#5-minute-quick-start","title":"5-Minute Quick Start","text":"<p>Want to see it in action immediately? Run this:</p> <pre><code># Step 1: Install core Phlo framework\nuv pip install phlo\n\n# Step 2: Add the services you need (modular)\nuv add phlo-dagster phlo-postgres phlo-trino phlo-nessie phlo-minio\n\n# Or install with defaults extra (all core services at once)\n# uv pip install phlo[defaults]\n\n# Step 3: Initialize and configure\nphlo init my-lakehouse\ncd my-lakehouse\n\n# Step 4: Start services\nphlo services start\n\n# Wait 2-3 minutes for services to start, then visit:\n# - Dagster UI: http://localhost:10006\n# - MinIO Console: http://localhost:10002\n# - Observatory: http://localhost:3001\n\n# Step 5: Materialize your first data pipeline\nphlo materialize --select \"dlt_glucose_entries+\"\n</code></pre>"},{"location":"blog/#adding-more-services-later","title":"Adding More Services Later","text":"<pre><code># Add observability stack\nuv add phlo-prometheus phlo-grafana phlo-loki\nphlo services start --profile observability\n\n# Add API layer\nuv add phlo-api phlo-hasura\nphlo services start --profile api\n\n# Add data catalog\nuv add phlo-openmetadata\nphlo services start --profile catalog\n</code></pre> <p>For detailed setup instructions, see Part 2: Getting Started.</p>"},{"location":"blog/#prerequisites","title":"Prerequisites","text":"<p>System Requirements:</p> <ul> <li>Minimum: 4 GB RAM, Docker, 10 GB disk space</li> <li>Comfortable: 8 GB RAM, 2+ CPU cores</li> <li>Optimal: 16+ GB RAM, 4+ CPU cores, SSD</li> </ul> <p>Skills:</p> <ul> <li>Required: Basic SQL knowledge, command line comfort</li> <li>Helpful: Python familiarity, Docker basics</li> <li>Optional: Data engineering experience (we'll teach you!)</li> </ul> <p>Software:</p> <ul> <li>Docker &amp; Docker Compose (required)</li> <li>Git (required)</li> <li>Python 3.11+ with uv (optional, for local development)</li> <li>A code editor (VSCode, PyCharm, etc.)</li> </ul> <p>Don't worry if you're missing some skills - the series is designed to teach you as you go!</p>"},{"location":"blog/#blog-posts","title":"Blog Posts","text":"# Title Topics Time 1 What is a Data Lakehouse? Architecture, Iceberg, Nessie, overview 15 min 2 Getting Started\u2014Setup Guide Installation, services, first pipeline 50 min 3 Apache Iceberg\u2014Table Format Snapshots, schema evolution, time travel 20 min 4 Project Nessie\u2014Git for Data Branching, versioning, governance 15 min 5 Data Ingestion Patterns DLT, PyIceberg, merge strategies, validation 22 min 6 dbt Transformations Models, testing, layers, best practices 22 min 7 Dagster Orchestration Assets, partitions, scheduling, monitoring 20 min 8 Real-World Example Complete glucose pipeline, end-to-end 25 min 9 Data Quality with Pandera Schemas, validation, asset checks 20 min 10 Metadata and Governance OpenMetadata, data contracts, schema evolution 25 min 11 Observability and Monitoring Metrics, alerting, lineage, debugging 25 min 12 Production Deployment Infrastructure config, Kubernetes, HA, scaling 35 min 13 Extending Phlo with Plugins Custom sources, quality checks, transforms 20 min <p>Total content: ~8,000 lines, 275+ KB of educational material Estimated reading time: 4.5-6 hours (complete series)</p>"},{"location":"blog/#phlo-architecture","title":"Phlo Architecture","text":"<p>Here's how all the pieces fit together:</p> <pre><code>graph TB\n    subgraph \"Data Sources\"\n        API[APIs&lt;br/&gt;Nightscout, GitHub]\n        DB[(Databases&lt;br/&gt;PostgreSQL, MySQL)]\n        Files[Files&lt;br/&gt;CSV, JSON, Parquet]\n    end\n\n    subgraph \"Orchestration Layer\"\n        Dagster[Dagster&lt;br/&gt;Workflow Engine]\n    end\n\n    subgraph \"Ingestion Layer\"\n        DLT[DLT&lt;br/&gt;Data Load Tool]\n        PyIceberg[PyIceberg&lt;br/&gt;Python Client]\n        Pandera[Pandera&lt;br/&gt;Validation]\n    end\n\n    subgraph \"Storage Layer\"\n        MinIO[MinIO&lt;br/&gt;S3-Compatible Object Storage]\n        Nessie[Nessie&lt;br/&gt;Catalog &amp; Versioning]\n\n        subgraph \"Iceberg Tables\"\n            Raw[raw&lt;br/&gt;Landing Zone]\n            Bronze[bronze&lt;br/&gt;Cleaned &amp; Typed]\n            Silver[silver&lt;br/&gt;Business Logic]\n            Gold[gold&lt;br/&gt;Aggregated Metrics]\n        end\n    end\n\n    subgraph \"Transformation Layer\"\n        dbt[dbt&lt;br/&gt;SQL Transformations]\n        Trino[Trino&lt;br/&gt;Query Engine]\n    end\n\n    subgraph \"Publishing Layer\"\n        Postgres[(PostgreSQL&lt;br/&gt;BI-Ready Marts)]\n    end\n\n    subgraph \"Presentation Layer\"\n        Superset[Superset&lt;br/&gt;Dashboards &amp; BI]\n        Notebooks[Jupyter&lt;br/&gt;Analysis]\n    end\n\n    subgraph \"Observability\"\n        OpenMetadata[OpenMetadata&lt;br/&gt;Data Catalog]\n        Logs[Logs &amp; Metrics&lt;br/&gt;Monitoring]\n    end\n\n    API --&gt; DLT\n    DB --&gt; DLT\n    Files --&gt; DLT\n\n    Dagster --&gt; DLT\n    Dagster --&gt; PyIceberg\n    Dagster --&gt; dbt\n    Dagster --&gt; Pandera\n\n    DLT --&gt; PyIceberg\n    PyIceberg --&gt; MinIO\n    PyIceberg --&gt; Nessie\n    Pandera --&gt; DLT\n\n    Nessie --&gt; Raw\n    Raw --&gt; Bronze\n    Bronze --&gt; Silver\n    Silver --&gt; Gold\n\n    dbt --&gt; Trino\n    Trino --&gt; Nessie\n    Trino --&gt; MinIO\n\n    Gold --&gt; Postgres\n    Postgres --&gt; Superset\n    MinIO --&gt; Notebooks\n\n    Trino --&gt; OpenMetadata\n    dbt --&gt; OpenMetadata\n    Dagster --&gt; Logs\n\n    style Dagster fill:#4A90E2\n    style Trino fill:#DD00A1\n    style MinIO fill:#C72C48\n    style dbt fill:#FF694B\n    style Nessie fill:#FFAA00\n</code></pre> <p>Key Components:</p> <ul> <li>Dagster: Orchestrates everything, schedules pipelines, monitors assets</li> <li>DLT + PyIceberg: Two-step ingestion pattern (stage \u2192 merge)</li> <li>MinIO + Nessie: S3-compatible storage with Git-like versioning</li> <li>Apache Iceberg: Open table format with ACID, time travel, schema evolution</li> <li>dbt: SQL transformations with testing and documentation</li> <li>Trino: Distributed query engine for analytics</li> <li>OpenMetadata: Data catalog for discovery and lineage</li> </ul>"},{"location":"blog/#learning-paths","title":"Learning Paths","text":""},{"location":"blog/#path-1-complete-beginner","title":"Path 1: Complete Beginner","text":"<p>Goal: Understand data lakehouses from scratch</p> <ol> <li>Read Part 1 (architecture concepts)</li> <li>Follow Part 2 setup (hands-on)</li> <li>Skim Part 8 (see it working)</li> <li>Deep dive Parts 3-7 (technical details)</li> <li>Skim Part 9 (why validation matters)</li> </ol> <p>Time: 2-3 hours reading + setup</p>"},{"location":"blog/#path-2-data-engineer","title":"Path 2: Data Engineer","text":"<p>Goal: Learn new tools and patterns</p> <ol> <li>Skim Part 1 (context)</li> <li>Part 2 setup</li> <li>Part 3 (Iceberg deep dive)</li> <li>Part 4 (Nessie concepts)</li> <li>Parts 5-7 (implementation)</li> <li>Part 8 (patterns)</li> <li>Part 9 (validation)</li> <li>Part 11 (monitoring)</li> </ol> <p>Time: 3 hours reading + hands-on</p>"},{"location":"blog/#path-3-architectdecision-maker","title":"Path 3: Architect/Decision Maker","text":"<p>Goal: Evaluate Phlo for your organization</p> <ol> <li>Part 1 (comparison to alternatives)</li> <li>Part 3 (open standards benefits)</li> <li>Part 4 (governance story)</li> <li>Part 8 (production patterns)</li> <li>Part 10 (governance and compliance)</li> <li>Part 12 (scaling and HA)</li> <li>Architecture Guide (deployment options)</li> </ol> <p>Time: 2 hours reading</p>"},{"location":"blog/#key-topics-by-post","title":"Key Topics by Post","text":""},{"location":"blog/#data-architecture","title":"Data Architecture","text":"<ul> <li>Part 1: Overview of lakehouse concept</li> <li>Part 3: Table format innovation</li> <li>Part 4: Git-like data versioning</li> </ul>"},{"location":"blog/#getting-started","title":"Getting Started","text":"<ul> <li>Part 2: Installation and first run</li> </ul>"},{"location":"blog/#technical-implementation","title":"Technical Implementation","text":"<ul> <li>Part 5: Ingestion patterns with DLT and PyIceberg</li> <li>Part 6: SQL transformations and testing</li> <li>Part 7: Scheduling and asset management</li> <li>Part 8: Complete end-to-end pipeline</li> </ul>"},{"location":"blog/#data-quality-and-governance","title":"Data Quality and Governance","text":"<ul> <li>Part 9: Schema validation and the <code>@phlo_quality</code> decorator</li> <li>Part 10: Data catalog, contracts, and schema evolution</li> <li>Part 11: Metrics, alerting, lineage, and debugging</li> <li>Part 12: Scaling and high availability</li> </ul>"},{"location":"blog/#extensibility","title":"Extensibility","text":"<ul> <li>Part 13: Building custom sources, quality checks, and transforms</li> </ul>"},{"location":"blog/#tools-reference","title":"Tools Reference","text":"<p>Each tool has dedicated coverage:</p> Tool Posts Key Topics Apache Iceberg 1, 3, 5, 8 Snapshots, time travel, schema evolution Project Nessie 1, 4, 8 Branching, merging, versioning DLT 5, 8 Data staging, schema normalization PyIceberg 5, 8 Iceberg operations, table management dbt 1, 6, 8 Transformations, testing, documentation Dagster 1, 7, 8 Orchestration, partitions, scheduling Trino 1, 3, 5, 6 Query engine, Iceberg integration MinIO 1, 2 Object storage setup and config Postgres 2, 6, 8 Marts, metadata, BI integration Superset 2, 8 Dashboarding, visualization Pandera 5, 9 Schema validation, data quality OpenMetadata 10 Data catalog, lineage, governance"},{"location":"blog/#hands-on-sections","title":"Hands-On Sections","text":"<p>These posts include runnable code and exercises:</p> <ul> <li>Part 2: Service access and first pipeline</li> <li>Part 3: Explore snapshots</li> <li>Part 4: Query different branches</li> <li>Part 5: Trace an ingestion</li> <li>Part 6: Run dbt transforms</li> <li>Part 7: Materialize assets</li> <li>Part 8: Complete pipeline walk-through</li> <li>Part 9: Set up Pandera validation</li> <li>Part 10: Query OpenMetadata catalog</li> <li>Part 11: Create monitoring dashboards</li> <li>Part 12: Deploy to Kubernetes</li> </ul>"},{"location":"blog/#common-questions-answered","title":"Common Questions Answered","text":"<ul> <li>\"Do I need to understand Iceberg?\" \u2192 Yes, Part 3 explains why it matters and how it's different</li> <li>\"Can I skip dbt and use Python instead?\" \u2192 You could, but Part 6 explains why SQL via dbt is better</li> <li>\"How does this compare to Snowflake/Databricks?\" \u2192 Part 1 compares. Key: open-source, no vendor lock-in</li> <li>\"Is this production-ready?\" \u2192 Yes, see Part 12 and Architecture Guide</li> <li>\"Can I ingest from databases, not just APIs?\" \u2192 Yes, Part 5 covers the pattern</li> <li>\"How do I ensure data quality?\" \u2192 Part 9 covers three validation layers</li> <li>\"How do I discover and catalog my data?\" \u2192 Part 10 covers OpenMetadata integration</li> <li>\"Is my pipeline actually running?\" \u2192 Part 11 covers monitoring and alerts</li> <li>\"How do I scale to millions of rows?\" \u2192 Part 12 covers scaling strategies</li> </ul>"},{"location":"blog/#what-youll-build","title":"What You'll Build","text":"<p>By the end of this series, you'll have:</p> <ul> <li>A running lakehouse with Iceberg tables</li> <li>Data flowing from an API to dashboards</li> <li>Transformations in multiple layers (bronze/silver/gold)</li> <li>Data quality checks</li> <li>Complete audit trail via versioning</li> <li>Operational monitoring and alerts</li> </ul> <p>Plus, knowledge to:</p> <ul> <li>Add more data sources</li> <li>Scale to millions of rows</li> <li>Deploy to production</li> <li>Implement advanced analytics</li> </ul>"},{"location":"blog/#hands-on-requirements","title":"Hands-On Requirements","text":"<p>To follow along:</p> <ul> <li>Minimum: 4 GB RAM, Docker, 10 GB disk space</li> <li>Comfortable: 8 GB RAM, local development tools (uv, git)</li> <li>Optimal: 16+ GB RAM, multiple CPU cores, SSD</li> </ul> <p>Most sections can run on minimum spec, just slower.</p>"},{"location":"blog/#next-steps-after-this-series","title":"Next Steps After This Series","text":"<ol> <li>Extend the example: Add more data sources (GitHub, Fitbit, weather)</li> <li>Advanced analytics: Anomaly detection, forecasting</li> <li>Production deployment: Kubernetes, cloud migration</li> <li>Compliance: Data governance, PII handling, retention policies</li> <li>ML integration: Train models on Phlo data</li> </ol> <p>See the main docs for references and advanced guides.</p>"},{"location":"blog/#contributing","title":"Contributing","text":"<p>Find issues in these posts? Want to add examples?</p> <ul> <li>Check AGENTS.md for contribution guidelines</li> <li>Issues and corrections are welcome</li> <li>Real-world examples especially appreciated</li> </ul>"},{"location":"blog/#resources","title":"Resources","text":"<ul> <li>Iceberg Docs: https://iceberg.apache.org/docs/</li> <li>Nessie Docs: https://projectnessie.org/docs/</li> <li>dbt Docs: https://docs.getdbt.com/</li> <li>Dagster Docs: https://docs.dagster.io/</li> <li>Trino Docs: https://trino.io/docs/current/</li> <li>OpenMetadata Docs: https://docs.open-metadata.org/</li> </ul>"},{"location":"blog/#license","title":"License","text":"<p>This blog series is part of Phlo, MIT License.</p> <p>Ready to start? Begin with Part 1: What is a Data Lakehouse?</p>"},{"location":"blog/01-intro-data-lakehouse/","title":"Part 1: What is a Data Lakehouse? Understanding Modern Data Architecture","text":""},{"location":"blog/01-intro-data-lakehouse/#the-problem-were-solving","title":"The Problem We're Solving","text":"<p>Traditional data pipelines have a fundamental problem: they force you to choose.</p> <p>Either you have:</p> <ul> <li>A Data Lake: cheap, flexible storage but chaotic and hard to query</li> <li>A Data Warehouse: organized, fast queries but rigid and expensive</li> </ul> <p>Phlo solves this by combining the best of both worlds into a lakehouse.</p>"},{"location":"blog/01-intro-data-lakehouse/#the-three-eras-of-data-architecture","title":"The Three Eras of Data Architecture","text":""},{"location":"blog/01-intro-data-lakehouse/#era-1-the-data-warehouse-1990s-2010s","title":"Era 1: The Data Warehouse (1990s-2010s)","text":"<ul> <li>Structured SQL queries</li> <li>Fast analytics</li> <li>Problem: Expensive, rigid schema, can't scale easily</li> </ul>"},{"location":"blog/01-intro-data-lakehouse/#era-2-the-data-lake-2010s-2020s","title":"Era 2: The Data Lake (2010s-2020s)","text":"<ul> <li>Store raw data cheaply in object storage</li> <li>Flexible schema</li> <li>Problem: \"Swamp\" syndrome\u2014data is disorganized, hard to query, poor governance</li> </ul>"},{"location":"blog/01-intro-data-lakehouse/#era-3-the-data-lakehouse-2020s","title":"Era 3: The Data Lakehouse (2020s+)","text":"<ul> <li>Open table formats (Apache Iceberg) provide structure on top of cheap object storage</li> <li>Git-like versioning (Project Nessie) for data governance</li> <li>ACID transactions and schema enforcement</li> <li>Query engines (Trino, DuckDB) for analytics</li> <li>Result: Lake economics + warehouse reliability</li> </ul>"},{"location":"blog/01-intro-data-lakehouse/#what-makes-phlo-a-lakehouse","title":"What Makes Phlo a Lakehouse?","text":"<pre><code>Traditional Warehouse    vs    Phlo Lakehouse\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nClosed format                  Apache Iceberg (open)\nVendor lock-in                 S3-compatible storage\nSingle query engine            Trino, DuckDB, Spark\nNo versioning                  Git-like branching\nExpensive storage              MinIO object storage\nRigid schemas                  Schema evolution\n</code></pre>"},{"location":"blog/01-intro-data-lakehouse/#the-phlo-stack-at-a-glance","title":"The Phlo Stack at a Glance","text":"<pre><code>graph TB\n    A[\"Data Sources&lt;br/&gt;APIs, Files, Databases\"]\n    B[\"Ingestion&lt;br/&gt;DLT, Python\"]\n    C[\"Storage&lt;br/&gt;MinIO S3\"]\n    D[\"Lakehouse&lt;br/&gt;Apache Iceberg\"]\n    E[\"Versioning&lt;br/&gt;Project Nessie\"]\n    F[\"Transform&lt;br/&gt;dbt, Trino\"]\n    G[\"Analytics&lt;br/&gt;Superset, API\"]\n\n    A --&gt;|Raw data| B\n    B --&gt;|Parquet files| C\n    C --&gt;|Read/Write| D\n    D --&gt;|Metadata| E\n    E --&gt;|Branch control| F\n    F --&gt;|Transformed data| G\n\n    style D fill:#e1f5ff\n    style E fill:#e1f5ff\n    style F fill:#fff4e1\n</code></pre>"},{"location":"blog/01-intro-data-lakehouse/#key-concepts-explained","title":"Key Concepts Explained","text":""},{"location":"blog/01-intro-data-lakehouse/#1-apache-iceberg-table-format","title":"1. Apache Iceberg (Table Format)","text":"<p>Imagine you're storing data in a filing cabinet. A table format is the file organization system that lets you:</p> <pre><code>Instead of:\ns3://lake/\n  entry-001.parquet\n  entry-002.parquet\n  (which file is current?)\n\nIceberg provides:\ns3://lake/entries/\n  metadata/\n    v1.metadata.json      \u2190 \"This is the current version\"\n    snap-123.avro         \u2190 \"What changed in this snapshot\"\n  data/\n    part-001.parquet      \u2190 \"Actual data files\"\n    part-002.parquet\n</code></pre> <p>Benefits:</p> <ul> <li>ACID transactions (consistent reads/writes)</li> <li>Time travel (query data as it was on Oct 15)</li> <li>Schema evolution (change structure without rewriting data)</li> <li>Hidden partitioning (faster queries automatically)</li> </ul>"},{"location":"blog/01-intro-data-lakehouse/#2-project-nessie-git-for-data","title":"2. Project Nessie (Git for Data)","text":"<p>Just like Git tracks code changes, Nessie tracks data changes:</p> <pre><code># Create a dev branch to test transformations\nnessie branch create dev\n\n# Work on dev independently\nSELECT * FROM iceberg.bronze.entries (branch: dev)\n\n# Validate, then merge to main\nnessie merge dev -&gt; main\n</code></pre> <p>Why it matters:</p> <ul> <li>Dev/test/prod data isolation</li> <li>Atomic multi-table commits</li> <li>Reproducibility (tag versions)</li> <li>Rollback if something breaks</li> </ul>"},{"location":"blog/01-intro-data-lakehouse/#3-trino-query-engine","title":"3. Trino (Query Engine)","text":"<p>A distributed SQL query engine that understands Iceberg tables:</p> <pre><code>-- Trino queries Iceberg tables natively\nSET SESSION iceberg.nessie_reference_name = 'main';\n\nSELECT\n  date_trunc('hour', reading_timestamp) as hour,\n  avg(glucose_mg_dl) as avg_glucose\nFROM iceberg.silver.fct_glucose_readings\nWHERE reading_date = DATE '2024-10-15'\nGROUP BY 1\nORDER BY 1 DESC;\n</code></pre>"},{"location":"blog/01-intro-data-lakehouse/#4-dbt-transform","title":"4. dbt (Transform)","text":"<p>SQL-based data transformation with version control and testing:</p> <pre><code>-- dbt model: silver/fct_glucose_readings.sql\nSELECT\n  entry_id,\n  glucose_mg_dl,\n  reading_timestamp,\n  CASE\n    WHEN glucose_mg_dl &lt; 70 THEN 'hypoglycemia'\n    WHEN glucose_mg_dl &lt;= 180 THEN 'in_range'\n    ELSE 'hyperglycemia'\n  END as glucose_category\nFROM {{ ref('stg_glucose_entries') }}\n</code></pre> <p>dbt handles:</p> <ul> <li>SQL execution and dependencies</li> <li>Data quality tests</li> <li>Documentation generation</li> <li>Lineage tracking</li> </ul>"},{"location":"blog/01-intro-data-lakehouse/#5-dagster-orchestration","title":"5. Dagster (Orchestration)","text":"<p>Declarative asset orchestration that tracks what data depends on what:</p> <pre><code>import phlo\n\n@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",\n    validation_schema=RawGlucoseEntries,\n    group=\"nightscout\",\n    cron=\"0 */1 * * *\",\n)\ndef glucose_entries(partition_date: str):\n    \"\"\"Ingest Nightscout data daily with automatic DLT + Iceberg merge\"\"\"\n    # Return DLT source - decorator handles staging, validation, merge\n    return rest_api(...)\n\n@asset\ndef dbt_transform(dbt: DbtCliResource) -&gt; None:\n    \"\"\"Run dbt models on latest data\"\"\"\n    # Execute transformations\n\n@asset(deps=[glucose_entries, dbt_transform])\ndef publish_marts() -&gt; None:\n    \"\"\"Publish marts to Postgres after ingestion and transform\"\"\"\n    # Copy data to marts\n</code></pre>"},{"location":"blog/01-intro-data-lakehouse/#the-data-flow-in-phlo","title":"The Data Flow in Phlo","text":"<pre><code>1. INGEST\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Nightscout API      \u2502\n   \u2502 (glucose data)      \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193 (DLT + PyIceberg)\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 S3 Staging (MinIO)              \u2502 \u2190 Temporary parquet files\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193 (Merge with dedup)\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Iceberg Table: raw.glucose_entries   \u2502 \u2190 Immutable, ACID\n   \u2502 Branch: main (production)            \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2. TRANSFORM\n              \u2193 (dbt + Trino)\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Iceberg Table: bronze.stg_entries    \u2502 \u2190 Type conversions\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Iceberg Table: silver.fct_readings   \u2502 \u2190 Business logic\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Iceberg Table: gold.dim_date         \u2502 \u2190 Dimensions\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n3. PUBLISH\n              \u2193 (Trino \u2192 Postgres)\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Postgres: marts.mrt_glucose_overview \u2502 \u2190 Fast for BI\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193 (SQL queries)\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Superset Dashboard                   \u2502 \u2190 Visualization\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/01-intro-data-lakehouse/#why-this-matters-real-benefits","title":"Why This Matters (Real Benefits)","text":"Problem Traditional Phlo Solution Data costs High (warehouse fees) Low (S3 storage) Query speed Fast Fast (Trino optimization) Schema changes Painful rewrites Easy evolution Governance Manual processes Git-like branching Vendor lock-in Yes (Snowflake, Redshift) No (open formats) Time travel Not available Query any past state Data quality Ad-hoc testing Built-in (Iceberg snapshots)"},{"location":"blog/01-intro-data-lakehouse/#what-youll-learn-in-this-series","title":"What You'll Learn in This Series","text":"<p>This blog series walks through:</p> <ol> <li>Part 1 (this): Architecture overview</li> <li>Part 2: Setting up Phlo locally</li> <li>Part 3: Apache Iceberg table format</li> <li>Part 4: Project Nessie versioning</li> <li>Part 5: Data ingestion with DLT and PyIceberg</li> <li>Part 6: Transform with dbt and Trino</li> <li>Part 7: Orchestration with Dagster</li> <li>Part 8: Real-world example (glucose monitoring)</li> <li>Part 9: Data quality with Pandera</li> <li>Part 10: Metadata and governance with OpenMetadata</li> <li>Part 11: Observability and monitoring</li> <li>Part 12: Production deployment and scaling</li> </ol> <p>Each post includes hands-on examples and code you can run.</p>"},{"location":"blog/01-intro-data-lakehouse/#next-steps","title":"Next Steps","text":"<p>Ready to build? In Part 2, we'll:</p> <ul> <li>Clone the Phlo repository</li> <li>Set up Docker and dependencies</li> <li>Start all services with one command</li> <li>Run your first data pipeline</li> </ul> <p>Next: Part 2: Getting Started\u2014Setup Guide</p>"},{"location":"blog/02-setup-guide/","title":"Part 2: Getting Started with Phlo\u2014Setup Guide","text":"<p>In this post, we'll get Phlo running on your machine. By the end, you'll have:</p> <ul> <li>All services running (Postgres, MinIO, Nessie, Trino, Dagster)</li> <li>Sample data ingested</li> <li>Your first data pipeline executed</li> <li>A dashboard showing results</li> </ul>"},{"location":"blog/02-setup-guide/#prerequisites","title":"Prerequisites","text":""},{"location":"blog/02-setup-guide/#what-you-need","title":"What You Need","text":"<ol> <li>Docker &amp; Docker Compose (required)</li> </ol> <p><code>bash    # Verify installation    docker --version    docker compose --version</code></p> <p>Install Docker if you don't have it</p> <ol> <li>uv (Python package manager, optional but recommended)</li> </ol> <p><code>bash    # Install uv (10x faster than pip)    curl -LsSf https://astral.sh/uv/install.sh | sh</code></p> <ol> <li>Python 3.11+ with Phlo installed    <code>bash    # Install Phlo (with uv, recommended)    uv pip install phlo[defaults]    # or with pip    pip install phlo[defaults]</code></li> </ol>"},{"location":"blog/02-setup-guide/#system-requirements","title":"System Requirements","text":"Component Minimum Recommended CPU 2 cores 4+ cores RAM 4 GB 8+ GB Disk 10 GB 20+ GB OS Linux/Mac/Windows Mac/Linux <p>If you have less than 4GB RAM, you can start a minimal setup (Postgres + MinIO only) and add services gradually.</p>"},{"location":"blog/02-setup-guide/#step-1-initialize-your-project","title":"Step 1: Initialize Your Project","text":"<pre><code># Create a new Phlo project\nphlo init my-lakehouse\ncd my-lakehouse\n\n# This creates:\n# - phlo.yaml (project configuration)\n# - .env.example (local secrets template)\n# - workflows/ (your data pipelines)\n# - workflows/transforms/dbt/ (dbt models)\n</code></pre> <p>Then initialize infra (generates <code>.phlo/.env</code> and <code>.phlo/.env.local</code>):</p> <pre><code>phlo services init\n</code></pre>"},{"location":"blog/02-setup-guide/#whats-in-phloyaml-env","title":"What's in phlo.yaml (env:)?","text":"<pre><code>env:\n  # Ports (all on 10xxx range)\n  POSTGRES_PORT: 10000\n  MINIO_API_PORT: 10001\n  MINIO_CONSOLE_PORT: 10002\n  NESSIE_PORT: 10003\n  TRINO_PORT: 10005\n  DAGSTER_PORT: 10006\n</code></pre>"},{"location":"blog/02-setup-guide/#whats-in-phloenvlocal","title":"What's in .phlo/.env.local?","text":"<pre><code># Database\nPOSTGRES_PASSWORD=phlo\n\n# Storage\nMINIO_ROOT_PASSWORD=minio123\n</code></pre> <p>For local development: Use the defaults as-is. For production: Change all passwords in <code>.phlo/.env.local</code> to strong values.</p> <p>SECURITY WARNING: The default configuration uses weak passwords (<code>admin/admin</code>, <code>minioadmin/minioadmin123</code>, etc.) and has no authentication enabled on most services. This is fine for local development, but NEVER expose these services to a network or the internet without:</p> <ul> <li>Changing all default passwords to strong, unique values</li> <li>Enabling authentication on all services (Dagster, Trino, MinIO, Superset)</li> <li>Using TLS/SSL for encrypted connections</li> <li>Implementing proper network security (firewall rules, VPNs)</li> </ul> <p>See Part 12: Production Deployment for production hardening guidance.</p>"},{"location":"blog/02-setup-guide/#step-2-install-phlo-packages","title":"Step 2: Install Phlo Packages","text":"<p>Phlo uses a modular architecture. Install the core framework, then add services as needed.</p>"},{"location":"blog/02-setup-guide/#option-a-install-all-defaults-quick-start","title":"Option A: Install All Defaults (Quick Start)","text":"<pre><code># Install Phlo with all default services at once\nuv pip install phlo[defaults]\n</code></pre> <p>This installs:</p> <ul> <li><code>phlo</code> - Core framework</li> <li><code>phlo-dagster</code> - Data orchestration platform</li> <li><code>phlo-postgres</code> - PostgreSQL database</li> <li><code>phlo-trino</code> - Distributed SQL query engine</li> <li><code>phlo-nessie</code> - Git-like catalog for Iceberg</li> <li><code>phlo-minio</code> - S3-compatible object storage</li> </ul>"},{"location":"blog/02-setup-guide/#option-b-install-incrementally-recommended-for-learning","title":"Option B: Install Incrementally (Recommended for Learning)","text":"<pre><code># Start with core framework\nuv pip install phlo\n\n# Add data orchestration\nuv add phlo-dagster\n\n# Add storage layer\nuv add phlo-postgres phlo-minio\n\n# Add Iceberg catalog\nuv add phlo-nessie\n\n# Add query engine\nuv add phlo-trino\n\n# Add transformations\nuv add phlo-dbt\n\n# Add data quality\nuv add phlo-quality\n</code></pre> <p>This modular approach lets you:</p> <ul> <li>Understand each component's role</li> <li>Start with minimal resources</li> <li>Add features as you need them</li> </ul>"},{"location":"blog/02-setup-guide/#option-c-add-optional-services-later","title":"Option C: Add Optional Services Later","text":"<pre><code># Add observability (monitoring, logging)\nuv add phlo-prometheus phlo-grafana phlo-loki phlo-alloy\n\n# Add API layer (REST, GraphQL)\nuv add phlo-api phlo-postgrest phlo-hasura\n\n# Add data catalog\nuv add phlo-openmetadata\n\n# Add Observatory UI (web interface)\nuv add phlo-observatory\n</code></pre>"},{"location":"blog/02-setup-guide/#step-3-start-services","title":"Step 3: Start Services","text":""},{"location":"blog/02-setup-guide/#option-a-start-everything-at-once-recommended","title":"Option A: Start Everything at Once (Recommended)","text":"<pre><code># Start all services\nphlo services start\n\n# View service status\nphlo services status\n</code></pre>"},{"location":"blog/02-setup-guide/#option-b-start-specific-services","title":"Option B: Start Specific Services","text":"<pre><code># Start only what you need\nphlo services start --service postgres --service dagster\n\n# Add more services later\nphlo services start --service trino --service observatory\n</code></pre>"},{"location":"blog/02-setup-guide/#verify-services-are-running","title":"Verify Services Are Running","text":"<pre><code># Check all services\nphlo services status\n\n# View logs\nphlo services logs -f\n\n# Or for a specific service\nphlo services logs dagster\n</code></pre> <p>If any show errors, check logs:</p> <pre><code>phlo services logs dagster\n</code></pre>"},{"location":"blog/02-setup-guide/#step-4-access-the-services","title":"Step 4: Access the Services","text":"<p>Each service runs on its own port:</p> <pre><code># Open services in browser\nphlo services open dagster\nphlo services open observatory\nphlo services open minio\n</code></pre> Service URL Purpose Dagster http://localhost:10006 Orchestration UI Observatory http://localhost:3001 Data exploration UI Trino http://localhost:10005 Query engine UI MinIO Console http://localhost:10002 Object storage PostgreSQL http://localhost:10000 Database Nessie http://localhost:10003 Catalog API"},{"location":"blog/02-setup-guide/#step-5-first-data-ingestion","title":"Step 5: First Data Ingestion","text":"<p>Now let's ingest some real glucose monitoring data and run the pipeline.</p>"},{"location":"blog/02-setup-guide/#5a-trigger-data-ingestion","title":"5a: Trigger Data Ingestion","text":"<p>Open Dagster at http://localhost:10006</p> <p>You should see the asset graph:</p> <pre><code>glucose_entries\n  \u2193\nstg_glucose_entries (dbt)\n  \u2193\nfct_glucose_readings (dbt)\n  \u2193\nfct_daily_glucose_metrics\n  \u2193\npostgres_marts\n</code></pre> <p>Click on <code>glucose_entries</code> \u2192 Click Materialize this asset</p> <p>In the modal, select Date range: pick yesterday's date (or any recent date)</p> <p>Click Materialize and watch the pipeline run.</p>"},{"location":"blog/02-setup-guide/#5b-monitor-in-logs","title":"5b: Monitor in Logs","text":"<pre><code># Watch service logs\nphlo services logs -f\n\n# Or just Dagster logs\nphlo services logs dagster\n\n# Watch asset progress in Dagster UI (it updates live)\n# Open http://localhost:10006\n</code></pre> <p>The ingestion does:</p> <ol> <li>Fetches glucose entries from Nightscout API</li> <li>Validates with Pandera schemas</li> <li>Stages to MinIO as parquet</li> <li>Merges to Iceberg with deduplication</li> </ol> <p>You should see output like:</p> <pre><code>2024-10-15 10:30:45 - Successfully fetched 288 entries from API\n2024-10-15 10:30:46 - Raw data validation passed for 288 entries\n2024-10-15 10:30:48 - DLT staging completed in 1.23s\n2024-10-15 10:30:50 - Merged 288 rows to raw.glucose_entries\n</code></pre>"},{"location":"blog/02-setup-guide/#5c-run-transformations","title":"5c: Run Transformations","text":"<p>Once ingestion completes, run dbt transforms:</p> <p>In Dagster, click Materialize this asset on <code>stg_glucose_entries</code></p> <p>This will:</p> <ol> <li>Run dbt bronze layer (staging)</li> <li>Run dbt silver layer (fact tables with business logic)</li> <li>Run dbt gold layer (dimensions)</li> <li>Publish to Postgres marts</li> </ol> <p>Watch it propagate through the graph:</p> <pre><code>glucose_entries [SUCCESS]\n  \u2193\nstg_glucose_entries \u23f3 (running)\n  \u2193\nfct_glucose_readings \u23f3 (waiting)\n  \u2193\npostgres_marts \u23f3 (waiting)\n</code></pre>"},{"location":"blog/02-setup-guide/#5d-check-results","title":"5d: Check Results","text":"<p>Once complete, verify data in the databases:</p> <p>Option 1: Observatory UI</p> <p>Open http://localhost:3001 in your browser:</p> <ol> <li>Click Data Explorer in the sidebar</li> <li>Select <code>silver.fct_glucose_readings</code> table</li> <li>View schema, preview data, and run queries</li> </ol> <p>Option 2: Trino CLI</p> <pre><code>docker exec -it trino trino \\\n  --catalog iceberg \\\n  --schema silver \\\n  --execute \"SELECT COUNT(*) as row_count FROM fct_glucose_readings;\"\n\n# Output:\n# row_count\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#      288\n</code></pre> <p>Option 3: DuckDB (Local Analysis)</p> <pre><code># If you have DuckDB installed locally\nduckdb\n\n-- Connect to MinIO data\nD SELECT COUNT(*) FROM read_parquet('s3://lake/warehouse/silver/fct_glucose_readings/**/*.parquet');\n</code></pre>"},{"location":"blog/02-setup-guide/#step-6-explore-with-observatory","title":"Step 6: Explore with Observatory","text":"<p>Phlo includes Observatory, a web UI for exploring your lakehouse.</p>"},{"location":"blog/02-setup-guide/#6a-open-observatory","title":"6a: Open Observatory","text":"<pre><code>phlo services open observatory\n# Opens http://localhost:3001\n</code></pre>"},{"location":"blog/02-setup-guide/#6b-explore-data","title":"6b: Explore Data","text":"<ol> <li>Click Data Explorer in the sidebar</li> <li>Browse schemas: <code>raw</code>, <code>bronze</code>, <code>silver</code>, <code>gold</code></li> <li>Click any table to see:</li> <li>Schema and column types</li> <li>Data preview</li> <li>Statistics</li> </ol>"},{"location":"blog/02-setup-guide/#6c-view-lineage","title":"6c: View Lineage","text":"<ol> <li>Click Lineage in the sidebar</li> <li>See how data flows from source \u2192 bronze \u2192 silver \u2192 gold</li> <li>Click nodes to see details</li> </ol>"},{"location":"blog/02-setup-guide/#6d-run-queries","title":"6d: Run Queries","text":"<ol> <li>Click SQL Workbench</li> <li>Run ad-hoc queries against your Iceberg tables:</li> </ol> <pre><code>SELECT\n  date_trunc('hour', reading_timestamp) as hour,\n  avg(glucose_mg_dl) as avg_glucose\nFROM silver.fct_glucose_readings\nGROUP BY 1\nORDER BY 1 DESC\nLIMIT 24\n</code></pre>"},{"location":"blog/02-setup-guide/#6c-create-a-chart","title":"6c: Create a Chart","text":"<ol> <li>Click + Data \u2192 Create Chart</li> <li>Choose Dataset: <code>mrt_glucose_overview</code> (Postgres table)</li> <li>Chart type: Line Chart</li> <li>Drag columns:</li> <li>X-Axis: <code>reading_date</code></li> <li>Y-Axis: <code>avg_glucose_mg_dl</code></li> <li>Click Update Chart</li> <li>Click Save Chart</li> </ol> <p>Congratulations! You've visualized real glucose data from a lakehouse.</p>"},{"location":"blog/02-setup-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"blog/02-setup-guide/#services-wont-start","title":"Services Won't Start","text":"<pre><code># Check service status\nphlo services status\n\n# View specific service logs\nphlo services logs postgres\nphlo services logs dagster\n\n# Restart all services\nphlo services restart\n</code></pre>"},{"location":"blog/02-setup-guide/#out-of-disk-space","title":"Out of Disk Space","text":"<pre><code># Clean up Docker resources\ndocker system prune\ndocker volume prune\n\n# Reset services (WARNING: deletes all data)\nphlo services reset\n</code></pre>"},{"location":"blog/02-setup-guide/#nessie-connection-error","title":"Nessie Connection Error","text":"<pre><code># Check Nessie status\nphlo services status nessie\n\n# View Nessie logs\nphlo services logs nessie\n\n# Verify Nessie is healthy\ncurl http://localhost:10003/api/v1/config\n</code></pre>"},{"location":"blog/02-setup-guide/#trino-cant-find-iceberg-connector","title":"Trino Can't Find Iceberg Connector","text":"<pre><code># Check Trino logs\nphlo services logs trino\n\n# Verify catalog is configured (once Trino is running)\ndocker exec trino trino --execute \"SHOW CATALOGS;\"\n\n# Should output:\n# catalog\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# iceberg\n# system\n</code></pre>"},{"location":"blog/02-setup-guide/#dagster-assets-not-appearing","title":"Dagster Assets Not Appearing","text":"<pre><code># Restart Dagster services\nphlo services restart dagster\n\n# Wait 10 seconds, refresh http://localhost:10006\n</code></pre>"},{"location":"blog/02-setup-guide/#whats-next","title":"What's Next?","text":"<p>You now have a working lakehouse! Next steps:</p> <ol> <li>Explore the Data (Part 3): Understand Iceberg and Nessie</li> <li>Understand Ingestion (Part 4): How DLT + PyIceberg works</li> <li>Learn dbt (Part 5): SQL transformations</li> <li>Master Dagster (Part 6): Orchestration and dependencies</li> </ol> <p>But first, let's make sure everything works by running a quick health check.</p>"},{"location":"blog/02-setup-guide/#quick-health-check","title":"Quick Health Check","text":"<pre><code># Check all services at once\nphlo services status\n\n# Or use the health endpoint\ncurl http://localhost:10005/v1/info     # Trino\ncurl http://localhost:10003/api/v1/config  # Nessie\ncurl http://localhost:10006/graphql     # Dagster\n</code></pre> <p>Or run the CLI check:</p> <pre><code>phlo services status --json\n</code></pre>"},{"location":"blog/02-setup-guide/#summary","title":"Summary","text":"<p>You've successfully:</p> <ul> <li>Set up Phlo with all services</li> <li>Ingested real glucose data</li> <li>Ran transformations</li> <li>Created a dashboard</li> </ul> <p>In Part 3, we'll dive deep into Apache Iceberg\u2014the magic that makes this lakehouse work.</p> <p>Next: Part 3: Apache Iceberg\u2014The Table Format That Changed Everything</p>"},{"location":"blog/03-apache-iceberg-explained/","title":"Part 3: Apache Iceberg\u2014The Table Format That Changed Everything","text":"<p>In Part 1, we mentioned Iceberg as the magic ingredient. Let's understand why it's such a game-changer.</p>"},{"location":"blog/03-apache-iceberg-explained/#the-problem-with-traditional-parquet","title":"The Problem With Traditional Parquet","text":"<p>Before Iceberg, storing data in S3 looked like this:</p> <pre><code>s3://lake/glucose-data/\n\u251c\u2500\u2500 2024-10-01_001.parquet  (100 rows)\n\u251c\u2500\u2500 2024-10-01_002.parquet  (100 rows)\n\u251c\u2500\u2500 2024-10-02_001.parquet  (100 rows)\n\u2502   \u2514\u2500\u2500 DELETED: superseded by version from 10:30 UTC\n\u251c\u2500\u2500 2024-10-02_001_v2.parquet (102 rows) \u2190 Confusion!\n\u251c\u2500\u2500 2024-10-02_002.parquet  (100 rows)\n\u2514\u2500\u2500 _old_backup_v1/          (Don't delete!)\n</code></pre> <p>Problems:</p> <ul> <li>\ud83e\udd14 Which files are \"current\"? You have to track metadata yourself</li> <li>Schema changes require rewriting all files</li> <li>Queries must scan ALL files (no partition pruning)</li> <li>Concurrent writes = conflicting files</li> <li>No time travel\u2014data is gone when you delete it</li> </ul>"},{"location":"blog/03-apache-iceberg-explained/#what-iceberg-provides","title":"What Iceberg Provides","text":"<p>Iceberg is a table format specification that layers atomic metadata on top of data files:</p> <pre><code>s3://lake/glucose-data/\n\u251c\u2500\u2500 metadata/\n\u2502   \u251c\u2500\u2500 v1.metadata.json       \u2190 \"CURRENT VERSION\"\n\u2502   \u251c\u2500\u2500 snap-1234.avro         \u2190 Snapshot 1 schema\n\u2502   \u251c\u2500\u2500 snap-5678.avro         \u2190 Snapshot 2 schema\n\u2502   \u2514\u2500\u2500 manifest-99.avro        \u2190 \"These files belong to snapshot 5678\"\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 year=2024/month=10/day=01/\n    \u2502   \u251c\u2500\u2500 00001-a1b2c.parquet (rows 1-100)\n    \u2502   \u2514\u2500\u2500 00002-d3e4f.parquet (rows 101-200)\n    \u2514\u2500\u2500 year=2024/month=10/day=02/\n        \u251c\u2500\u2500 00003-g5h6i.parquet (rows 1-100, v2)\n        \u2514\u2500\u2500 00004-j7k8l.parquet (rows 101-200, v2)\n</code></pre> <p>The metadata files answer:</p> <ul> <li>\"What's the current version?\" \u2192 v1.metadata.json</li> <li>\"What files make up snapshot 5678?\" \u2192 manifest-99.avro</li> <li>\"What schema is this data?\" \u2192 snap-5678.avro</li> <li>\"Are these writes in conflict?\" \u2192 atomic metadata updates</li> </ul>"},{"location":"blog/03-apache-iceberg-explained/#core-iceberg-concepts","title":"Core Iceberg Concepts","text":""},{"location":"blog/03-apache-iceberg-explained/#1-snapshots-immutable-versions","title":"1. Snapshots (Immutable Versions)","text":"<p>Each write creates a new snapshot\u2014a complete, immutable view of the table at that moment:</p> <pre><code># In Python, using PyIceberg\nfrom phlo_iceberg.catalog import get_catalog\n\ncatalog = get_catalog()\ntable = catalog.load_table(\"raw.glucose_entries\")\n\n# View all snapshots\nfor snapshot in table.snapshots():\n    print(f\"Snapshot {snapshot.snapshot_id}:\")\n    print(f\"  Created: {snapshot.timestamp_ms}\")\n    print(f\"  Files: {len(snapshot.manifest_list)}\")\n\n# Output:\n# Snapshot 1234567890:\n#   Created: 2024-10-15 10:30:00\n#   Files: 3\n# Snapshot 1234567891:\n#   Created: 2024-10-15 10:35:00\n#   Files: 4 (one new file added)\n# Snapshot 1234567892:\n#   Created: 2024-10-15 10:40:00\n#   Files: 4 (one file deleted, one added)\n</code></pre> <p>Each snapshot points to:</p> <ul> <li>Data files: Actual parquet/avro/orc files with rows</li> <li>Manifest files: Which data files are in this snapshot</li> <li>Schema: Table structure at that moment</li> </ul>"},{"location":"blog/03-apache-iceberg-explained/#2-manifests-file-tracking","title":"2. Manifests (File Tracking)","text":"<p>A manifest is a list: \"These files make up this snapshot\"</p> <pre><code>Snapshot 1234567892:\n\u251c\u2500\u2500 manifest-001.avro\n\u2502   \u251c\u2500\u2500 data/year=2024/month=10/day=01/00001.parquet \u2192 rows 1-100\n\u2502   \u251c\u2500\u2500 data/year=2024/month=10/day=01/00002.parquet \u2192 rows 101-200\n\u2502   \u2514\u2500\u2500 data/year=2024/month=10/day=02/00003.parquet \u2192 rows 1-100\n\u2514\u2500\u2500 manifest-002.avro\n    \u2514\u2500\u2500 data/year=2024/month=10/day=02/00004.parquet \u2192 rows 101-200\n</code></pre> <p>Why manifests? Query optimization:</p> <ul> <li>Scanner reads manifest, not S3 listing</li> <li>Knows exact file count before scanning</li> <li>Filters files by partition before opening</li> </ul>"},{"location":"blog/03-apache-iceberg-explained/#3-hidden-partitioning","title":"3. Hidden Partitioning","text":"<p>Traditional table partitioning:</p> <pre><code>-- You explicitly filter by partition\nSELECT * FROM glucose_data\nWHERE year=2024 AND month=10 AND day=15;\n</code></pre> <p>Iceberg partitioning:</p> <pre><code>-- Iceberg does this automatically!\nSELECT * FROM glucose_data\nWHERE reading_timestamp = '2024-10-15';\n\n-- Iceberg transforms to:\n-- WHERE year=2024 AND month=10 AND day=15\n-- (you don't need to know the partition scheme)\n</code></pre> <p>In Phlo's code, this is handled automatically:</p> <pre><code># In iceberg/tables.py\n# No need to specify partition columns in queries\ntable = catalog.load_table(\"raw.glucose_entries\")\n\n# Iceberg automatically prunes partitions based on WHERE clause\n# Query engine skips files that don't match the predicate\n</code></pre>"},{"location":"blog/03-apache-iceberg-explained/#4-schema-evolution-adding-columns-without-rewriting","title":"4. Schema Evolution (Adding Columns Without Rewriting)","text":"<p>Before Iceberg (bad):</p> <pre><code>-- Want to add a field? Must rewrite all files\nALTER TABLE glucose_entries ADD COLUMN a1c_level FLOAT;\n-- ^ Takes hours, costs money\n</code></pre> <p>With Iceberg (good):</p> <pre><code>-- Add a column with a default\nALTER TABLE iceberg.raw.glucose_entries\nADD COLUMN a1c_level FLOAT DEFAULT 0.0;\n\n-- Old files don't have this column?\n-- Iceberg fills in the default when reading\n-- Query still works, no rewrite needed\n</code></pre> <p>In dbt, this happens automatically when you add a column to a model.</p>"},{"location":"blog/03-apache-iceberg-explained/#time-travel-query-the-past","title":"Time Travel: Query the Past","text":"<p>The killer feature of Iceberg: travel back in time.</p> <pre><code>-- Current state (latest snapshot)\nSELECT COUNT(*) FROM iceberg.raw.glucose_entries;\n-- Result: 5,000 rows\n\n-- As it was 1 hour ago\nSELECT COUNT(*) FROM iceberg.raw.glucose_entries\nFOR VERSION AS OF 1728992400000;  -- Unix milliseconds\n-- Result: 4,500 rows (before recent ingestion)\n\n-- As it was yesterday\nSELECT COUNT(*) FROM iceberg.raw.glucose_entries\nFOR TIMESTAMP AS OF '2024-10-14 10:00:00';\n-- Result: 4,200 rows\n\n-- Show me what was added in the last hour\nSELECT DISTINCT sgv, device, date_string\nFROM iceberg.raw.glucose_entries\nFOR VERSION AS OF 1728992400000\nMINUS\nSELECT DISTINCT sgv, device, date_string\nFROM iceberg.raw.glucose_entries;  -- Current\n</code></pre> <p>Why time travel matters:</p> <ul> <li>\ud83d\udc1b Data quality issue today? Check what you ingested yesterday</li> <li>Audit trail: see exactly what changed and when</li> <li>Reproducibility: re-run yesterday's analysis with yesterday's data</li> <li>\u21a9\ufe0f No \"undo\" button needed\u2014just query the previous snapshot</li> </ul>"},{"location":"blog/03-apache-iceberg-explained/#acid-transactions","title":"ACID Transactions","text":"<p>Iceberg ensures Atomicity, Consistency, Isolation, Durability.</p> <p>Atomicity: Either all changes or none</p> <pre><code>Write to iceberg.raw.glucose_entries:\n  \u2713 Write 500 new rows\n  \u2713 Update 10 existing rows (deduplication)\n  \u2713 Update metadata.json to point to new snapshot\n  \u2192 All or nothing (no partial writes)\n</code></pre> <p>Isolation: Readers see consistent snapshots</p> <pre><code>Writer is updating glucose_entries (slow, 1 minute)\nReader queries same table (right now)\n  \u2192 Reader sees previous complete snapshot\n  \u2192 Reader doesn't see partial writes\n  \u2192 Writer completes, new readers see new snapshot\n</code></pre> <p>In Phlo's Code:</p> <pre><code># From workflows/ingestion/dlt_assets.py\n# Merge with idempotent deduplication\n\nmerge_metrics = iceberg.merge_parquet(\n    table_name=\"raw.glucose_entries\",\n    data_path=\"s3://lake/stage/entries/2024-10-15/data.parquet\",\n    unique_key=\"_id\",  # Nightscout's unique key\n)\n\n# Iceberg ensures:\n# 1. New rows are inserted\n# 2. Duplicates (same _id) are replaced atomically\n# 3. If write fails, table unchanged\n# Result: Safe to run multiple times (idempotent)\n</code></pre>"},{"location":"blog/03-apache-iceberg-explained/#iceberg-in-phlo","title":"Iceberg in Phlo","text":"<p>Let's see how Phlo uses Iceberg in practice.</p>"},{"location":"blog/03-apache-iceberg-explained/#reading-data-in-dbt","title":"Reading Data (in dbt)","text":"<pre><code>-- File: phlo-examples/nightscout/workflows/transforms/dbt/models/bronze/stg_glucose_entries.sql\n{{ config(\n    materialized='view',\n) }}\n\nWITH raw_data AS (\n    SELECT * FROM {{ source('dagster_assets', 'glucose_entries') }}\n)\nSELECT\n    _id as entry_id,\n    sgv as glucose_mg_dl,\n    date_string as timestamp_iso,\n    ...\nFROM raw_data\nWHERE sgv IS NOT NULL\n  AND sgv BETWEEN 20 AND 600  -- Data quality filter\n</code></pre> <p>This dbt model:</p> <ul> <li>Reads from Iceberg table <code>glucose_entries</code> (created by @phlo_ingestion)</li> <li>Applies transformations</li> <li>Writes to Iceberg table <code>bronze.stg_glucose_entries</code></li> <li>All tracked as a snapshot</li> </ul>"},{"location":"blog/03-apache-iceberg-explained/#writing-data-with-phlo_ingestion","title":"Writing Data (with @phlo_ingestion)","text":"<p>The <code>@phlo_ingestion</code> decorator handles Iceberg writes automatically:</p> <pre><code># From phlo-examples/nightscout/workflows/ingestion/nightscout/readings.py\n\nimport phlo\n\n@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",  # Deduplicate on this column\n    validation_schema=RawGlucoseEntries,\n    group=\"nightscout\",\n)\ndef glucose_entries(partition_date: str):\n    \"\"\"Ingest glucose entries with automatic Iceberg merge.\"\"\"\n    # Return DLT source\n    return rest_api(...)\n\n# The decorator automatically:\n# 1. Ensures Iceberg table exists\n# 2. Stages data to parquet via DLT\n# 3. Merges to Iceberg with deduplication on \"_id\"\n# 4. Creates new snapshot\n# 5. Tracks metadata in Dagster\n</code></pre>"},{"location":"blog/03-apache-iceberg-explained/#querying-with-time-travel","title":"Querying with Time Travel","text":"<pre><code># Query current version\ndocker exec trino trino \\\n  --catalog iceberg \\\n  --schema raw \\\n  --execute \"SELECT COUNT(*) FROM glucose_entries;\"\n\n# Query specific snapshot ID (from Iceberg metadata)\ndocker exec trino trino \\\n  --catalog iceberg \\\n  --schema raw \\\n  --execute \"\n  SELECT COUNT(*) FROM glucose_entries\n  FOR VERSION AS OF 1728992400000;\n  \"\n</code></pre>"},{"location":"blog/03-apache-iceberg-explained/#comparison-before-vs-after-iceberg","title":"Comparison: Before vs After Iceberg","text":"Aspect Without Iceberg (S3 Files) With Iceberg Current version Manual tracking (fragile) Metadata files (reliable) Schema changes Rewrite all files Add columns instantly Time travel Impossible <code>FOR VERSION AS OF</code> Concurrent writes Risk of conflicts Atomic snapshots Partition pruning Manual WHERE clauses Automatic Data quality Corrupted files? Undetectable Checksums in metadata Query cost Scan ALL files Scan only necessary files"},{"location":"blog/03-apache-iceberg-explained/#how-phlo-stores-data","title":"How Phlo Stores Data","text":"<p>Here's Phlo's actual storage structure in MinIO:</p> <pre><code>s3://lake/\n\u251c\u2500\u2500 stage/                      \u2190 DLT staging area (temporary)\n\u2502   \u2514\u2500\u2500 entries/2024-10-15/\n\u2502       \u2514\u2500\u2500 data.parquet        \u2190 Parquet from API\n\u2502\n\u2514\u2500\u2500 warehouse/                  \u2190 Iceberg tables\n    \u251c\u2500\u2500 raw/\n    \u2502   \u2514\u2500\u2500 glucose_entries/\n    \u2502       \u251c\u2500\u2500 metadata/\n    \u2502       \u2502   \u251c\u2500\u2500 v1.metadata.json\n    \u2502       \u2502   \u251c\u2500\u2500 snap-1234.avro\n    \u2502       \u2502   \u2514\u2500\u2500 manifest-99.avro\n    \u2502       \u2514\u2500\u2500 data/\n    \u2502           \u2514\u2500\u2500 year=2024/month=10/day=15/\n    \u2502               \u251c\u2500\u2500 00001.parquet\n    \u2502               \u2514\u2500\u2500 00002.parquet\n    \u2502\n    \u251c\u2500\u2500 bronze/\n    \u2502   \u2514\u2500\u2500 stg_glucose_entries/\n    \u2502       \u251c\u2500\u2500 metadata/\n    \u2502       \u2502   \u251c\u2500\u2500 v1.metadata.json\n    \u2502       \u2502   \u2514\u2500\u2500 ...\n    \u2502       \u2514\u2500\u2500 data/\n    \u2502           \u2514\u2500\u2500 year=2024/month=10/day=15/\n    \u2502\n    \u2514\u2500\u2500 silver/\n        \u2514\u2500\u2500 fct_glucose_readings/\n            \u251c\u2500\u2500 metadata/\n            \u2502   \u251c\u2500\u2500 v1.metadata.json\n            \u2502   \u2514\u2500\u2500 ...\n            \u2514\u2500\u2500 data/\n                \u2514\u2500\u2500 year=2024/month=10/day=15/\n</code></pre> <p>Note: Staging is temporary (cleaned up after merge). Only warehouse tables persist.</p>"},{"location":"blog/03-apache-iceberg-explained/#hands-on-explore-snapshots","title":"Hands-On: Explore Snapshots","text":"<pre><code># Use Python to explore snapshots\npython3 &lt;&lt; 'EOF'\nfrom phlo_iceberg.catalog import get_catalog\n\ncatalog = get_catalog()\ntable = catalog.load_table(\"glucose_entries\")\n\nprint(f\"Table: {table.name()}\")\nprint(f\"Total snapshots: {len(table.snapshots())}\")\n\n# Show recent snapshots\nfor snapshot in sorted(table.snapshots(),\n                       key=lambda s: s.timestamp_ms,\n                       reverse=True)[:3]:\n    print(f\"\\nSnapshot {snapshot.snapshot_id}:\")\n    print(f\"  Time: {snapshot.timestamp_ms}\")\n    print(f\"  Manifests: {len(snapshot.manifest_list)}\")\n\n    # Show files in this snapshot\n    for manifest_entry in snapshot.manifest_list:\n        print(f\"    File: {manifest_entry.manifest_path}\")\nEOF\n</code></pre>"},{"location":"blog/03-apache-iceberg-explained/#next-project-nessie-git-for-data","title":"Next: Project Nessie (Git for Data)","text":"<p>Iceberg gives us time travel. Nessie adds branching on top.</p> <p>We'll explore that in the next post.</p>"},{"location":"blog/03-apache-iceberg-explained/#summary","title":"Summary","text":"<p>Apache Iceberg:</p> <ul> <li>Tracks table versions with metadata (snapshots)</li> <li>Enables time travel (query any historical version)</li> <li>ACID transactions (safe concurrent access)</li> <li>Schema evolution (add columns instantly)</li> <li>Hidden partitioning (automatic pruning)</li> <li>Works with any S3-compatible storage</li> </ul> <p>Phlo uses Iceberg to ensure:</p> <ul> <li>Safe ingestion (idempotent merges)</li> <li>Reliable transformations (atomic snapshots)</li> <li>Data governance (audit trail via time travel)</li> </ul> <p>Next: Part 4: Project Nessie\u2014Git-Like Versioning for Data</p>"},{"location":"blog/04-project-nessie-versioning/","title":"Part 4: Project Nessie\u2014Git-Like Versioning for Data","text":"<p>Iceberg gave us time travel. Now let's add branching, merging, and tags to our data with Project Nessie.</p>"},{"location":"blog/04-project-nessie-versioning/#why-nessie-the-git-analogy","title":"Why Nessie? The Git Analogy","text":"<p>You already know Git:</p> <pre><code># Code versioning\ngit branch feature/new-glucose-model\ngit commit -m \"Add glucose categories\"\ngit push origin feature/new-glucose-model\ngit pull request  # Review changes\ngit merge  # Promote to main\n</code></pre> <p>Nessie brings this same workflow to data:</p> <pre><code>main branch (production)     dev branch (development)\n      \u2502                            \u2502\n      \u2502  \u2190 stable, validated       \u2502  \u2190 experimental, testing\n      \u2502                            \u2502\n      \u2514\u2500\u2500\u2500\u2500 merge when ready \u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#the-problem-nessie-solves","title":"The Problem Nessie Solves","text":"<p>Without versioning, data work looks like:</p> <pre><code>Production Data\n    \u2193\n  (Dev transforms it)\n    \u2193\n  (Oops! Broke something)\n    \u2193\nProduction Data is CORRUPTED\n    \u2193\n(Back up from last night? Lost today's data!)\n</code></pre> <p>With Nessie:</p> <pre><code>main (production)\n  \u2193\n  \u251c\u2500 dev (development)\n  \u2502   \u2514\u2500 (Test transformations)\n  \u2502   \u2514\u2500 (Validate quality)\n  \u2502   \u2514\u2500 (If bad, delete branch, main unchanged)\n  \u2502\n  \u2514\u2500 (If good, merge dev \u2192 main atomically)\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#core-nessie-concepts","title":"Core Nessie Concepts","text":""},{"location":"blog/04-project-nessie-versioning/#1-branches","title":"1. Branches","text":"<p>A branch is an independent copy of all table metadata.</p> <pre><code>Database State:\n\u251c\u2500\u2500 main (production)\n\u2502   \u251c\u2500\u2500 raw.glucose_entries (snapshot v1)\n\u2502   \u251c\u2500\u2500 bronze.stg_entries (snapshot v3)\n\u2502   \u2514\u2500\u2500 silver.fct_readings (snapshot v5)\n\u2502\n\u2514\u2500\u2500 dev (development, created from main)\n    \u251c\u2500\u2500 raw.glucose_entries (snapshot v1) \u2190 same as main\n    \u251c\u2500\u2500 bronze.stg_entries (snapshot v3) \u2190 same as main\n    \u2514\u2500\u2500 silver.fct_readings (snapshot v5) \u2190 same as main\n</code></pre> <p>Now you work on dev:</p> <pre><code>After transformations on dev:\n\u251c\u2500\u2500 main (unchanged)\n\u2502   \u251c\u2500\u2500 raw.glucose_entries (snapshot v1)\n\u2502   \u251c\u2500\u2500 bronze.stg_entries (snapshot v3)\n\u2502   \u2514\u2500\u2500 silver.fct_readings (snapshot v5)\n\u2502\n\u2514\u2500\u2500 dev (has new snapshots)\n    \u251c\u2500\u2500 raw.glucose_entries (snapshot v1) \u2190 unchanged\n    \u251c\u2500\u2500 bronze.stg_entries (snapshot v4) \u2190 NEW\n    \u2514\u2500\u2500 silver.fct_readings (snapshot v6) \u2190 NEW\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#2-commits-and-merges","title":"2. Commits and Merges","text":"<p>Each change on a branch creates a commit (pointer to metadata).</p> <pre><code>Branch History:\nmain:\n  \u251c\u2500\u2500 Commit A: Initial data load\n  \u251c\u2500\u2500 Commit B: Quality fixes\n  \u2514\u2500\u2500 Commit C: Schema evolution (HEAD)\n\ndev (branched from Commit B):\n  \u251c\u2500\u2500 Commit B': Quality fixes (inherited)\n  \u251c\u2500\u2500 Commit D: New transformations\n  \u2514\u2500\u2500 Commit E: Schema optimizations (HEAD)\n\nMerge dev \u2192 main:\n  \u251c\u2500\u2500 Commit A: Initial data load\n  \u251c\u2500\u2500 Commit B: Quality fixes\n  \u251c\u2500\u2500 Commit C: Schema evolution\n  \u251c\u2500\u2500 Commit F: Merge commit (combines D + E)\n  \u2514\u2500\u2500 (now HEAD points to F)\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#3-tags-releases","title":"3. Tags (Releases)","text":"<p>Tag specific commits for releases:</p> <pre><code>main:\n  \u251c\u2500\u2500 Commit A\n  \u251c\u2500\u2500 Commit B (tag: v1.0-released)\n  \u251c\u2500\u2500 Commit C\n  \u2514\u2500\u2500 Commit D (tag: v1.1-released, HEAD)\n\n-- Query data as it was at v1.0:\nSELECT * FROM iceberg.silver.fct_readings\nFOR TAG v1.0-released;\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#nessie-in-phlo","title":"Nessie in Phlo","text":""},{"location":"blog/04-project-nessie-versioning/#setup-nessie-runs-in-docker","title":"Setup: Nessie Runs in Docker","text":"<pre><code># Nessie REST API is available at port 19120\ncurl http://localhost:19120/api/v2/config\n\n# Response:\n# {\n#   \"defaultBranch\": \"main\",\n#   \"maxSupportedApiVersion\": \"2\",\n#   \"repositories\": []\n# }\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#default-main-branch","title":"Default: main Branch","text":"<p>When you start Phlo, the <code>main</code> branch exists:</p> <pre><code># List branches\ncurl http://localhost:19120/api/v2/trees\n\n# Response:\n# {\n#   \"trees\": [\n#     {\n#       \"name\": \"main\",\n#       \"hash\": \"abc123def456\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#creating-a-development-branch","title":"Creating a Development Branch","text":"<p>In Phlo, Dagster automatically creates <code>dev</code> branch:</p> <pre><code># From workflows/nessie/operations.py\n\n@asset(name=\"nessie_dev_branch\")\ndef create_dev_branch(nessie_client: NessieResource) -&gt; None:\n    \"\"\"Ensure dev branch exists for safe transformations.\"\"\"\n\n    # List existing branches\n    branches = nessie_client.list_branches()\n    branch_names = [b.name for b in branches]\n\n    # Create dev if it doesn't exist\n    if 'dev' not in branch_names:\n        nessie_client.create_branch(\n            name='dev',\n            from_branch='main'\n        )\n        print(\"Created dev branch from main\")\n    else:\n        print(\"Dev branch already exists\")\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#phlos-write-audit-publish-pattern","title":"Phlo's Write-Audit-Publish Pattern","text":"<p>Phlo implements the Write-Audit-Publish (WAP) pattern automatically:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     FEATURE BRANCH (dev)                         \u2502\n\u2502  Catalog: iceberg_dev                                           \u2502\n\u2502                                                                  \u2502\n\u2502  1. Ingestion \u2500\u2500\u25ba 2. Transforms (dbt) \u2500\u2500\u25ba 3. Quality Checks     \u2502\n\u2502     (PyIceberg)      (bronze\u2192silver\u2192gold)    (validation)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2502 4. All Checks Pass?\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  AUTO-MERGE     \u2502 (Nessie merge via sensor)\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       MAIN BRANCH                                \u2502\n\u2502  Catalog: iceberg                                               \u2502\n\u2502                                                                  \u2502\n\u2502  5. Publishing \u2500\u2500\u25ba Postgres (marts for BI dashboards)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insight: All writes happen on the feature branch. Only validated data reaches main.</p>"},{"location":"blog/04-project-nessie-versioning/#automatic-branch-management-with-sensors","title":"Automatic Branch Management with Sensors","text":"<p>Phlo handles branching automatically via Dagster sensors:</p> <pre><code># 1. branch_creation_sensor - Creates branch when pipeline starts\n@run_status_sensor(run_status=DagsterRunStatus.STARTING)\ndef branch_creation_sensor(context, branch_manager):\n    \"\"\"Auto-create pipeline/run-{id} branch for isolation.\"\"\"\n    branch_manager.create_pipeline_branch(context.dagster_run.run_id)\n\n# 2. auto_promotion_sensor - Merges when quality checks pass\n@sensor(minimum_interval_seconds=30)\ndef auto_promotion_sensor(context, nessie, branch_manager):\n    \"\"\"Auto-merge to main when all ERROR-severity checks pass.\"\"\"\n    # Check recent runs for passing quality checks\n    # If all pass \u2192 merge branch to main\n    # Create timestamped tag (v20251126_143000)\n\n# 3. branch_cleanup_sensor - Deletes old branches\n@sensor(minimum_interval_seconds=3600)\ndef branch_cleanup_sensor(context, branch_manager):\n    \"\"\"Clean up branches after retention period (default: 7 days).\"\"\"\n</code></pre> <p>You don't need to manually merge - the sensor handles it when quality checks pass.</p>"},{"location":"blog/04-project-nessie-versioning/#trino-catalog-configuration-not-session-properties","title":"Trino Catalog Configuration (Not Session Properties)","text":"<p>Nessie branching in Trino is configured at the catalog level, not via session properties:</p> <pre><code># .phlo/trino/catalog/iceberg.properties (main branch)\nconnector.name=iceberg\niceberg.catalog.type=rest\niceberg.rest-catalog.uri=http://nessie:19120/iceberg/main\niceberg.rest-catalog.prefix=main\n</code></pre> <pre><code># .phlo/trino/catalog/iceberg_dev.properties (dev branch)\nconnector.name=iceberg\niceberg.catalog.type=rest\niceberg.rest-catalog.uri=http://nessie:19120/iceberg/dev\niceberg.rest-catalog.prefix=dev\n</code></pre> <p>Query different branches by using different catalogs:</p> <pre><code>-- Query main (production)\nSELECT COUNT(*) FROM iceberg.raw.glucose_entries;\n\n-- Query dev (development)\nSELECT COUNT(*) FROM iceberg_dev.raw.glucose_entries;\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#using-branch-in-code","title":"Using Branch in Code","text":"<pre><code>from phlo_trino.resource import TrinoResource\n\ntrino = TrinoResource()\n\n# Query main branch (default)\nrows = trino.execute(\"SELECT * FROM iceberg.marts.readings\", branch=\"main\")\n\n# Query dev branch\nrows = trino.execute(\"SELECT * FROM iceberg_dev.marts.readings\", branch=\"dev\")\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#hands-on-explore-nessie","title":"Hands-On: Explore Nessie","text":""},{"location":"blog/04-project-nessie-versioning/#list-all-branches","title":"List All Branches","text":"<pre><code>curl http://localhost:19120/api/v2/trees\n\n# Response (pretty-printed):\n# {\n#   \"trees\": [\n#     {\n#       \"name\": \"main\",\n#       \"hash\": \"def456xyz\"\n#     },\n#     {\n#       \"name\": \"dev\",\n#       \"hash\": \"abc123def\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#view-branch-history","title":"View Branch History","text":"<pre><code># Get commit history for a branch\ncurl \"http://localhost:19120/api/v2/trees/main/history\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"maxResults\": 10,\n    \"pageToken\": null\n  }'\n\n# Response:\n# {\n#   \"logEntries\": [\n#     {\n#       \"commitMeta\": {\n#         \"hash\": \"abc123def456\",\n#         \"message\": \"Promote validated transforms to production\",\n#         \"authorTime\": 1729027800000,\n#         \"commitTime\": 1729027800000\n#       },\n#       \"operations\": [\n#         {\n#           \"type\": \"Put\",\n#           \"key\": {\n#             \"elements\": [\"silver\", \"fct_glucose_readings\"]\n#           }\n#         }\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#query-on-a-specific-branch","title":"Query on a Specific Branch","text":"<p>Use different Trino catalogs to query different branches:</p> <pre><code>-- Query main (production) - uses 'iceberg' catalog\nSELECT COUNT(*) FROM iceberg.raw.glucose_entries;\n-- Result: 5000\n\n-- Query dev (development) - uses 'iceberg_dev' catalog\nSELECT COUNT(*) FROM iceberg_dev.raw.glucose_entries;\n-- Result: 5500 (includes new test data)\n\n-- Compare between branches\nSELECT\n    'main' as branch, COUNT(*) as rows FROM iceberg.raw.glucose_entries\nUNION ALL\nSELECT\n    'dev' as branch, COUNT(*) as rows FROM iceberg_dev.raw.glucose_entries;\n</code></pre> <p>In dbt, select the target to use the appropriate catalog:</p> <pre><code># workflows/transforms/dbt/profiles.yml\n\nphlo:\n  outputs:\n    dev:\n      type: trino\n      host: trino\n      catalog: iceberg_dev # \u2190 Dev branch catalog\n      schema: bronze\n\n    prod:\n      type: trino\n      host: trino\n      catalog: iceberg # \u2190 Main branch catalog\n      schema: bronze\n</code></pre> <pre><code># Run dbt on dev branch\ndbt run --target dev\n\n# Run dbt on main branch (production)\ndbt run --target prod\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#advanced-manual-branch-operations","title":"Advanced: Manual Branch Operations","text":"<p>Using the Nessie REST API (v1):</p> <pre><code># Get main branch hash\nMAIN_HASH=$(curl -s http://localhost:19120/api/v1/trees/tree/main | jq -r '.hash')\n\n# Create a feature branch from main\ncurl -X POST http://localhost:19120/api/v1/trees/tree \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"type\\\": \\\"BRANCH\\\", \\\"name\\\": \\\"feature/new-metrics\\\", \\\"hash\\\": \\\"$MAIN_HASH\\\"}\"\n\n# List all branches\ncurl -s http://localhost:19120/api/v1/trees | jq '.references[].name'\n\n# Merge feature branch to main\nTARGET_HASH=$(curl -s http://localhost:19120/api/v1/trees/tree/main | jq -r '.hash')\nSOURCE_HASH=$(curl -s http://localhost:19120/api/v1/trees/tree/feature/new-metrics | jq -r '.hash')\n\ncurl -X POST \"http://localhost:19120/api/v1/trees/tree/main/merge?expectedHash=$TARGET_HASH\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"fromRefName\\\": \\\"feature/new-metrics\\\", \\\"fromHash\\\": \\\"$SOURCE_HASH\\\"}\"\n\n# Delete feature branch after merge\nBRANCH_HASH=$(curl -s http://localhost:19120/api/v1/trees/tree/feature/new-metrics | jq -r '.hash')\ncurl -X DELETE \"http://localhost:19120/api/v1/trees/branch/feature/new-metrics?expectedHash=$BRANCH_HASH\"\n</code></pre> <p>Or use the <code>NessieResource</code> in Python:</p> <pre><code>from phlo_nessie.resource import NessieResource\n\nnessie = NessieResource()\n\n# Create branch\nnessie.create_branch(\"feature/new-metrics\", source_ref=\"main\")\n\n# Merge branch\nnessie.merge_branch(\"feature/new-metrics\", \"main\")\n\n# Delete branch\nnessie.delete_branch(\"feature/new-metrics\")\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#branch-management-via-cli","title":"Branch Management via CLI","text":"<p>The REST API and Python SDK work, but for day-to-day operations, <code>phlo branch</code> is simpler.</p>"},{"location":"blog/04-project-nessie-versioning/#list-branches","title":"List Branches","text":"<pre><code>$ phlo branch list\n\nBranches:\n  NAME                  HASH         TABLES  LAST COMMIT\n  main (default)        a1b2c3d4     15      2h ago\n  dev                   e5f6g7h8     15      4h ago\n  feature/new-metrics   i9j0k1l2     16      1d ago\n\nTags:\n  release-2024-01       m3n4o5p6     15      7d ago\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#create-a-feature-branch","title":"Create a Feature Branch","text":"<pre><code># Create from main (default)\n$ phlo branch create feature/new-transform\n\nCreating branch: feature/new-transform\nSource: main (a1b2c3d4)\n\n\u2713 Branch created\n\n# Create from specific branch\n$ phlo branch create experiment/risky-change --from dev\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#compare-branches","title":"Compare Branches","text":"<p>See what's different between branches before merging:</p> <pre><code>$ phlo branch diff main feature/new-transform\n\nBranch Diff: main \u2190 feature/new-transform\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nTables Modified:\n  silver.fct_glucose_readings\n    + Column: estimated_a1c (float)\n    ~ Column: glucose_category (type unchanged, constraint added)\n\nTables Added:\n  gold.dim_glucose_ranges (new table)\n\nCommits on feature/new-transform not in main:\n  i9j0k1l2  Add estimated A1C calculation\n  k2l3m4n5  Add glucose range dimension\n\nSafe to merge: Yes (no conflicts detected)\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#merge-branches","title":"Merge Branches","text":"<pre><code># Preview merge (dry run)\n$ phlo branch merge feature/new-transform main --dry-run\n\nMerge Preview: feature/new-transform \u2192 main\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nChanges to apply:\n  + gold.dim_glucose_ranges (new table)\n  ~ silver.fct_glucose_readings (schema change)\n\nNo conflicts detected.\nRun without --dry-run to merge.\n\n# Perform merge\n$ phlo branch merge feature/new-transform main\n\nMerging: feature/new-transform \u2192 main\n\n\u2713 Merge successful\n  Commit: q8r9s0t1\n  Tables affected: 2\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#delete-branches","title":"Delete Branches","text":"<pre><code># Delete merged branch\n$ phlo branch delete feature/new-transform\n\nDeleting branch: feature/new-transform\n\n\u26a0 This branch has been merged to main.\nProceed? [y/N] y\n\n\u2713 Branch deleted\n\n# Force delete unmerged branch\n$ phlo branch delete experiment/abandoned --force\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#practical-workflow-safe-schema-changes","title":"Practical Workflow: Safe Schema Changes","text":"<p>Here's how to use branches for safe data development:</p> <pre><code># 1. Create a feature branch\n$ phlo branch create feature/add-a1c-calculation\n\n# 2. Switch Trino to use the branch (in your SQL client)\n#    USE iceberg_feature_add_a1c_calculation.silver;\n\n# 3. Make changes (run dbt, materialize assets)\n$ dbt run --select fct_glucose_readings\n$ phlo materialize fct_glucose_readings --partition 2024-01-15\n\n# 4. Validate changes\n$ phlo contract validate glucose_readings\n$ phlo quality run silver.fct_glucose_readings\n\n# 5. Compare to main\n$ phlo branch diff main feature/add-a1c-calculation\n\n# 6. If everything looks good, merge\n$ phlo branch merge feature/add-a1c-calculation main\n\n# 7. Clean up\n$ phlo branch delete feature/add-a1c-calculation\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#branch-naming-conventions","title":"Branch Naming Conventions","text":"Pattern Use Case <code>feature/xyz</code> New features, schema changes <code>fix/xyz</code> Bug fixes to transformations <code>experiment/xyz</code> Exploratory work (may be abandoned) <code>release/v1.2</code> Tagged releases for rollback points <code>dev</code> Shared development branch"},{"location":"blog/04-project-nessie-versioning/#when-to-use-branches","title":"When to Use Branches","text":"Scenario Use Branch? Adding new column to existing table Yes Changing data type Yes Testing new transformation logic Yes Daily data ingestion No (use main) Bug fix to production Yes (then merge quickly) Exploratory analysis Optional (nice for isolation)"},{"location":"blog/04-project-nessie-versioning/#nessie-vs-iceberg-understanding-the-layers","title":"Nessie vs Iceberg: Understanding the Layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Nessie (Catalog Layer)                         \u2502\n\u2502 - Branch: main, dev, feature/metrics           \u2502\n\u2502 - Commit: \"Promote validated data\"             \u2502\n\u2502 - References: Which branch has which tables    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502 (Points to)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Iceberg (Table Format Layer)                   \u2502\n\u2502 - Snapshot: v1.metadata.json                  \u2502\n\u2502 - Manifest: Files in this snapshot             \u2502\n\u2502 - Data: S3 parquet files                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Nessie = \"which version of which table per branch\" Iceberg = \"what files make up this table version\"</p> <p>Together: Complete versioning from storage to queries.</p>"},{"location":"blog/04-project-nessie-versioning/#real-world-scenario-handling-a-bug","title":"Real-World Scenario: Handling a Bug","text":"<p>Let's say your transformation has a bug:</p> <pre><code>-- Bug: All glucose values are multiplied by 2!\nSELECT glucose_mg_dl * 2 as glucose_mg_dl  -- WRONG\nFROM stg_glucose_entries;\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#without-nessie-disaster","title":"Without Nessie (Disaster)","text":"<pre><code>1. Bug deployed to main\n   \u2193 Production dashboards show 2x glucose values\n   \u2193 People think blood sugar is spiking\n   \u2193 Alerts fire for high glucose\n   \u2193 (This happened to real patients, very bad)\n   \u2193\n2. Discover bug 2 hours later\n   \u2193\n3. Fix and re-run\n   \u2193\n4. Need to clean up corrupted 2 hours of data (hard!)\n   \u2193\n5. Audit trail: ? (who made the change?)\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#with-nessie-safe","title":"With Nessie (Safe)","text":"<pre><code>1. Bug caught during dev branch testing\n   \u2193 Run quality checks on dev branch\n   \u2193 Tests fail: \"glucose_mg_dl should be &lt; 500\"\n   \u2193\n2. Fix bug, re-run on dev\n   \u2193 Tests pass\n   \u2193\n3. Merge dev \u2192 main only when validated\n   \u2193 main branch still shows correct data\n   \u2193\n4. Audit trail: commit \"Fix glucose calculation bug\"\n   \u2193 Can query dev branch to see what was wrong\n</code></pre>"},{"location":"blog/04-project-nessie-versioning/#phlos-nessie-configuration","title":"Phlo's Nessie Configuration","text":"<p>In <code>docker-compose.yml</code>:</p> <pre><code>nessie:\n  image: ghcr.io/projectnessie/nessie:${NESSIE_VERSION}\n  environment:\n    NESSIE_VERSION_STORE_TYPE: JDBC\n    # Nessie metadata stored in Postgres\n    QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres:5432/lakehouse\n    # Iceberg warehouse location\n    nessie.catalog.warehouses.warehouse.location: s3://lake/warehouse\n    # MinIO S3 access\n    nessie.catalog.service.s3.default-options.endpoint: http://minio:9000/\n    nessie.catalog.service.s3.default-options.access-key: minioadmin\n    nessie.catalog.service.s3.default-options.secret: minioadmin\n</code></pre> <p>Breaking this down:</p> <ul> <li>JDBC: Nessie metadata (commits, branches) in Postgres</li> <li>Warehouse: Iceberg data files in MinIO S3</li> <li>S3 access: Credentials for MinIO</li> </ul>"},{"location":"blog/04-project-nessie-versioning/#next-data-ingestion","title":"Next: Data Ingestion","text":"<p>Now we understand:</p> <ul> <li>Iceberg: Table format with snapshots and time travel</li> <li>Nessie: Git-like branching on top of Iceberg</li> </ul> <p>Next: How does data actually get into this system?</p> <p>Part 5: Data Ingestion with DLT and PyIceberg</p> <p>See you then!</p>"},{"location":"blog/04-project-nessie-versioning/#summary","title":"Summary","text":"<p>Project Nessie:</p> <ul> <li>Branch isolation (dev/staging/prod)</li> <li>Atomic merges (all-or-nothing)</li> <li>Commit history (audit trail)</li> <li>Tags for releases</li> <li>REST API for automation</li> </ul> <p>In Phlo (Write-Audit-Publish):</p> <ul> <li><code>branch_creation_sensor</code> - Auto-creates pipeline branch on job start</li> <li><code>auto_promotion_sensor</code> - Auto-merges to main when quality checks pass</li> <li><code>branch_cleanup_sensor</code> - Cleans up old branches after retention period</li> <li>Catalog-based branching: <code>iceberg</code> (main) vs <code>iceberg_dev</code> (dev)</li> <li>All writes happen on feature branch - only validated data reaches main</li> </ul> <p>Next: Part 5: Data Ingestion\u2014Getting Data Into the Lakehouse</p>"},{"location":"blog/05-data-ingestion/","title":"Part 5: Data Ingestion\u2014Getting Data Into the Lakehouse","text":"<p>We have our lakehouse infrastructure. Now: how does data actually get in?</p> <p>Phlo uses a two-step pattern:</p> <ol> <li>DLT (Data Load Tool): Fetch and stage data</li> <li>PyIceberg: Merge staged data into Iceberg tables</li> </ol>"},{"location":"blog/05-data-ingestion/#the-two-step-ingestion-pattern","title":"The Two-Step Ingestion Pattern","text":"<p>Why two steps instead of one?</p> <pre><code>Single Step (Risky)\n    External API\n      \u2193 (network fails?)\n    Iceberg Table\n      (Corruption if interrupted)\n\nTwo Steps (Safe)\n    External API\n      \u2193 (network fails? No problem, retry)\n    S3 Staging (Temporary)\n      \u2193 (Has backup of raw data)\n    Iceberg Table\n      (Merge with idempotent deduplication)\n</code></pre> <p>The two-step pattern ensures:</p> <ul> <li>If network fails during fetch \u2192 restart from API</li> <li>\ud83d\udce6 If staging fails \u2192 S3 has backup</li> <li>If merge fails \u2192 can retry with same data</li> <li>Idempotent: run multiple times safely</li> </ul>"},{"location":"blog/05-data-ingestion/#step-1-dlt-data-load-tool","title":"Step 1: DLT (Data Load Tool)","text":"<p>DLT is a Python library that:</p> <ul> <li>Fetches data from sources</li> <li>Normalizes schema (makes consistent)</li> <li>Stages to parquet files</li> </ul>"},{"location":"blog/05-data-ingestion/#the-phlo_ingestion-decorator","title":"The @phlo_ingestion Decorator","text":"<p>Phlo provides the <code>@phlo_ingestion</code> decorator to simplify DLT ingestion. Here's the actual implementation from the glucose platform:</p> <pre><code># From phlo-examples/nightscout/workflows/ingestion/nightscout/readings.py\n\nimport phlo\nfrom dlt.sources.rest_api import rest_api\nfrom workflows.schemas.nightscout import RawGlucoseEntries\n\n@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",\n    validation_schema=RawGlucoseEntries,\n    group=\"nightscout\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef glucose_entries(partition_date: str):\n    \"\"\"\n    Ingest Nightscout glucose entries using DLT rest_api source.\n\n    Fetches CGM glucose readings from the Nightscout API for a specific partition date,\n    stages to parquet, and merges to Iceberg with idempotent deduplication.\n\n    Features:\n    - Idempotent ingestion: safe to run multiple times without duplicates\n    - Deduplication based on _id field (Nightscout's unique entry ID)\n    - Daily partitioning by timestamp\n    - Automatic validation with Pandera schema\n    - Branch-aware writes to Iceberg\n\n    Args:\n        partition_date: Date partition in YYYY-MM-DD format\n\n    Returns:\n        DLT resource for glucose entries, or None if no data\n    \"\"\"\n    start_time_iso = f\"{partition_date}T00:00:00.000Z\"\n    end_time_iso = f\"{partition_date}T23:59:59.999Z\"\n\n    source = rest_api(\n        client={\n            \"base_url\": \"https://gwp-diabetes.fly.dev/api/v1\",\n        },\n        resources=[\n            {\n                \"name\": \"entries\",\n                \"endpoint\": {\n                    \"path\": \"entries.json\",\n                    \"params\": {\n                        \"count\": 10000,\n                        \"find[dateString][$gte]\": start_time_iso,\n                        \"find[dateString][$lt]\": end_time_iso,\n                    },\n                },\n            }\n        ],\n    )\n\n    return source\n</code></pre>"},{"location":"blog/05-data-ingestion/#what-phlo_ingestion-does","title":"What @phlo_ingestion Does","text":"<p>The decorator handles all the complexity:</p> <ol> <li>DLT Pipeline Setup: Automatically configures DLT staging and execution</li> <li>Schema Validation: Validates data with Pandera schema before ingestion</li> <li>Iceberg Merge: Performs idempotent upsert to Iceberg table using unique_key</li> <li>Scheduling: Supports cron-based scheduling</li> <li>Freshness Checks: Monitors data freshness (1-24 hours in this example)</li> <li>Asset Metadata: Tracks lineage and dependencies in Dagster</li> </ol>"},{"location":"blog/05-data-ingestion/#merge-strategies","title":"Merge Strategies","text":"<p>Phlo supports two merge strategies, allowing you to optimize for different data patterns:</p>"},{"location":"blog/05-data-ingestion/#append-strategy-insert-only","title":"Append Strategy (Insert-Only)","text":"<p>Best for immutable event streams where you never update existing records:</p> <pre><code>@phlo_ingestion(\n    table_name=\"api_events\",\n    unique_key=\"event_id\",\n    validation_schema=EventSchema,\n    merge_strategy=\"append\",  # Insert-only, no deduplication\n    group=\"events\",\n)\ndef api_events(partition_date: str):\n    return rest_api(...)\n</code></pre> <p>Characteristics:</p> <ul> <li>Fastest performance (no deduplication overhead)</li> <li>No checking for duplicates</li> <li>Simply appends all new records</li> <li>Use for: Server logs, clickstream events, time-series sensor data, immutable audit trails</li> </ul> <p>Trade-offs:</p> <ul> <li>If you accidentally run the same partition twice, you'll get duplicates</li> <li>No way to update existing records</li> <li>Requires careful pipeline design to avoid re-runs</li> </ul>"},{"location":"blog/05-data-ingestion/#merge-strategy-upsert-with-deduplication","title":"Merge Strategy (Upsert with Deduplication)","text":"<p>Best for dimension tables and data that may need updates:</p> <pre><code>@phlo_ingestion(\n    table_name=\"user_profiles\",\n    unique_key=\"user_id\",\n    validation_schema=UserSchema,\n    merge_strategy=\"merge\",      # Upsert mode\n    merge_config={\"deduplication_method\": \"last\"},  # Keep most recent\n    group=\"users\",\n)\ndef user_profiles(partition_date: str):\n    return rest_api(...)\n</code></pre> <p>Deduplication Strategies:</p> <ol> <li><code>last</code> (default): Keep the most recent occurrence</li> </ol> <p><code>python    merge_config={\"deduplication_method\": \"last\"}</code></p> <ul> <li>Based on insertion order during the pipeline run</li> <li>Most common choice for dimension tables</li> <li> <p>Example: User profile updates (keep latest email, phone, etc.)</p> </li> <li> <p><code>first</code>: Keep the earliest occurrence</p> </li> </ul> <p><code>python    merge_config={\"deduplication_method\": \"first\"}</code></p> <ul> <li>Useful when first value is authoritative</li> <li> <p>Example: Initial signup timestamp, first purchase date</p> </li> <li> <p><code>hash</code>: Keep based on content hash    <code>python    merge_config={\"deduplication_method\": \"hash\"}</code></p> </li> <li> <p>Compares full record content, not just timestamp</p> </li> <li>Useful when you want to detect actual data changes</li> <li>Example: Configuration snapshots (only update if content differs)</li> </ul> <p>Characteristics:</p> <ul> <li>Performs upsert: UPDATE if <code>unique_key</code> exists, INSERT if new</li> <li>Removes duplicates within the same batch</li> <li>Idempotent: running multiple times produces same result</li> <li>Use for: User profiles, product catalogs, reference data, slowly changing dimensions</li> </ul> <p>Trade-offs:</p> <ul> <li>Slower than append (requires deduplication logic)</li> <li>More memory usage during merge</li> <li>Worth it for data correctness</li> </ul>"},{"location":"blog/05-data-ingestion/#strategy-comparison","title":"Strategy Comparison","text":"Aspect Append Merge (last) Merge (first) Merge (hash) Performance Fastest Medium Medium Slowest Deduplication None By order By order By content Idempotency No Yes Yes Yes Updates No Yes (keeps latest) No (keeps first) Yes (if changed) Use Case Logs, events Dimensions Historical records Config snapshots"},{"location":"blog/05-data-ingestion/#real-world-example-glucose-data","title":"Real-World Example: Glucose Data","text":"<p>The glucose ingestion uses merge strategy because:</p> <ol> <li>API may return overlapping data: Querying \"last 24 hours\" twice gives duplicates</li> <li>Data corrections: Nightscout allows retroactive corrections to glucose readings</li> <li>Idempotency: We want <code>materialize --partition 2024-10-15</code> to be safe to run multiple times</li> </ol> <pre><code>@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",              # Nightscout's unique entry ID\n    merge_strategy=\"merge\",        # Upsert mode\n    merge_config={\"deduplication_method\": \"last\"},  # Keep most recent reading\n    validation_schema=RawGlucoseEntries,\n    ...\n)\n</code></pre> <p>If we used <code>append</code> strategy instead:</p> <ul> <li>Running the same partition twice would create duplicates</li> <li>Corrected readings wouldn't update (you'd have both old and new)</li> <li>dbt transformations downstream would need to handle deduplication</li> </ul>"},{"location":"blog/05-data-ingestion/#dlt-schema-normalization","title":"DLT Schema Normalization","text":"<p>DLT normalizes messy API responses:</p> <pre><code># API Response (original)\n[\n  {\n    \"dateString\": \"2024-10-15T10:30:00.000Z\",\n    \"_id\": \"abc123\",\n    \"sgv\": 145,\n    \"direction\": \"Flat\",\n    \"device\": \"iPhone\",\n    \"type\": \"sgv\"\n  },\n  ...\n]\n\n# After DLT (normalized schema)\nParquet file with columns:\n\u251c\u2500\u2500 date_string: string (converted from dateString)\n\u251c\u2500\u2500 _id: string\n\u251c\u2500\u2500 sgv: int64\n\u251c\u2500\u2500 direction: string\n\u251c\u2500\u2500 device: string\n\u251c\u2500\u2500 type: string\n</code></pre> <p>DLT automatically:</p> <ul> <li>Infers column types</li> <li>\ud83d\udeab Handles nulls</li> <li>\ud83d\udcdb Renames fields (snake_case)</li> <li>Validates structure</li> </ul>"},{"location":"blog/05-data-ingestion/#pandera-schema-validation","title":"Pandera Schema Validation","text":"<p>The validation schema is defined in <code>phlo-examples/nightscout/workflows/schemas/nightscout.py</code>:</p> <pre><code># From workflows/schemas/nightscout.py\n\nfrom pandera.pandas import DataFrameModel, Field\n\nclass RawGlucoseEntries(DataFrameModel):\n    \"\"\"\n    Schema for raw Nightscout glucose entries from the API.\n\n    Validates raw glucose data at ingestion time:\n    - Valid glucose ranges (1-1000 mg/dL for raw data)\n    - Proper field types and nullability\n    - Required metadata fields\n    - Unique entry IDs\n    \"\"\"\n\n    _id: str = Field(\n        nullable=False,\n        unique=True,\n        description=\"Nightscout entry ID (unique identifier)\",\n    )\n\n    sgv: int = Field(\n        ge=1,\n        le=1000,\n        nullable=False,\n        description=\"Sensor glucose value in mg/dL (1-1000 for raw data)\",\n    )\n\n    date: int = Field(\n        nullable=False,\n        description=\"Unix timestamp in milliseconds\",\n    )\n\n    date_string: datetime = Field(\n        nullable=False,\n        description=\"ISO 8601 timestamp\",\n    )\n\n    direction: str | None = Field(\n        isin=[\"Flat\", \"FortyFiveUp\", \"FortyFiveDown\", \"SingleUp\", \"SingleDown\", \"DoubleUp\", \"DoubleDown\", \"NONE\"],\n        nullable=True,\n        description=\"Trend direction (e.g., 'SingleUp', 'Flat')\",\n    )\n\n    class Config:\n        strict = False  # Allow DLT metadata fields\n        coerce = True\n</code></pre>"},{"location":"blog/05-data-ingestion/#step-2-pyiceberg-merge-into-lakehouse","title":"Step 2: PyIceberg (Merge into Lakehouse)","text":"<p>PyIceberg is the Python client for Iceberg. It:</p> <ul> <li>Loads the staged parquet</li> <li>Creates/updates Iceberg table</li> <li>Performs idempotent merge (upsert)</li> </ul>"},{"location":"blog/05-data-ingestion/#creating-the-iceberg-table","title":"Creating the Iceberg Table","text":"<p>First, ensure the table exists:</p> <pre><code># From packages/phlo-iceberg/src/phlo_iceberg/tables.py\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, IntegerType, TimestampType\n\nschema = Schema(\n    NestedField(1, \"_id\", StringType(), required=True),\n    NestedField(2, \"sgv\", IntegerType(), required=False),\n    NestedField(3, \"date_string\", StringType(), required=False),\n    NestedField(4, \"direction\", StringType(), required=False),\n    NestedField(5, \"timestamp_iso\", StringType(), required=False),\n    NestedField(6, \"_cascade_ingested_at\", TimestampType(), required=False),\n)\n\ncatalog.create_table(\n    identifier=\"raw.glucose_entries\",\n    schema=schema,\n    partition_spec=None  # Iceberg will auto-partition by date\n)\n</code></pre> <p>Result in MinIO:</p> <pre><code>s3://lake/warehouse/raw/glucose_entries/\n\u251c\u2500\u2500 metadata/\n\u2502   \u2514\u2500\u2500 v1.metadata.json      \u2190 Table created\n\u2514\u2500\u2500 data/ (empty)\n</code></pre>"},{"location":"blog/05-data-ingestion/#merging-data-idempotent-upsert","title":"Merging Data (Idempotent Upsert)","text":"<p>Now merge staged parquet into Iceberg:</p> <pre><code># From workflows/resources/iceberg.py\n\ndef merge_parquet(\n    self,\n    table_name: str,\n    data_path: str,\n    unique_key: str = \"_id\"\n) -&gt; dict:\n    \"\"\"\n    Merge parquet file into Iceberg table (idempotent upsert).\n\n    Args:\n        table_name: \"raw.glucose_entries\"\n        data_path: \"/path/to/data.parquet\"\n        unique_key: Column to deduplicate on\n\n    Returns:\n        Metrics: rows_inserted, rows_deleted, etc.\n    \"\"\"\n\n    # Load table\n    table = self.catalog.load_table(table_name)\n\n    # Read new data from parquet\n    new_df = read_parquet(data_path)\n\n    # SQL merge operation:\n    # - If _id exists: delete old, insert new (deduplication)\n    # - If _id new: insert it\n    merge_query = f\"\"\"\n    MERGE INTO {table_name} t\n    USING (SELECT * FROM read_parquet('{data_path}')) n\n    ON t.{unique_key} = n.{unique_key}\n    WHEN MATCHED THEN DELETE\n    WHEN NOT MATCHED THEN INSERT *\n    \"\"\"\n\n    result = self.trino.execute(merge_query)\n\n    return {\n        'rows_inserted': result['inserted'],\n        'rows_deleted': result['deleted'],\n        'rows_total': len(table.scan().to_pandas())\n    }\n</code></pre> <p>This ensures idempotency: running the same ingestion multiple times produces the same result.</p>"},{"location":"blog/05-data-ingestion/#real-example-glucose-ingestion-with-phlo_ingestion","title":"Real Example: Glucose Ingestion with @phlo_ingestion","text":"<p>Let's trace through what happens when you materialize a <code>@phlo_ingestion</code> asset:</p> <pre><code># Timeline: 2024-10-15\n\n# 1. Materialize the asset\ndagster asset materialize --select glucose_entries \\\n  --partition \"2024-10-15\"\n\n# 2. The @phlo_ingestion decorator executes your function\n# Your function returns a DLT source configured for 2024-10-15\n\n# 3. Decorator automatically stages data via DLT\nFetching data from Nightscout API...\nSuccessfully fetched 288 entries from API\nStaging data to parquet via DLT...\nDLT staging completed in 1.23s\n\n# 4. Decorator validates with Pandera schema (RawGlucoseEntries)\nValidating raw glucose data with Pandera schema...\nRaw data validation passed for 288 entries\n\n# 5. Decorator creates Iceberg table if needed\nEnsuring Iceberg table glucose_entries exists...\n\n# 6. Decorator merges with deduplication (using unique_key=\"_id\")\nMerging data to Iceberg table (idempotent upsert)...\nMerged 288 rows to glucose_entries\n  (deleted 0 existing duplicates)\n\n# 7. Decorator tracks metadata in Dagster\nAsset materialized successfully\nMetadata:\n  - rows_ingested: 288\n  - table_name: glucose_entries\n  - partition_date: 2024-10-15\n\n# Success!\nIngestion completed successfully in 2.45s\n</code></pre> <p>You wrote: ~10 lines of code (just the DLT source configuration) You got: Full ingestion pipeline with validation, staging, merging, and monitoring</p> <p>Now the data lives in Iceberg:</p> <pre><code>s3://lake/warehouse/raw/glucose_entries/\n\u251c\u2500\u2500 metadata/\n\u2502   \u251c\u2500\u2500 v1.metadata.json\n\u2502   \u2514\u2500\u2500 snap-1234.avro         \u2190 New snapshot for this ingestion\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 year=2024/month=10/day=15/\n        \u251c\u2500\u2500 00001.parquet (100 rows)\n        \u251c\u2500\u2500 00002.parquet (100 rows)\n        \u2514\u2500\u2500 00003.parquet (88 rows)\n</code></pre>"},{"location":"blog/05-data-ingestion/#quality-checks-with-phlo_quality","title":"Quality Checks with @phlo_quality","text":"<p>After ingestion and transformation, Phlo validates data with quality checks. The <code>@phlo_quality</code> decorator provides a declarative way to define quality checks:</p> <pre><code># From phlo-examples/nightscout/workflows/quality/nightscout.py\n\nimport phlo\nfrom phlo_quality import FreshnessCheck, NullCheck, RangeCheck\n\n@phlo_quality(\n    table=\"silver.fct_glucose_readings\",\n    checks=[\n        NullCheck(columns=[\"entry_id\", \"glucose_mg_dl\", \"reading_timestamp\"]),\n        RangeCheck(column=\"glucose_mg_dl\", min_value=20, max_value=600),\n        RangeCheck(column=\"hour_of_day\", min_value=0, max_value=23),\n        FreshnessCheck(column=\"reading_timestamp\", max_age_hours=24),\n    ],\n    group=\"nightscout\",\n    blocking=True,\n)\ndef glucose_readings_quality():\n    \"\"\"Declarative quality checks for glucose readings using @phlo_quality.\"\"\"\n    pass\n</code></pre> <p>The <code>@phlo_quality</code> decorator provides:</p> <ol> <li>NullCheck: Ensures critical columns have no null values</li> <li>RangeCheck: Validates numeric values are within expected ranges</li> <li>FreshnessCheck: Ensures data is not stale (within 24 hours)</li> <li>Blocking: If <code>blocking=True</code>, downstream assets wait for checks to pass</li> </ol>"},{"location":"blog/05-data-ingestion/#traditional-asset-checks-alternative","title":"Traditional Asset Checks (Alternative)","text":"<p>You can also use traditional Dagster asset checks for more control:</p> <pre><code># From workflows/quality/nightscout.py\n\nfrom dagster import AssetCheckResult, AssetKey, asset_check\nfrom workflows.schemas.nightscout import FactGlucoseReadings\n\n@asset_check(\n    name=\"nightscout_glucose_quality\",\n    asset=AssetKey([\"fct_glucose_readings\"]),\n    blocking=True,\n)\ndef nightscout_glucose_quality_check(context, trino: TrinoResource) -&gt; AssetCheckResult:\n    \"\"\"Quality check using Pandera for type-safe schema validation.\"\"\"\n\n    # Query data from Iceberg via Trino\n    query = \"SELECT * FROM iceberg_dev.silver.fct_glucose_readings\"\n    with trino.cursor(schema=\"silver\") as cursor:\n        cursor.execute(query)\n        rows = cursor.fetchall()\n        columns = [desc[0] for desc in cursor.description]\n\n    fact_df = pd.DataFrame(rows, columns=columns)\n\n    # Validate with Pandera schema\n    try:\n        FactGlucoseReadings.validate(fact_df, lazy=True)\n        return AssetCheckResult(\n            passed=True,\n            metadata={\n                \"rows_validated\": MetadataValue.int(len(fact_df)),\n            }\n        )\n    except pandera.errors.SchemaErrors as err:\n        return AssetCheckResult(\n            passed=False,\n            metadata={\n                \"failed_checks\": MetadataValue.int(len(err.failure_cases)),\n            }\n        )\n</code></pre> <p>This approach gives you more control over the validation logic and error handling.</p>"},{"location":"blog/05-data-ingestion/#handling-different-data-sources","title":"Handling Different Data Sources","text":"<p>The <code>@phlo_ingestion</code> decorator works with any DLT source. You just return a DLT source/resource and the decorator handles the rest.</p> <p>Pattern: Define your data source, return it, and let <code>@phlo_ingestion</code> handle staging, validation, and merging.</p> <pre><code># Example: Custom API ingestion\n\nimport phlo\nfrom dlt.sources.rest_api import rest_api\n\n@phlo_ingestion(\n    table_name=\"github_events\",\n    unique_key=\"event_id\",\n    validation_schema=GitHubEventSchema,\n    group=\"github\",\n)\ndef github_events(partition_date: str):\n    \"\"\"Ingest GitHub events using DLT.\"\"\"\n\n    # DLT rest_api source handles pagination and retries\n    source = rest_api(\n        client={\n            \"base_url\": \"https://api.github.com\",\n            \"auth\": {\n                \"token\": os.getenv(\"GITHUB_TOKEN\"),\n            },\n        },\n        resources=[\n            {\n                \"name\": \"events\",\n                \"endpoint\": {\n                    \"path\": \"/users/{username}/events\",\n                    \"params\": {\n                        \"per_page\": 100,\n                    },\n                },\n            }\n        ],\n    )\n\n    return source\n    # @phlo_ingestion automatically:\n    # 1. Runs DLT pipeline to stage to parquet\n    # 2. Validates with GitHubEventSchema\n    # 3. Merges to Iceberg table with deduplication on event_id\n</code></pre>"},{"location":"blog/05-data-ingestion/#ingestion-patterns-in-phlo","title":"Ingestion Patterns in Phlo","text":""},{"location":"blog/05-data-ingestion/#pattern-1-api-ingestion-nightscout-github","title":"Pattern 1: API Ingestion (Nightscout, GitHub)","text":"<pre><code>API (network)\n  \u2193 (requests.get)\nPython dict\n  \u2193 (DLT pipeline)\nS3 parquet\n  \u2193 (PyIceberg merge)\nIceberg table\n</code></pre>"},{"location":"blog/05-data-ingestion/#pattern-2-file-upload-csv-excel","title":"Pattern 2: File Upload (CSV, Excel)","text":"<pre><code>Local file\n  \u2193 (read_csv, openpyxl)\nPandas DataFrame\n  \u2193 (DLT pipeline)\nS3 parquet\n  \u2193 (PyIceberg merge)\nIceberg table\n</code></pre>"},{"location":"blog/05-data-ingestion/#pattern-3-database-replication","title":"Pattern 3: Database Replication","text":"<pre><code>Source Database (PostgreSQL, MySQL)\n  \u2193 (SELECT * from table)\nPandas DataFrame\n  \u2193 (DLT pipeline)\nS3 parquet\n  \u2193 (PyIceberg merge)\nIceberg table\n</code></pre> <p>All follow the same pattern for safety and idempotency.</p>"},{"location":"blog/05-data-ingestion/#hands-on-trace-an-ingestion","title":"Hands-On: Trace an Ingestion","text":"<pre><code># Run ingestion and watch the flow\n# This uses the @phlo_ingestion decorated function\ndagster asset materialize \\\n  --select glucose_entries \\\n  --partition \"2024-10-15\"\n\n# Check Iceberg table via PyIceberg\npython3 &lt;&lt; 'EOF'\nfrom phlo_iceberg.catalog import get_catalog\nimport pandas as pd\n\ncatalog = get_catalog()\ntable = catalog.load_table(\"glucose_entries\")\n\n# Load all data\ndf = table.scan().to_pandas()\nprint(f\"Total rows: {len(df)}\")\nprint(f\"\\nLatest snapshot: {table.current_snapshot().snapshot_id}\")\nprint(f\"\\nColumns:\\n{df.columns.tolist()}\")\nprint(f\"\\nSample data:\\n{df.head(3)}\")\nEOF\n\n# Or query via Trino\ndocker exec trino trino \\\n  --catalog iceberg_dev \\\n  --schema raw \\\n  --execute \"SELECT COUNT(*) as total FROM glucose_entries;\"\n</code></pre>"},{"location":"blog/05-data-ingestion/#performance-considerations","title":"Performance Considerations","text":""},{"location":"blog/05-data-ingestion/#batch-size","title":"Batch Size","text":"<p>DLT chunks data into batches:</p> <pre><code># Small batches = more overhead\ninfo = pipeline.run(\n    provide_entries(),\n    loader_file_format=\"parquet\",\n)  # Default: batches of ~10K rows\n\n# For large datasets, you want good batch size\n# 100K-1M rows per batch is typical\n</code></pre>"},{"location":"blog/05-data-ingestion/#deduplication-key","title":"Deduplication Key","text":"<p>Choose a column that's truly unique:</p> <pre><code># Good: Nightscout API's unique ID\nunique_key=\"_id\"  # MongoDB ObjectId, guaranteed unique\n\n# Bad: Reading data multiple times\nunique_key=\"date_string\"  # Multiple readings per minute!\n</code></pre>"},{"location":"blog/05-data-ingestion/#idempotency","title":"Idempotency","text":"<p>Always design for idempotency:</p> <pre><code># Good: Can run any time, same result\nmerge_parquet(table, data, unique_key=\"_id\")\nmerge_parquet(table, data, unique_key=\"_id\")  # Second run does nothing\n# Result: 288 rows\n\n# Bad: Non-idempotent (duplicates!)\nappend_parquet(table, data)\nappend_parquet(table, data)  # Second run duplicates!\n# Result: 576 rows (corrupted)\n</code></pre>"},{"location":"blog/05-data-ingestion/#next-transformations","title":"Next: Transformations","text":"<p>Data is now in the lakehouse. Next: Transform it with dbt and Trino.</p> <p>Part 6: SQL Transformations with dbt</p> <p>See you there!</p>"},{"location":"blog/05-data-ingestion/#summary","title":"Summary","text":"<p>Phlo's Ingestion with @phlo_ingestion:</p> <p>The <code>@phlo_ingestion</code> decorator simplifies data ingestion by handling:</p> <ol> <li>DLT pipeline execution: Stages data from source to parquet</li> <li>Schema validation: Validates with Pandera before loading</li> <li>Iceberg merge: Performs idempotent upsert using unique_key</li> <li>Monitoring: Tracks metrics in Dagster (rows, freshness, etc.)</li> <li>Scheduling: Supports cron-based execution</li> </ol> <p>Decorator Parameters:</p> <ul> <li><code>table_name</code>: Iceberg table to write to</li> <li><code>unique_key</code>: Column for deduplication</li> <li><code>validation_schema</code>: Pandera schema for validation</li> <li><code>group</code>: Asset group for organization</li> <li><code>cron</code>: Schedule (optional)</li> <li><code>freshness_hours</code>: Expected data freshness (optional)</li> </ul> <p>Your Function:</p> <ul> <li>Takes <code>partition_date: str</code> parameter</li> <li>Returns a DLT source or resource</li> <li>The decorator handles everything else</li> </ul> <p>Why This Pattern Works:</p> <ul> <li>Simple: Write 10 lines, get full pipeline</li> <li>Idempotent: Safe to retry/rerun</li> <li>Atomic: All-or-nothing commits</li> <li>Validated: Pandera schema checks</li> <li>Auditable: Iceberg snapshot history</li> <li>Scalable: Works from KB to TB</li> </ul> <p>Quality Checks:</p> <ul> <li>Use <code>@phlo_quality</code> for declarative checks (NullCheck, RangeCheck, FreshnessCheck)</li> <li>Or use traditional <code>@asset_check</code> for custom validation logic</li> <li>Both integrate with Dagster's asset check system</li> </ul> <p>Next: Part 6: SQL Transformations with dbt\u2014The Right Way</p>"},{"location":"blog/06-dbt-transformations/","title":"Part 6: SQL Transformations with dbt\u2014The Right Way","text":"<p>Raw data is in the lakehouse. Now we transform it into analysis-ready datasets using dbt (data build tool).</p> <p>dbt solves a critical problem: How do you manage SQL transformations professionally?</p>"},{"location":"blog/06-dbt-transformations/#the-problem-without-dbt","title":"The Problem Without dbt","text":"<pre><code># Without dbt: SQL in Python files (nightmare)\n\ndef transform_glucose():\n    sql1 = \"\"\"\n    CREATE TABLE bronze_stg_entries AS\n    SELECT _id, sgv, date_string FROM raw.entries\n    WHERE sgv IS NOT NULL\n    \"\"\"\n    trino.execute(sql1)\n\n    sql2 = \"\"\"\n    CREATE TABLE silver_fct_readings AS\n    SELECT\n        entry_id,\n        glucose_mg_dl,\n        CASE WHEN glucose_mg_dl &lt; 70 THEN 'hypoglycemia' END as category\n    FROM bronze_stg_entries\n    \"\"\"\n    trino.execute(sql2)\n\n    sql3 = \"\"\"\n    CREATE TABLE gold_dim_date AS\n    SELECT DISTINCT DATE(reading_timestamp) as reading_date\n    FROM silver_fct_readings\n    \"\"\"\n    trino.execute(sql3)\n\n    # Problems:\n    # 1. No documentation\n    # 2. No testing (did the join work?)\n    # 3. No lineage (which table depends on which?)\n    # 4. No version control (what changed?)\n    # 5. Manual dependency management (run sql3 after sql2)\n</code></pre>"},{"location":"blog/06-dbt-transformations/#dbt-solves-this","title":"dbt Solves This","text":"<pre><code>-- dbt: SQL with structure and discipline\n\n-- File: models/bronze/stg_glucose_entries.sql\nSELECT\n    _id as entry_id,\n    sgv as glucose_mg_dl,\n    date_string as timestamp_iso,\n    -- Comments documented\nFROM {{ source('dagster_assets', 'glucose_entries') }}\nWHERE sgv IS NOT NULL\n\n-- File: models/silver/fct_glucose_readings.sql\nSELECT\n    entry_id,\n    glucose_mg_dl,\n    CASE\n        WHEN glucose_mg_dl &lt; 70 THEN 'hypoglycemia'\n        WHEN glucose_mg_dl &lt;= 180 THEN 'in_range'\n        ELSE 'hyperglycemia'\n    END as glucose_category\nFROM {{ ref('stg_glucose_entries') }}  -- Auto-dependency!\n\n-- File: models/gold/dim_date.sql\nSELECT DISTINCT DATE(reading_timestamp) as reading_date\nFROM {{ ref('fct_glucose_readings') }}  -- dbt finds dependencies\n</code></pre> <p>One command runs everything:</p> <pre><code>dbt build\n# dbt figures out: run bronze first, then silver, then gold\n# Tests each transformation\n# Generates documentation\n</code></pre>"},{"location":"blog/06-dbt-transformations/#dbts-four-core-features","title":"dbt's Four Core Features","text":""},{"location":"blog/06-dbt-transformations/#1-models-reusable-sql","title":"1. Models (Reusable SQL)","text":"<p>A model is a <code>.sql</code> file that transforms data:</p> <pre><code>-- models/bronze/stg_glucose_entries.sql\n{{ config(\n    materialized='view',  -- or 'table' for persistent storage\n    tags=['nightscout'],   -- organize models\n) }}\n\n-- CTEs for clarity\nWITH raw_data AS (\n    SELECT * FROM {{ source('dagster_assets', 'glucose_entries') }}\n),\n\ncleaned AS (\n    SELECT\n        _id as entry_id,\n        sgv as glucose_mg_dl,\n        date_string as timestamp_iso,\n        -- More transformations\n    FROM raw_data\n    WHERE sgv IS NOT NULL\n)\n\nSELECT * FROM cleaned\n</code></pre> <p>dbt materializes this as:</p> <ul> <li>View: Query source every time (slow for complex transforms)</li> <li>Table: Precompute once, fast queries (Phlo uses this)</li> <li>Incremental: Only process new data (for huge tables)</li> </ul> <p>Phlo's config:</p> <pre><code>-- Bronze models\n{{ config(materialized='view') }}\n-- Staging layer, not persisted\n\n-- Silver models\n{{ config(materialized='table') }}\n-- Fact tables, persisted in Iceberg\n\n-- Gold models\n{{ config(materialized='table') }}\n-- Dimensions, persisted in Iceberg\n</code></pre>"},{"location":"blog/06-dbt-transformations/#2-dependencies-auto-resolved","title":"2. Dependencies (Auto-Resolved)","text":"<p>Instead of manual ordering, dbt resolves dependencies:</p> <pre><code>-- Model A: source data\nFROM {{ source('dagster_assets', 'glucose_entries') }}\n\n-- Model B: depends on Model A\nFROM {{ ref('stg_glucose_entries') }}  -- ref('A')\n\n-- Model C: depends on Model B\nFROM {{ ref('fct_glucose_readings') }}  -- ref('B')\n</code></pre> <p>dbt builds a DAG (directed acyclic graph):</p> <pre><code>glucose_entries (source)\n    \u2193 (Model A)\nstg_glucose_entries\n    \u2193 (Model B)\nfct_glucose_readings\n    \u2193 (Model C)\ndim_date, mrt_glucose_readings\n    \u2193 (Model D)\ngold/marts tables\n</code></pre> <p>Execute order: automatic!</p>"},{"location":"blog/06-dbt-transformations/#3-testing-data-quality","title":"3. Testing (Data Quality)","text":"<p>dbt includes built-in tests:</p> <pre><code># models/silver/fct_glucose_readings.yml\nversion: 2\n\nmodels:\n  - name: fct_glucose_readings\n    description: Enriched glucose readings with business logic\n    columns:\n      - name: entry_id\n        tests:\n          - unique # Each entry_id appears once\n          - not_null # No missing values\n\n      - name: glucose_mg_dl\n        tests:\n          - not_null\n          - accepted_values:\n              values: [0] # 0 for missing data\n\n      - name: glucose_category\n        tests:\n          - accepted_values:\n              values:\n                [\n                  \"hypoglycemia\",\n                  \"in_range\",\n                  \"hyperglycemia_mild\",\n                  \"hyperglycemia_severe\",\n                ]\n</code></pre> <p>Run tests:</p> <pre><code>dbt test\n\n# Output\nRunning with dbt=1.8.0\nFound 3 models, 8 tests...\n\nTesting fct_glucose_readings\n  Running test unique_fct_glucose_readings_entry_id ... PASS\n  Running test not_null_fct_glucose_readings_entry_id ... PASS\n  Running test accepted_values_fct_glucose_readings_glucose_category ... PASS\n</code></pre>"},{"location":"blog/06-dbt-transformations/#4-documentation-auto-generated","title":"4. Documentation (Auto-Generated)","text":"<p>dbt generates a knowledge base from YAML:</p> <pre><code># models/silver/schema.yml\nversion: 2\n\nmodels:\n  - name: fct_glucose_readings\n    description: |\n      Enriched glucose readings with calculated metrics.\n      Serves as foundation for all downstream analytics.\n\n    columns:\n      - name: entry_id\n        description: Unique Nightscout entry ID (MongoDB ObjectId)\n\n      - name: glucose_mg_dl\n        description: Glucose reading in mg/dL\n        tests:\n          - not_null\n\n      - name: glucose_category\n        description: Classification (hypoglycemia/in_range/hyperglycemia)\n\n      - name: hour_of_day\n        description: Hour extracted from reading_timestamp (0-23)\n</code></pre> <p>Run:</p> <pre><code>dbt docs generate\ndbt docs serve  # Opens http://localhost:8000\n\n# Generates interactive documentation with:\n# - Column descriptions\n# - Test results\n# - Data lineage (visual DAG)\n# - Query execution stats\n</code></pre>"},{"location":"blog/06-dbt-transformations/#phlos-dbt-structure","title":"Phlo's dbt Structure","text":""},{"location":"blog/06-dbt-transformations/#directory-layout","title":"Directory Layout","text":"<pre><code>workflows/transforms/dbt/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 sources.yml         # External data sources (raw Iceberg tables)\n\u2502   \u251c\u2500\u2500 bronze/\n\u2502   \u2502   \u251c\u2500\u2500 stg_glucose_entries.sql    # Staging (filter, rename, validate)\n\u2502   \u2502   \u251c\u2500\u2500 stg_github_user_events.sql\n\u2502   \u2502   \u2514\u2500\u2500 stg_*.sql\n\u2502   \u251c\u2500\u2500 silver/\n\u2502   \u2502   \u251c\u2500\u2500 fct_glucose_readings.sql   # Fact tables (business logic)\n\u2502   \u2502   \u251c\u2500\u2500 fct_github_user_events.sql\n\u2502   \u2502   \u2514\u2500\u2500 fct_*.sql\n\u2502   \u251c\u2500\u2500 gold/\n\u2502   \u2502   \u251c\u2500\u2500 dim_date.sql               # Dimensions\n\u2502   \u2502   \u251c\u2500\u2500 mrt_glucose_readings.sql   # Metrics\n\u2502   \u2502   \u2514\u2500\u2500 *.sql\n\u2502   \u2514\u2500\u2500 marts_postgres/\n\u2502       \u251c\u2500\u2500 mrt_glucose_overview.sql   # Publish to Postgres\n\u2502       \u2514\u2500\u2500 *.sql\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 custom_tests.sql               # Custom SQL tests\n\u251c\u2500\u2500 profiles.yml                       # Connection config\n\u2514\u2500\u2500 dbt_project.yml                   # Project config\n</code></pre>"},{"location":"blog/06-dbt-transformations/#4-layer-architecture","title":"4-Layer Architecture","text":"<ol> <li> <p>Bronze (Staging):</p> </li> <li> <p>Raw data cleanup</p> </li> <li>Type conversions</li> <li>Rename columns</li> <li>Filter obvious errors</li> </ol> <p><code>sql    -- stg_glucose_entries.sql    SELECT        _id as entry_id,        CAST(sgv as INT) as glucose_mg_dl,        CAST(date_string as TIMESTAMP) as timestamp_iso    FROM {{ source('dagster_assets', 'glucose_entries') }}    WHERE sgv IS NOT NULL</code></p> <ol> <li> <p>Silver (Fact Tables):</p> </li> <li> <p>Business logic</p> </li> <li>Calculations</li> <li>Joins and enrichment</li> <li>Metrics</li> </ol> <p><code>sql    -- fct_glucose_readings.sql    SELECT        entry_id,        glucose_mg_dl,        CASE            WHEN glucose_mg_dl &lt; 70 THEN 'hypoglycemia'            WHEN glucose_mg_dl &lt;= 180 THEN 'in_range'            ELSE 'hyperglycemia'        END as glucose_category,        -- Rate of change (lag window function)        glucose_mg_dl - LAG(glucose_mg_dl) OVER (            PARTITION BY device ORDER BY reading_timestamp        ) as glucose_change_mg_dl    FROM {{ ref('stg_glucose_entries') }}</code></p> <ol> <li> <p>Gold (Dimensions):</p> </li> <li> <p>Pre-computed dimensions</p> </li> <li>Slow-changing dimensions</li> <li>Reference tables</li> </ol> <p><code>sql    -- dim_date.sql    SELECT DISTINCT        DATE(reading_timestamp) as reading_date,        EXTRACT(YEAR FROM reading_timestamp) as year,        EXTRACT(QUARTER FROM reading_timestamp) as quarter,        EXTRACT(MONTH FROM reading_timestamp) as month,        EXTRACT(WEEK FROM reading_timestamp) as week,        EXTRACT(DAY_OF_WEEK FROM reading_timestamp) as day_of_week    FROM {{ ref('fct_glucose_readings') }}    ORDER BY reading_date</code></p> <ol> <li> <p>Marts (Published):</p> </li> <li> <p>Business-ready tables</p> </li> <li>Published to Postgres for BI tools</li> <li>Aggregations and summaries</li> </ol> <p><code>sql    -- marts_postgres/mrt_glucose_overview.sql    SELECT        reading_date,        ROUND(AVG(glucose_mg_dl), 1) as avg_glucose,        MIN(glucose_mg_dl) as min_glucose,        MAX(glucose_mg_dl) as max_glucose,        COUNT(*) as reading_count,        ROUND(100.0 * SUM(CASE WHEN is_in_range THEN 1 ELSE 0 END)              / COUNT(*), 1) as percent_in_range    FROM {{ ref('fct_glucose_readings') }}    GROUP BY reading_date    ORDER BY reading_date DESC</code></p>"},{"location":"blog/06-dbt-transformations/#integration-with-phlo","title":"Integration with Phlo","text":""},{"location":"blog/06-dbt-transformations/#connection-configuration","title":"Connection Configuration","text":"<p>Nessie branching is configured via different Trino catalogs:</p> <pre><code># profiles.yml\nphlo:\n  target: dev # development by default\n\n  outputs:\n    dev:\n      type: trino\n      host: trino\n      port: 8080\n      catalog: iceberg_dev # Dev branch catalog\n      schema: bronze\n\n    prod:\n      type: trino\n      host: trino\n      port: 8080\n      catalog: iceberg # Main branch catalog\n      schema: bronze\n</code></pre> <p>The <code>iceberg_dev</code> catalog points to the Nessie dev branch, while <code>iceberg</code> points to main. This is configured in Trino's catalog properties, not via session properties.</p>"},{"location":"blog/06-dbt-transformations/#partition-aware-execution","title":"Partition-Aware Execution","text":"<p>dbt runs on daily partitions (via Dagster):</p> <pre><code>-- models/silver/fct_glucose_readings.sql\n{{ config(\n    materialized='table',\n) }}\n\nSELECT\n    entry_id,\n    glucose_mg_dl,\n    ...\nFROM {{ ref('stg_glucose_entries') }}\n{% if var('partition_date_str', None) is not none %}\n-- Filter to partition when running daily\nWHERE DATE(reading_timestamp) = DATE('{{ var(\"partition_date_str\") }}')\n{% endif %}\n</code></pre> <p>Dagster passes partition date:</p> <pre><code># From workflows/transform/dbt.py\n\nif context.has_partition_key:\n    partition_date = context.partition_key\n    build_args.extend([\n        \"--vars\",\n        f'{{\"partition_date_str\": \"{partition_date}\"}}'\n    ])\n</code></pre>"},{"location":"blog/06-dbt-transformations/#hands-on-run-dbt-transforms","title":"Hands-On: Run dbt Transforms","text":""},{"location":"blog/06-dbt-transformations/#option-1-via-dagster-ui","title":"Option 1: Via Dagster UI","text":"<ol> <li>Open http://localhost:10006</li> <li>Click asset: <code>stg_glucose_entries</code></li> <li>Click Materialize</li> <li>Watch dbt run in logs</li> </ol>"},{"location":"blog/06-dbt-transformations/#option-2-direct-command","title":"Option 2: Direct Command","text":"<pre><code># Run all models\ndocker exec dagster-webserver dbt build \\\n  --project-dir /app/workflows/transforms/dbt \\\n  --profiles-dir /app/workflows/transforms/dbt/profiles \\\n  --target dev\n\n# Run specific model\ndocker exec dagster-webserver dbt run \\\n  --project-dir /app/workflows/transforms/dbt \\\n  --profiles-dir /app/workflows/transforms/dbt/profiles \\\n  --select stg_glucose_entries\n\n# Run with tests\ndocker exec dagster-webserver dbt test \\\n  --project-dir /app/workflows/transforms/dbt \\\n  --profiles-dir /app/workflows/transforms/dbt/profiles\n</code></pre>"},{"location":"blog/06-dbt-transformations/#option-3-local-if-you-have-uv-installed","title":"Option 3: Local (if you have uv installed)","text":"<pre><code>cd workflows/transforms/dbt\n\n# Install dbt\nuv pip install dbt-trino\n\n# Create dbt profiles\nmkdir -p ~/.dbt\ncat &gt; ~/.dbt/profiles.yml &lt;&lt; EOF\nphlo:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      host: localhost\n      port: 8080\n      catalog: iceberg\n      schema: bronze\nEOF\n\n# Run dbt\ndbt run --select stg_glucose_entries\ndbt test\ndbt docs generate &amp;&amp; dbt docs serve\n</code></pre>"},{"location":"blog/06-dbt-transformations/#best-practices-in-dbt","title":"Best Practices in dbt","text":""},{"location":"blog/06-dbt-transformations/#1-naming-convention","title":"1. Naming Convention","text":"<pre><code>bronze/stg_*              Staging (from source)\nsilver/fct_*              Fact tables\nsilver/dim_*              Dimension tables\ngold/*                    Summarized/published\nmarts_postgres/*          Published to Postgres\n</code></pre>"},{"location":"blog/06-dbt-transformations/#2-ctes-for-clarity","title":"2. CTEs for Clarity","text":"<pre><code>-- Good: logical steps with CTEs\nWITH source_data AS (\n    SELECT * FROM {{ source(...) }}\n),\ncleaned AS (\n    SELECT ... FROM source_data WHERE ...\n),\nenriched AS (\n    SELECT ... FROM cleaned\n)\nSELECT * FROM enriched\n\n-- Bad: nested subqueries (hard to read)\nSELECT * FROM (\n    SELECT ... FROM (\n        SELECT * FROM ...\n    ) sub\n) outer_sub\n</code></pre>"},{"location":"blog/06-dbt-transformations/#3-comments-for-complex-logic","title":"3. Comments for Complex Logic","text":"<pre><code>-- Document the \"why\" not the \"what\"\nSELECT\n    entry_id,\n    -- Rate of change: glucose difference from previous reading\n    -- Used to identify rapid spikes (potential errors)\n    glucose_mg_dl - LAG(glucose_mg_dl) OVER (...) as glucose_change\nFROM {{ ref('stg_glucose_entries') }}\n</code></pre>"},{"location":"blog/06-dbt-transformations/#4-tests-for-critical-columns","title":"4. Tests for Critical Columns","text":"<pre><code>columns:\n  - name: entry_id\n    tests:\n      - unique # Must be unique\n      - not_null # Cannot be missing\n\n  - name: glucose_mg_dl\n    tests:\n      - not_null\n      - dbt_utils.accepted_range:\n          min_value: 20\n          max_value: 600\n</code></pre>"},{"location":"blog/06-dbt-transformations/#performance-tips","title":"Performance Tips","text":""},{"location":"blog/06-dbt-transformations/#1-incremental-models-large-tables","title":"1. Incremental Models (Large Tables)","text":"<p>For tables with millions of rows, use incremental:</p> <pre><code>{{ config(\n    materialized='incremental',\n    unique_key='entry_id',\n    on_schema_change='fail',\n) }}\n\nSELECT *\nFROM {{ source('raw', 'entries') }}\n\n{% if execute %}\n  {% set max_partition = run_started_at %}\n  WHERE _cascade_ingested_at &gt; '{{ max_partition }}'\n{% endif %}\n</code></pre> <p>Only processes new data since last run.</p>"},{"location":"blog/06-dbt-transformations/#2-limit-in-development","title":"2. Limit in Development","text":"<pre><code>SELECT * FROM {{ ref('large_table') }}\n{% if execute and execute_sql %}\n  LIMIT 1000  -- Don't scan 100M rows while developing\n{% endif %}\n</code></pre>"},{"location":"blog/06-dbt-transformations/#3-pre-filter-before-joins","title":"3. Pre-filter Before Joins","text":"<pre><code>-- Bad: Join large tables then filter\nSELECT * FROM {{ ref('fact') }}\nLEFT JOIN {{ ref('dimension') }} ...\nWHERE dimension.active = true\n\n-- Good: Filter dimension first\nSELECT * FROM {{ ref('fact') }}\nLEFT JOIN (\n    SELECT * FROM {{ ref('dimension') }}\n    WHERE active = true\n) dim ...\n</code></pre>"},{"location":"blog/06-dbt-transformations/#next-orchestration","title":"Next: Orchestration","text":"<p>We have ingestion and transformation. Now: Who runs this, and when?</p> <p>Part 7: Orchestration with Dagster</p> <p>See you there!</p>"},{"location":"blog/06-dbt-transformations/#summary","title":"Summary","text":"<p>dbt provides:</p> <ul> <li>Reusable SQL models</li> <li>Auto-resolved dependencies</li> <li>Built-in data quality tests</li> <li>Documentation generation</li> <li>Version control (git-friendly)</li> </ul> <p>Phlo uses dbt for:</p> <ul> <li>Bronze: Raw data staging</li> <li>Silver: Fact tables with business logic</li> <li>Gold: Dimensions and metrics</li> <li>Marts: Published to Postgres for BI</li> </ul> <p>Key Pattern: Define models as SQL files, dbt handles orchestration and testing.</p> <p>Next: Part 7: Orchestration with Dagster\u2014Running Your Pipelines</p>"},{"location":"blog/07-orchestration-dagster/","title":"Part 7: Orchestration with Dagster\u2014Running Your Pipelines","text":"<p>We have data flowing in (DLT + Iceberg) and transformations defined (dbt). Now: Who runs this? When? What happens if it fails?</p> <p>That's Dagster's job\u2014orchestration.</p>"},{"location":"blog/07-orchestration-dagster/#the-orchestration-problem","title":"The Orchestration Problem","text":"<p>Without orchestration:</p> <pre><code># 6:00 AM\ncurl nightscout-api | dlt_ingest.py          # Manual - easy to forget\n# 6:30 AM\ndbt build                                      # Manual - depends on above\n# 7:00 AM\npublish_to_postgres.py                        # Manual - depends on dbt\n# 7:30 AM (oops! API was down)\n# Pipeline failed silently - nobody knows!\n# Dashboards show stale data for 24 hours\n</code></pre> <p>With Dagster:</p> <pre><code>6:00 AM: Dagster scheduler triggers ingestion\n         \u2193 watches for completion\n6:02 AM: Ingestion done \u2192 Dagster auto-triggers dbt\n         \u2193 watches for completion\n6:04 AM: dbt done \u2192 Dagster auto-triggers publishing\n         \u2193 watches for completion\n6:06 AM: All complete\n         \u2193\n         If anything fails \u2192 Alert via email/Slack\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#dagsters-core-concepts","title":"Dagster's Core Concepts","text":""},{"location":"blog/07-orchestration-dagster/#1-assets-declarative-data-dependencies","title":"1. Assets (Declarative Data Dependencies)","text":"<p>Instead of telling Dagster \"run this code\", you declare:</p> <ul> <li>What: What data does this produce?</li> <li>When: When should it run?</li> <li>Where: Where does it come from?</li> </ul> <pre><code># Instead of: \"Run this function at 6am\"\n# You declare: \"This is an asset\"\n\n@dg.asset\ndef dlt_glucose_entries() -&gt; MaterializeResult:\n    \"\"\"\n    Produces: raw.glucose_entries table\n    Source: Nightscout API\n    \"\"\"\n    # Ingest data...\n    return MaterializeResult(metadata={...})\n\n\n@dg.asset\ndef dbt_bronze(dbt: DbtCliResource) -&gt; None:\n    \"\"\"\n    Produces: bronze.stg_glucose_entries\n    Depends on: dlt_glucose_entries (auto-detected)\n    \"\"\"\n    # Run dbt bronze models...\n    dbt.cli([\"build\", \"--select\", \"tag:bronze\"])\n\n\n@dg.asset(deps=[dbt_bronze])\ndef dbt_silver() -&gt; None:\n    \"\"\"\n    Produces: silver.fct_glucose_readings\n    Depends on: dbt_bronze (explicit)\n    \"\"\"\n    dbt.cli([\"build\", \"--select\", \"tag:silver\"])\n</code></pre> <p>Dagster automatically:</p> <ul> <li>Detects dependencies (bronze depends on ingestion)</li> <li>Builds DAG (directed acyclic graph)</li> <li>Executes in correct order</li> <li>Skips if already done (idempotent)</li> <li>Retries on failure</li> <li>Tracks lineage</li> </ul>"},{"location":"blog/07-orchestration-dagster/#2-partitions-time-based-splitting","title":"2. Partitions (Time-Based Splitting)","text":"<p>Large datasets need splitting. Dagster partitions by time:</p> <pre><code># Define daily partitions starting from a date\nfrom phlo_dagster.partitions import daily_partition\n\n@dg.asset(\n    partitions_def=daily_partition,\n    description=\"Daily ingestion of glucose entries\"\n)\ndef dlt_glucose_entries(context) -&gt; MaterializeResult:\n    \"\"\"\n    Materialize for each day independently.\n\n    Daily partition ensures:\n    - Idempotency (re-run one day without affecting others)\n    - Incremental loading (only process new data per day)\n    - Easy recovery (re-run single failed day)\n    \"\"\"\n    partition_date = context.partition_key  # \"2024-10-15\"\n\n    # Fetch data for this date range only\n    start = f\"{partition_date}T00:00:00.000Z\"\n    end = f\"{partition_date}T23:59:59.999Z\"\n\n    entries = fetch_from_nightscout(start, end)\n    # ... ingest ...\n    return MaterializeResult(metadata={...})\n</code></pre> <p>In the UI, you can materialize specific partitions:</p> <pre><code>Oct 14 (complete)\nOct 15 \u23f3 (running)\nOct 16 (failed - can re-run)\nOct 17 \u26aa (not run yet)\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#3-automation-schedules-sensors","title":"3. Automation (Schedules &amp; Sensors)","text":"<p>Run assets automatically:</p> <p>Schedule: Run at specific times</p> <pre><code>@dg.daily_schedule\ndef daily_ingestion():\n    \"\"\"Ingest new data every morning at 6 AM.\"\"\"\n    return dg.build_asset_context(\n        partition_key=get_today_date()\n    )\n</code></pre> <p>Sensor: Run when something happens</p> <pre><code>@dg.sensor\ndef nightscout_api_sensor():\n    \"\"\"Ingest when new data available in Nightscout API.\"\"\"\n    if has_new_data():\n        yield dg.SensorResult(\n            cursor=get_latest_timestamp(),\n            run_requests=[dg.RunRequest(tags={\"source\": \"nightscout\"})]\n        )\n</code></pre> <p>In Phlo's code:</p> <pre><code># From workflows/ingestion/dlt_assets.py\n\n@dg.asset(\n    partitions_def=daily_partition,\n    # ... other config ...\n    automation_condition=dg.AutomationCondition.on_cron(\"0 */1 * * *\"),\n    # ^ Run every hour on :00 minute (0 * * * * = UTC cron format)\n)\ndef entries(context) -&gt; MaterializeResult:\n    # Runs automatically hourly\n    pass\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#4-resources-connections-clients","title":"4. Resources (Connections &amp; Clients)","text":"<p>Instead of hardcoding connections, pass as resources:</p> <pre><code>@dg.resource\ndef trino_resource(config) -&gt; trino.dbapi.Connection:\n    \"\"\"Trino connection (configured via phlo.yaml + .phlo/.env.local).\"\"\"\n    return trino.dbapi.connect(\n        host=config.trino_host,\n        port=config.trino_port,\n        user=\"phlo\",\n    )\n\n@dg.resource\ndef iceberg_resource(config) -&gt; IcebergCatalog:\n    \"\"\"PyIceberg catalog for Iceberg operations.\"\"\"\n    return get_catalog(ref=\"dev\")\n\n@dg.asset\ndef dlt_glucose_entries(iceberg: IcebergResource) -&gt; MaterializeResult:\n    \"\"\"iceberg resource injected automatically.\"\"\"\n    # Use iceberg resource\n    iceberg.ensure_table(...)\n    iceberg.merge_parquet(...)\n</code></pre> <p>Resources are:</p> <ul> <li>Configured centrally (no hardcoding)</li> <li>Testable (swap real for mock)</li> <li>Shared across assets</li> <li>Lifecycle managed (connect/disconnect)</li> </ul>"},{"location":"blog/07-orchestration-dagster/#5-asset-checks-data-quality","title":"5. Asset Checks (Data Quality)","text":"<p>Beyond dbt tests, add explicit checks:</p> <pre><code>@dg.asset_check(asset=dlt_glucose_entries)\ndef glucose_not_null(context) -&gt; dg.AssetCheckResult:\n    \"\"\"Ensure no null glucose values.\"\"\"\n\n    table = get_table(\"raw.glucose_entries\")\n    null_count = table.scan().filter(\"sgv IS NULL\").count()\n\n    return dg.AssetCheckResult(\n        passed=null_count == 0,\n        metadata={\n            \"null_count\": dg.MetadataValue.int(null_count),\n            \"total_rows\": dg.MetadataValue.int(table.count_rows()),\n        }\n    )\n\n\n@dg.asset_check(asset=fct_glucose_readings)\ndef range_check(context) -&gt; dg.AssetCheckResult:\n    \"\"\"Ensure glucose values in physiologically plausible range.\"\"\"\n\n    table = get_table(\"silver.fct_glucose_readings\")\n\n    out_of_range = table.scan().filter(\n        \"(glucose_mg_dl &lt; 20) OR (glucose_mg_dl &gt; 600)\"\n    ).count()\n\n    return dg.AssetCheckResult(\n        passed=out_of_range == 0,\n        metadata={\n            \"out_of_range_count\": dg.MetadataValue.int(out_of_range),\n        }\n    )\n</code></pre> <p>Checks run after asset materialization:</p> <pre><code>dlt_glucose_entries (complete)\n  \u251c\u2500 Check: glucose_not_null [PASSED]\n  \u2514\u2500 Check: row_count_increased [PASSED]\n\u2193 (checks pass, continue)\ndbt_bronze [SUCCESS]\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#phlos-asset-graph","title":"Phlo's Asset Graph","text":"<p>Here's Phlo's actual orchestration (simplified):</p> <pre><code>Sources\n\u251c\u2500\u2500 Nightscout API\n\u2514\u2500\u2500 GitHub API\n    \u2193\nIngestion Layer\n\u251c\u2500\u2500 dlt_glucose_entries (raw.glucose_entries)\n\u251c\u2500\u2500 dlt_github_user_events (raw.github_user_events)\n\u2514\u2500\u2500 Quality checks\n    \u2193\nTransform Layer\n\u251c\u2500\u2500 stg_glucose_entries (bronze)\n\u251c\u2500\u2500 stg_github_user_events (bronze)\n    \u251c\u2500\u2500 fct_glucose_readings (silver)\n    \u251c\u2500\u2500 fct_github_user_events (silver)\n        \u251c\u2500\u2500 dim_date (gold)\n        \u251c\u2500\u2500 mrt_glucose_readings (gold)\n        \u2514\u2500\u2500 More metrics\n            \u2193\nPublish Layer\n\u251c\u2500\u2500 mrt_glucose_overview (postgres)\n\u251c\u2500\u2500 mrt_glucose_hourly_patterns (postgres)\n\u2514\u2500\u2500 mrt_github_activity_overview (postgres)\n    \u2193\nAnalytics\n\u2514\u2500\u2500 Superset Dashboards\n</code></pre> <p>All dependencies auto-detected by Dagster:</p> <pre><code># File: workflows/ingestion/dlt_assets.py\n@dg.asset(name=\"dlt_glucose_entries\")\ndef entries() -&gt; MaterializeResult:\n    \"\"\"Produces raw.glucose_entries\"\"\"\n    pass\n\n# File: workflows/transform/dbt.py\n@dbt_assets(...)\ndef all_dbt_assets(dbt: DbtCliResource):\n    \"\"\"\n    Produces bronze.*, silver.*, gold.*\n    Depends on dlt_glucose_entries (via dbt source definition)\n    \"\"\"\n    dbt.cli([\"build\"])\n\n# Dagster automatically:\n# - Sees dbt reads from dlt_glucose_entries\n# - Waits for ingestion before running dbt\n# - Re-runs dbt when ingestion changes\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#viewing-the-lineage","title":"Viewing the Lineage","text":"<p>Open Dagster UI (http://localhost:10006):</p> <p>Asset Graph:</p> <ul> <li>Visual DAG of dependencies</li> <li>Color-coded by status (complete, \u23f3 running, failed)</li> <li>Click to drill down</li> </ul> <p>Asset Details:</p> <pre><code>dlt_glucose_entries\n\u251c\u2500\u2500 Dependencies\n\u2502   \u251c\u2500\u2500 Upstream: None (source)\n\u2502   \u2514\u2500\u2500 Downstream: stg_glucose_entries\n\u251c\u2500\u2500 Recent Materializations\n\u2502   \u251c\u2500\u2500 2024-10-15 10:30:00 (2.45s)\n\u2502   \u251c\u2500\u2500 2024-10-15 09:30:00 (2.51s)\n\u2502   \u2514\u2500\u2500 2024-10-15 08:30:00 (2.48s)\n\u251c\u2500\u2500 Runs\n\u2502   \u2514\u2500\u2500 2024-10-15-103001 (view logs, re-run, etc.)\n\u2514\u2500\u2500 Metadata\n    \u251c\u2500\u2500 rows_loaded: 288\n    \u251c\u2500\u2500 rows_inserted: 288\n    \u251c\u2500\u2500 partition: 2024-10-15\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#running-pipelines-manually","title":"Running Pipelines Manually","text":""},{"location":"blog/07-orchestration-dagster/#via-ui","title":"Via UI","text":"<ol> <li>Open http://localhost:10006</li> <li>Click asset: <code>stg_glucose_entries</code></li> <li>Click Materialize this asset</li> <li>Select partition date (or use defaults)</li> <li>Click Materialize</li> <li>Watch progress in sidebar</li> </ol>"},{"location":"blog/07-orchestration-dagster/#via-cli","title":"Via CLI","text":"<pre><code># Materialize single asset\nphlo materialize --select dlt_glucose_entries\n\n# Materialize with partition\nphlo materialize --select dlt_glucose_entries --partition 2024-10-15\n\n# Materialize multiple assets\nphlo materialize --select \"dlt_glucose_entries,stg_glucose_entries\" --partition 2024-10-15\n\n# Materialize all downstream of ingestion\nphlo materialize --select \"dlt_glucose_entries+\"  # Plus = all downstream\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#via-python-api","title":"Via Python API","text":"<pre><code># From workflows/...\n\nfrom dagster import materialize\n\n# Programmatic trigger\nmaterialize(\n    [dlt_glucose_entries],\n    partition_key=\"2024-10-15\"\n)\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#backfilling-historical-data","title":"Backfilling Historical Data","text":"<p>Single materializations work for daily operations, but what about:</p> <ul> <li>Loading historical data when you first set up</li> <li>Re-processing after a bug fix</li> <li>Filling gaps from outages</li> </ul> <p>That's where backfills come in.</p>"},{"location":"blog/07-orchestration-dagster/#the-backfill-problem","title":"The Backfill Problem","text":"<pre><code>Scenario: You fixed a bug in your glucose transformation.\nNeed to re-process the last 90 days.\n\nManual approach:\n  for date in 2024-07-01 to 2024-09-30:\n    phlo materialize glucose_entries --partition $date\n    # Wait for each to complete...\n\nTime: 90 days \u00d7 2 minutes = 3 hours of babysitting\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#using-phlo-backfill","title":"Using phlo backfill","text":"<p>The <code>phlo backfill</code> command handles date ranges intelligently:</p> <pre><code># Backfill a date range\n$ phlo backfill glucose_entries --start-date 2024-07-01 --end-date 2024-09-30\n\nBackfill Plan: glucose_entries\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nDate Range: 2024-07-01 to 2024-09-30\nPartitions: 92 days\nEstimated Time: ~45 minutes (parallel)\n\nProceed? [y/N] y\n\n[1/92]  2024-07-01 \u2713 (32s)\n[2/92]  2024-07-02 \u2713 (28s)\n[3/92]  2024-07-03 \u2713 (31s)\n...\n[92/92] 2024-09-30 \u2713 (29s)\n\nBackfill complete: 92 partitions in 43m\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#parallel-execution","title":"Parallel Execution","text":"<p>For large backfills, run multiple partitions simultaneously:</p> <pre><code># Run 4 partitions in parallel\n$ phlo backfill glucose_entries \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-12-31 \\\n    --parallel 4\n\nBackfill Plan: glucose_entries\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nDate Range: 2024-01-01 to 2024-12-31\nPartitions: 365 days\nParallel Workers: 4\nEstimated Time: ~90 minutes\n\n[Worker 1] 2024-01-01 \u2713\n[Worker 2] 2024-01-02 \u2713\n[Worker 3] 2024-01-03 \u2713\n[Worker 4] 2024-01-04 \u2713\n[Worker 1] 2024-01-05 \u2713\n...\n</code></pre> <p>Parallel considerations:</p> <ul> <li>More workers = faster, but more resource usage</li> <li>Don't exceed your database connection pool</li> <li>Start with 2-4 workers, increase if stable</li> </ul>"},{"location":"blog/07-orchestration-dagster/#explicit-partitions","title":"Explicit Partitions","text":"<p>Sometimes you need specific dates, not a range:</p> <pre><code># Only these specific dates\n$ phlo backfill glucose_entries \\\n    --partitions 2024-01-01,2024-01-15,2024-02-01,2024-03-01\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#resuming-failed-backfills","title":"Resuming Failed Backfills","text":"<p>If a backfill fails partway through (network issue, resource limits), resume it:</p> <pre><code># Backfill gets interrupted at partition 45\n$ phlo backfill glucose_entries --start-date 2024-01-01 --end-date 2024-03-31\n...\n[45/90] 2024-02-14 \u2717 Connection timeout\nBackfill interrupted. Run with --resume to continue.\n\n# Later, resume from where it stopped\n$ phlo backfill --resume\n\nResuming backfill: glucose_entries\nCompleted: 44/90\nRemaining: 46 partitions\n\n[45/90] 2024-02-14 \u2713\n[46/90] 2024-02-15 \u2713\n...\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#dry-run","title":"Dry Run","text":"<p>Preview what will happen without executing:</p> <pre><code>$ phlo backfill glucose_entries \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-31 \\\n    --dry-run\n\nBackfill Plan (DRY RUN)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nAsset: glucose_entries\nPartitions to process: 31\n\n  2024-01-01 (not materialized)\n  2024-01-02 (not materialized)\n  2024-01-03 (stale - upstream changed)\n  2024-01-04 (not materialized)\n  ...\n  2024-01-15 (already fresh) &lt;- would skip\n  ...\n\nWould process: 30 partitions\nWould skip: 1 partition (already fresh)\n\nRun without --dry-run to execute.\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#backfill-strategies","title":"Backfill Strategies","text":"Scenario Strategy Initial load <code>--start-date</code> from earliest data, <code>--parallel 4</code> Bug fix re-process Date range of affected data, <code>--parallel 2</code> Fill gaps <code>--partitions</code> with explicit list Monthly refresh <code>--start-date</code> first of month, <code>--end-date</code> last Testing <code>--dry-run</code> first, then small range"},{"location":"blog/07-orchestration-dagster/#backfills-in-production","title":"Backfills in Production","text":"<p>For production backfills:</p> <ol> <li>Test first: Run on a few partitions manually</li> <li>Monitor resources: Watch CPU, memory, connections</li> <li>Off-peak hours: Schedule large backfills overnight</li> <li>Incremental: Better to run multiple smaller backfills than one huge one</li> <li>Notify team: Large backfills can impact query performance</li> </ol> <pre><code># Production backfill pattern\n$ phlo backfill glucose_entries \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-12-31 \\\n    --parallel 2 \\           # Conservative parallelism\n    --dry-run                # Preview first\n\n# If dry-run looks good:\n$ phlo backfill glucose_entries \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-12-31 \\\n    --parallel 2\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#monitoring-and-alerts","title":"Monitoring and Alerts","text":""},{"location":"blog/07-orchestration-dagster/#event-logging","title":"Event Logging","text":"<p>Dagster logs every action:</p> <pre><code>@dg.asset\ndef dlt_glucose_entries(context) -&gt; MaterializeResult:\n    # All logs captured\n    context.log.info(\"Starting ingestion...\")\n    context.log.warning(\"Got 288 entries\")\n    context.log.error(\"Failed to connect to MinIO\")\n\n    # Metadata attached to run\n    return dg.MaterializeResult(\n        metadata={\n            \"rows_loaded\": dg.MetadataValue.int(288),\n            \"duration_seconds\": dg.MetadataValue.float(2.45),\n            \"partition\": dg.MetadataValue.text(\"2024-10-15\"),\n        }\n    )\n</code></pre> <p>View logs in Dagster UI \u2192 Runs \u2192 Click run \u2192 Logs tab</p>"},{"location":"blog/07-orchestration-dagster/#failure-monitoring","title":"Failure Monitoring","text":"<pre><code>@dg.sensor\ndef failure_alert_sensor(context):\n    \"\"\"Alert on pipeline failures.\"\"\"\n\n    failed_runs = context.instance.get_runs(\n        filters=[\n            DagsterRunStatus.FAILURE\n        ],\n        limit=10\n    )\n\n    for run in failed_runs:\n        send_slack_alert(f\"Pipeline failed: {run.asset_selection}\")\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#freshness-policies","title":"Freshness Policies","text":"<p>Ensure data is fresh:</p> <pre><code>@dg.asset(\n    freshness_policy=FreshnessPolicy(\n        maximum_lag_minutes=60,  # Data must be &lt;1hr old\n        # If not met:\n        # - \u26a0\ufe0f Warning in UI\n        # - Can trigger automatic re-run\n    ),\n    partitions_def=daily_partition\n)\ndef dlt_glucose_entries() -&gt; MaterializeResult:\n    pass\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#configuration-and-environment","title":"Configuration and Environment","text":"<p>Phlo uses <code>phlo/config.py</code> for centralized config:</p> <pre><code># phlo/config.py\nfrom pydantic_settings import BaseSettings\n\nclass PhloConfig(BaseSettings):\n    # Iceberg\n    iceberg_warehouse_path: str = \"s3://lake/warehouse\"\n    iceberg_staging_path: str = \"s3://lake/stage\"\n\n    # Nessie\n    nessie_uri: str = \"http://nessie:10003\"\n    nessie_branch: str = \"dev\"\n\n    # Trino\n    trino_host: str = \"trino\"\n    trino_port: int = 10005\n\n    class Config:\n        env_file = (\".phlo/.env\", \".phlo/.env.local\")\n\nconfig = PhloConfig()  # Singleton\n</code></pre> <p>In assets:</p> <pre><code>@dg.asset\ndef dlt_glucose_entries(context, iceberg: IcebergResource) -&gt; MaterializeResult:\n    context.log.info(f\"Using warehouse: {config.iceberg_warehouse_path}\")\n    context.log.info(f\"Using branch: {config.nessie_branch}\")\n    # ...\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#advanced-custom-ops-and-jobs","title":"Advanced: Custom Ops and Jobs","text":"<p>For complex workflows, you can define Jobs (lower-level than assets):</p> <pre><code># Assets are preferred, but sometimes you need Jobs\n\nfrom dagster import op, job\n\n@op\ndef fetch_data():\n    return requests.get(...).json()\n\n@op\ndef validate_data(data):\n    return pandera_schema.validate(data)\n\n@op\ndef write_to_iceberg(data):\n    iceberg.merge_parquet(...)\n\n@job\ndef ingestion_pipeline():\n    \"\"\"Explicit workflow if you need more control.\"\"\"\n    data = fetch_data()\n    validated = validate_data(data)\n    write_to_iceberg(validated)\n\n# Most of Phlo uses Assets instead (more declarative)\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#performance-considerations","title":"Performance Considerations","text":""},{"location":"blog/07-orchestration-dagster/#dagster-daemon","title":"Dagster Daemon","text":"<p>The daemon runs scheduled assets:</p> <pre><code>Dagster Daemon\n\u251c\u2500\u2500 Runs scheduled assets\n\u251c\u2500\u2500 Monitors sensors\n\u2514\u2500\u2500 Manages run queue\n</code></pre> <p>In Docker:</p> <pre><code>dagster-daemon:\n  image: dagster/dagster:1.8.0\n  command: dagster-daemon run\n  depends_on:\n    - postgres\n    - dagster-webserver\n</code></pre> <p>The daemon needs:</p> <ul> <li>Access to PostgreSQL (run history)</li> <li>Access to code (asset definitions)</li> <li>Compute resources (run actual ops)</li> </ul>"},{"location":"blog/07-orchestration-dagster/#concurrency","title":"Concurrency","text":"<p>By default, Dagster runs one partition at a time. For parallel:</p> <pre><code>@dg.asset(\n    partitions_def=daily_partition,\n    # Run up to 4 partitions in parallel\n    automation_condition=dg.AutomationCondition.eager(),\n    tags={\"dagster/max_concurrency\": 4}\n)\ndef dlt_glucose_entries() -&gt; MaterializeResult:\n    pass\n</code></pre>"},{"location":"blog/07-orchestration-dagster/#next-data-quality","title":"Next: Data Quality","text":"<p>Orchestration keeps pipelines running. But are they running correctly?</p> <p>Part 8: Data Quality and Testing</p> <p>See you there!</p>"},{"location":"blog/07-orchestration-dagster/#summary","title":"Summary","text":"<p>Dagster provides:</p> <ul> <li>Asset-based orchestration (declare dependencies)</li> <li>Automatic scheduling (run on cron)</li> <li>Partitioning (split by time)</li> <li>Lineage tracking (visual DAG)</li> <li>Error handling and retries</li> <li>Monitoring and alerting</li> </ul> <p>In Phlo:</p> <ul> <li>Assets for ingestion, transformation, publishing</li> <li>Daily partitions for scalability</li> <li>Automatic dependency resolution</li> <li>Asset checks for data quality</li> <li>Freshness policies for monitoring</li> </ul> <p>Key Pattern: Declare assets, Dagster handles orchestration.</p> <p>Next: Part 8: Real-World Example\u2014Building a Complete Data Pipeline</p>"},{"location":"blog/08-real-world-example/","title":"Part 8: Real-World Example\u2014Building a Complete Data Pipeline","text":"<p>We've covered all the pieces. Now let's build a complete, working pipeline from start to finish: Nightscout Glucose Monitoring.</p>"},{"location":"blog/08-real-world-example/#the-use-case","title":"The Use Case","text":"<p>Nightscout is an open-source glucose monitoring system used by people with diabetes. It sends glucose readings every 5 minutes to a cloud API.</p> <p>Goal: Build analytics to understand:</p> <ul> <li>Daily glucose averages and ranges</li> <li>Time spent in range (70-180 mg/dL)</li> <li>Hour-by-hour patterns</li> <li>Overnight stability</li> </ul>"},{"location":"blog/08-real-world-example/#the-architecture","title":"The Architecture","text":"<pre><code>Nightscout API\n  \u2193 (5-min readings)\nPhlo Ingestion\n  \u251c\u2500 DLT stages to S3\n  \u251c\u2500 PyIceberg merges to raw.glucose_entries\n  \u2514\u2500 Nessie tracks via snapshot\n    \u2193\ndbt Transformation\n  \u251c\u2500 Bronze: stg_glucose_entries (staging)\n  \u251c\u2500 Silver: fct_glucose_readings (enriched)\n  \u251c\u2500 Gold: dim_date, mrt_glucose_readings (metrics)\n  \u2514\u2500 Marts: mrt_glucose_overview (aggregated)\n    \u2193\nPostgres Publishing\n  \u2514\u2500 Superset Dashboard\n</code></pre>"},{"location":"blog/08-real-world-example/#step-1-understanding-the-api","title":"Step 1: Understanding the API","text":"<p>Nightscout's glucose API:</p> <pre><code># Fetch glucose readings\ncurl \"https://gwp-diabetes.fly.dev/api/v1/entries.json\" \\\n  -G \\\n  --data-urlencode 'count=10000' \\\n  --data-urlencode 'find[dateString][$gte]=2024-10-15T00:00:00.000Z' \\\n  --data-urlencode 'find[dateString][$lt]=2024-10-15T23:59:59.999Z'\n\n# Response:\n[\n  {\n    \"_id\": \"507f1f77bcf86cd799439011\",  # MongoDB ObjectId\n    \"sgv\": 145,                          # Glucose in mg/dL\n    \"date\": 1729027800000,               # Unix ms\n    \"dateString\": \"2024-10-15T10:30:00.000Z\",\n    \"direction\": \"Flat\",                 # Trend direction\n    \"trend\": 0,                          # Numeric trend\n    \"device\": \"share2\",                  # Device type\n    \"type\": \"sgv\",                       # Type\n    \"rssi\": 100,                         # Signal strength\n  },\n  ...\n]\n</code></pre>"},{"location":"blog/08-real-world-example/#step-2-data-ingestion","title":"Step 2: Data Ingestion","text":""},{"location":"blog/08-real-world-example/#using-phlo_ingestion-decorator","title":"Using @phlo_ingestion Decorator","text":"<p>Phlo simplifies ingestion with the <code>@phlo_ingestion</code> decorator that handles validation, staging, and Iceberg merging:</p> <pre><code># File: phlo-examples/nightscout/workflows/ingestion/nightscout/readings.py\n\nimport phlo\nfrom dlt.sources.rest_api import rest_api\nfrom workflows.schemas.nightscout import RawGlucoseEntries\n\n@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",\n    validation_schema=RawGlucoseEntries,\n    group=\"nightscout\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef glucose_entries(partition_date: str):\n    \"\"\"\n    Ingest Nightscout glucose entries using DLT rest_api source.\n\n    Features:\n    - Idempotent ingestion: safe to run multiple times without duplicates\n    - Deduplication based on _id field (Nightscout's unique entry ID)\n    - Daily partitioning by timestamp\n    - Automatic validation with Pandera schema\n    - Branch-aware writes to Iceberg\n\n    Args:\n        partition_date: Date partition in YYYY-MM-DD format\n\n    Returns:\n        DLT resource for glucose entries, or None if no data\n    \"\"\"\n    start_time_iso = f\"{partition_date}T00:00:00.000Z\"\n    end_time_iso = f\"{partition_date}T23:59:59.999Z\"\n\n    source = rest_api(\n        client={\n            \"base_url\": \"https://gwp-diabetes.fly.dev/api/v1\",\n        },\n        resources=[\n            {\n                \"name\": \"entries\",\n                \"endpoint\": {\n                    \"path\": \"entries.json\",\n                    \"params\": {\n                        \"count\": 10000,\n                        \"find[dateString][$gte]\": start_time_iso,\n                        \"find[dateString][$lt]\": end_time_iso,\n                    },\n                },\n            }\n        ],\n    )\n\n    return source\n</code></pre> <p>What the @phlo_ingestion decorator does automatically:</p> <ol> <li>Creates Dagster asset with daily partitioning</li> <li>Runs DLT pipeline to fetch and stage data to parquet</li> <li>Validates with RawGlucoseEntries Pandera schema</li> <li>Creates Iceberg table if it doesn't exist</li> <li>Merges data idempotently using <code>_id</code> as unique key</li> <li>Adds freshness checks and cron scheduling</li> <li>Returns MaterializeResult with metadata</li> </ol> <p>Run it:</p> <pre><code>phlo materialize --select dlt_glucose_entries --partition 2024-10-15\n</code></pre>"},{"location":"blog/08-real-world-example/#step-3-bronze-layer-transformation","title":"Step 3: Bronze Layer Transformation","text":""},{"location":"blog/08-real-world-example/#dbt-model-staging","title":"dbt Model: Staging","text":"<pre><code>-- File: workflows/transforms/dbt/models/bronze/stg_glucose_entries.sql\n\n{{ config(\n    materialized='view',\n    tags=['nightscout', 'stg']\n) }}\n\nWITH raw_data AS (\n    SELECT * FROM {{ source('dagster_assets', 'glucose_entries') }}\n)\n\nSELECT\n    -- Rename and type columns\n    _id as entry_id,\n    CAST(sgv AS INT) as glucose_mg_dl,\n    CAST(date_string AS TIMESTAMP) as reading_timestamp,\n    date_string as timestamp_iso,\n    direction,\n    trend,\n    device,\n    type as reading_type,\n    CAST(utc_offset AS INT) as utc_offset_minutes,\n\n    -- Metadata\n    _cascade_ingested_at as ingested_at,\n    _dlt_load_id,\n    _dlt_id\n\nFROM raw_data\n\n-- Data quality filters\nWHERE sgv IS NOT NULL\n  AND sgv BETWEEN 20 AND 600  -- Physiologically plausible\n  {% if var('partition_date_str', None) is not none %}\n    AND DATE(date_string) = DATE('{{ var(\"partition_date_str\") }}')\n  {% endif %}\n</code></pre> <p>Purpose:</p> <ul> <li>Clean types (string \u2192 timestamp)</li> <li>Rename for clarity (_id \u2192 entry_id)</li> <li>Filter out bad data (null, out-of-range)</li> </ul>"},{"location":"blog/08-real-world-example/#step-4-silver-layer-transformation","title":"Step 4: Silver Layer Transformation","text":""},{"location":"blog/08-real-world-example/#dbt-model-enriched-facts","title":"dbt Model: Enriched Facts","text":"<pre><code>-- File: workflows/transforms/dbt/models/silver/fct_glucose_readings.sql\n\n{{ config(\n    materialized='table',\n    tags=['nightscout', 'int']\n) }}\n\nWITH glucose_data AS (\n    SELECT * FROM {{ ref('stg_glucose_entries') }}\n),\n\nenriched AS (\n    SELECT\n        entry_id,\n        glucose_mg_dl,\n        reading_timestamp,\n        timestamp_iso,\n        direction,\n        device,\n\n        -- Time dimensions\n        DATE(reading_timestamp) as reading_date,\n        EXTRACT(HOUR FROM reading_timestamp) as hour_of_day,\n        DAY_OF_WEEK(reading_timestamp) as day_of_week,\n        FORMAT_DATETIME(reading_timestamp, 'EEEE') as day_name,\n\n        -- Glucose classification (ADA guidelines)\n        CASE\n            WHEN glucose_mg_dl &lt; 70 THEN 'hypoglycemia'\n            WHEN glucose_mg_dl &gt;= 70 AND glucose_mg_dl &lt;= 180 THEN 'in_range'\n            WHEN glucose_mg_dl &gt; 180 AND glucose_mg_dl &lt;= 250 THEN 'hyperglycemia_mild'\n            WHEN glucose_mg_dl &gt; 250 THEN 'hyperglycemia_severe'\n        END as glucose_category,\n\n        -- Time in range flag\n        CASE\n            WHEN glucose_mg_dl &gt;= 70 AND glucose_mg_dl &lt;= 180 THEN 1\n            ELSE 0\n        END as is_in_range,\n\n        -- Rate of change\n        glucose_mg_dl - LAG(glucose_mg_dl) OVER (\n            PARTITION BY device ORDER BY reading_timestamp\n        ) as glucose_change_mg_dl,\n\n        -- Minutes since last reading\n        DATE_DIFF('minute',\n            LAG(reading_timestamp) OVER (\n                PARTITION BY device ORDER BY reading_timestamp\n            ),\n            reading_timestamp\n        ) as minutes_since_last_reading\n\n    FROM glucose_data\n)\n\nSELECT * FROM enriched\nORDER BY reading_timestamp DESC\n</code></pre> <p>Features added:</p> <ul> <li>Time dimensions (hour, day, etc.)</li> <li>Glucose categories (hypoglycemia/in-range/hyperglycemia)</li> <li>Time in range indicator</li> <li>Rate of change (lag window function)</li> <li>Interval between readings</li> </ul>"},{"location":"blog/08-real-world-example/#step-5-gold-layer-metrics","title":"Step 5: Gold Layer Metrics","text":""},{"location":"blog/08-real-world-example/#dbt-model-summarized-metrics","title":"dbt Model: Summarized Metrics","text":"<pre><code>-- File: workflows/transforms/dbt/models/gold/mrt_glucose_readings.sql\n\n{{ config(\n    materialized='table',\n    tags=['nightscout', 'metrics']\n) }}\n\nSELECT\n    reading_date,\n    hour_of_day,\n\n    -- Glucose statistics\n    COUNT(*) as reading_count,\n    ROUND(AVG(glucose_mg_dl), 1) as avg_glucose,\n    MIN(glucose_mg_dl) as min_glucose,\n    MAX(glucose_mg_dl) as max_glucose,\n\n    -- Time in range\n    ROUND(100.0 * SUM(is_in_range) / COUNT(*), 1) as percent_in_range,\n\n    -- Glucose categories\n    COUNT(CASE WHEN glucose_category = 'hypoglycemia' THEN 1 END) as hypoglycemia_count,\n    COUNT(CASE WHEN glucose_category = 'in_range' THEN 1 END) as in_range_count,\n    COUNT(CASE WHEN glucose_category = 'hyperglycemia_mild' THEN 1 END) as hyperglycemia_mild_count,\n    COUNT(CASE WHEN glucose_category = 'hyperglycemia_severe' THEN 1 END) as hyperglycemia_severe_count\n\nFROM {{ ref('fct_glucose_readings') }}\n\nGROUP BY reading_date, hour_of_day\nORDER BY reading_date DESC, hour_of_day DESC\n</code></pre> <p>Metrics:</p> <ul> <li>Average glucose per hour</li> <li>Time in range percentage</li> <li>Hypoglycemia warnings</li> <li>Hyperglycemia counts</li> </ul>"},{"location":"blog/08-real-world-example/#step-6-publishing-to-postgres","title":"Step 6: Publishing to Postgres","text":""},{"location":"blog/08-real-world-example/#dbt-model-bi-ready-marts","title":"dbt Model: BI-Ready Marts","text":"<pre><code>-- File: workflows/transforms/dbt/models/marts_postgres/mrt_glucose_overview.sql\n\n{{ config(\n    materialized='table',\n    meta={'external_database': 'postgres'}\n) }}\n\nSELECT\n    reading_date,\n\n    -- Daily stats\n    COUNT(*) as total_readings,\n    ROUND(AVG(glucose_mg_dl), 1) as avg_glucose_mg_dl,\n    MIN(glucose_mg_dl) as min_glucose_mg_dl,\n    MAX(glucose_mg_dl) as max_glucose_mg_dl,\n    ROUND(STDDEV(glucose_mg_dl), 1) as stddev_glucose,\n\n    -- Time in range\n    ROUND(100.0 * SUM(is_in_range) / COUNT(*), 1) as percent_in_range,\n\n    -- Alerts\n    CASE\n        WHEN COUNT(CASE WHEN glucose_category = 'hypoglycemia' THEN 1 END) &gt; 3 THEN 'CRITICAL'\n        WHEN COUNT(CASE WHEN glucose_category = 'hyperglycemia_severe' THEN 1 END) &gt; 5 THEN 'ALERT'\n        ELSE 'OK'\n    END as day_status\n\nFROM {{ ref('fct_glucose_readings') }}\n\nWHERE reading_date &gt;= CURRENT_DATE - INTERVAL '30' DAY\n\nGROUP BY reading_date\nORDER BY reading_date DESC\n</code></pre> <p>Note: Marts are built in Iceberg first, then auto-published to Postgres.</p>"},{"location":"blog/08-real-world-example/#auto-publishing-iceberg-marts-postgres","title":"Auto-Publishing: Iceberg Marts \u2192 Postgres","text":"<p>Phlo automatically discovers dbt models in the <code>marts</code> schema and creates a <code>publish_marts_to_postgres</code> asset:</p> <pre><code># This is auto-generated by phlo.framework.discovery._discover_publishing_assets()\n# You don't need to write this code - Phlo creates it automatically!\n\n@asset(\n    name=\"publish_marts_to_postgres\",\n    group_name=\"publishing\",\n    deps=[AssetKey(\"mrt_glucose_overview\"), AssetKey(\"mrt_glucose_hourly_patterns\")],\n    kinds={\"trino\", \"postgres\"},\n    description=\"Publish mart tables from Iceberg to PostgreSQL for BI\",\n)\ndef publish_marts_to_postgres(context):\n    \"\"\"Auto-generated publishing asset for dbt marts.\"\"\"\n    trino = TrinoResource()\n    conn = trino.get_connection()\n    cursor = conn.cursor()\n\n    # Create marts schema in postgres if not exists\n    cursor.execute(\"CREATE SCHEMA IF NOT EXISTS postgres.marts\")\n\n    # For each mart table discovered in dbt manifest:\n    for table_name in [\"mrt_glucose_overview\", \"mrt_glucose_hourly_patterns\"]:\n        source = f\"iceberg.marts.{table_name}\"\n        target = f\"postgres.marts.{table_name}\"\n\n        # Drop and recreate (simple refresh)\n        cursor.execute(f\"DROP TABLE IF EXISTS {target}\")\n        cursor.execute(f\"CREATE TABLE {target} AS SELECT * FROM {source}\")\n\n        context.log.info(f\"Published {table_name} to Postgres\")\n</code></pre> <p>How it works:</p> <ol> <li>Phlo scans the dbt <code>manifest.json</code> for models in the <code>marts</code> schema</li> <li>Auto-generates a publishing asset with dependencies on those marts</li> <li>Uses Trino to copy data from Iceberg (<code>iceberg.marts.*</code>) to Postgres (<code>postgres.marts.*</code>)</li> </ol> <p>You don't need to write any publishing code - just create dbt models in the <code>marts</code> schema!</p>"},{"location":"blog/08-real-world-example/#step-7-running-the-full-pipeline","title":"Step 7: Running the Full Pipeline","text":""},{"location":"blog/08-real-world-example/#materialize-everything","title":"Materialize Everything","text":"<pre><code># Run all glucose assets for a specific date\nphlo materialize \\\n  --select \"dlt_glucose_entries,stg_glucose_entries,fct_glucose_readings,mrt_glucose_readings,publish_glucose_marts\" \\\n  --partition 2024-10-15\n\n# Output:\n# Materializing dlt_glucose_entries [2024-10-15]\n#   Successfully fetched 288 entries\n#   Merged 288 rows to iceberg\n#   Asset materialized in 2.45s\n#\n# Materializing stg_glucose_entries [2024-10-15]\n#   dbt build completed\n#   Asset materialized in 1.23s\n#\n# Materializing fct_glucose_readings [2024-10-15]\n#   dbt run completed\n#   Asset materialized in 3.42s\n#\n# Materializing mrt_glucose_readings [2024-10-15]\n#   dbt run completed\n#   Asset materialized in 1.15s\n#\n# Materializing publish_glucose_marts\n#   Published 1 daily record to Postgres\n#   Published 24 hourly records to Postgres\n#   Asset materialized in 0.67s\n#\n# All assets materialized successfully in 8.92s\n</code></pre> <p>In Dagster:</p> <pre><code>http://localhost:10006\n\u2192 Assets tab\n\u2192 dlt_glucose_entries\n\u2192 Click to view lineage graph with status\n</code></pre> <p>In Observatory:</p> <pre><code>http://localhost:3001\n\u2192 Data Explorer\n\u2192 Browse marts.mrt_glucose_overview\n\u2192 Preview data and run queries\n</code></pre> <p>In Postgres:</p> <pre><code>docker exec -it pg psql -U phlo lakehouse\n\nlakehouse=# SELECT * FROM marts.mrt_glucose_overview ORDER BY reading_date DESC LIMIT 1;\n\nreading_date | avg_glucose_mg_dl | min_glucose_mg_dl | max_glucose_mg_dl | percent_in_range\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n2024-10-15   | 145.3             | 89                | 210               | 78.2\n</code></pre>"},{"location":"blog/08-real-world-example/#step-8-monitoring-and-alerts","title":"Step 8: Monitoring and Alerts","text":""},{"location":"blog/08-real-world-example/#quality-checks-with-phlo_quality","title":"Quality Checks with @phlo_quality","text":"<p>Phlo provides two approaches for quality checks. The declarative <code>@phlo_quality</code> decorator:</p> <pre><code># File: phlo-examples/nightscout/workflows/quality/nightscout.py\n\nimport phlo\nfrom phlo_quality import NullCheck, RangeCheck, FreshnessCheck\n\n@phlo_quality(\n    table=\"silver.fct_glucose_readings\",\n    checks=[\n        NullCheck(columns=[\"entry_id\", \"glucose_mg_dl\", \"reading_timestamp\"]),\n        RangeCheck(column=\"glucose_mg_dl\", min_value=20, max_value=600),\n        RangeCheck(column=\"hour_of_day\", min_value=0, max_value=23),\n        FreshnessCheck(column=\"reading_timestamp\", max_age_hours=24),\n    ],\n    group=\"nightscout\",\n    blocking=True,\n)\ndef glucose_readings_quality():\n    \"\"\"Declarative quality checks for glucose readings using @phlo_quality.\"\"\"\n    pass\n</code></pre> <p>And the traditional <code>@asset_check</code> for custom logic:</p> <pre><code># File: phlo-examples/nightscout/workflows/quality/nightscout.py\n\nfrom dagster import AssetCheckResult, AssetKey, asset_check\nfrom workflows.schemas.nightscout import FactGlucoseReadings\n\n@asset_check(\n    name=\"nightscout_glucose_quality\",\n    asset=AssetKey([\"fct_glucose_readings\"]),\n    blocking=True,\n    description=\"Validate processed Nightscout glucose data using Pandera schema validation.\",\n)\ndef nightscout_glucose_quality_check(context, trino: TrinoResource) -&gt; AssetCheckResult:\n    \"\"\"Quality check using Pandera for type-safe schema validation.\"\"\"\n\n    query = \"\"\"\n    SELECT\n        entry_id, glucose_mg_dl, reading_timestamp, direction,\n        hour_of_day, day_of_week, glucose_category, is_in_range\n    FROM iceberg_dev.silver.fct_glucose_readings\n    \"\"\"\n\n    with trino.cursor(schema=\"silver\") as cursor:\n        cursor.execute(query)\n        rows = cursor.fetchall()\n        columns = [desc[0] for desc in cursor.description]\n\n    fact_df = pd.DataFrame(rows, columns=columns)\n\n    # Validate with Pandera schema\n    try:\n        FactGlucoseReadings.validate(fact_df, lazy=True)\n        return AssetCheckResult(\n            passed=True,\n            metadata={\n                \"rows_validated\": len(fact_df),\n                \"columns_validated\": len(fact_df.columns),\n            },\n        )\n    except pandera.errors.SchemaErrors as err:\n        return AssetCheckResult(\n            passed=False,\n            metadata={\n                \"failed_checks\": len(err.failure_cases),\n                \"failures_by_column\": err.failure_cases.groupby(\"column\").size().to_dict(),\n            },\n        )\n</code></pre> <p>Alerts: If checks fail:</p> <ul> <li>Dagster UI shows red</li> <li>\ud83d\udce7 Optional: send to Slack/email</li> <li>\ud83d\udd14 Dashboard shows warnings</li> </ul>"},{"location":"blog/08-real-world-example/#complete-data-flow-diagram","title":"Complete Data Flow Diagram","text":"<pre><code>\u250c\u2500 Nightscout API \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 288 readings/day                  \u2502\n\u2502 5-min intervals                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 INGESTION (2.45s) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dlt_glucose_entries                       \u2502\n\u2502 \u251c\u2500 Fetch from API (288 rows)              \u2502\n\u2502 \u251c\u2500 Validate with Pandera [PASSED]         \u2502\n\u2502 \u251c\u2500 Stage to S3 parquet                    \u2502\n\u2502 \u2514\u2500 Merge to iceberg raw.glucose_entries   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 TRANSFORM BRONZE (1.23s) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 stg_glucose_entries                       \u2502\n\u2502 \u251c\u2500 Type conversions                       \u2502\n\u2502 \u251c\u2500 Rename columns                         \u2502\n\u2502 \u251c\u2500 Filter nulls &amp; out-of-range            \u2502\n\u2502 \u2514\u2500 Create view in bronze.*                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 TRANSFORM SILVER (3.42s) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fct_glucose_readings                      \u2502\n\u2502 \u251c\u2500 Add time dimensions                    \u2502\n\u2502 \u251c\u2500 Classify glucose (hypo/in-range/hyper) \u2502\n\u2502 \u251c\u2500 Calculate rate of change (window fn)   \u2502\n\u2502 \u2514\u2500 Create table in silver.*               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 TRANSFORM GOLD (1.15s) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 mrt_glucose_readings                      \u2502\n\u2502 \u251c\u2500 Aggregate by hour                      \u2502\n\u2502 \u251c\u2500 Calculate % time in range               \u2502\n\u2502 \u251c\u2500 Count by category                      \u2502\n\u2502 \u2514\u2500 Create table in gold.*                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 PUBLISH (0.67s) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 publish_glucose_marts                    \u2502\n\u2502 \u251c\u2500 Query Iceberg gold tables              \u2502\n\u2502 \u251c\u2500 Truncate Postgres marts                \u2502\n\u2502 \u2514\u2500 Insert results (1 daily + 24 hourly)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 ANALYTICS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Superset Dashboard                \u2502\n\u2502 \u251c\u2500 Daily avg glucose: 145.3 mg/dL \u2502\n\u2502 \u251c\u2500 Time in range: 78.2%           \u2502\n\u2502 \u251c\u2500 Hourly patterns graph           \u2502\n\u2502 \u2514\u2500 30-day trend                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTotal pipeline time: 8.92s\n</code></pre>"},{"location":"blog/08-real-world-example/#key-takeaways","title":"Key Takeaways","text":"<p>End-to-end pipeline from API to dashboard Idempotent ingestion - safe to re-run Type-safe transformations via dbt Data quality checks with Pandera &amp; Dagster Audit trail via Nessie versioning Scalable to millions of rows Observable with logs, metrics, dashboards</p> <p>This is real-world data engineering:</p> <ul> <li>Start with raw data (APIs, files, databases)</li> <li>Validate early (Pandera schemas)</li> <li>Transform incrementally (bronze \u2192 silver \u2192 gold)</li> <li>Publish for consumption (Postgres, dashboards)</li> <li>Monitor quality (tests, checks, alerts)</li> </ul>"},{"location":"blog/08-real-world-example/#next-steps","title":"Next Steps","text":"<p>To extend this example:</p> <ol> <li>Add more data sources: GitHub, Fitbit, weather, etc.</li> <li>Advanced analytics: Anomaly detection, forecasting</li> <li>Real-time alerts: Slack notifications for hypoglycemia</li> <li>Retention policies: Archive old data, keep recent data hot</li> <li>ML integration: Predict glucose trends</li> </ol> <p>The pattern remains: Ingest \u2192 Validate \u2192 Transform \u2192 Publish \u2192 Monitor</p>"},{"location":"blog/08-real-world-example/#summary","title":"Summary","text":"<p>You now understand:</p> <ul> <li>Modern data lakehouse architecture (Iceberg, Nessie)</li> <li>Complete ingestion pattern (DLT, PyIceberg)</li> <li>SQL transformation best practices (dbt layers)</li> <li>Production orchestration (Dagster assets)</li> <li>Data quality and testing</li> <li>Real-world example building dashboards</li> </ul> <p>Time to build your own pipelines!</p> <p>See the main docs for API references, troubleshooting, and production deployment guides.</p> <p>Next: Part 9: Data Quality with Pandera</p>"},{"location":"blog/09-data-quality-with-pandera/","title":"Part 9: Data Quality\u2014Pandera Schemas and Asset Checks","text":"<p>In Part 8, we built a complete pipeline. But how do we ensure data quality throughout? This post covers validation at multiple layers.</p>"},{"location":"blog/09-data-quality-with-pandera/#the-data-quality-problem","title":"The Data Quality Problem","text":"<p>Without validation, bad data silently propagates:</p> <pre><code># This data is... problematic\nglucose_reading = {\n    \"glucose_mg_dl\": -50,  # Negative? Impossible\n    \"timestamp\": \"2024-13-45\",  # Invalid date\n    \"device\": None,  # Required field missing\n    \"reading_type\": \"unknown\",  # Invalid enum\n}\n\n# Query downstream just sees rows\n# Dashboard shows glucose values from -50 to 5000\n# Alerts fire for impossible \"low\" readings\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#three-layers-of-validation","title":"Three Layers of Validation","text":"<p>Phlo uses validation at three points:</p> <pre><code>API Data\n    \u2193\n[1] Ingestion: Pandera schema validation\n    \u2193\nDLT Staging Tables\n    \u2193\n[2] dbt Tests: Business logic validation\n    \u2193\nIceberg/Postgres Marts\n    \u2193\n[3] Dagster Asset Checks: Runtime monitoring\n    \u2193\nDashboards/Alerts\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#layer-1-pandera-schemas-ingestion","title":"Layer 1: Pandera Schemas (Ingestion)","text":"<p>Pandera provides type-safe validation with detailed error reporting.</p>"},{"location":"blog/09-data-quality-with-pandera/#setting-up-a-schema","title":"Setting Up a Schema","text":"<p>Phlo uses Pandera's DataFrameModel approach for cleaner, class-based schemas:</p> <pre><code># File: phlo-examples/nightscout/workflows/schemas/nightscout.py\nfrom pandera.pandas import DataFrameModel, Field\n\n# Validation constants\nMIN_GLUCOSE_MG_DL = 20\nMAX_GLUCOSE_MG_DL = 600\n\nVALID_DIRECTIONS = [\n    \"Flat\", \"FortyFiveUp\", \"FortyFiveDown\",\n    \"SingleUp\", \"SingleDown\", \"DoubleUp\", \"DoubleDown\", \"NONE\"\n]\n\n\nclass RawGlucoseEntries(DataFrameModel):\n    \"\"\"\n    Schema for raw Nightscout glucose entries from the API.\n\n    Validates raw glucose data at ingestion time:\n    - Valid glucose ranges (1-1000 mg/dL for raw data)\n    - Proper field types and nullability\n    - Required metadata fields\n    - Unique entry IDs\n    \"\"\"\n\n    _id: str = Field(\n        nullable=False,\n        unique=True,\n        description=\"Nightscout entry ID (unique identifier)\",\n    )\n\n    sgv: int = Field(\n        ge=1,\n        le=1000,\n        nullable=False,\n        description=\"Sensor glucose value in mg/dL (1-1000 for raw data)\",\n    )\n\n    date: int = Field(\n        nullable=False,\n        description=\"Unix timestamp in milliseconds\",\n    )\n\n    date_string: datetime = Field(\n        nullable=False,\n        description=\"ISO 8601 timestamp\",\n    )\n\n    direction: str | None = Field(\n        isin=VALID_DIRECTIONS,\n        nullable=True,\n        description=\"Trend direction (e.g., 'SingleUp', 'Flat')\",\n    )\n\n    device: str | None = Field(\n        nullable=True,\n        description=\"Device name that recorded the entry\",\n    )\n\n    class Config:\n        strict = False  # Allow DLT metadata fields\n        coerce = True\n\n\nclass FactGlucoseReadings(DataFrameModel):\n    \"\"\"\n    Schema for the fct_glucose_readings table (silver layer).\n\n    Validates processed Nightscout glucose data including:\n    - Valid glucose ranges (20-600 mg/dL)\n    - Proper timestamp formatting\n    - Valid direction indicators\n    - Time dimension fields (hour, day of week)\n    - Glucose categorization\n    \"\"\"\n\n    entry_id: str = Field(\n        nullable=False,\n        unique=True,\n        description=\"Unique identifier for each glucose reading entry\",\n    )\n\n    glucose_mg_dl: int = Field(\n        ge=MIN_GLUCOSE_MG_DL,\n        le=MAX_GLUCOSE_MG_DL,\n        nullable=False,\n        description=f\"Blood glucose in mg/dL ({MIN_GLUCOSE_MG_DL}-{MAX_GLUCOSE_MG_DL})\",\n    )\n\n    reading_timestamp: datetime = Field(\n        nullable=False,\n        description=\"Timestamp when the glucose reading was taken\",\n    )\n\n    hour_of_day: int = Field(\n        ge=0,\n        le=23,\n        nullable=False,\n        description=\"Hour of day when reading was taken (0-23)\",\n    )\n\n    glucose_category: str = Field(\n        isin=[\"hypoglycemia\", \"in_range\", \"hyperglycemia_mild\", \"hyperglycemia_severe\"],\n        nullable=False,\n        description=\"Categorized glucose level based on ADA guidelines\",\n    )\n\n    is_in_range: int = Field(\n        isin=[0, 1],\n        nullable=False,\n        description=\"Whether glucose level is within target range (0=no, 1=yes)\",\n    )\n\n    class Config:\n        strict = True\n        coerce = True\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#using-pandera-in-phlo_ingestion","title":"Using Pandera in @phlo_ingestion","text":"<p>The <code>@phlo_ingestion</code> decorator automatically validates data with Pandera schemas:</p> <pre><code># File: phlo-examples/nightscout/workflows/ingestion/nightscout/readings.py\n\nimport phlo\nfrom dlt.sources.rest_api import rest_api\nfrom workflows.schemas.nightscout import RawGlucoseEntries\n\n@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",\n    validation_schema=RawGlucoseEntries,  # Automatic validation\n    group=\"nightscout\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef glucose_entries(partition_date: str):\n    \"\"\"\n    Ingest Nightscout glucose entries with automatic validation.\n\n    The decorator validates data against RawGlucoseEntries schema:\n    - Checks all field types and constraints\n    - Validates glucose ranges (1-1000 for raw)\n    - Ensures unique entry IDs\n    - Logs validation failures with details\n    \"\"\"\n    start_time_iso = f\"{partition_date}T00:00:00.000Z\"\n    end_time_iso = f\"{partition_date}T23:59:59.999Z\"\n\n    source = rest_api(\n        client={\"base_url\": \"https://gwp-diabetes.fly.dev/api/v1\"},\n        resources=[\n            {\n                \"name\": \"entries\",\n                \"endpoint\": {\n                    \"path\": \"entries.json\",\n                    \"params\": {\n                        \"count\": 10000,\n                        \"find[dateString][$gte]\": start_time_iso,\n                        \"find[dateString][$lt]\": end_time_iso,\n                    },\n                },\n            }\n        ],\n    )\n\n    return source\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#detailed-error-messages","title":"Detailed Error Messages","text":"<p>When validation fails, Pandera provides actionable feedback:</p> <pre><code>SchemaError: Column 'sgv' has an out-of-range value:\n\n  row_num  sgv\n       42  -50   \u2190 Glucose -50? Impossible\n\n  Check failed: lambda x: (x &gt;= 20) &amp; (x &lt;= 600)\n  Glucose must be 20-600 mg/dL\n\nFailure counts:\n  Total: 3 failures\n  Unique values failing: 1\n</code></pre> <p>This helps you:</p> <ul> <li>Identify exact problematic rows</li> <li>Understand which rule failed</li> <li>Decide: drop, fix, or investigate</li> </ul>"},{"location":"blog/09-data-quality-with-pandera/#layer-2-dbt-tests-transformations","title":"Layer 2: dbt Tests (Transformations)","text":"<p>After ingestion, dbt tests validate business logic during transformations.</p>"},{"location":"blog/09-data-quality-with-pandera/#schema-tests-yaml-based","title":"Schema Tests (YAML-based)","text":"<pre><code># workflows/transforms/dbt/models/bronze/stg_glucose_entries.yml\nversion: 2\n\nmodels:\n  - name: stg_glucose_entries\n    description: Staged glucose entries with basic cleaning\n\n    columns:\n      - name: entry_id\n        description: Unique identifier\n        tests:\n          - unique\n          - not_null\n\n      - name: glucose_mg_dl\n        description: Glucose in mg/dL\n        tests:\n          - not_null\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 20\n              max_value: 600\n              strictly: false\n          - dbt_expectations.expect_column_values_to_match_regex:\n              regex: \"^\\\\d+$\"\n\n      - name: timestamp_iso\n        description: ISO 8601 timestamp\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"timestamp_iso ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}T'\"\n\n      - name: device_type\n        description: Device type (enum)\n        tests:\n          - not_null\n          - accepted_values:\n              values: [\"dexcom\", \"freestyle\", \"medtronic\"]\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#custom-tests-sql","title":"Custom Tests (SQL)","text":"<pre><code>-- workflows/transforms/dbt/tests/no_duplicate_readings.sql\n-- Test: Ensure no duplicate readings within 5 minutes\n\nSELECT\n  device_type,\n  COUNT(*) as reading_count,\n  MIN(timestamp_iso) as earliest,\n  MAX(timestamp_iso) as latest\nFROM {{ ref('stg_glucose_entries') }}\nGROUP BY\n  device_type,\n  DATE_TRUNC('5 minutes', timestamp_iso)\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If this query returns rows, the test fails (duplicates found).</p>"},{"location":"blog/09-data-quality-with-pandera/#running-dbt-tests","title":"Running dbt Tests","text":"<pre><code># Test all transformations\ndbt test --select stg_glucose_entries\n\n# Test specific column\ndbt test --select stg_glucose_entries.unique:entry_id\n\n# Show detailed failure output\ndbt test --select stg_glucose_entries --debug\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#layer-3-dagster-asset-checks-runtime","title":"Layer 3: Dagster Asset Checks (Runtime)","text":"<p>After orchestration, Dagster asset checks monitor data quality in production. Phlo provides two approaches: the declarative <code>@phlo_quality</code> decorator and traditional <code>@asset_check</code> for custom logic.</p>"},{"location":"blog/09-data-quality-with-pandera/#approach-1-phlo_quality-decorator-declarative","title":"Approach 1: @phlo_quality Decorator (Declarative)","text":"<p>For common checks (null, range, freshness), use the <code>@phlo_quality</code> decorator to reduce boilerplate by 70-80%:</p> <pre><code># File: phlo-examples/nightscout/workflows/quality/nightscout.py\n\nimport phlo\nfrom phlo_quality import NullCheck, RangeCheck, FreshnessCheck\n\n@phlo_quality(\n    table=\"silver.fct_glucose_readings\",\n    checks=[\n        NullCheck(columns=[\"entry_id\", \"glucose_mg_dl\", \"reading_timestamp\"]),\n        RangeCheck(column=\"glucose_mg_dl\", min_value=20, max_value=600),\n        RangeCheck(column=\"hour_of_day\", min_value=0, max_value=23),\n        FreshnessCheck(column=\"reading_timestamp\", max_age_hours=24),\n    ],\n    group=\"nightscout\",\n    blocking=True,\n)\ndef glucose_readings_quality():\n    \"\"\"Declarative quality checks for glucose readings using @phlo_quality.\"\"\"\n    pass\n\n\n@phlo_quality(\n    table=\"gold.fct_daily_glucose_metrics\",\n    checks=[\n        NullCheck(columns=[\"reading_date\", \"reading_count\", \"avg_glucose_mg_dl\"]),\n        RangeCheck(column=\"avg_glucose_mg_dl\", min_value=20, max_value=600),\n        RangeCheck(column=\"time_in_range_pct\", min_value=0, max_value=100),\n    ],\n    group=\"nightscout\",\n    blocking=True,\n)\ndef daily_metrics_quality():\n    \"\"\"Declarative quality checks for daily glucose metrics.\"\"\"\n    pass\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#approach-2-traditional-asset_check-custom-logic","title":"Approach 2: Traditional @asset_check (Custom Logic)","text":"<p>For complex validation with Pandera schemas or custom business logic:</p> <pre><code># File: phlo-examples/nightscout/workflows/quality/nightscout.py\n\nfrom dagster import AssetCheckResult, AssetKey, asset_check\nfrom phlo_trino.resource import TrinoResource\nfrom workflows.schemas.nightscout import FactGlucoseReadings\nimport pandera.errors\n\n\n@asset_check(\n    name=\"nightscout_glucose_quality\",\n    asset=AssetKey([\"fct_glucose_readings\"]),\n    blocking=True,\n    description=\"Validate processed Nightscout glucose data using Pandera schema validation.\",\n)\ndef nightscout_glucose_quality_check(context, trino: TrinoResource) -&gt; AssetCheckResult:\n    \"\"\"\n    Quality check using Pandera for type-safe schema validation.\n\n    Validates glucose readings against the FactGlucoseReadings schema,\n    checking data types, ranges, and business rules directly against Iceberg via Trino.\n    \"\"\"\n    query = \"\"\"\n    SELECT\n        entry_id,\n        glucose_mg_dl,\n        reading_timestamp,\n        direction,\n        hour_of_day,\n        day_of_week,\n        glucose_category,\n        is_in_range\n    FROM iceberg_dev.silver.fct_glucose_readings\n    \"\"\"\n\n    partition_key = getattr(context, \"partition_key\", None)\n    if partition_key:\n        query = f\"{query}\\nWHERE DATE(reading_timestamp) = DATE '{partition_key}'\"\n        context.log.info(f\"Validating partition: {partition_key}\")\n\n    try:\n        with trino.cursor(schema=\"silver\") as cursor:\n            cursor.execute(query)\n            rows = cursor.fetchall()\n            columns = [desc[0] for desc in cursor.description]\n\n        fact_df = pd.DataFrame(rows, columns=columns)\n\n        # Type conversions\n        fact_df[\"glucose_mg_dl\"] = fact_df[\"glucose_mg_dl\"].astype(\"int64\")\n        fact_df[\"hour_of_day\"] = fact_df[\"hour_of_day\"].astype(\"int64\")\n        fact_df[\"day_of_week\"] = fact_df[\"day_of_week\"].astype(\"int64\")\n        fact_df[\"is_in_range\"] = fact_df[\"is_in_range\"].astype(\"int64\")\n        fact_df[\"reading_timestamp\"] = pd.to_datetime(fact_df[\"reading_timestamp\"])\n\n        context.log.info(f\"Loaded {len(fact_df)} rows for validation\")\n\n    except Exception as exc:\n        context.log.error(f\"Failed to load data from Trino: {exc}\")\n        return AssetCheckResult(\n            passed=False,\n            metadata={\n                \"reason\": \"trino_query_failed\",\n                \"error\": str(exc),\n            },\n        )\n\n    if fact_df.empty:\n        return AssetCheckResult(\n            passed=True,\n            metadata={\n                \"rows_validated\": 0,\n                \"note\": \"No data available for selected partition\",\n            },\n        )\n\n    # Validate with Pandera schema\n    context.log.info(\"Validating data with Pandera schema...\")\n    try:\n        FactGlucoseReadings.validate(fact_df, lazy=True)\n        context.log.info(\"All validation checks passed!\")\n\n        return AssetCheckResult(\n            passed=True,\n            metadata={\n                \"rows_validated\": len(fact_df),\n                \"columns_validated\": len(fact_df.columns),\n            },\n        )\n\n    except pandera.errors.SchemaErrors as err:\n        failure_cases = err.failure_cases\n        context.log.warning(f\"Validation failed with {len(failure_cases)} check failures\")\n\n        return AssetCheckResult(\n            passed=False,\n            metadata={\n                \"rows_evaluated\": len(fact_df),\n                \"failed_checks\": len(failure_cases),\n                \"failures_by_column\": failure_cases.groupby(\"column\").size().to_dict(),\n                \"sample_failures\": failure_cases.head(10).to_dict(orient=\"records\"),\n            },\n        )\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#viewing-check-results","title":"Viewing Check Results","text":"<p>In Dagster UI:</p> <pre><code>Asset: fct_glucose_readings\n\u251c\u2500 \u2713 glucose_range_check (PASSED)\n\u2502  \u2514\u2500 valid_count: 4,987 / 5,000\n\u2502  \u2514\u2500 percentage_valid: 99.74%\n\u251c\u2500 \u2713 glucose_freshness_check (PASSED)\n\u2502  \u2514\u2500 latest_reading_hours_ago: 0.15\n\u251c\u2500 \u2717 glucose_statistical_bounds_check (FAILED)\n\u2502  \u2514\u2500 outlier_count: 3\n\u2502  \u2514\u2500 bounds: [45.2, 215.8]\n\u2502  \u2514\u2500 Action: Investigate readings outside [45, 215]\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#comparing-both-approaches","title":"Comparing Both Approaches","text":"<p>Both approaches are used in the actual Phlo implementation and serve different purposes:</p>"},{"location":"blog/09-data-quality-with-pandera/#phlo_quality-declarative-10-lines","title":"@phlo_quality: Declarative (10 lines)","text":"<p>Best for standard checks - reduces boilerplate by 70-80%:</p> <pre><code># File: phlo-examples/nightscout/workflows/quality/nightscout.py\n\nimport phlo\nfrom phlo_quality import NullCheck, RangeCheck, FreshnessCheck\n\n@phlo_quality(\n    table=\"silver.fct_glucose_readings\",\n    checks=[\n        NullCheck(columns=[\"entry_id\", \"glucose_mg_dl\", \"reading_timestamp\"]),\n        RangeCheck(column=\"glucose_mg_dl\", min_value=20, max_value=600),\n        RangeCheck(column=\"hour_of_day\", min_value=0, max_value=23),\n        FreshnessCheck(column=\"reading_timestamp\", max_age_hours=24),\n    ],\n    group=\"nightscout\",\n    blocking=True,\n)\ndef glucose_readings_quality():\n    \"\"\"Declarative quality checks for glucose readings.\"\"\"\n    pass\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#traditional-asset_check-custom-logic-80-lines","title":"Traditional @asset_check: Custom Logic (80+ lines)","text":"<p>Best for complex validation with Pandera schemas or custom business logic:</p> <pre><code># File: phlo-examples/nightscout/workflows/quality/nightscout.py\n\n@asset_check(\n    name=\"nightscout_glucose_quality\",\n    asset=AssetKey([\"fct_glucose_readings\"]),\n    blocking=True,\n)\ndef nightscout_glucose_quality_check(context, trino: TrinoResource) -&gt; AssetCheckResult:\n    \"\"\"Full Pandera schema validation with custom error handling.\"\"\"\n\n    # Query data from Trino\n    query = \"\"\"SELECT entry_id, glucose_mg_dl, ... FROM iceberg_dev.silver.fct_glucose_readings\"\"\"\n    with trino.cursor() as cursor:\n        cursor.execute(query)\n        rows = cursor.fetchall()\n\n    fact_df = pd.DataFrame(rows, columns=columns)\n\n    # Validate with Pandera schema\n    try:\n        FactGlucoseReadings.validate(fact_df, lazy=True)\n        return AssetCheckResult(passed=True, metadata={...})\n    except pandera.errors.SchemaErrors as err:\n        return AssetCheckResult(passed=False, metadata={...})\n</code></pre> <p>Both approaches are valid - use <code>@phlo_quality</code> for common checks, <code>@asset_check</code> for complex logic.</p>"},{"location":"blog/09-data-quality-with-pandera/#available-check-types","title":"Available Check Types","text":"Check Type Purpose Parameters <code>NullCheck</code> Verify no nulls <code>columns</code>, <code>tolerance</code> (% allowed) <code>RangeCheck</code> Verify numeric bounds <code>column</code>, <code>min_value</code>, <code>max_value</code> <code>FreshnessCheck</code> Verify data recency <code>column</code>, <code>max_age_hours</code> <code>UniqueCheck</code> Verify uniqueness <code>columns</code> (can be composite) <code>CountCheck</code> Verify row count <code>min_count</code>, <code>max_count</code> <code>SchemaCheck</code> Validate against Pandera <code>schema</code> (DataFrameModel class) <code>CustomSQLCheck</code> Run arbitrary SQL <code>sql</code>, <code>expected_result</code>"},{"location":"blog/09-data-quality-with-pandera/#check-parameters-in-detail","title":"Check Parameters in Detail","text":"<p>NullCheck with tolerance:</p> <pre><code># Strict: no nulls allowed\nNullCheck(columns=[\"sgv\", \"timestamp\"])\n\n# Lenient: allow up to 1% nulls\nNullCheck(columns=[\"device\"], tolerance=0.01)\n</code></pre> <p>RangeCheck:</p> <pre><code># Both bounds\nRangeCheck(column=\"sgv\", min_value=20, max_value=600)\n\n# Only lower bound\nRangeCheck(column=\"price\", min_value=0)\n\n# Only upper bound\nRangeCheck(column=\"percentage\", max_value=100)\n</code></pre> <p>FreshnessCheck:</p> <pre><code># Data must be less than 2 hours old\nFreshnessCheck(column=\"timestamp\", max_age_hours=2)\n\n# Different column name\nFreshnessCheck(column=\"created_at\", max_age_hours=24)\n</code></pre> <p>UniqueCheck:</p> <pre><code># Single column unique\nUniqueCheck(columns=[\"id\"])\n\n# Composite unique (combination must be unique)\nUniqueCheck(columns=[\"user_id\", \"timestamp\"])\n</code></pre> <p>CustomSQLCheck for complex rules:</p> <pre><code>CustomSQLCheck(\n    name=\"business_hours_only\",\n    sql=\"\"\"\n        SELECT COUNT(*) as violations\n        FROM {table}\n        WHERE HOUR(timestamp) &lt; 6 OR HOUR(timestamp) &gt; 22\n    \"\"\",\n    expected_result=0,  # Zero violations expected\n)\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#decorator-parameters","title":"Decorator Parameters","text":"<pre><code>@phlo_quality(\n    table=\"silver.fct_glucose_readings\",  # Fully qualified table name\n    checks=[...],                          # List of check instances\n    group=\"glucose\",                       # Asset group (optional)\n    blocking=True,                         # Fail downstream if check fails\n    warn_threshold=0.1,                    # Warn if &gt;10% of checks fail\n    backend=\"trino\",                       # Query backend: \"trino\" or \"duckdb\"\n)\n</code></pre> <p>blocking parameter:</p> <ul> <li><code>blocking=True</code> (default): Failed checks prevent downstream assets from running</li> <li><code>blocking=False</code>: Failed checks log warnings but don't block execution</li> </ul> <p>warn_threshold:</p> <ul> <li>Set to <code>0.0</code> for strict mode (any failure = warning)</li> <li>Set to <code>0.1</code> to allow 10% of checks to fail before warning</li> </ul>"},{"location":"blog/09-data-quality-with-pandera/#combining-with-pandera-schemas","title":"Combining with Pandera Schemas","text":"<p>For complex validation, combine the decorator with Pandera:</p> <pre><code>from phlo_quality import SchemaCheck\nfrom workflows.schemas.glucose import FactGlucoseReadings\n\n@phlo_quality(\n    table=\"silver.fct_glucose_readings\",\n    checks=[\n        # Use Pandera for full schema validation\n        SchemaCheck(schema=FactGlucoseReadings),\n\n        # Plus additional runtime checks\n        FreshnessCheck(column=\"timestamp\", max_age_hours=2),\n    ],\n)\ndef glucose_comprehensive_quality():\n    pass\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#when-to-use-each-approach","title":"When to Use Each Approach","text":"Scenario Recommended Approach Example Standard null/range/freshness checks <code>@phlo_quality</code> decorator <code>NullCheck</code>, <code>RangeCheck</code>, <code>FreshnessCheck</code> Full Pandera schema validation Traditional <code>@asset_check</code> <code>FactGlucoseReadings.validate()</code> Complex business logic Traditional <code>@asset_check</code> Custom distribution checks Statistical analysis Traditional <code>@asset_check</code> Outlier detection Multiple simple checks <code>@phlo_quality</code> decorator Combine <code>NullCheck</code> + <code>RangeCheck</code> Custom error handling Traditional <code>@asset_check</code> Detailed failure reporting <p>Real-world usage in phlo-examples/nightscout:</p> <ul> <li><code>@phlo_quality</code>: <code>glucose_readings_quality()</code>, <code>daily_metrics_quality()</code> - standard checks</li> <li><code>@asset_check</code>: <code>nightscout_glucose_quality_check()</code> - full Pandera validation with custom error handling</li> </ul> <p>Both approaches are valid and complement each other. The decorator handles common cases efficiently, while traditional checks provide full control for complex scenarios.</p>"},{"location":"blog/09-data-quality-with-pandera/#validation-at-each-layer","title":"Validation at Each Layer","text":""},{"location":"blog/09-data-quality-with-pandera/#why-three-layers","title":"Why Three Layers?","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Layer 1: Pandera (Ingestion)        \u2502\n\u2502 \u2713 Type correctness                  \u2502\n\u2502 \u2713 Basic constraints (range, enum)   \u2502\n\u2502 \u2713 Prevent bad data entering system  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2193 (only clean data passes)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Layer 2: dbt Tests (Transformation) \u2502\n\u2502 \u2713 Business logic rules              \u2502\n\u2502 \u2713 Cross-table consistency           \u2502\n\u2502 \u2713 Catch issues during transform     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2193 (only valid transforms apply)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Layer 3: Asset Checks (Runtime)     \u2502\n\u2502 \u2713 Production data quality           \u2502\n\u2502 \u2713 Anomaly detection                 \u2502\n\u2502 \u2713 Freshness monitoring              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each layer catches different issues:</p> <ul> <li>Pandera: Bad API responses</li> <li>dbt: Broken business logic</li> <li>Asset Checks: Unexpected data patterns</li> </ul>"},{"location":"blog/09-data-quality-with-pandera/#practical-example-catching-a-bug","title":"Practical Example: Catching a Bug","text":"<pre><code>Tuesday 3am: Nightscout API starts returning SGV = NULL\n\n[1] Pandera catches it:\n    \u2717 Column 'sgv' has null values (not nullable)\n    \u2192 Ingestion stops, alert sent\n    \u2192 Manual investigation before data corrupts\n\nWithout Layer 1:\n    [2] dbt Test would catch it:\n        \u2717 not_null check fails\n        \u2192 Build fails\n        \u2192 Data already written to staging\n\n    Without Layer 2:\n        [3] Asset Check catches it:\n            \u2717 All values are NULL\n            \u2192 Dashboard shows \"N/A\"\n            \u2192 Users question data validity\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#configuring-validation-strictness","title":"Configuring Validation Strictness","text":"<pre><code># phlo/config.py\nfrom pydantic import BaseSettings\n\nclass DataQualityConfig(BaseSettings):\n    # Validation behavior\n    pandera_strict: bool = True  # Fail on any schema error\n    allow_null_in_required: bool = False\n\n    # Thresholds for warnings\n    max_invalid_percentage: float = 1.0  # Warn if &gt;1% invalid\n    freshness_threshold_hours: float = 2.0\n\n    # Anomaly detection\n    enable_statistical_checks: bool = True\n    outlier_std_devs: float = 3.0\n\n    class Config:\n        env_file = (\".phlo/.env\", \".phlo/.env.local\")\n\nconfig = DataQualityConfig()\n</code></pre> <p>Use in code:</p> <pre><code># Ingestion: strict\nif config.pandera_strict:\n    validated_df = glucose_entries_schema.validate(data)\nelse:\n    # Lenient: log but continue\n    try:\n        validated_df = glucose_entries_schema.validate(data)\n    except pa.errors.SchemaError as e:\n        context.log.warning(f\"Schema validation failed: {e}\")\n        validated_df = data  # Proceed anyway\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#monitoring-dashboard","title":"Monitoring Dashboard","text":"<p>Create a Superset dashboard for data quality:</p> <pre><code>-- Query: Validation failures by day\nSELECT\n  DATE(check_timestamp) as date,\n  check_name,\n  COUNT(*) as failure_count,\n  COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY DATE(check_timestamp)) as pct\nFROM data_quality_logs\nWHERE status = 'FAILED'\nGROUP BY 1, 2\nORDER BY 1 DESC, 3 DESC;\n\n-- Query: Freshness by asset\nSELECT\n  asset_name,\n  MAX(data_date) as latest_data,\n  NOW() - MAX(data_date) as hours_stale,\n  CASE\n    WHEN NOW() - MAX(data_date) &lt; '2 hours'::interval THEN '\u2713 Fresh'\n    WHEN NOW() - MAX(data_date) &lt; '24 hours'::interval THEN '\u26a0 Stale'\n    ELSE '\u2717 Very Stale'\n  END as freshness_status\nFROM asset_metadata\nGROUP BY 1\nORDER BY 3 DESC;\n</code></pre>"},{"location":"blog/09-data-quality-with-pandera/#summary","title":"Summary","text":"<p>Phlo uses three-layer validation:</p> <ol> <li>Pandera (ingestion): Type and constraint checking</li> <li>dbt (transformation): Business logic and consistency</li> <li>Dagster (runtime): Production monitoring and anomaly detection</li> </ol> <p>This ensures:</p> <ul> <li>Bad data never enters the system</li> <li>Transforms execute correctly</li> <li>Production issues are caught quickly</li> </ul> <p>Next: Part 10: Metadata and Governance with OpenMetadata</p> <p>See you there!</p>"},{"location":"blog/10-metadata-governance/","title":"Part 10: Metadata and Governance with OpenMetadata","text":"<p>Data quality is important. But knowing what you have, where it came from, and who can use it is equally critical. This post covers metadata and governance with OpenMetadata.</p>"},{"location":"blog/10-metadata-governance/#the-metadata-problem","title":"The Metadata Problem","text":"<p>Without metadata tracking:</p> <pre><code>Tuesday 3pm: Someone asks \"Where did this dataset come from?\"\n\nAnswers from your team:\n- Engineer 1: \"I think Nightscout API?\"\n- Engineer 2: \"Maybe it's in the glucose_readings table\"\n- Data analyst: \"I don't know, it's in the dashboard\"\n- Manager: \"How many people depend on this?\"\n</code></pre> <p>Nobody knows because metadata is scattered:</p> <ul> <li>Table definitions in dbt YAML</li> <li>Column notes in dbt docs</li> <li>Data source info in Dagster assets</li> <li>Lineage unclear</li> <li>Ownership unknown</li> <li>Change history nowhere</li> </ul> <p>Note: For detailed OpenMetadata setup instructions, see docs/setup/openmetadata.md</p>"},{"location":"blog/10-metadata-governance/#openmetadata-the-open-source-data-catalog","title":"OpenMetadata: The Open-Source Data Catalog","text":"<p>OpenMetadata is an open-source data catalog that answers:</p> <ul> <li>What data exists?</li> <li>Where is it stored?</li> <li>How is it related?</li> <li>Who owns it?</li> <li>What does it mean?</li> <li>How often is it updated?</li> <li>What quality checks does it have?</li> </ul>"},{"location":"blog/10-metadata-governance/#why-openmetadata-for-phlo","title":"Why OpenMetadata for Phlo?","text":"<p>OpenMetadata integrates seamlessly with Phlo's tech stack:</p> <ul> <li>Trino connector - Auto-discovers Iceberg tables</li> <li>Modern UI - Intuitive search and browsing experience</li> <li>Active development - Regular updates and improvements</li> <li>Simple architecture - MySQL + Elasticsearch (6GB RAM required)</li> <li>Open source - No licensing costs</li> </ul>"},{"location":"blog/10-metadata-governance/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         OpenMetadata Server (UI)           \u2502\n\u2502         http://localhost:10020              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    MySQL    \u2502 \u2502 Elasticsearch  \u2502\n\u2502  (metadata) \u2502 \u2502   (search)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 Ingests metadata from:\n       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Trino \u2192 Iceberg Tables (Nessie)\u2502\n\u2502  - raw.glucose_entries           \u2502\n\u2502  - bronze.stg_glucose_entries    \u2502\n\u2502  - silver.fct_glucose_readings   \u2502\n\u2502  - gold.dim_date                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/10-metadata-governance/#quick-start-with-openmetadata","title":"Quick Start with OpenMetadata","text":""},{"location":"blog/10-metadata-governance/#1-start-openmetadata-services","title":"1. Start OpenMetadata Services","text":"<pre><code># Start the data catalog stack\nmake up-catalog\n\n# Check health status\nmake health-catalog\n</code></pre> <p>Expected output:</p> <pre><code>=== Data Catalog Health Check ===\nOpenMetadata:\n  Ready\n  UI: http://localhost:10020\n  Default credentials: admin / admin\nMySQL:\n  Ready\nElasticsearch:\n  Ready\n</code></pre>"},{"location":"blog/10-metadata-governance/#2-access-openmetadata-ui","title":"2. Access OpenMetadata UI","text":"<pre><code># Open in browser\nmake catalog\n# Or manually visit: http://localhost:10020\n</code></pre> <p>Default credentials:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul> <p>Security Note: Change the default password in production by updating <code>OPENMETADATA_ADMIN_PASSWORD</code> in <code>.phlo/.env.local</code></p>"},{"location":"blog/10-metadata-governance/#setting-up-trino-data-source","title":"Setting Up Trino Data Source","text":""},{"location":"blog/10-metadata-governance/#step-1-add-trino-database-service","title":"Step 1: Add Trino Database Service","text":"<ol> <li>Click Settings (gear icon) in the top-right corner</li> <li>Navigate to Integrations \u2192 Databases</li> <li>Click Add New Service</li> <li>Select Trino from the list of database types</li> <li>Click Next</li> </ol>"},{"location":"blog/10-metadata-governance/#step-2-configure-trino-connection","title":"Step 2: Configure Trino Connection","text":"<p>Service Name:</p> <pre><code>trino\n</code></pre> <p>Description:</p> <pre><code>Phlo lakehouse Trino query engine with Iceberg catalog\n</code></pre> <p>Connection Configuration:</p> <p>Click on Basic authentication type, then configure:</p> Field Value Notes Host <code>trino</code> Docker service name (internal network) Port <code>8080</code> Internal container port Username <code>phlo</code> Any username (no auth in dev) Catalog Leave empty We'll filter by catalog in ingestion Database Schema Leave empty - <p>Port Note: Trino runs on port <code>8080</code> inside the Docker network. The external host port <code>10005</code> is only for accessing Trino from your laptop. OpenMetadata uses the internal port <code>8080</code>.</p> <p>Click Test Connection - you should see:</p> <pre><code>Connection test was successful\n</code></pre> <p>Click Submit to save the service.</p>"},{"location":"blog/10-metadata-governance/#step-3-configure-metadata-ingestion-pipeline","title":"Step 3: Configure Metadata Ingestion Pipeline","text":"<p>After creating the service, you'll be prompted to set up metadata ingestion.</p> <ol> <li>Pipeline Name: <code>trino-metadata</code></li> <li>Pipeline Type: Select Metadata Ingestion</li> <li>Click Next</li> </ol> <p>Filter Patterns (CRITICAL - prevents crashes):</p> <pre><code>Database Filter Pattern:\n  Include: iceberg\n  Exclude: system\n\nSchema Filter Pattern:\n  Include: raw, bronze, silver, gold\n  Exclude: information_schema\n\nTable Filter Pattern:\n  Include: .*\n  Exclude: (leave empty)\n</code></pre> <p>Advanced Configuration:</p> <p>Enable/disable these options:</p> Option Enable? Reason Include Tables Yes Core metadata Include Views Yes Include views Include Tags Yes Catalog tags Include Owners No Not used in dev Include Stored Procedures NO Causes crashes Mark Deleted Stored Procedures NO Causes crashes Include DDL No Not needed Override Metadata No - <p>Ingestion Settings:</p> <ul> <li>Thread Count: <code>1</code> (default)</li> <li>Timeout: <code>300</code> seconds (default)</li> </ul> <p>Click Next.</p>"},{"location":"blog/10-metadata-governance/#step-4-configure-scheduling","title":"Step 4: Configure Scheduling","text":"<p>Schedule Type: Choose one:</p> <p>Option A: Manual (Recommended for Development)</p> <ul> <li>Select Manual</li> <li>Run ingestion on-demand when you need to refresh metadata</li> <li>Good for: Development, testing</li> </ul> <p>Option B: Scheduled (Recommended for Production)</p> <ul> <li>Select Scheduled</li> <li>Choose Cron Expression</li> <li>Enter: <code>0 3 * * *</code> (runs daily at 3 AM, after Dagster pipelines complete)</li> <li>Timezone: <code>UTC</code></li> </ul> <p>Click Next \u2192 Deploy.</p>"},{"location":"blog/10-metadata-governance/#step-5-run-initial-ingestion","title":"Step 5: Run Initial Ingestion","text":"<p>Via OpenMetadata UI:</p> <ol> <li>Go to Settings \u2192 Integrations \u2192 Databases</li> <li>Click on trino service</li> <li>Click Ingestions tab</li> <li>Find <code>trino-metadata</code> pipeline</li> <li>Click Run (play button)</li> <li>Monitor progress in real-time</li> </ol> <p>Expected output:</p> <pre><code>INFO - Starting metadata ingestion\nINFO - Connecting to Trino at trino:8080\nINFO - Processing catalog: iceberg\nINFO - Processing schema: raw\nINFO - Discovered 1 tables in schema raw\nINFO - Processing table: glucose_entries\nINFO - Successfully ingested table: trino.iceberg.raw.glucose_entries\nINFO - Processing schema: bronze\nINFO - Processing schema: silver\nINFO - Processing schema: gold\nINFO - Metadata ingestion completed\nINFO - Total tables ingested: 15\nINFO - Total schemas ingested: 4\nINFO - Total errors: 0\n</code></pre>"},{"location":"blog/10-metadata-governance/#step-6-enable-search-critical","title":"Step 6: Enable Search (CRITICAL)","text":"<p>After initial ingestion, search will NOT work until you populate the search index. This is a required step.</p> <p>Navigate to Search Settings:</p> <ol> <li>Go to Settings (gear icon) \u2192 OpenMetadata \u2192 Search</li> <li>Click on SearchIndexingApplication</li> <li>Click Run Now button</li> </ol> <p>Configure the Reindex Job:</p> <ol> <li>Enable \"Recreate Indexes\" toggle (IMPORTANT)</li> <li>Select \"All\" entities (or leave default)</li> <li>Click Submit</li> </ol> <p>Monitor Progress:</p> <ul> <li>The job will run for 1-2 minutes</li> <li>You'll see \"Success\" when complete</li> </ul> <p>What This Does:</p> <ul> <li>Creates the <code>all</code> search alias</li> <li>Populates search indices from metadata</li> <li>Enables Explore page and search functionality</li> </ul> <p>Without this step:</p> <ul> <li>Explore page will show error: \"Search failed due to Elasticsearch exception\"</li> <li>Global search will not work</li> <li>You can only navigate by direct URLs</li> </ul>"},{"location":"blog/10-metadata-governance/#using-the-data-catalog","title":"Using the Data Catalog","text":""},{"location":"blog/10-metadata-governance/#search-for-data","title":"Search for Data","text":"<ol> <li>Use the search bar at the top</li> <li>Search by:</li> <li>Table name: <code>glucose_daily_stats</code></li> <li>Column name: <code>mean_glucose</code></li> <li>Description keywords: <code>\"blood sugar\"</code></li> <li>Tags: <code>#glucose</code> (after adding tags)</li> </ol>"},{"location":"blog/10-metadata-governance/#view-table-details","title":"View Table Details","text":"<p>Click on any table to see:</p> <ul> <li>Schema: Column names, types, descriptions</li> <li>Sample Data: Preview of actual data</li> <li>Lineage: Visual graph showing upstream/downstream tables</li> <li>Queries: Recent SQL queries (if query log enabled)</li> <li>Usage: Access patterns and popularity</li> </ul>"},{"location":"blog/10-metadata-governance/#add-documentation","title":"Add Documentation","text":"<ol> <li>Click on a table (e.g., <code>silver.fct_glucose_readings</code>)</li> <li>Click Edit (pencil icon)</li> <li>Add description:</li> </ol> <pre><code>## Description\n\nFact table of glucose readings with calculated categories and metrics.\n\n## Update Schedule\n\nUpdated every 5 minutes via Dagster pipeline.\n\n## Business Logic\n\n- `glucose_category`: Categorized as hypoglycemia (&lt;70), in_range (70-180), or hyperglycemia (&gt;180)\n- `reading_timestamp`: UTC timestamp of the reading\n</code></pre> <ol> <li>Add column descriptions:</li> <li><code>reading_id</code>: Unique identifier for each glucose reading</li> <li><code>glucose_mg_dl</code>: Glucose value in mg/dL (validated range: 20-600)</li> <li><code>glucose_category</code>: Categorized glucose level</li> <li> <p><code>reading_timestamp</code>: When the reading was taken (UTC)</p> </li> <li> <p>Click Save</p> </li> </ol>"},{"location":"blog/10-metadata-governance/#add-tags","title":"Add Tags","text":"<ol> <li>Click on a table</li> <li>Click Add Tag</li> <li>Use built-in tags or create custom:</li> <li><code>PII.None</code> - No personal information</li> <li><code>Tier.Bronze</code> / <code>Tier.Silver</code> / <code>Tier.Gold</code></li> <li>Create custom: <code>Healthcare</code>, <code>CGM</code>, <code>Analytics</code></li> </ol>"},{"location":"blog/10-metadata-governance/#set-ownership","title":"Set Ownership","text":"<ol> <li>Click on a table</li> <li>Click Add Owner</li> <li>Select user or team (create teams in Settings)</li> </ol>"},{"location":"blog/10-metadata-governance/#data-lineage","title":"Data Lineage","text":"<p>OpenMetadata can show visual lineage graphs:</p> <pre><code>raw.glucose_entries\n    \u2193\nbronze.stg_glucose_entries (dbt model)\n    \u2193\nsilver.fct_glucose_readings (dbt model)\n    \u2193\ngold.dim_date (dbt model)\n    \u2193\nmarts.mrt_glucose_overview (Trino publish)\n</code></pre>"},{"location":"blog/10-metadata-governance/#enable-lineage-tracking-with-dbt","title":"Enable Lineage Tracking with dbt","text":"<p>Lineage is automatically extracted from:</p> <ul> <li>dbt models - Shows dependencies between models and tables</li> <li>SQL queries - Enable query log ingestion (advanced)</li> </ul> <p>Step 1: Add dbt Service</p> <ol> <li>Go to Settings \u2192 Services \u2192 Pipeline Services</li> <li>Click Add Service</li> <li>Select dbt</li> <li>Configure:</li> </ol> Field Value Name <code>phlo-dbt</code> dbt Cloud API URL Leave empty (we use local files) dbt Cloud Account ID Leave empty <p>Click Next.</p> <p>Step 2: Configure dbt Metadata Ingestion</p> <ol> <li>Source Configuration:</li> </ol> Field Value Notes dbt Configuration Source <code>Local</code> We're using local files, not dbt Cloud dbt Catalog File Path <code>/dbt/target/catalog.json</code> Contains column-level metadata dbt Manifest File Path <code>/dbt/target/manifest.json</code> Contains lineage and dependencies dbt Run Results File Path <code>/dbt/target/run_results.json</code> Optional: test results <ol> <li>Database Service Name: <code>trino</code></li> <li>This links dbt models to your Trino tables</li> <li> <p>Must match the name of your Trino service</p> </li> <li> <p>Include Tags: <code>Yes</code> (Enable)</p> </li> <li>Imports dbt model tags as OpenMetadata tags</li> </ol> <p>Click Next.</p> <p>Step 3: Schedule dbt Ingestion</p> <p>For Development:</p> <ul> <li>Select Manual</li> <li>Run after <code>dbt run</code> or <code>dbt build</code> completes</li> </ul> <p>For Production:</p> <ul> <li>Select Scheduled</li> <li>Cron: <code>0 4 * * *</code> (4 AM, after Dagster + Trino ingestion)</li> </ul> <p>Click Next \u2192 Deploy.</p> <p>Step 4: Run dbt Ingestion</p> <ol> <li>Ensure dbt artifacts are fresh:</li> </ol> <p><code>bash    cd workflows/transforms/dbt    dbt compile --profiles-dir ./profiles</code></p> <ol> <li>Go to Settings \u2192 Integrations \u2192 Pipeline \u2192 phlo-dbt</li> <li>Click Ingestions tab</li> <li>Find <code>phlo-dbt-metadata</code> pipeline</li> <li>Click Run (play button)</li> </ol> <p>Expected output:</p> <pre><code>INFO - Starting dbt metadata ingestion\nINFO - Reading manifest from /dbt/target/manifest.json\nINFO - Found 12 dbt models\nINFO - Processing model: fct_glucose_readings\nINFO - Linking model to table: trino.iceberg.silver.fct_glucose_readings\nINFO - Extracted lineage: bronze.stg_glucose_entries \u2192 silver.fct_glucose_readings\nINFO - Successfully ingested dbt metadata\n</code></pre>"},{"location":"blog/10-metadata-governance/#governance-workflows","title":"Governance Workflows","text":""},{"location":"blog/10-metadata-governance/#1-impact-analysis","title":"1. Impact Analysis","text":"<p>Question: \"Can I delete the <code>raw.glucose_entries</code> table?\"</p> <p>You can use the <code>phlo lineage impact</code> command (see Part 11) or check OpenMetadata:</p> <pre><code>raw.glucose_entries\n\u251c\u2500 Downstream: bronze.stg_glucose_entries\n\u2502  \u251c\u2500 Downstream: silver.fct_glucose_readings\n\u2502  \u2502  \u251c\u2500 Downstream: gold.mrt_glucose_overview\n\u2502  \u2502  \u2502  \u2514\u2500 Used by: Dashboard \"Glucose Monitoring\"\n\u2502  \u2502  \u2514\u2500 Used by: 3 dbt models\n\u2502  \u2514\u2500 Used by: 2 dbt models\n\u2502\n\u2514\u2500 Owner: data-platform-team\n\nAnswer: NO!\n  It impacts:\n  - 3 downstream datasets\n  - 1 dashboard\n  - Multiple dbt models\n</code></pre>"},{"location":"blog/10-metadata-governance/#2-search-and-discovery","title":"2. Search and Discovery","text":"<pre><code>OpenMetadata Search: \"glucose\"\n\nResults:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1. fct_glucose_readings (Dataset)\n   Silver layer \u2022 Iceberg \u2022 487K rows\n   \"Glucose readings fact table\"\n   Owner: data-platform-team\n\n2. stg_glucose_entries (Dataset)\n   Bronze layer \u2022 Iceberg \u2022 500K rows\n   \"Staged glucose entries\"\n\n3. mrt_glucose_overview (Dataset)\n   Gold layer \u2022 Postgres marts \u2022 5K rows\n   \"Marketing-ready glucose metrics\"\n</code></pre>"},{"location":"blog/10-metadata-governance/#3-data-access-governance","title":"3. Data Access Governance","text":"<p>Track who has access to what:</p> <ol> <li>Navigate to table</li> <li>View Activity Feeds</li> <li>See who accessed, queried, or modified</li> </ol>"},{"location":"blog/10-metadata-governance/#integration-with-phlo-workflows","title":"Integration with Phlo Workflows","text":""},{"location":"blog/10-metadata-governance/#update-ingestion-schedule","title":"Update Ingestion Schedule","text":"<p>Match OpenMetadata ingestion with your Dagster pipelines:</p> <pre><code>Dagster Pipeline: Daily at 2:00 AM\nOpenMetadata Ingestion: Daily at 3:00 AM (1 hour after data refresh)\n</code></pre>"},{"location":"blog/10-metadata-governance/#document-in-dbt-models","title":"Document in dbt Models","text":"<p>Add descriptions to dbt models that will appear in OpenMetadata:</p> <pre><code># workflows/transforms/dbt/models/silver/fct_glucose_readings.yml\nversion: 2\n\nmodels:\n  - name: fct_glucose_readings\n    description: |\n      Fact table of glucose readings with calculated categories.\n      Source: bronze.stg_glucose_entries\n      Refresh: Every 5 minutes\n    columns:\n      - name: reading_id\n        description: Unique identifier\n      - name: glucose_mg_dl\n        description: Glucose value in mg/dL (validated 20-600)\n        tests:\n          - not_null\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 20\n              max_value: 600\n      - name: glucose_category\n        description: Categorized as hypoglycemia, in_range, or hyperglycemia\n</code></pre>"},{"location":"blog/10-metadata-governance/#troubleshooting","title":"Troubleshooting","text":""},{"location":"blog/10-metadata-governance/#openmetadata-ui-not-loading","title":"OpenMetadata UI Not Loading","text":"<pre><code># Check service health\nmake health-catalog\n\n# Check logs\ndocker logs openmetadata-server\ndocker logs openmetadata-mysql\ndocker logs openmetadata-elasticsearch\n</code></pre>"},{"location":"blog/10-metadata-governance/#search-not-working","title":"Search Not Working","text":"<p>Symptom:</p> <ul> <li>Explore page shows: \"Search failed due to Elasticsearch exception\"</li> <li>Global search returns no results</li> </ul> <p>Solution:</p> <ol> <li>Go to Settings \u2192 OpenMetadata \u2192 Search</li> <li>Click on SearchIndexingApplication</li> <li>Click Run Now</li> <li>IMPORTANT: Enable \"Recreate Indexes\" toggle</li> <li>Click Submit</li> <li>Wait 1-2 minutes for completion</li> </ol>"},{"location":"blog/10-metadata-governance/#trino-connection-failed","title":"Trino Connection Failed","text":"<p>Ensure Trino is running:</p> <pre><code>make health\n\n# Start Trino if not running\nmake up-query\n</code></pre> <p>Check connection from OpenMetadata container:</p> <pre><code>docker exec -it openmetadata-server curl http://trino:8080/v1/info\n</code></pre>"},{"location":"blog/10-metadata-governance/#data-contracts-formalizing-data-agreements","title":"Data Contracts: Formalizing Data Agreements","text":"<p>As data platforms grow, informal agreements break down. The ML team assumes glucose readings update hourly. The analytics team expects certain columns to never be null. The reporting system depends on specific value ranges. When someone changes the schema or update frequency, things break.</p> <p>Data contracts formalize these agreements between data producers (your pipelines) and data consumers (dashboards, ML models, downstream teams).</p>"},{"location":"blog/10-metadata-governance/#the-problem-contracts-solve","title":"The Problem Contracts Solve","text":"<pre><code>Without contracts:\n\nMonday:    Engineer adds new column, removes old one\nTuesday:   ML model training fails silently\nWednesday: Dashboard shows \"No Data\"\nThursday:  Analyst reports: \"Numbers look wrong\"\nFriday:    Fire drill to understand what changed and why\n</code></pre> <p>With contracts, breaking changes are caught before deployment.</p>"},{"location":"blog/10-metadata-governance/#anatomy-of-a-data-contract","title":"Anatomy of a Data Contract","text":"<p>Contracts live in your <code>contracts/</code> directory as YAML files. Here's a real example from the glucose platform:</p> <pre><code># phlo-examples/nightscout/contracts/glucose_readings.yaml\nname: glucose_readings\nversion: 1.0.0\nowner: data-team\ndescription: \"Contract for glucose readings from Nightscout API\"\n\nschema:\n  required_columns:\n    - name: reading_id\n      type: string\n      description: \"Unique identifier for each glucose reading\"\n      constraints:\n        unique: true\n        nullable: false\n\n    - name: sgv\n      type: integer\n      description: \"Sensor glucose value in mg/dL\"\n      constraints:\n        min: 20\n        max: 600\n        nullable: false\n\n    - name: reading_timestamp\n      type: timestamp\n      description: \"When the reading was taken (UTC)\"\n      constraints:\n        nullable: false\n\nsla:\n  freshness_hours: 2 # Data must be &lt; 2 hours old\n  quality_threshold: 0.99 # 99% of rows must pass validation\n  availability_percentage: 99.9\n\nconsumers:\n  - name: analytics-team\n    usage: \"BI dashboards and ad-hoc analysis\"\n    contact: \"analytics@example.com\"\n\n  - name: ml-team\n    usage: \"Model training and feature engineering\"\n    contact: \"ml@example.com\"\n\nnotifications:\n  channels:\n    - type: slack\n      channel: \"#data-alerts\"\n  on_events:\n    - schema_change_proposed\n    - sla_breach\n    - quality_violation\n</code></pre>"},{"location":"blog/10-metadata-governance/#how-contract-validation-works","title":"How Contract Validation Works","text":"<p>When you run <code>phlo contract validate glucose_readings</code>, Phlo:</p> <ol> <li>Loads the contract from <code>contracts/glucose_readings.yaml</code> (or <code>phlo-examples/nightscout/contracts/glucose_readings.yaml</code> for examples)</li> <li>Validates the contract schema and structure</li> <li>Reports expected schema and SLA requirements</li> </ol> <p>Note: Full table schema comparison against live Iceberg tables is planned for a future release. Currently, the command validates contract syntax and displays expected requirements.</p> <pre><code>$ phlo contract validate glucose_readings\n\nContract Validation: glucose_readings\n\nNote: Requires live catalog access to validate actual schema\n\nRequired Columns:\n  reading_id   string\n  sgv          integer\n  reading_timestamp timestamp\n  direction    string\n  device       string\n</code></pre> <p>To check for contract violations against actual tables, you would use:</p> <pre><code>$ phlo contract show glucose_readings  # View full contract details\n$ phlo catalog describe raw.glucose_entries  # View actual table schema\n</code></pre>"},{"location":"blog/10-metadata-governance/#schema-evolution-and-breaking-changes","title":"Schema Evolution and Breaking Changes","text":"<p>The real power of contracts is preventing breaking changes. When you modify a schema, Phlo classifies changes:</p> Change Type Classification Action Add nullable column SAFE Auto-approve Add column with default SAFE Auto-approve Change column description WARNING Review required Add new constraint WARNING Review required Remove column BREAKING Block merge Change column type BREAKING Block merge Remove nullable BREAKING Block merge <p>In CI/CD, run <code>phlo contract check --pr</code> to validate changes before merge:</p> <pre><code>$ phlo contract check --pr\n\nChecking contracts against PR changes...\n\nglucose_readings:\n  BREAKING: Column 'device_type' removed\n\n  Impact:\n    - analytics-team: BI dashboards (contact: analytics@example.com)\n    - ml-team: Model training (contact: ml@example.com)\n\n  Action Required:\n    1. Notify consumers before removing column\n    2. Add deprecation period (recommended: 30 days)\n    3. Get explicit approval from consumers\n    4. Use --force to override (not recommended)\n\nContract check FAILED - 1 breaking change detected\n</code></pre>"},{"location":"blog/10-metadata-governance/#consumer-notifications","title":"Consumer Notifications","text":"<p>When contracts change, affected teams get notified automatically:</p> <pre><code>#data-alerts Slack Channel:\n\n\ud83d\udd14 Schema Change Proposed: glucose_readings\n\nChanges:\n  \u2022 Column 'legacy_id' marked for removal\n  \u2022 New column 'device_model' added (nullable)\n\nAffected Consumers:\n  \u2022 analytics-team (BI dashboards)\n  \u2022 ml-team (Model training)\n\nPR: https://github.com/org/repo/pull/123\nReview by: Friday 5pm\n\nReact with \u2705 to approve or \ud83d\udeab to block\n</code></pre>"},{"location":"blog/10-metadata-governance/#schema-management-via-cli","title":"Schema Management via CLI","text":"<p>Beyond contracts, Phlo provides tools for managing Pandera schemas themselves.</p>"},{"location":"blog/10-metadata-governance/#why-schema-management-matters","title":"Why Schema Management Matters","text":"<p>Pandera schemas define the expected structure of your data at each layer:</p> <pre><code># workflows/schemas/nightscout.py\nclass RawGlucoseEntries(pa.DataFrameModel):\n    \"\"\"Schema for raw glucose entries from Nightscout API.\"\"\"\n\n    _id: str = pa.Field(description=\"Nightscout entry ID\")\n    sgv: int = pa.Field(ge=20, le=600, description=\"Glucose in mg/dL\")\n    dateString: str = pa.Field(description=\"ISO timestamp string\")\n    direction: str = pa.Field(nullable=True)\n</code></pre> <p>As your platform grows, you'll have dozens of schemas. The CLI helps you manage them.</p>"},{"location":"blog/10-metadata-governance/#discovering-schemas","title":"Discovering Schemas","text":"<pre><code>$ phlo schema list\n\nAvailable Schemas\nName                     Fields  Module\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nFactDailyMetrics           15   workflows.schemas.nightscout\nFactGlucoseReadings        12   workflows.schemas.nightscout\nMartGlucoseOverview         6   workflows.schemas.nightscout\nRawGlucoseEntries           8   workflows.schemas.nightscout\nRawWeatherObservations     10   workflows.schemas.weather\n</code></pre>"},{"location":"blog/10-metadata-governance/#inspecting-schema-details","title":"Inspecting Schema Details","text":"<pre><code>$ phlo schema show RawGlucoseEntries\n\nRawGlucoseEntries\nModule: workflows.schemas.nightscout\nFields: 8\n\nFields\nName         Type     Required  Description\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n_id          str      \u2713\nsgv          int      \u2713\ndateString   str      \u2713\ndirection    str\ndevice       str\ntype         str\n</code></pre> <p>You can also view the Iceberg schema equivalent:</p> <pre><code>$ phlo schema show RawGlucoseEntries --iceberg\n</code></pre>"},{"location":"blog/10-metadata-governance/#comparing-schema-versions","title":"Comparing Schema Versions","text":"<p>When schemas change, use <code>diff</code> to understand what's different:</p> <pre><code>$ phlo schema diff RawGlucoseEntries --old HEAD~5\n\nSchema Diff: RawGlucoseEntries\n\nAdded:\n  + transmitter_id: str (optional) - \"CGM transmitter serial\"\n\nModified:\n  ~ sgv: constraint changed\n    - was: ge=0, le=500\n    + now: ge=20, le=600\n\nRemoved:\n  - legacy_timestamp: str\n\nClassification: WARNING (1 safe, 1 warning, 0 breaking)\n</code></pre>"},{"location":"blog/10-metadata-governance/#browsing-your-catalog-via-cli","title":"Browsing Your Catalog via CLI","text":"<p>While OpenMetadata provides a powerful UI, you can also explore your catalog from the command line.</p>"},{"location":"blog/10-metadata-governance/#listing-tables","title":"Listing Tables","text":"<p>View all Iceberg tables in your catalog:</p> <pre><code>$ phlo catalog tables\n\nIceberg Tables (ref: main)\nNamespace  Table Name              Full Name\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nraw        glucose_entries         raw.glucose_entries\nbronze     stg_glucose_entries     bronze.stg_glucose_entries\nsilver     fct_glucose_readings    silver.fct_glucose_readings\ngold       fct_daily_glucose_metrics  gold.fct_daily_glucose_metrics\n\nTotal: 4 tables\n</code></pre> <p>Filter by namespace:</p> <pre><code>$ phlo catalog tables --namespace silver\n</code></pre>"},{"location":"blog/10-metadata-governance/#describing-table-metadata","title":"Describing Table Metadata","text":"<p>View detailed schema information:</p> <pre><code>$ phlo catalog describe raw.glucose_entries\n\nTable: raw.glucose_entries\nLocation: s3://lake/warehouse/raw/glucose_entries\nCurrent Snapshot ID: 1234567890\nFormat Version: 2\n\nSchema:\nColumn Name        Type      Required\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n_id                string    \u2713\nsgv                int       \u2713\ndateString         string    \u2713\ndirection          string\ndevice             string\n</code></pre>"},{"location":"blog/10-metadata-governance/#viewing-table-history","title":"Viewing Table History","text":"<p>Check snapshot history to understand table evolution:</p> <pre><code>$ phlo catalog history raw.glucose_entries\n\nSnapshot History: raw.glucose_entries\nSnapshot ID   Timestamp            Operation  Current\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nabc12345...   2025-11-27 10:35:00  append     \u25cf\ndef67890...   2025-11-27 09:30:00  append\nghi12345...   2025-11-27 08:25:00  append\n\nShowing 3 most recent snapshots\n</code></pre> <p>Future Feature: Automated metadata sync to OpenMetadata (<code>phlo catalog sync</code>) is planned for a future release. For now, use OpenMetadata's built-in ingestion pipelines as described in the setup guide.</p>"},{"location":"blog/10-metadata-governance/#exposing-data-via-apis","title":"Exposing Data via APIs","text":"<p>Your data platform isn't just for analysts running SQL. Applications, mobile apps, and external partners often need API access to your curated datasets.</p>"},{"location":"blog/10-metadata-governance/#the-api-layer-problem","title":"The API Layer Problem","text":"<pre><code>Traditional approach:\n\n1. Data team builds mart table\n2. Backend team builds REST endpoint\n3. They disagree on schema\n4. Manual sync between dbt model and API\n5. Schema changes break integrations\n6. Nobody knows what APIs exist\n</code></pre> <p>Phlo automates this with PostgREST (REST) and Hasura (GraphQL).</p> <p>Implementation Details: For comprehensive API setup guides, see:</p> <ul> <li>docs/setup/postgrest.md - PostgREST configuration</li> <li>docs/setup/hasura.md - Hasura GraphQL setup</li> </ul>"},{"location":"blog/10-metadata-governance/#auto-generating-rest-apis-with-postgrest","title":"Auto-Generating REST APIs with PostgREST","text":"<p>PostgREST turns PostgreSQL tables into REST endpoints automatically. The challenge is keeping API views in sync with your dbt models.</p> <p>The manual way:</p> <pre><code>-- Write this by hand for every model\nCREATE VIEW api.glucose_readings AS\nSELECT reading_id, timestamp, sgv, direction\nFROM marts_postgres.mrt_glucose_readings;\n\nGRANT SELECT ON api.glucose_readings TO analyst;\n</code></pre> <p>The automated way:</p> <pre><code>$ phlo postgrest generate-views\n\nGenerating API views from dbt models...\n\nSource: dbt manifest (12 models in marts_postgres)\n\nViews to generate:\n  api.glucose_readings     &lt;- mrt_glucose_readings (tag: api)\n  api.daily_metrics       &lt;- mrt_daily_glucose_metrics (tag: api)\n  api.user_summary        &lt;- mrt_user_summary (tag: api, analyst)\n\nPermissions:\n  api.glucose_readings: analyst, admin\n  api.daily_metrics: analyst, admin\n  api.user_summary: admin (restricted)\n\nGenerated SQL saved to: api_views.sql\n\nApply with: phlo postgrest generate-views --apply\n</code></pre>"},{"location":"blog/10-metadata-governance/#how-view-generation-works","title":"How View Generation Works","text":"<ol> <li>Phlo reads your dbt <code>manifest.json</code></li> <li>Filters models tagged with <code>api</code> (configurable)</li> <li>Generates <code>CREATE VIEW</code> statements</li> <li>Maps dbt tags to PostgreSQL roles</li> <li>Optionally applies Row-Level Security</li> </ol> <p>In your dbt model:</p> <pre><code># models/marts_postgres/mrt_glucose_readings.yml\nmodels:\n  - name: mrt_glucose_readings\n    description: \"Curated glucose readings for API access\"\n    config:\n      tags: [\"api\", \"analyst\"] # Exposed to API, readable by analyst role\n    columns:\n      - name: reading_id\n        description: \"Unique reading identifier\"\n      - name: sgv\n        description: \"Glucose value in mg/dL\"\n</code></pre> <p>Generated SQL:</p> <pre><code>-- Auto-generated by: phlo postgrest generate-views\n-- Source: mrt_glucose_readings\n-- Tags: api, analyst\n\nCREATE OR REPLACE VIEW api.glucose_readings AS\nSELECT\n    reading_id,\n    timestamp,\n    sgv,\n    direction,\n    device\nFROM marts_postgres.mrt_glucose_readings;\n\nCOMMENT ON VIEW api.glucose_readings IS 'Curated glucose readings for API access';\n\n-- Permissions from tags\nGRANT SELECT ON api.glucose_readings TO analyst;\nGRANT SELECT ON api.glucose_readings TO admin;\n</code></pre>"},{"location":"blog/10-metadata-governance/#graphql-with-hasura","title":"GraphQL with Hasura","text":"<p>For richer query capabilities, Phlo integrates with Hasura:</p> <pre><code># Auto-track new tables in Hasura\n$ phlo hasura track\n\n\u2713 Tracked 3/3 tables\n</code></pre> <p>You can also set up relationships and permissions:</p> <pre><code># Auto-create relationships from foreign keys\n$ phlo hasura relationships\n\n\u2713 Created 1/1 relationships\n\n# Set up default permissions\n$ phlo hasura permissions\n\n\u2713 Created 6/6 permissions\n</code></pre> <p>Or do all three at once:</p> <pre><code>$ phlo hasura auto-setup\n\nAuto-tracking tables, setting up relationships and permissions...\n\u2713 Complete\n</code></pre> <p>Now you get GraphQL automatically:</p> <pre><code>query {\n  glucose_readings(\n    where: { sgv: { _gt: 180 } }\n    order_by: { timestamp: desc }\n    limit: 10\n  ) {\n    reading_id\n    timestamp\n    sgv\n    direction\n  }\n}\n</code></pre>"},{"location":"blog/10-metadata-governance/#permission-management","title":"Permission Management","text":"<p>Define permissions in YAML, sync to Hasura:</p> <pre><code># hasura-permissions.yaml\ntables:\n  api.glucose_readings:\n    select:\n      anon: false # No public access\n      analyst:\n        columns: [reading_id, timestamp, sgv, direction]\n        filter: {} # All rows\n      admin:\n        columns: \"*\" # All columns\n        filter: {}\n\n  api.user_summary:\n    select:\n      analyst: false # Restricted\n      admin:\n        columns: \"*\"\n        filter: {}\n</code></pre> <p>Apply permissions from a config file:</p> <pre><code>$ phlo hasura sync-permissions --config hasura-permissions.yaml\n\n\u2713 Permissions synced\n</code></pre>"},{"location":"blog/10-metadata-governance/#when-to-use-rest-vs-graphql","title":"When to Use REST vs GraphQL","text":"Use Case Recommendation Simple CRUD operations REST (PostgREST) Mobile apps with varied queries GraphQL (Hasura) External partner integrations REST (simpler) Internal dashboards GraphQL (flexible) High-volume batch reads Direct SQL"},{"location":"blog/10-metadata-governance/#api-layer-in-the-architecture","title":"API Layer in the Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Consumers                               \u2502\n\u2502  Mobile App    Web App    Partner API    Internal Tools     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                                   \u2502\n         \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   PostgREST     \u2502               \u2502     Hasura      \u2502\n\u2502   (REST API)    \u2502               \u2502   (GraphQL)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   PostgreSQL    \u2502\n                \u2502   api schema    \u2502\n                \u2502   (views)       \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502  marts_postgres \u2502\n                \u2502  (dbt models)   \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/10-metadata-governance/#best-practices","title":"Best Practices","text":"<ol> <li>Document Everything: Add descriptions to all tables and columns</li> <li>Use Tags: Create a consistent tagging strategy (layers, domains, sensitivity)</li> <li>Set Ownership: Assign owners to all datasets</li> <li>Regular Updates: Run <code>phlo catalog sync</code> daily to keep metadata fresh</li> <li>Quality Checks: Link data quality tests to tables</li> <li>Glossary: Maintain business terms for domain-specific language</li> <li>Data Contracts: Define contracts for critical datasets with clear SLAs</li> </ol>"},{"location":"blog/10-metadata-governance/#benefits-of-metadata-management","title":"Benefits of Metadata Management","text":"<p>Self-Service: New analysts discover datasets without asking Compliance: Track who accessed what, when, and why Impact Analysis: Understand dependencies before changes Accountability: Clear ownership and change history Quality: Quality checks visible and tracked Documentation: Single source of truth for data definitions</p>"},{"location":"blog/10-metadata-governance/#summary","title":"Summary","text":"<p>OpenMetadata provides:</p> <ol> <li>Catalog: Discover all datasets with descriptions</li> <li>Lineage: Understand data flow end-to-end</li> <li>Governance: Track ownership and access</li> <li>Quality: Link validation checks to datasets</li> <li>History: Change tracking and audit trail</li> <li>Impact: See what breaks when data changes</li> </ol> <p>Integrated with dbt, Dagster, and Iceberg, OpenMetadata becomes your data OS.</p> <p>Next: Part 11: Observability and Monitoring</p> <p>See you there!</p>"},{"location":"blog/11-observability-monitoring/","title":"Part 11: Observability and Monitoring\u2014Knowing Your Pipeline","text":"<p>You've built a data lakehouse with validation and governance. But what happens at 3am when something breaks? This post covers observability: monitoring, alerting, and troubleshooting.</p>"},{"location":"blog/11-observability-monitoring/#the-observability-problem","title":"The Observability Problem","text":"<p>Without proper monitoring, failures hide:</p> <pre><code>Tuesday 3am:\n  \u2022 DLT fails to fetch from Nightscout API\n  \u2022 Data stops flowing\n  \u2022 Nobody notices\n\nWednesday 9am:\n  \u2022 Users report: \"Dashboard shows stale data\"\n  \u2022 Investigation: \"Last update was 30 hours ago\"\n  \u2022 Impact: 500+ people using outdated metrics\n  \u2022 Root cause: API timeout at 3:14am, log buried in Dagster logs\n</code></pre> <p>Observability solves this with:</p> <pre><code>[Monitoring] \u2192 Understand what's happening\n[Alerting]   \u2192 Get notified of problems\n[Tracing]    \u2192 Find root causes quickly\n[Dashboards] \u2192 Visualize pipeline health\n</code></pre>"},{"location":"blog/11-observability-monitoring/#three-pillars-of-observability","title":"Three Pillars of Observability","text":""},{"location":"blog/11-observability-monitoring/#1-metrics-numbers","title":"1. Metrics (Numbers)","text":"<p>Track quantitative data:</p> <pre><code>\u2022 Pipeline runtime: 5.2 seconds\n\u2022 Data quality: 99.7% valid rows\n\u2022 Row throughput: 12,500 rows/minute\n\u2022 Freshness: 2 hours since last update\n\u2022 API latency: 150ms average\n</code></pre>"},{"location":"blog/11-observability-monitoring/#2-logs-events","title":"2. Logs (Events)","text":"<p>Track what happened and when:</p> <pre><code>[2024-10-15 10:35:42] \u2713 dlt_glucose_entries started\n[2024-10-15 10:35:45] \u2713 Fetched 487 rows from API\n[2024-10-15 10:35:47] \u2713 Pandera validation: 487/487 rows valid\n[2024-10-15 10:35:49] \u26a0 2 rows with invalid device type (logged)\n[2024-10-15 10:35:51] \u2713 Merged to Iceberg (487 inserts, 2 updates)\n[2024-10-15 10:35:53] \u2713 dlt_glucose_entries succeeded\n</code></pre>"},{"location":"blog/11-observability-monitoring/#3-traces-flows","title":"3. Traces (Flows)","text":"<p>Track execution paths:</p> <pre><code>Request: dlt_glucose_entries asset materialization\n\u251c\u2500 Fetch from API (45ms)\n\u2502  \u251c\u2500 Auth (5ms)\n\u2502  \u2514\u2500 Network (40ms)\n\u251c\u2500 Pandera validation (12ms)\n\u2502  \u251c\u2500 Type checking (8ms)\n\u2502  \u2514\u2500 Constraint checking (4ms)\n\u251c\u2500 Merge to Iceberg (80ms)\n\u2502  \u251c\u2500 Read current snapshot (20ms)\n\u2502  \u251c\u2500 Merge operation (40ms)\n\u2502  \u2514\u2500 Write metadata (20ms)\n\u2514\u2500 Asset check (15ms)\n\nTotal: 152ms\n</code></pre>"},{"location":"blog/11-observability-monitoring/#phlos-observability-stack","title":"Phlo's Observability Stack","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application Layer           \u2502\n\u2502  (Dagster, dbt, DLT)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193 (emits events)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phlo CLI &amp; Monitoring       \u2502\n\u2502  \u2022 phlo logs                 \u2502\n\u2502  \u2022 phlo metrics              \u2502\n\u2502  \u2022 phlo lineage              \u2502\n\u2502  \u2022 phlo alerts               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Storage &amp; Collection        \u2502\n\u2502  \u2022 Dagster Event Log         \u2502\n\u2502  \u2022 Metrics Collector         \u2502\n\u2502  \u2022 Lineage Graph             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Alerting &amp; Notifications    \u2502\n\u2502  \u2022 Slack Webhooks            \u2502\n\u2502  \u2022 PagerDuty                 \u2502\n\u2502  \u2022 Email                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/11-observability-monitoring/#metrics-tracking-pipeline-health","title":"Metrics: Tracking Pipeline Health","text":"<p>Phlo provides built-in metrics collection through the <code>phlo metrics</code> CLI command and the metrics collector module.</p>"},{"location":"blog/11-observability-monitoring/#viewing-metrics-via-cli","title":"Viewing Metrics via CLI","text":"<p>The easiest way to check pipeline health:</p> <pre><code># View overall pipeline metrics\n$ phlo metrics summary\n\nPlatform Metrics Summary\n\nRuns (last 24h)\n  Total:     288\n  Success:   285 (98.96%)\n  Failure:   3 (1.04%)\n\nData Volume\n  Rows:      1.2M\n  Bytes:     450 MB\n\nLatency (seconds)\n  p50:       0.32s\n  p95:       0.85s\n  p99:       1.20s\n\nAssets\n  Active:    12\n  Success:   11\n  Warning:   0\n  Failure:   1\n</code></pre> <p>For per-asset details:</p> <pre><code># View metrics for specific asset\n$ phlo metrics asset glucose_entries --runs 20\n\nMetrics for glucose_entries\nMetric              Value\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nLast Run Status     success\nLast Run Duration   5.20s\nAverage Duration    5.10s\nFailure Rate        5.0%\nAvg Rows/Run        498\nData Size           2.5 MB\n\nLast 20 Runs\nRun ID    Status   Duration  Rows\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nabc123    success  5.2s      487\ndef456    success  4.8s      512\nghi789    success  5.1s      495\n</code></pre> <p>Export metrics for external analysis:</p> <pre><code># Export to JSON\n$ phlo metrics export --format json --period 7d --output metrics.json\n\n# Export to CSV\n$ phlo metrics export --format csv --period 30d --output metrics.csv\n</code></pre>"},{"location":"blog/11-observability-monitoring/#logs-structured-logging","title":"Logs: Structured Logging","text":"<p>Use structured logs for easy searching:</p> <pre><code># workflows/ingestion/dlt_assets.py\nimport structlog\n\nlogger = structlog.get_logger()\n\n\n@asset\ndef dlt_glucose_entries(context) -&gt; None:\n    \"\"\"Ingest glucose entries with structured logging.\"\"\"\n\n    logger.info(\n        \"asset_started\",\n        asset_name=\"dlt_glucose_entries\",\n        timestamp=datetime.utcnow().isoformat(),\n    )\n\n    try:\n        # Fetch API\n        logger.info(\n            \"api_fetch_started\",\n            endpoint=\"https://api.nightscout.info/api/v1/entries\",\n            timeout_seconds=30,\n        )\n\n        response = fetch_from_api()\n\n        logger.info(\n            \"api_fetch_success\",\n            rows_returned=len(response),\n            response_time_ms=response.elapsed.total_seconds() * 1000,\n        )\n\n        # Validate\n        logger.info(\n            \"validation_started\",\n            validator=\"pandera\",\n            schema=\"glucose_entries_v1\",\n        )\n\n        validated = validate(response)\n        invalid_count = len(response) - len(validated)\n\n        logger.info(\n            \"validation_complete\",\n            total_rows=len(response),\n            valid_rows=len(validated),\n            invalid_rows=invalid_count,\n            pass_rate=100.0 * len(validated) / len(response),\n        )\n\n        # Merge\n        logger.info(\n            \"merge_started\",\n            table=\"raw.glucose_entries\",\n            rows_to_merge=len(validated),\n            unique_key=\"_id\",\n        )\n\n        result = merge_to_iceberg(validated)\n\n        logger.info(\n            \"merge_complete\",\n            table=\"raw.glucose_entries\",\n            inserts=result[\"inserts\"],\n            updates=result[\"updates\"],\n            merge_time_ms=result[\"duration_ms\"],\n        )\n\n        # Success\n        logger.info(\n            \"asset_succeeded\",\n            asset_name=\"dlt_glucose_entries\",\n            total_time_ms=get_total_elapsed(),\n        )\n\n    except Exception as e:\n        logger.exception(\n            \"asset_failed\",\n            asset_name=\"dlt_glucose_entries\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n        )\n        raise\n</code></pre> <p>In Grafana, search logs:</p> <pre><code>{job=\"dagster\"} | json | asset_name=\"dlt_glucose_entries\" | status=\"success\"\n\nLast 24 hours:\n\u251c\u2500 10/15 10:35 \u2713 Succeeded in 152ms\n\u251c\u2500 10/15 10:30 \u2713 Succeeded in 145ms\n\u251c\u2500 10/15 10:25 \u2713 Succeeded in 168ms\n\u251c\u2500 10/15 10:20 \u26a0 Succeeded in 1,240ms (slow)\n\u2514\u2500 10/15 10:15 \u2713 Succeeded in 156ms\n</code></pre>"},{"location":"blog/11-observability-monitoring/#alerting-getting-notified-when-things-break","title":"Alerting: Getting Notified When Things Break","text":"<p>Phlo includes a built-in alerting system that can send notifications to Slack, PagerDuty, and email.</p>"},{"location":"blog/11-observability-monitoring/#configuring-alert-destinations","title":"Configuring Alert Destinations","text":"<p>Set up alert destinations via environment variables:</p> <pre><code># Slack Integration\nexport PHLO_ALERT_SLACK_WEBHOOK=\"https://hooks.slack.com/services/T.../B.../xxx\"\nexport PHLO_ALERT_SLACK_CHANNEL=\"#data-alerts\"  # Optional\n\n# PagerDuty Integration\nexport PHLO_ALERT_PAGERDUTY_KEY=\"your-integration-key\"\n\n# Email Integration\nexport PHLO_ALERT_EMAIL_SMTP_HOST=\"smtp.gmail.com\"\nexport PHLO_ALERT_EMAIL_SMTP_PORT=\"587\"\nexport PHLO_ALERT_EMAIL_SMTP_USER=\"alerts@yourcompany.com\"\nexport PHLO_ALERT_EMAIL_SMTP_PASSWORD=\"your-password\"\nexport PHLO_ALERT_EMAIL_RECIPIENTS=\"data-team@yourcompany.com,oncall@yourcompany.com\"\n</code></pre>"},{"location":"blog/11-observability-monitoring/#managing-alerts-via-cli","title":"Managing Alerts via CLI","text":"<p>Check alert system status:</p> <pre><code>$ phlo alerts status\n\nAlert System Status\n\nConfigured Destinations: 2\n  \u2022 slack\n  \u2022 email\n\nRecent Alerts Sent: 5\nDeduplication Window: 60 minutes\n</code></pre> <p>List configured destinations:</p> <pre><code>$ phlo alerts list\n\nConfigured Alert Destinations\nName   Type              Status\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nslack  SlackDestination  \u2713 Ready\nemail  EmailDestination  \u2713 Ready\n</code></pre> <p>Test your alert configuration:</p> <pre><code># Send test alert to all destinations\n$ phlo alerts test\n\n\u2713 Test alert sent successfully! Check your configured alert destinations.\n\n# Test specific destination\n$ phlo alerts test --destination slack --severity critical\n</code></pre>"},{"location":"blog/11-observability-monitoring/#what-triggers-alerts","title":"What Triggers Alerts","text":"<p>Phlo automatically sends alerts for:</p> <ol> <li>Asset Failures - When materialization fails</li> <li>Quality Check Violations - When data quality checks fail</li> <li>SLA Breaches - When freshness or quality thresholds are exceeded</li> </ol>"},{"location":"blog/11-observability-monitoring/#alert-severity-levels","title":"Alert Severity Levels","text":"Severity Description Default Routing INFO FYI notifications Slack only WARNING Needs attention Slack + Email ERROR Something failed Slack + Email + PagerDuty (low urgency) CRITICAL Production impact All channels + PagerDuty (high urgency)"},{"location":"blog/11-observability-monitoring/#dashboards-visualizing-health","title":"Dashboards: Visualizing Health","text":""},{"location":"blog/11-observability-monitoring/#main-operations-dashboard","title":"Main Operations Dashboard","text":"<pre><code>Phlo Data Pipeline - Operations Dashboard\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Pipeline Status \u2502  Data Freshness \u2502  Quality Score  \u2502\n\u2502      \u2713 HEALTHY  \u2502      2.3 hours  \u2502    99.74%       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAsset Execution Times (last 24 hours)\n\u251c\u2500 dlt_glucose_entries:    150ms avg \u2713\n\u251c\u2500 stg_glucose_entries:    45ms avg \u2713\n\u251c\u2500 fct_glucose_readings:   320ms avg \u2713\n\u251c\u2500 mrt_glucose_readings:   85ms avg \u2713\n\u2514\u2500 publish_to_postgres:   1,240ms avg \u26a0\n\nData Quality Checks (pass rate)\n\u251c\u2500 glucose_range_check:         100% \u2713\n\u251c\u2500 glucose_freshness_check:     100% \u2713\n\u251c\u2500 no_duplicates:               100% \u2713\n\u251c\u2500 statistical_bounds_check:    99.9% \u2713\n\u2514\u2500 validation_pass_rate:        99.74% \u2713\n\nActive Alerts\n\u251c\u2500 \u26a0 publish_to_postgres running slow (1240ms vs 500ms avg)\n\u2514\u2500 \u2139 API latency slightly elevated (180ms vs 150ms avg)\n\nResource Utilization\n\u251c\u2500 Disk: 45% used (180 GB / 400 GB)\n\u251c\u2500 Memory: 62% used (10 GB / 16 GB)\n\u2514\u2500 MinIO lake bucket: 280 GB\n</code></pre>"},{"location":"blog/11-observability-monitoring/#asset-health-dashboard","title":"Asset Health Dashboard","text":"<pre><code>Asset: fct_glucose_readings\n\nStatus:        \u2713 HEALTHY\nLast Update:   2024-10-15 10:35:42 UTC (2.3 hours ago)\nOwner:         data-platform-team\nLayer:         Gold (Marts)\nRow Count:     487,239\n\nExecution Metrics (24 hours)\n\u251c\u2500 Total runs: 288\n\u251c\u2500 Success: 285 (98.96%)\n\u251c\u2500 Failures: 3 (1.04%)\n\u251c\u2500 Avg time: 320ms\n\u251c\u2500 P95 time: 580ms\n\u251c\u2500 P99 time: 950ms\n\nQuality Checks\n\u251c\u2500 glucose_range_check:         \u2713 487,239/487,239 valid\n\u251c\u2500 glucose_freshness_check:     \u2713 Latest: 2.3h ago\n\u251c\u2500 no_duplicates:               \u2713 0 duplicates\n\u2514\u2500 statistical_bounds_check:    \u26a0 2 outliers detected\n\nData Distribution\n\u251c\u2500 Mean: 150 mg/dL\n\u251c\u2500 Std Dev: 45 mg/dL\n\u251c\u2500 Min: 22 mg/dL\n\u251c\u2500 Max: 598 mg/dL\n\u2514\u2500 Nulls: 0 (0%)\n\nDownstream Usage\n\u251c\u2500 mrt_glucose_readings (Gold) \u2192 100K reads/day\n\u251c\u2500 Superset Dashboard (Glucose Monitoring) \u2192 450 views/day\n\u2514\u2500 Alert: Low Glucose Detection \u2192 12 alerts/day avg\n</code></pre>"},{"location":"blog/11-observability-monitoring/#tracing-deep-debugging","title":"Tracing: Deep Debugging","text":"<p>Use distributed tracing to understand slow operations:</p> <pre><code># phlo/monitoring/tracing.py\nfrom jaeger_client import Config\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom contextlib import contextmanager\n\n\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\n\ntracer = trace.get_tracer(__name__)\n\n\n@contextmanager\ndef trace_operation(operation_name: str, attributes: dict = None):\n    \"\"\"Context manager for tracing operations.\"\"\"\n    with tracer.start_as_current_span(operation_name) as span:\n        if attributes:\n            for key, value in attributes.items():\n                span.set_attribute(key, value)\n        yield span\n\n\n# Usage in code\n@asset\ndef dlt_glucose_entries(context):\n    \"\"\"Ingest with tracing.\"\"\"\n\n    with trace_operation(\"dlt_glucose_entries\") as span:\n        # Fetch\n        with trace_operation(\"fetch_from_api\") as fetch_span:\n            fetch_span.set_attribute(\"endpoint\", \"nightscout_api\")\n            data = fetch_api()\n            fetch_span.set_attribute(\"rows_fetched\", len(data))\n\n        # Validate\n        with trace_operation(\"pandera_validation\") as val_span:\n            val_span.set_attribute(\"schema\", \"glucose_entries_v1\")\n            valid = validate(data)\n            val_span.set_attribute(\"rows_valid\", len(valid))\n\n        # Merge\n        with trace_operation(\"iceberg_merge\") as merge_span:\n            merge_span.set_attribute(\"table\", \"raw.glucose_entries\")\n            result = merge_to_iceberg(valid)\n            merge_span.set_attribute(\"inserts\", result[\"inserts\"])\n            merge_span.set_attribute(\"updates\", result[\"updates\"])\n</code></pre> <p>In Jaeger UI, you see:</p> <pre><code>Trace: dlt_glucose_entries\nDuration: 152ms\n\n\u251c\u2500 dlt_glucose_entries [0ms - 152ms] (main)\n\u2502  \u251c\u2500 fetch_from_api [0ms - 45ms]\n\u2502  \u2502  \u2514\u2500 http.request GET /api/v1/entries [5ms - 40ms]\n\u2502  \u251c\u2500 pandera_validation [50ms - 62ms]\n\u2502  \u2502  \u251c\u2500 type_checking [50ms - 55ms]\n\u2502  \u2502  \u2514\u2500 constraint_checking [55ms - 60ms]\n\u2502  \u2514\u2500 iceberg_merge [65ms - 152ms]\n\u2502     \u251c\u2500 read_snapshot [65ms - 85ms]\n\u2502     \u251c\u2500 merge_operation [85ms - 125ms]\n\u2502     \u2514\u2500 write_metadata [125ms - 152ms]\n</code></pre> <p>Click on any span to see:</p> <ul> <li>Start time and duration</li> <li>Attributes (table name, row count, etc.)</li> <li>Logs within that span</li> <li>Errors or exceptions</li> </ul>"},{"location":"blog/11-observability-monitoring/#monitoring-as-code","title":"Monitoring as Code","text":"<pre><code># phlo/monitoring/observability_assets.py\nfrom dagster import asset, schedule\n\n\n@asset(group_name=\"monitoring\")\ndef freshness_dashboard(context):\n    \"\"\"Generate freshness dashboard.\"\"\"\n    queries = {\n        \"dlt_glucose_entries\": \"\"\"\n            (time() - asset_last_update_timestamp{'asset'='dlt_glucose_entries'}) / 3600\n        \"\"\",\n        \"fct_glucose_readings\": \"\"\"\n            (time() - asset_last_update_timestamp{'asset'='fct_glucose_readings'}) / 3600\n        \"\"\",\n    }\n\n    dashboard = create_grafana_dashboard(\n        name=\"Data Freshness\",\n        panels=[\n            create_gauge_panel(\n                title=f\"{asset} Age (hours)\",\n                query=query,\n                thresholds={\"warning\": 2, \"critical\": 4},\n            )\n            for asset, query in queries.items()\n        ],\n    )\n\n    context.log.info(f\"\u2713 Created freshness dashboard: {dashboard.url}\")\n\n\n@asset(group_name=\"monitoring\")\ndef sla_tracker(context):\n    \"\"\"Track SLA compliance.\"\"\"\n    slas = {\n        \"dlt_glucose_entries\": {\n            \"freshness\": \"2 hours\",\n            \"availability\": \"99.9%\",\n        },\n        \"fct_glucose_readings\": {\n            \"freshness\": \"1 hour\",\n            \"availability\": \"99.95%\",\n        },\n    }\n\n    for asset_name, sla in slas.items():\n        record_sla_metric(asset_name, sla)\n        context.log.info(f\"\u2713 Updated SLA for {asset_name}\")\n\n\n@schedule(\n    name=\"observability_updates\",\n    cron_schedule=\"* * * * *\",  # Every minute\n)\ndef update_observability():\n    \"\"\"Update monitoring dashboards and alerts.\"\"\"\n    return {}\n</code></pre>"},{"location":"blog/11-observability-monitoring/#lineage-visualization-understanding-your-data-flow","title":"Lineage Visualization: Understanding Your Data Flow","text":"<p>One of the hardest questions in data engineering: \"If I change this table, what breaks?\"</p> <p>Without lineage, you're guessing. With lineage, you know exactly what depends on what.</p>"},{"location":"blog/11-observability-monitoring/#why-lineage-matters","title":"Why Lineage Matters","text":"<pre><code>Scenario: You need to rename a column in glucose_entries\n\nWithout lineage:\n  1. Grep through code looking for references\n  2. Miss the dbt model that uses a different alias\n  3. Deploy change\n  4. Dashboard breaks in production\n  5. 2am phone call\n\nWith lineage:\n  1. Run: phlo lineage impact glucose_entries\n  2. See all 6 downstream dependencies\n  3. Update them first\n  4. Deploy safely\n</code></pre>"},{"location":"blog/11-observability-monitoring/#viewing-lineage-from-cli","title":"Viewing Lineage from CLI","text":"<p>The <code>phlo lineage</code> commands let you explore dependencies without opening a browser:</p> <pre><code>$ phlo lineage show glucose_entries\n\nglucose_entries\n\u251c\u2500\u2500 [upstream]\n\u2502   \u2514\u2500\u2500 (external) Nightscout API\n\u2502       \u2514\u2500\u2500 Fetched via DLT rest_api\n\u2514\u2500\u2500 [downstream]\n    \u2514\u2500\u2500 stg_glucose_entries (dbt model)\n        \u2514\u2500\u2500 fct_glucose_readings (dbt model)\n            \u251c\u2500\u2500 mrt_glucose_readings (dbt model)\n            \u2502   \u2514\u2500\u2500 publish_glucose_marts (Dagster asset)\n            \u2502       \u2514\u2500\u2500 (external) Superset Dashboard\n            \u2514\u2500\u2500 fct_daily_glucose_metrics (dbt model)\n                \u2514\u2500\u2500 mrt_glucose_hourly_patterns (dbt model)\n</code></pre> <p>This tree shows your entire data flow: from API source through transformations to dashboards.</p>"},{"location":"blog/11-observability-monitoring/#impact-analysis","title":"Impact Analysis","text":"<p>Before making changes, check what's affected:</p> <pre><code>$ phlo lineage impact glucose_entries\n\nImpact Analysis: glucose_entries\n\nDirect Dependencies (depth 1):\n  \u2022 stg_glucose_entries (dbt) - 12 columns reference source\n\nTransitive Dependencies (depth 2+):\n  \u2022 fct_glucose_readings - 8 columns derived\n  \u2022 fct_daily_glucose_metrics - aggregates from fact table\n\nPublishing Impact:\n  \u26a0 mrt_glucose_readings \u2192 PostgreSQL marts (used by Superset)\n  \u26a0 publish_glucose_marts \u2192 External consumers\n\nTotal Impact:\n  6 assets affected\n  2 publishing endpoints\n  1 external dashboard\n\nRecommendation: Coordinate change with dashboard owners before deploying\n</code></pre>"},{"location":"blog/11-observability-monitoring/#exporting-lineage","title":"Exporting Lineage","text":"<p>Generate lineage diagrams for documentation:</p> <pre><code># Graphviz DOT format\n$ phlo lineage export --format dot --output lineage.dot\n$ dot -Tpng lineage.dot -o lineage.png\n\n# Mermaid format (for Markdown docs)\n$ phlo lineage export --format mermaid --output lineage.md\n</code></pre> <p>The Mermaid output can be embedded directly in GitHub READMEs or Notion docs:</p> <pre><code>graph LR\n    A[Nightscout API] --&gt; B[glucose_entries]\n    B --&gt; C[stg_glucose_entries]\n    C --&gt; D[fct_glucose_readings]\n    D --&gt; E[mrt_glucose_readings]\n    D --&gt; F[fct_daily_glucose_metrics]\n    E --&gt; G[Superset Dashboard]\n</code></pre>"},{"location":"blog/11-observability-monitoring/#alerting-getting-notified-when-things-break_1","title":"Alerting: Getting Notified When Things Break","text":"<p>Dashboards are great, but you can't watch them 24/7. Alerting ensures you know about problems before your users do.</p>"},{"location":"blog/11-observability-monitoring/#example-alert-format","title":"Example Alert Format","text":"<p>When alerts are sent, they include context:</p> <pre><code>\ud83d\udd34 Asset Materialization Failed\n\nAsset: glucose_entries\nPartition: 2024-01-15\nRun ID: abc123-def456\n\nError: Connection timeout after 30s\n\nTimestamp: 2024-01-15 10:35:42 UTC\n</code></pre> <p>The AlertManager automatically handles:</p> <ul> <li>Deduplication - Same asset + same error = 1 alert per hour (configurable)</li> <li>Severity Routing - Routes to appropriate channels based on severity</li> <li>Context - Includes run ID, asset name, error details</li> </ul>"},{"location":"blog/11-observability-monitoring/#log-access-and-filtering","title":"Log Access and Filtering","text":"<p>When something goes wrong, you need logs. The <code>phlo logs</code> command gives you fast access without diving into Docker or Dagster UI.</p>"},{"location":"blog/11-observability-monitoring/#why-cli-logs","title":"Why CLI Logs?","text":"<pre><code>Scenario: Dashboard is showing stale data\n\nWithout CLI logs:\n  1. Open browser\n  2. Navigate to Dagster UI\n  3. Find the run\n  4. Scroll through logs\n  5. Try to filter\n  6. Give up and grep Docker logs\n\nWith phlo logs:\n  $ phlo logs --asset glucose_entries --level ERROR --since 1h\n\n  [10:35:42] ERROR glucose_entries: Connection refused to nightscout.api\n  [10:35:42] ERROR glucose_entries: Retry 3/3 failed, aborting\n  [10:35:43] ERROR glucose_entries: Run failed after 45s\n</code></pre>"},{"location":"blog/11-observability-monitoring/#filtering-options","title":"Filtering Options","text":"<pre><code># By asset\nphlo logs --asset glucose_entries\n\n# By time\nphlo logs --since 1h          # Last hour\nphlo logs --since 30m         # Last 30 minutes\nphlo logs --since 7d          # Last week\n\n# By severity\nphlo logs --level ERROR       # Errors only\nphlo logs --level WARNING     # Warnings and above\n\n# Combined filters\nphlo logs --asset glucose_entries --level ERROR --since 2h\n\n# By specific run\nphlo logs --run-id abc123-def456\n\n# By job (for scheduled jobs)\nphlo logs --job daily_glucose_pipeline\n</code></pre>"},{"location":"blog/11-observability-monitoring/#real-time-tailing","title":"Real-Time Tailing","text":"<p>Watch logs as they happen:</p> <pre><code>$ phlo logs --follow\n\n[10:35:40] INFO  glucose_entries: Starting materialization\n[10:35:41] INFO  glucose_entries: Fetching from Nightscout API\n[10:35:42] INFO  glucose_entries: Retrieved 487 entries\n[10:35:43] INFO  glucose_entries: Validating with Pandera schema\n[10:35:44] INFO  glucose_entries: Writing to Iceberg table\n[10:35:45] INFO  glucose_entries: Materialization complete (5.2s)\n^C  # Ctrl+C to stop\n</code></pre>"},{"location":"blog/11-observability-monitoring/#json-output-for-scripting","title":"JSON Output for Scripting","text":"<p>Pipe logs to other tools:</p> <pre><code># Find all unique error messages in the last day\nphlo logs --level ERROR --since 1d --json | \\\n  jq -r '.[] | .message' | \\\n  sort | uniq -c | sort -rn\n\n# Export to file for analysis\nphlo logs --asset glucose_entries --since 7d --json &gt; glucose_logs.json\n\n# Grep for specific patterns\nphlo logs --json | jq 'select(.message | contains(\"timeout\"))'\n</code></pre>"},{"location":"blog/11-observability-monitoring/#metrics-quantifying-pipeline-health","title":"Metrics: Quantifying Pipeline Health","text":"<p>Logs tell you what happened. Metrics tell you how well things are running over time.</p>"},{"location":"blog/11-observability-monitoring/#the-metrics-dashboard","title":"The Metrics Dashboard","text":"<pre><code>$ phlo metrics\n\nPipeline Health Dashboard (Last 24 Hours)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nOverall Status: \u2713 HEALTHY\n\nRuns:\n  Total:     288\n  Success:   285 (98.96%)\n  Failed:    3 (1.04%)\n\nData Volume:\n  Rows Processed:  1.2M\n  Bytes Written:   450 MB\n\nLatency:\n  P50:   320ms\n  P95:   850ms\n  P99:   1.2s\n\nQuality:\n  Pass Rate:     99.74%\n  Checks Run:    1,440\n  Failures:      4\n\nActive Alerts: 1\n  \u26a0 publish_to_postgres running slow (1.2s avg vs 500ms baseline)\n</code></pre>"},{"location":"blog/11-observability-monitoring/#per-asset-metrics","title":"Per-Asset Metrics","text":"<p>Drill into specific assets:</p> <pre><code>$ phlo metrics asset glucose_entries --runs 20\n\nAsset: glucose_entries\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nLast 20 Runs:\n  Run ID          Time      Duration  Rows   Status\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  abc123          10:35     5.2s      487    \u2713\n  def456          10:30     4.8s      512    \u2713\n  ghi789          10:25     5.1s      495    \u2713\n  ...\n  xyz000          09:05     45.2s     0      \u2717 (timeout)\n\nStatistics:\n  Success Rate:   95% (19/20)\n  Avg Duration:   5.1s\n  P95 Duration:   8.2s\n  Avg Rows:       498\n\nTrend:\n  Duration: \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 (stable)\n  Failures: \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 (1 in window)\n</code></pre>"},{"location":"blog/11-observability-monitoring/#exporting-for-analysis","title":"Exporting for Analysis","text":"<p>Export metrics for external tools (Grafana, spreadsheets, custom analysis):</p> <pre><code># CSV for spreadsheets\n$ phlo metrics export --format csv --period 30d --output metrics.csv\n\n# JSON for custom analysis\n$ phlo metrics export --format json --period 7d | \\\n  python analyze_metrics.py\n</code></pre>"},{"location":"blog/11-observability-monitoring/#summary","title":"Summary","text":"<p>Phlo's observability stack provides:</p> <p>Metrics: Track what's happening via <code>phlo metrics</code> (execution time, throughput, quality) Logs: Understand why with <code>phlo logs</code> (structured logs, searchable, real-time tailing) Lineage: Understand impact with <code>phlo lineage</code> (CLI visualization, export, impact analysis) Alerts: Get notified via <code>phlo alerts</code> (Slack, PagerDuty, email with deduplication)</p> <p>Combined, you have:</p> <ul> <li>Visibility: Know your pipeline state at any time via CLI</li> <li>Reliability: Automated alerts detect failures before users do</li> <li>Speed: Find root causes in minutes with structured logs and lineage</li> <li>Confidence: Deploy with safety nets and impact analysis in place</li> </ul> <p>All accessible through simple CLI commands that integrate with your existing workflows.</p> <p>Next: Part 12: Production Deployment and Scaling</p> <p>See you there!</p>"},{"location":"blog/12-production-deployment/","title":"Part 12: Production Deployment and Scaling","text":"<p>You've built, tested, and monitored your data lakehouse. Now let's deploy it to production and scale it reliably.</p>"},{"location":"blog/12-production-deployment/#development-vs-production","title":"Development vs Production","text":"<p>What differs between your laptop and production:</p> Aspect Development Production Uptime SLA None (stop anytime) 99.9%+ Data Retention Days Years Backup Strategy Optional Required (PITR) Access Control All devs have all access Role-based, audited Failure Recovery Restart containers Auto-recovery, failover Monitoring None (ad-hoc) Continuous Capacity 16GB RAM, 1TB disk 256GB+ RAM, PB+ storage Cost Minimal Optimized Compliance None HIPAA, GDPR, etc."},{"location":"blog/12-production-deployment/#architecture-from-laptop-to-kubernetes","title":"Architecture: From Laptop to Kubernetes","text":""},{"location":"blog/12-production-deployment/#stage-1-docker-compose-development","title":"Stage 1: Docker Compose (Development)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Docker Compose Host       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 \u2502 Dagster  \u2502 \u2502 Trino    \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 \u2502 MinIO    \u2502 \u2502 Postgres \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 \u2502 Nessie   \u2502 \u2502 Superset \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nUse: docker-compose up\n</code></pre>"},{"location":"blog/12-production-deployment/#stage-2-single-server-small-prod","title":"Stage 2: Single Server (Small Prod)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Production Server            \u2502\n\u2502     (AWS EC2, DigitalOcean)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502 \u2502   Systemd Services         \u2502   \u2502\n\u2502 \u2502  (instead of docker-compose)   \u2502\n\u2502 \u2502                            \u2502   \u2502\n\u2502 \u2502 \u251c\u2500 Dagster (web + daemon) \u2502   \u2502\n\u2502 \u2502 \u251c\u2500 Trino                   \u2502   \u2502\n\u2502 \u2502 \u251c\u2500 Postgres                \u2502   \u2502\n\u2502 \u2502 \u2514\u2500 MinIO                   \u2502   \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                  \u2502\n\u2502  + S3-compatible external storage\u2502\n\u2502  + Managed RDS for Postgres      \u2502\n\u2502  + CloudWatch monitoring         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/12-production-deployment/#stage-3-kubernetes-high-scale-prod","title":"Stage 3: Kubernetes (High-Scale Prod)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Kubernetes Cluster                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502    Namespace: dagster          \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502  \u2502  \u2502  Dagster Webserver Pods    \u2502 \u2502   \u2502\n\u2502  \u2502  \u2502  (replicas: 2)             \u2502 \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502  \u2502  \u2502  Dagster Daemon Pods       \u2502 \u2502   \u2502\n\u2502  \u2502  \u2502  (replicas: 1)             \u2502 \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502  \u2502  \u2502  Compute Pods (auto-scale) \u2502 \u2502   \u2502\n\u2502  \u2502  \u2502  (replicas: 1-10)          \u2502 \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502    Namespace: data-warehouse    \u2502   \u2502\n\u2502  \u2502  \u251c\u2500 Trino Coordinator           \u2502   \u2502\n\u2502  \u2502  \u251c\u2500 Trino Workers (3-10)        \u2502   \u2502\n\u2502  \u2502  \u2514\u2500 Nessie                      \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                          \u2502\n\u2502  External Services:                      \u2502\n\u2502  \u251c\u2500 AWS RDS (Postgres, HA)              \u2502\n\u2502  \u251c\u2500 AWS S3 (data lake)                  \u2502\n\u2502  \u251c\u2500 AWS ElastiCache (caching)           \u2502\n\u2502  \u2514\u2500 AWS CloudWatch (monitoring)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/12-production-deployment/#deployment-steps","title":"Deployment Steps","text":""},{"location":"blog/12-production-deployment/#step-1-prepare-the-environment","title":"Step 1: Prepare the Environment","text":"<p>Phlo uses environment variables for configuration. Generate local env files and set defaults:</p> <pre><code># Generate .phlo/.env and .phlo/.env.local\nphlo services init\n\n# Copy secrets template\ncp .env.example .phlo/.env.local\n</code></pre> <p>Based on the actual <code>.env.example</code>, here are the critical production settings:</p> <pre><code># phlo.yaml (committed defaults)\nenv:\n  # Database (consider managed RDS for production)\n  POSTGRES_USER: lake\n  POSTGRES_DB: lakehouse\n  POSTGRES_PORT: 10000\n\n  # MinIO / S3 Storage\n  MINIO_API_PORT: 10001\n  MINIO_CONSOLE_PORT: 10002\n\n  # Nessie (Data Catalog)\n  NESSIE_VERSION: 0.105.5\n  NESSIE_PORT: 10003\n\n  # Trino (Query Engine)\n  TRINO_VERSION: 477\n  TRINO_PORT: 10005\n\n  # Iceberg Configuration\n  ICEBERG_WAREHOUSE_PATH: s3://lake/warehouse\n  ICEBERG_STAGING_PATH: s3://lake/stage\n  ICEBERG_NESSIE_REF: main\n\n  # Dagster (Orchestration)\n  DAGSTER_PORT: 10006\n\n  # Superset (BI)\n  SUPERSET_PORT: 10007\n\n  # API Layer\n  API_PORT: 10010\n\n  # Observability Stack\n  GRAFANA_PORT: 10016\n  PROMETHEUS_PORT: 10013\n\n  # Data Catalog (OpenMetadata)\n  OPENMETADATA_PORT: 10020\n</code></pre> <pre><code># .phlo/.env.local (secrets, not committed)\nPOSTGRES_PASSWORD=&lt;SECURE_PASSWORD&gt;  # Change from default!\nMINIO_ROOT_USER=&lt;SECURE_USER&gt;        # Change from default!\nMINIO_ROOT_PASSWORD=&lt;SECURE_PASSWORD&gt;\nSUPERSET_ADMIN_PASSWORD=&lt;SECURE_PASSWORD&gt;\nJWT_SECRET=&lt;SECURE_JWT_SECRET&gt;\nHASURA_ADMIN_SECRET=&lt;SECURE_ADMIN_SECRET&gt;\nPOSTGREST_AUTHENTICATOR_PASSWORD=&lt;SECURE_PASSWORD&gt;\nGRAFANA_ADMIN_PASSWORD=&lt;SECURE_PASSWORD&gt;\nOPENMETADATA_ADMIN_PASSWORD=&lt;SECURE_PASSWORD&gt;\nOPENMETADATA_MYSQL_PASSWORD=&lt;SECURE_PASSWORD&gt;\n</code></pre>"},{"location":"blog/12-production-deployment/#step-1b-infrastructure-configuration-phloyaml","title":"Step 1b: Infrastructure Configuration (phlo.yaml)","text":"<p>For production deployments, especially when running multiple Phlo projects or customizing service configurations, use <code>phlo.yaml</code> for project-level infrastructure settings.</p>"},{"location":"blog/12-production-deployment/#why-infrastructure-configuration","title":"Why Infrastructure Configuration?","text":"<p>Secrets in <code>.phlo/.env.local</code> and defaults in <code>phlo.yaml</code> (env:) handle configuration, but they don't handle:</p> <ul> <li>Multi-project deployments: Running multiple Phlo instances on the same host</li> <li>Container naming patterns: Custom naming for service discovery</li> <li>Port customization: Per-project port assignments</li> <li>Service-specific overrides: Custom configurations for individual services</li> </ul>"},{"location":"blog/12-production-deployment/#creating-phloyaml","title":"Creating phlo.yaml","text":"<p>Create a <code>phlo.yaml</code> file in your project root:</p> <pre><code># phlo.yaml - Production infrastructure configuration\n\nname: production-lakehouse\ndescription: Production data lakehouse for analytics\n\ninfrastructure:\n  # Container naming pattern (supports {{project}} and {{service}} variables)\n  container_naming_pattern: \"{{project}}-{{service}}-1\"\n\n  # Service-specific configuration\n  services:\n    dagster_webserver:\n      container_name: null # Use pattern above\n      service_name: dagster-webserver\n      host: localhost\n      internal_host: dagster-webserver\n      port: 10006\n\n    postgres:\n      container_name: null\n      service_name: postgres\n      host: localhost\n      internal_host: postgres\n      port: 10000\n      credentials:\n        user: postgres\n        password: ${POSTGRES_PASSWORD} # References .phlo/.env.local\n        database: cascade\n\n    minio:\n      container_name: null\n      service_name: minio\n      host: localhost\n      internal_host: minio\n      api_port: 10001\n      console_port: 10002\n\n    nessie:\n      container_name: null\n      service_name: nessie\n      host: localhost\n      internal_host: nessie\n      port: 10003\n\n    trino:\n      container_name: null\n      service_name: trino\n      host: localhost\n      internal_host: trino\n      port: 10005\n</code></pre>"},{"location":"blog/12-production-deployment/#multi-project-example","title":"Multi-Project Example","text":"<p>Running two Phlo projects on the same server:</p> <p>Project 1: Analytics Lakehouse</p> <pre><code># analytics/phlo.yaml\nname: analytics\ndescription: Analytics data lakehouse\n\ninfrastructure:\n  container_naming_pattern: \"{{project}}-{{service}}-1\"\n  services:\n    dagster_webserver:\n      port: 11006 # Different port\n    postgres:\n      port: 11000\n    # ... other services with unique ports\n</code></pre> <p>Project 2: ML Platform</p> <pre><code># ml-platform/phlo.yaml\nname: ml-platform\ndescription: Machine learning data platform\n\ninfrastructure:\n  container_naming_pattern: \"{{project}}-{{service}}-1\"\n  services:\n    dagster_webserver:\n      port: 12006 # Different port\n    postgres:\n      port: 12000\n    # ... other services with unique ports\n</code></pre> <p>Now you can run both simultaneously:</p> <pre><code># Terminal 1: Analytics project\ncd analytics/\nphlo services start\n\n# Terminal 2: ML platform\ncd ml-platform/\nphlo services start\n\n# Check running containers\ndocker ps\n# Shows:\n# - analytics-dagster-webserver-1 (port 11006)\n# - analytics-postgres-1 (port 11000)\n# - ml-platform-dagster-webserver-1 (port 12006)\n# - ml-platform-postgres-1 (port 12000)\n</code></pre>"},{"location":"blog/12-production-deployment/#configuration-loading","title":"Configuration Loading","text":"<p>Phlo automatically loads configuration in this order:</p> <ol> <li>phlo.yaml (project-level infrastructure + env defaults)</li> <li>.phlo/.env (generated non-secret defaults)</li> <li>.phlo/.env.local (secrets and local overrides)</li> <li>Environment variables (runtime overrides)</li> <li>Defaults (built-in fallbacks)</li> </ol> <p>Access configuration programmatically:</p> <pre><code>from phlo.infrastructure.config import (\n    load_infrastructure_config,\n    get_container_name,\n    get_service_config\n)\n\n# Load project configuration\nconfig = load_infrastructure_config()\nprint(config.name)  # \"production-lakehouse\"\n\n# Get container name for a service\ncontainer = get_container_name(\"dagster-webserver\")\n# Returns: \"production-lakehouse-dagster-webserver-1\"\n\n# Get service configuration\npostgres_config = get_service_config(\"postgres\")\nprint(postgres_config[\"port\"])  # 10000\nprint(postgres_config[\"internal_host\"])  # \"postgres\"\n</code></pre>"},{"location":"blog/12-production-deployment/#production-best-practices","title":"Production Best Practices","text":"<p>1. Use descriptive project names:</p> <pre><code>name: prod-analytics-us-east\ndescription: Production analytics lakehouse (US East region)\n</code></pre> <p>2. Document service purposes:</p> <pre><code>services:\n  dagster_webserver:\n    description: Dagster UI and GraphQL API\n    port: 10006\n</code></pre> <p>3. Reference secrets from .phlo/.env.local:</p> <pre><code>postgres:\n  credentials:\n    password: ${POSTGRES_PASSWORD} # Never hardcode secrets\n</code></pre> <p>4. Version control phlo.yaml:</p> <pre><code># Commit to git (no secrets here)\ngit add phlo.yaml\ngit commit -m \"Add infrastructure configuration\"\n\n# But NOT .phlo/.env.local (contains secrets)\necho \".phlo/.env.local\" &gt;&gt; .gitignore\n</code></pre> <p>5. Use different configs per environment:</p> <pre><code>phlo.yaml              # Base configuration\nphlo.staging.yaml      # Staging overrides\nphlo.production.yaml   # Production overrides\n</code></pre>"},{"location":"blog/12-production-deployment/#service-discovery","title":"Service Discovery","text":"<p>With infrastructure configuration, services can discover each other:</p> <pre><code>from phlo.infrastructure.config import get_service_config\n\n# Get Postgres connection from config\npg_config = get_service_config(\"postgres\")\nconnection_string = (\n    f\"postgresql://{pg_config['credentials']['user']}:\"\n    f\"{pg_config['credentials']['password']}@\"\n    f\"{pg_config['internal_host']}:{pg_config['port']}/\"\n    f\"{pg_config['credentials']['database']}\"\n)\n\n# Get Trino endpoint\ntrino_config = get_service_config(\"trino\")\ntrino_endpoint = f\"http://{trino_config['internal_host']}:{trino_config['port']}\"\n</code></pre>"},{"location":"blog/12-production-deployment/#kubernetes-integration","title":"Kubernetes Integration","text":"<p>For Kubernetes deployments, <code>phlo.yaml</code> provides a single source of truth:</p> <pre><code># Generate k8s manifests from phlo.yaml\nphlo k8s generate --config phlo.yaml\n\n# Deploys with:\n# - Service names from phlo.yaml\n# - Port mappings from phlo.yaml\n# - Resource limits from phlo.yaml\n</code></pre> <p>This ensures consistency between Docker Compose (dev) and Kubernetes (prod).</p>"},{"location":"blog/12-production-deployment/#step-2-deploy-with-docker-compose","title":"Step 2: Deploy with Docker Compose","text":"<p>Phlo includes a comprehensive <code>docker-compose.yml</code> that orchestrates all services. For production, you have options:</p> <p>Option A: Docker Compose (Current Implementation)</p> <pre><code># Start all core services\ndocker-compose up -d\n\n# Or start with observability stack\ndocker-compose --profile observability up -d\n\n# Or start with API layer\ndocker-compose --profile api up -d\n\n# Or start with data catalog\ndocker-compose --profile catalog up -d\n\n# Or start everything\ndocker-compose --profile all up -d\n\n# Check service health\ndocker-compose ps\n</code></pre> <p>The actual <code>docker-compose.yml</code> includes:</p> <ul> <li>Core Services: postgres, minio, nessie, trino, dagster-webserver, dagster-daemon, superset</li> <li>Observability (profile: observability): prometheus, loki, grafana, alloy, postgres-exporter</li> <li>API Layer (profile: api): FastAPI, Hasura GraphQL, PostgREST</li> <li>Data Catalog (profile: catalog): OpenMetadata with MySQL and Elasticsearch</li> <li>Documentation (profile: docs): MkDocs server</li> </ul> <p>Option B: Managed Services (Recommended for Production)</p> <p>For production workloads, consider replacing containerized services with managed alternatives:</p> <pre><code># Use AWS RDS for PostgreSQL\nPOSTGRES_HOST=phlo-prod.xxxxx.rds.amazonaws.com\nPOSTGRES_PORT=5432\n\n# Use AWS S3 instead of MinIO\nICEBERG_WAREHOUSE_PATH=s3://your-prod-bucket/warehouse\nMINIO_API_PORT=9000  # Or S3 endpoint\n\n# Keep Dagster, Trino, Nessie containerized with docker-compose\ndocker-compose up -d dagster-webserver dagster-daemon trino nessie\n</code></pre>"},{"location":"blog/12-production-deployment/#step-3-verify-service-health","title":"Step 3: Verify Service Health","text":"<p>The docker-compose configuration includes health checks for all services:</p> <pre><code># View service status\ndocker-compose ps\n\n# Check logs for specific service\ndocker-compose logs -f dagster-webserver\ndocker-compose logs -f trino\ndocker-compose logs -f nessie\n\n# Access services\n# Dagster UI: http://localhost:10006\n# Trino: http://localhost:10005\n# Nessie API: http://localhost:10003\n# Superset: http://localhost:10007\n# MinIO Console: http://localhost:10002\n# Grafana (with observability profile): http://localhost:10016\n# OpenMetadata (with catalog profile): http://localhost:10020\n</code></pre>"},{"location":"blog/12-production-deployment/#step-4-storage-configuration-production-s3","title":"Step 4: Storage Configuration (Production S3)","text":"<p>For production, replace MinIO with AWS S3:</p> <pre><code># Create S3 bucket\naws s3 mb s3://phlo-prod-lake --region us-east-1\n\n# Enable versioning (for Nessie and time-travel)\naws s3api put-bucket-versioning \\\n  --bucket phlo-prod-lake \\\n  --versioning-configuration Status=Enabled\n\n# Enable encryption\naws s3api put-bucket-encryption \\\n  --bucket phlo-prod-lake \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n\n# Update environment variables\nICEBERG_WAREHOUSE_PATH=s3://phlo-prod-lake/warehouse\nICEBERG_STAGING_PATH=s3://phlo-prod-lake/stage\nAWS_ACCESS_KEY_ID=&lt;your-access-key&gt;\nAWS_SECRET_ACCESS_KEY=&lt;your-secret-key&gt;\nAWS_REGION=us-east-1\n</code></pre>"},{"location":"blog/12-production-deployment/#step-5-observability-stack","title":"Step 5: Observability Stack","text":"<p>Enable Grafana, Prometheus, and Loki for monitoring:</p> <pre><code># Start with observability profile\ndocker-compose --profile observability up -d\n\n# Access Grafana at http://localhost:10016\n# Default credentials from .phlo/.env.local:\n# Username: admin\n# Password: admin123 (change in production!)\n\n# Prometheus metrics: http://localhost:10013\n# Loki logs: Accessible via Grafana data source\n</code></pre> <p>The observability stack includes:</p> <ul> <li>Prometheus: Metrics collection and storage</li> <li>Loki: Log aggregation</li> <li>Grafana: Dashboards and visualization</li> <li>Alloy: Metrics and log forwarding</li> <li>postgres-exporter: PostgreSQL metrics</li> </ul>"},{"location":"blog/12-production-deployment/#future-kubernetes-deployment","title":"Future: Kubernetes Deployment","text":"<p>Note: Kubernetes manifests are not yet included in the repository. The following is a reference architecture for future implementation.</p> <p>For large-scale production deployments, Kubernetes provides better orchestration:</p> <pre><code># Future: k8s/dagster-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dagster-webserver\n  namespace: dagster\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dagster-webserver\n  template:\n    metadata:\n      labels:\n        app: dagster-webserver\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n        prometheus.io/path: \"/metrics\"\n    spec:\n      serviceAccountName: dagster\n      containers:\n        - name: dagster-webserver\n          image: &lt;ACCOUNT_ID&gt;.dkr.ecr.us-east-1.amazonaws.com/phlo:1.0.0\n          ports:\n            - containerPort: 3000\n              name: http\n            - containerPort: 9090\n              name: metrics\n\n          # Resource limits\n          resources:\n            requests:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n\n          # Environment from secrets\n          envFrom:\n            - secretRef:\n                name: phlo-secrets\n            - configMapRef:\n                name: phlo-config\n\n          # Health checks\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 40\n            periodSeconds: 30\n            timeoutSeconds: 10\n            failureThreshold: 3\n\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 3000\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            timeoutSeconds: 5\n            failureThreshold: 1\n\n          # Logging\n          volumeMounts:\n            - name: logs\n              mountPath: /app/logs\n\n      # Node selection\n      nodeSelector:\n        workload: compute\n\n      # Pod disruption budget (for rolling updates)\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                    - key: app\n                      operator: In\n                      values:\n                        - dagster-webserver\n                topologyKey: kubernetes.io/hostname\n\n      volumes:\n        - name: logs\n          emptyDir: {}\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: dagster-webserver\n  namespace: dagster\nspec:\n  type: LoadBalancer\n  selector:\n    app: dagster-webserver\n  ports:\n    - port: 80\n      targetPort: 3000\n      protocol: TCP\n</code></pre> <p>Future Kubernetes deployment would look like:</p> <pre><code># Create namespace\nkubectl create namespace dagster\n\n# Create secrets from .phlo/.env.local\nkubectl create secret generic phlo-secrets \\\n  --from-env-file=.phlo/.env.local \\\n  -n dagster\n\n# Deploy (when k8s/ manifests are available)\nkubectl apply -f k8s/dagster-deployment.yaml\nkubectl apply -f k8s/trino-deployment.yaml\nkubectl apply -f k8s/nessie-deployment.yaml\n\n# Monitor rollout\nkubectl rollout status deployment/dagster-webserver -n dagster\n</code></pre>"},{"location":"blog/12-production-deployment/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"blog/12-production-deployment/#vertical-scaling-bigger-machines","title":"Vertical Scaling (Bigger Machines)","text":"<pre><code># Increase machine size for compute-intensive workloads\nkubectl set resources deployment dagster-compute \\\n  --requests=cpu=2000m,memory=8Gi \\\n  --limits=cpu=4000m,memory=16Gi \\\n  -n dagster\n</code></pre>"},{"location":"blog/12-production-deployment/#horizontal-scaling-more-machines","title":"Horizontal Scaling (More Machines)","text":"<pre><code># Add more Trino workers\nkubectl scale deployment trino-worker --replicas=10 -n data-warehouse\n\n# Add more Dagster compute pods\nkubectl set env deployment/dagster-compute \\\n  DAGSTER_K8S_INSTANCE_CONFIG_WORKERS=10 \\\n  -n dagster\n</code></pre>"},{"location":"blog/12-production-deployment/#autoscaling","title":"Autoscaling","text":"<pre><code># k8s/hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: dagster-compute-hpa\n  namespace: dagster\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: dagster-compute\n  minReplicas: 1\n  maxReplicas: 20\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n        - type: Percent\n          value: 50\n          periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n        - type: Percent\n          value: 100\n          periodSeconds: 30\n      selectPolicy: Max\n</code></pre>"},{"location":"blog/12-production-deployment/#high-availability","title":"High Availability","text":""},{"location":"blog/12-production-deployment/#database-failover","title":"Database Failover","text":"<pre><code># RDS Multi-AZ setup (automatic failover)\naws rds modify-db-instance \\\n  --db-instance-identifier phlo-prod \\\n  --multi-az \\\n  --apply-immediately\n\n# Automated backups\naws rds modify-db-instance \\\n  --db-instance-identifier phlo-prod \\\n  --backup-retention-period 30 \\\n  --preferred-backup-window \"03:00-04:00\" \\\n  --apply-immediately\n\n# Point-in-time recovery\naws rds restore-db-instance-to-point-in-time \\\n  --source-db-instance-identifier phlo-prod \\\n  --target-db-instance-identifier phlo-prod-restored \\\n  --restore-time 2024-10-15T10:30:00Z\n</code></pre>"},{"location":"blog/12-production-deployment/#service-redundancy","title":"Service Redundancy","text":"<pre><code># k8s/pdb.yaml (Pod Disruption Budget)\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: dagster-pdb\n  namespace: dagster\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: dagster-webserver\n</code></pre> <p>This ensures at least 1 pod is always running during maintenance.</p>"},{"location":"blog/12-production-deployment/#disaster-recovery","title":"Disaster Recovery","text":"<p>Current Implementation: Docker Compose Backups</p> <pre><code># Backup PostgreSQL database\ndocker-compose exec postgres pg_dump -U lake lakehouse &gt; backup_$(date +%Y%m%d).sql\n\n# Backup MinIO data (if using MinIO)\ndocker run --rm -v $(pwd)/volumes/minio:/data -v $(pwd)/backups:/backup \\\n  alpine tar czf /backup/minio_$(date +%Y%m%d).tar.gz /data\n\n# For production S3, use cross-region replication\naws s3api put-bucket-replication \\\n  --bucket phlo-prod-lake \\\n  --replication-configuration file://replication.json\n</code></pre> <p>Future: Kubernetes Backup Automation</p> <pre><code># Future backup automation with CronJob\ncat &gt; k8s/backup-cronjob.yaml &lt;&lt; 'EOF'\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: phlo-backup\n  namespace: dagster\nspec:\n  schedule: \"0 2 * * *\"  # 2 AM daily\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: dagster\n          containers:\n          - name: backup\n            image: amazon/aws-cli:latest\n            command:\n            - /bin/sh\n            - -c\n            - |\n              aws s3 sync s3://phlo-prod-lake s3://phlo-prod-backup \\\n                --delete \\\n                --exclude \"tmp/*\" \\\n                --region us-east-1\n            env:\n            - name: AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name: aws-credentials\n                  key: access_key\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: aws-credentials\n                  key: secret_key\n          restartPolicy: OnFailure\nEOF\n\nkubectl apply -f k8s/backup-cronjob.yaml\n</code></pre>"},{"location":"blog/12-production-deployment/#cost-optimization","title":"Cost Optimization","text":"<pre><code># phlo/monitoring/cost_tracking.py\nimport boto3\n\ndef estimate_monthly_cost():\n    \"\"\"Estimate AWS costs.\"\"\"\n\n    # S3 storage costs\n    s3 = boto3.client('s3')\n    response = s3.list_bucket_metrics_configurations(Bucket='phlo-prod-lake')\n\n    storage_size_gb = get_bucket_size() / (1024**3)\n    storage_cost = storage_size_gb * 0.023  # $0.023/GB/month\n\n    # RDS costs\n    rds = boto3.client('rds')\n    instances = rds.describe_db_instances()\n    rds_cost = len(instances) * 365  # Estimate\n\n    # Compute costs (EC2/ECS)\n    ec2 = boto3.client('ec2')\n    instances = ec2.describe_instances()\n    compute_cost = len(instances) * 150  # Estimate\n\n    total = storage_cost + rds_cost + compute_cost\n\n    print(f\"Monthly cost estimate:\")\n    print(f\"  Storage (S3): ${storage_cost:,.0f}\")\n    print(f\"  Database (RDS): ${rds_cost:,.0f}\")\n    print(f\"  Compute (K8s): ${compute_cost:,.0f}\")\n    print(f\"  Total: ${total:,.0f}\")\n\n    return {\n        \"storage\": storage_cost,\n        \"database\": rds_cost,\n        \"compute\": compute_cost,\n        \"total\": total,\n    }\n\n# Optimization strategies\ndef optimize_costs():\n    \"\"\"Implement cost optimization.\"\"\"\n\n    # 1. S3 Intelligent-Tiering\n    # Automatically move old data to cheaper storage classes\n\n    # 2. Reserved Instances\n    # Commit to 1-3 year terms for 40% discount\n\n    # 3. Spot instances for Trino workers\n    # Use spot instances for non-critical compute\n\n    # 4. Data lifecycle policies\n    # Archive to Glacier after 90 days\n\n    # 5. Compression\n    # Compress old Parquet files\n    pass\n</code></pre>"},{"location":"blog/12-production-deployment/#monitoring-production","title":"Monitoring Production","text":"<p>Current Implementation: Grafana + Prometheus</p> <pre><code># Start observability stack\ndocker-compose --profile observability up -d\n\n# Access Grafana dashboards\n# http://localhost:10016\n\n# View Prometheus metrics\n# http://localhost:10013\n\n# Query logs with Loki\n# Available via Grafana Explore interface\n</code></pre> <p>Grafana dashboards are pre-provisioned in <code>.phlo/grafana/dashboards/</code>:</p> <ul> <li>Lakehouse overview</li> <li>Dagster pipeline metrics</li> <li>PostgreSQL database metrics</li> <li>Trino query performance</li> </ul> <p>Future: Cloud-Native Monitoring</p> <p>For AWS deployments, integrate with CloudWatch:</p> <pre><code># CloudWatch dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name PhloProductionStatus \\\n  --dashboard-body file://dashboard.json\n\n# Set up alerts\naws cloudwatch put-metric-alarm \\\n  --alarm-name phlo-dagster-failures \\\n  --alarm-description \"Alert on Dagster failures\" \\\n  --metric-name PipelineFailures \\\n  --namespace Phlo \\\n  --statistic Sum \\\n  --period 300 \\\n  --threshold 1 \\\n  --comparison-operator GreaterThanThreshold\n</code></pre>"},{"location":"blog/12-production-deployment/#summary","title":"Summary","text":"<p>Production deployment with Phlo:</p>"},{"location":"blog/12-production-deployment/#current-implementation-docker-compose","title":"Current Implementation (Docker Compose)","text":"<p>Deployment Method: <code>docker-compose up -d</code> with profiles Infrastructure: Containerized services with health checks Storage: MinIO (dev) or S3 (production) Database: PostgreSQL (containerized or RDS) Monitoring: Grafana + Prometheus + Loki Scaling: Vertical (increase container resources)</p>"},{"location":"blog/12-production-deployment/#production-readiness-checklist","title":"Production Readiness Checklist","text":"<p>\u2705 Implemented:</p> <ul> <li>Docker Compose orchestration with health checks</li> <li>Environment-based configuration (<code>phlo.yaml</code> + <code>.phlo/.env.local</code>)</li> <li>Observability stack (Grafana, Prometheus, Loki)</li> <li>API layer with authentication (FastAPI, Hasura, PostgREST)</li> <li>Data catalog integration (OpenMetadata)</li> <li>Multi-profile deployment (core, observability, api, catalog)</li> </ul> <p>\ud83d\udccb Recommended for Production:</p> <ul> <li>Replace MinIO with AWS S3 or similar object storage</li> <li>Use managed PostgreSQL (RDS, Cloud SQL)</li> <li>Implement backup automation</li> <li>Set up SSL/TLS certificates</li> <li>Configure firewall rules and VPC</li> <li>Change all default passwords in <code>.phlo/.env.local</code></li> <li>Enable audit logging</li> </ul> <p>\ud83d\udd2e Future (Kubernetes):</p> <ul> <li>Kubernetes manifests for k8s/ directory</li> <li>Horizontal pod autoscaling</li> <li>Multi-region deployment</li> <li>Service mesh integration</li> </ul>"},{"location":"blog/12-production-deployment/#quick-start","title":"Quick Start","text":"<pre><code># Development\nphlo services init\ndocker-compose up -d\n\n# Production (all features)\ncp .env.example .phlo/.env.local\n# Edit .phlo/.env.local with production credentials\ndocker-compose --profile all up -d\n\n# Access services\n# Dagster: http://localhost:10006\n# Grafana: http://localhost:10016\n# Superset: http://localhost:10007\n# OpenMetadata: http://localhost:10020\n</code></pre> <p>Next: Part 13 - Plugin System - Extend Phlo with custom plugins</p> <p>Series:</p> <ol> <li>Data Lakehouse concepts</li> <li>Getting started</li> <li>Apache Iceberg</li> <li>Project Nessie</li> <li>Data ingestion</li> <li>dbt transformations</li> <li>Dagster orchestration</li> <li>Real-world example</li> <li>Data quality with Pandera</li> <li>Metadata and governance</li> <li>Observability and monitoring</li> <li>Production deployment \u2190 You are here</li> <li>Plugin system</li> </ol> <p>Happy data engineering!</p>"},{"location":"blog/13-plugin-system/","title":"Part 13: Extending Phlo with Plugins","text":"<p>You've built pipelines, added quality checks, and set up monitoring. But what happens when you need something Phlo doesn't provide out of the box? A custom data source, a specialized validation rule, or a domain-specific transformation?</p> <p>That's where the plugin system comes in.</p>"},{"location":"blog/13-plugin-system/#why-plugins","title":"Why Plugins?","text":"<p>Every data platform eventually hits limitations:</p> <pre><code>Week 1:  \"Phlo is great! It has everything we need.\"\nWeek 4:  \"Can we add a Salesforce source?\"\nWeek 8:  \"We need a custom quality check for our business rules.\"\nWeek 12: \"The finance team wants a specific transformation pattern.\"\n</code></pre> <p>Without plugins, you'd fork the codebase or hack around limitations. With plugins, you extend Phlo cleanly.</p>"},{"location":"blog/13-plugin-system/#the-three-plugin-types","title":"The Three Plugin Types","text":"<p>Phlo supports three types of plugins:</p> Type Purpose Example Source Connectors Fetch data from external systems Salesforce, HubSpot, custom APIs Quality Checks Custom validation rules Business logic, compliance rules Transforms Data transformation helpers Domain-specific calculations <p>Each type has a base class you inherit from, and Phlo discovers your plugins automatically via Python entry points.</p>"},{"location":"blog/13-plugin-system/#how-plugin-discovery-works","title":"How Plugin Discovery Works","text":"<p>When Phlo starts, it scans for installed packages that declare entry points:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Python Environment                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  phlo (core)                                                \u2502\n\u2502  phlo-plugin-salesforce (installed via pip)                 \u2502\n\u2502  phlo-plugin-custom-checks (your internal package)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Entry Point Discovery                           \u2502\n\u2502  importlib.metadata.entry_points()                          \u2502\n\u2502                                                             \u2502\n\u2502  Groups scanned:                                            \u2502\n\u2502    \u2022 phlo.plugins.sources                                   \u2502\n\u2502    \u2022 phlo.plugins.quality                                   \u2502\n\u2502    \u2022 phlo.plugins.transforms                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Plugin Registry                                 \u2502\n\u2502                                                             \u2502\n\u2502  Sources:     [rest_api, salesforce, hubspot]               \u2502\n\u2502  Quality:     [null_check, range_check, threshold_check]    \u2502\n\u2502  Transforms:  [uppercase, currency_convert]                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This means:</p> <ul> <li>No manual registration required</li> <li>Install a package, restart Phlo, plugin is available</li> <li>Bad plugins don't crash the system (logged and skipped)</li> </ul>"},{"location":"blog/13-plugin-system/#creating-a-source-connector-plugin","title":"Creating a Source Connector Plugin","text":"<p>Let's walk through a concrete example.</p>"},{"location":"blog/13-plugin-system/#step-1-project-structure","title":"Step 1: Project Structure","text":"<p>Example structure:</p> <pre><code>my-plugin/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 phlo_example/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 source.py       # JSONPlaceholderSource\n\u2502       \u251c\u2500\u2500 quality.py      # ThresholdCheckPlugin\n\u2502       \u2514\u2500\u2500 transform.py    # UppercaseTransformPlugin\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 test_source.py\n    \u251c\u2500\u2500 test_quality.py\n    \u2514\u2500\u2500 test_transform.py\n</code></pre>"},{"location":"blog/13-plugin-system/#step-2-source-plugin-implementation","title":"Step 2: Source Plugin Implementation","text":"<p>Example implementation:</p> <pre><code>\"\"\"Example source connector plugin using JSONPlaceholder API.\"\"\"\n\nfrom typing import Any, Iterator\nimport requests\nfrom phlo.plugins import PluginMetadata, SourceConnectorPlugin\n\n\nclass JSONPlaceholderSource(SourceConnectorPlugin):\n    \"\"\"\n    Source connector for JSONPlaceholder API.\n\n    Fetches posts, comments, or other data from the free JSONPlaceholder API.\n    \"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        \"\"\"Return plugin metadata.\"\"\"\n        return PluginMetadata(\n            name=\"jsonplaceholder\",\n            version=\"1.0.0\",\n            description=\"Fetch data from JSONPlaceholder API\",\n            author=\"Phlo Team\",\n            homepage=\"https://github.com/iamgp/phlo\",\n            tags=[\"api\", \"example\", \"public\"],\n            license=\"MIT\",\n        )\n\n    def fetch_data(self, config: dict[str, Any]) -&gt; Iterator[dict[str, Any]]:\n        \"\"\"\n        Fetch data from JSONPlaceholder API.\n\n        Args:\n            config: Configuration dictionary with:\n                - base_url: API base URL (default: https://jsonplaceholder.typicode.com)\n                - resource: Resource to fetch (default: posts)\n                - limit: Max items to fetch (default: 0 = all)\n\n        Yields:\n            Dictionary representing each item from the API\n        \"\"\"\n        # Validate configuration\n        if not self.validate_config(config):\n            raise ValueError(\"Invalid configuration\")\n\n        base_url = config.get(\"base_url\", \"https://jsonplaceholder.typicode.com\")\n        resource = config.get(\"resource\", \"posts\")\n        limit = config.get(\"limit\", 0)\n\n        url = f\"{base_url}/{resource}\"\n\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            items = response.json()\n            if not isinstance(items, list):\n                items = [items]\n\n            # Apply limit if specified\n            if limit &gt; 0:\n                items = items[:limit]\n\n            for item in items:\n                yield item\n\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Failed to fetch from {url}: {e}\")\n\n    def get_schema(self, config: dict[str, Any]) -&gt; dict[str, str] | None:\n        \"\"\"Get expected schema for the resource.\"\"\"\n        resource = config.get(\"resource\", \"posts\")\n\n        schemas = {\n            \"posts\": {\n                \"userId\": \"int\",\n                \"id\": \"int\",\n                \"title\": \"string\",\n                \"body\": \"string\",\n            },\n            \"comments\": {\n                \"postId\": \"int\",\n                \"id\": \"int\",\n                \"name\": \"string\",\n                \"email\": \"string\",\n                \"body\": \"string\",\n            },\n            \"users\": {\n                \"id\": \"int\",\n                \"name\": \"string\",\n                \"username\": \"string\",\n                \"email\": \"string\",\n                \"address\": \"object\",\n                \"phone\": \"string\",\n                \"website\": \"string\",\n            },\n        }\n\n        return schemas.get(resource, None)\n\n    def test_connection(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Test if the API is accessible.\"\"\"\n        try:\n            base_url = config.get(\"base_url\", \"https://jsonplaceholder.typicode.com\")\n            resource = config.get(\"resource\", \"posts\")\n            url = f\"{base_url}/{resource}\"\n\n            response = requests.get(url, timeout=5)\n            return response.status_code == 200\n\n        except Exception:\n            return False\n\n    def validate_config(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Validate configuration.\"\"\"\n        if not isinstance(config, dict):\n            return False\n\n        # Validate base_url if provided\n        base_url = config.get(\"base_url\")\n        if base_url and not isinstance(base_url, str):\n            return False\n\n        # Validate resource if provided\n        resource = config.get(\"resource\")\n        if resource and not isinstance(resource, str):\n            return False\n\n        # Validate limit if provided\n        limit = config.get(\"limit\", 0)\n        if not isinstance(limit, int) or limit &lt; 0:\n            return False\n\n        return True\n</code></pre>"},{"location":"blog/13-plugin-system/#step-3-entry-points-registration","title":"Step 3: Entry Points Registration","text":"<p>Example entry points from <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"phlo-plugin-example\"\nversion = \"1.0.0\"\ndescription = \"Example Phlo plugin package demonstrating all plugin types\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\n    \"pandas&gt;=1.5.0\",\n    \"requests&gt;=2.28.0\",\n]\n\n[project.entry-points.\"phlo.plugins.sources\"]\njsonplaceholder = \"phlo_example.source:JSONPlaceholderSource\"\n\n[project.entry-points.\"phlo.plugins.quality\"]\nthreshold_check = \"phlo_example.quality:ThresholdCheckPlugin\"\n\n[project.entry-points.\"phlo.plugins.transforms\"]\nuppercase = \"phlo_example.transform:UppercaseTransformPlugin\"\n</code></pre>"},{"location":"blog/13-plugin-system/#step-4-install-and-use","title":"Step 4: Install and Use","text":"<pre><code># Install the example plugin\ncd my-plugin\npip install -e .\n\n# Verify it's discovered\nphlo plugin list\n</code></pre> <p>Now use it in your pipeline:</p> <pre><code>from phlo.plugins import get_source_connector\n\n# Get the plugin\nsource = get_source_connector(\"jsonplaceholder\")\n\n# Fetch data\nconfig = {\n    \"resource\": \"posts\",\n    \"limit\": 10,\n}\n\nfor post in source.fetch_data(config):\n    print(post)\n</code></pre>"},{"location":"blog/13-plugin-system/#creating-a-quality-check-plugin","title":"Creating a Quality Check Plugin","text":"<p>Example threshold check plugin:</p>"},{"location":"blog/13-plugin-system/#example-threshold-check-plugin","title":"Example: Threshold Check Plugin","text":"<p>Example quality plugin implementation:</p> <pre><code>\"\"\"Example quality check plugin.\"\"\"\n\nfrom typing import Any\nimport pandas as pd\nfrom phlo.plugins import PluginMetadata, QualityCheckPlugin\n\n\nclass ThresholdCheckPlugin(QualityCheckPlugin):\n    \"\"\"\n    Quality check plugin for threshold validation.\n\n    Creates checks that verify numeric values fall within specified thresholds.\n    \"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        \"\"\"Return plugin metadata.\"\"\"\n        return PluginMetadata(\n            name=\"threshold_check\",\n            version=\"1.0.0\",\n            description=\"Validate numeric values within thresholds\",\n            author=\"Phlo Team\",\n            homepage=\"https://github.com/iamgp/phlo\",\n            tags=[\"validation\", \"numeric\", \"example\"],\n            license=\"MIT\",\n        )\n\n    def create_check(self, **kwargs) -&gt; \"ThresholdCheck\":\n        \"\"\"\n        Create a threshold check instance.\n\n        Args:\n            column: Column name to validate\n            min: Minimum value (inclusive)\n            max: Maximum value (inclusive)\n            tolerance: Fraction of rows allowed to fail (0.0 = strict, 1.0 = allow all)\n        \"\"\"\n        return ThresholdCheck(\n            column=kwargs.get(\"column\"),\n            min_value=kwargs.get(\"min\"),\n            max_value=kwargs.get(\"max\"),\n            tolerance=kwargs.get(\"tolerance\", 0.0),\n        )\n\n\nclass ThresholdCheck:\n    \"\"\"Threshold-based quality check.\"\"\"\n\n    def __init__(\n        self,\n        column: str,\n        min_value: float | None = None,\n        max_value: float | None = None,\n        tolerance: float = 0.0,\n    ):\n        \"\"\"\n        Initialize threshold check.\n\n        Args:\n            column: Column to validate\n            min_value: Minimum allowed value\n            max_value: Maximum allowed value\n            tolerance: Fraction of rows allowed to fail (0.0-1.0)\n        \"\"\"\n        self.column = column\n        self.min_value = min_value\n        self.max_value = max_value\n        self.tolerance = max(0.0, min(1.0, tolerance))  # Clamp to 0.0-1.0\n\n    def execute(self, df: pd.DataFrame, context: Any = None) -&gt; dict:\n        \"\"\"\n        Execute the quality check.\n\n        Returns:\n            Dictionary with check results:\n            {\n                \"passed\": bool,\n                \"violations\": int,\n                \"total\": int,\n                \"violation_rate\": float,\n            }\n        \"\"\"\n        if self.column not in df.columns:\n            return {\n                \"passed\": False,\n                \"violations\": len(df),\n                \"total\": len(df),\n                \"violation_rate\": 1.0,\n                \"error\": f\"Column '{self.column}' not found\",\n            }\n\n        # Count violations\n        violations = 0\n        for value in df[self.column]:\n            if pd.isna(value):\n                violations += 1\n                continue\n\n            if self.min_value is not None and value &lt; self.min_value:\n                violations += 1\n                continue\n\n            if self.max_value is not None and value &gt; self.max_value:\n                violations += 1\n\n        total = len(df)\n        violation_rate = violations / total if total &gt; 0 else 0.0\n\n        # Check if within tolerance\n        passed = violation_rate &lt;= self.tolerance\n\n        return {\n            \"passed\": passed,\n            \"violations\": violations,\n            \"total\": total,\n            \"violation_rate\": violation_rate,\n        }\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return check name.\"\"\"\n        bounds = []\n        if self.min_value is not None:\n            bounds.append(f\"min={self.min_value}\")\n        if self.max_value is not None:\n            bounds.append(f\"max={self.max_value}\")\n\n        bound_str = \",\".join(bounds) if bounds else \"unbounded\"\n        return f\"threshold_check({self.column},{bound_str})\"\n</code></pre> <p>Usage:</p> <pre><code>from phlo.plugins import get_quality_check\n\n# Get the plugin\nplugin = get_quality_check(\"threshold_check\")\n\n# Create a check instance\ncheck = plugin.create_check(\n    column=\"temperature\",\n    min=0,\n    max=100,\n    tolerance=0.05,  # Allow 5% of rows to fail\n)\n\n# Execute the check\nresult = check.execute(df)\nprint(f\"Passed: {result['passed']}\")\nprint(f\"Violations: {result['violations']} / {result['total']}\")\n</code></pre>"},{"location":"blog/13-plugin-system/#creating-a-transform-plugin","title":"Creating a Transform Plugin","text":"<p>Example transform plugin:</p>"},{"location":"blog/13-plugin-system/#example-uppercase-transform","title":"Example: Uppercase Transform","text":"<p>Example transform plugin implementation:</p> <pre><code>\"\"\"Example transformation plugin.\"\"\"\n\nfrom typing import Any\nimport pandas as pd\nfrom phlo.plugins import PluginMetadata, TransformationPlugin\n\n\nclass UppercaseTransformPlugin(TransformationPlugin):\n    \"\"\"\n    Transformation plugin for uppercase conversion.\n\n    Converts specified string columns to uppercase.\n    \"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        \"\"\"Return plugin metadata.\"\"\"\n        return PluginMetadata(\n            name=\"uppercase\",\n            version=\"1.0.0\",\n            description=\"Convert string columns to uppercase\",\n            author=\"Phlo Team\",\n            homepage=\"https://github.com/iamgp/phlo\",\n            tags=[\"string\", \"transform\", \"example\"],\n            license=\"MIT\",\n        )\n\n    def transform(self, df: pd.DataFrame, config: dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform DataFrame by converting columns to uppercase.\n\n        Args:\n            df: Input DataFrame\n            config: Configuration with:\n                - columns: List of column names to transform\n                - skip_na: Skip null values (default: True)\n\n        Returns:\n            Transformed DataFrame with uppercase values\n        \"\"\"\n        if not self.validate_config(config):\n            raise ValueError(\"Invalid configuration\")\n\n        # Copy to avoid modifying original\n        result = df.copy()\n\n        columns = config.get(\"columns\", [])\n        skip_na = config.get(\"skip_na\", True)\n\n        # Transform each column\n        for column in columns:\n            if column not in result.columns:\n                raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n            # Apply uppercase transformation\n            if skip_na:\n                result[column] = result[column].apply(\n                    lambda x: x.upper() if pd.notna(x) else x\n                )\n            else:\n                result[column] = result[column].str.upper()\n\n        return result\n\n    def get_output_schema(\n        self, input_schema: dict[str, str], config: dict[str, Any]\n    ) -&gt; dict[str, str] | None:\n        \"\"\"\n        Get the schema of transformed data.\n\n        Uppercase transformation doesn't change types.\n        \"\"\"\n        return input_schema\n\n    def validate_config(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Validate transformation configuration.\"\"\"\n        if not isinstance(config, dict):\n            return False\n\n        # Columns must be a list\n        columns = config.get(\"columns\", [])\n        if not isinstance(columns, (list, tuple)):\n            return False\n\n        # Each column must be a string\n        for column in columns:\n            if not isinstance(column, str):\n                return False\n\n        # skip_na must be a boolean if provided\n        skip_na = config.get(\"skip_na\", True)\n        if not isinstance(skip_na, bool):\n            return False\n\n        return True\n</code></pre> <p>Usage:</p> <pre><code>from phlo.plugins import get_transformation\nimport pandas as pd\n\n# Get the plugin\nplugin = get_transformation(\"uppercase\")\n\n# Create sample data\ndf = pd.DataFrame({\n    \"title\": [\"hello world\", \"foo bar\"],\n    \"body\": [\"test data\", \"more test\"],\n})\n\n# Transform\nresult = plugin.transform(df, config={\n    \"columns\": [\"title\", \"body\"],\n    \"skip_na\": True,\n})\n\nprint(result)\n# Output:\n#         title       body\n# 0  HELLO WORLD  TEST DATA\n# 1     FOO BAR  MORE TEST\n</code></pre>"},{"location":"blog/13-plugin-system/#managing-plugins-via-cli","title":"Managing Plugins via CLI","text":"<p>The actual CLI implementation provides these commands:</p>"},{"location":"blog/13-plugin-system/#list-installed-plugins","title":"List Installed Plugins","text":"<pre><code>$ phlo plugin list\n\nSources:\n  NAME              VERSION  AUTHOR\n  jsonplaceholder   1.0.0    Phlo Team\n\nQuality Checks:\n  NAME              VERSION  AUTHOR\n  threshold_check   1.0.0    Phlo Team\n\nTransforms:\n  NAME              VERSION  AUTHOR\n  uppercase         1.0.0    Phlo Team\n\n# Filter by type\n$ phlo plugin list --type sources\n\n# Output as JSON\n$ phlo plugin list --json\n</code></pre>"},{"location":"blog/13-plugin-system/#get-plugin-details","title":"Get Plugin Details","text":"<pre><code>$ phlo plugin info jsonplaceholder\n\njsonplaceholder\nType: sources\nVersion: 1.0.0\nAuthor: Phlo Team\nDescription: Fetch data from JSONPlaceholder API\nLicense: MIT\nHomepage: https://github.com/iamgp/phlo\nTags: api, example, public\n\n# Auto-detect plugin type\n$ phlo plugin info threshold_check\n\n# Specify type explicitly\n$ phlo plugin info uppercase --type transforms\n\n# JSON output\n$ phlo plugin info jsonplaceholder --json\n</code></pre>"},{"location":"blog/13-plugin-system/#validate-plugins","title":"Validate Plugins","text":"<pre><code>$ phlo plugin check\n\nValidating plugins...\n\n\u2713 Valid Plugins: 3\n  \u2713 source_connectors:jsonplaceholder\n  \u2713 quality_checks:threshold_check\n  \u2713 transformations:uppercase\n\nAll plugins are valid!\n\n# JSON output\n$ phlo plugin check --json\n</code></pre>"},{"location":"blog/13-plugin-system/#create-new-plugin-scaffold","title":"Create New Plugin Scaffold","text":"<p>The actual CLI implementation scaffolds complete plugin packages:</p> <pre><code>$ phlo plugin create my-api-source --type source\n\n\u2713 Plugin created successfully!\n\nNext steps:\n  1. cd phlo-plugin-my-api-source\n  2. Edit the plugin in src/phlo_my_api_source/\n  3. Run tests: pytest tests/\n  4. Install: pip install -e .\n\n# Create quality check plugin\n$ phlo plugin create my-validation --type quality\n\n# Create transform plugin\n$ phlo plugin create my-transform --type transform\n\n# Specify custom path\n$ phlo plugin create my-plugin --type source --path ./plugins/my-plugin\n</code></pre> <p>The scaffold creates a complete package structure:</p> <pre><code>phlo-plugin-my-api-source/\n\u251c\u2500\u2500 pyproject.toml           # Package config with entry points\n\u251c\u2500\u2500 README.md                # Documentation\n\u251c\u2500\u2500 MANIFEST.in              # Package manifest\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 phlo_my_api_source/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 plugin.py        # Plugin implementation\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_plugin.py       # Test suite\n</code></pre>"},{"location":"blog/13-plugin-system/#best-practices","title":"Best Practices","text":""},{"location":"blog/13-plugin-system/#1-keep-plugins-focused","title":"1. Keep Plugins Focused","text":"<p>One plugin = one responsibility. Don't create a \"kitchen sink\" plugin.</p> <pre><code># Good: focused plugins\nclass SalesforceSource(SourceConnectorPlugin): ...\nclass HubSpotSource(SourceConnectorPlugin): ...\n\n# Bad: monolithic plugin\nclass CRMSource(SourceConnectorPlugin):\n    def fetch_salesforce(self): ...\n    def fetch_hubspot(self): ...\n    def fetch_dynamics(self): ...\n</code></pre>"},{"location":"blog/13-plugin-system/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<p>Plugins should never crash Phlo. Catch exceptions and return meaningful errors.</p> <pre><code>def fetch_data(self, config: dict) -&gt; Iterator[dict]:\n    try:\n        response = self.client.get(url)\n        response.raise_for_status()\n        yield from response.json()\n    except httpx.HTTPStatusError as e:\n        raise PluginError(f\"API returned {e.response.status_code}: {e.response.text}\")\n    except httpx.RequestError as e:\n        raise PluginError(f\"Network error: {e}\")\n</code></pre>"},{"location":"blog/13-plugin-system/#3-include-metadata","title":"3. Include Metadata","text":"<p>Good metadata makes plugins discoverable:</p> <pre><code>@property\ndef metadata(self) -&gt; PluginMetadata:\n    return PluginMetadata(\n        name=\"salesforce\",\n        version=\"2.1.0\",\n        description=\"Ingest Salesforce objects (Accounts, Contacts, Opportunities)\",\n        author=\"Data Platform Team\",\n        documentation_url=\"https://docs.yourcompany.com/plugins/salesforce\",\n    )\n</code></pre>"},{"location":"blog/13-plugin-system/#4-write-tests","title":"4. Write Tests","text":"<pre><code># tests/test_source.py\ndef test_fetch_data_returns_records():\n    source = JSONPlaceholderSource()\n    records = list(source.fetch_data({\"resource\": \"posts\", \"limit\": 5}))\n\n    assert len(records) == 5\n    assert all(\"id\" in r for r in records)\n    assert all(\"title\" in r for r in records)\n\ndef test_connection_check():\n    source = JSONPlaceholderSource()\n    assert source.test_connection({}) is True\n\ndef test_invalid_resource_handled():\n    source = JSONPlaceholderSource()\n    with pytest.raises(PluginError):\n        list(source.fetch_data({\"resource\": \"invalid\"}))\n</code></pre>"},{"location":"blog/13-plugin-system/#5-version-your-plugins","title":"5. Version Your Plugins","text":"<p>Use semantic versioning. Breaking changes = major version bump.</p> <pre><code>[project]\nversion = \"2.0.0\"  # Breaking: changed config schema\nversion = \"1.1.0\"  # Feature: added new resource type\nversion = \"1.0.1\"  # Fix: handled edge case\n</code></pre>"},{"location":"blog/13-plugin-system/#plugin-security","title":"Plugin Security","text":"<p>Plugins run with full access to your environment. Only install trusted plugins.</p> <p>For organizations:</p> <ul> <li>Maintain an internal plugin registry</li> <li>Review plugin code before deployment</li> <li>Use <code>plugins_whitelist</code> in config to restrict allowed plugins:</li> </ul> <pre><code># config.py\nclass PhloConfig:\n    plugins_whitelist: list[str] = [\n        \"rest_api\",\n        \"salesforce\",\n        \"internal_*\",  # Allow all internal plugins\n    ]\n</code></pre>"},{"location":"blog/13-plugin-system/#summary","title":"Summary","text":"<p>The plugin system lets you extend Phlo without modifying core code:</p> <ul> <li>Source Connectors: Fetch data from any system</li> <li>Quality Checks: Encode custom business rules</li> <li>Transforms: Reusable transformation logic</li> </ul> <p>Plugins are discovered automatically via Python entry points, managed via CLI, and integrate seamlessly with Phlo's decorators and assets.</p> <p>When to use plugins:</p> <ul> <li>You need a data source Phlo doesn't support</li> <li>You have organization-specific quality rules</li> <li>You want to share reusable logic across teams</li> </ul> <p>When NOT to use plugins:</p> <ul> <li>One-off transformations (just write Python)</li> <li>Simple quality checks (use built-in checks)</li> <li>Anything that could be a dbt model</li> </ul>"},{"location":"blog/13-plugin-system/#try-the-example-plugin","title":"Try the Example Plugin","text":"<p>A complete working example layout:</p> <pre><code># Install the example plugin\ncd my-plugin\npip install -e .\n\n# List discovered plugins\nphlo plugin list\n\n# Get plugin info\nphlo plugin info jsonplaceholder\nphlo plugin info threshold_check\nphlo plugin info uppercase\n\n# Test the source connector\npython -c \"\nfrom phlo.plugins import get_source_connector\nsource = get_source_connector('jsonplaceholder')\nfor post in source.fetch_data({'resource': 'posts', 'limit': 3}):\n    print(post['title'])\n\"\n</code></pre> <p>Actual Files to Study:</p> <ul> <li>Source plugin: <code>src/phlo_example/source.py</code></li> <li>Quality check plugin: <code>src/phlo_example/quality.py</code></li> <li>Transform plugin: <code>src/phlo_example/transform.py</code></li> <li>Entry points: <code>pyproject.toml</code></li> <li>Tests: <code>tests/</code></li> </ul> <p>Base Classes:</p> <ul> <li><code>phlo.plugins.SourceConnectorPlugin</code> - Inherit for source connectors</li> <li><code>phlo.plugins.QualityCheckPlugin</code> - Inherit for quality checks</li> <li><code>phlo.plugins.TransformationPlugin</code> - Inherit for transforms</li> </ul> <p>Discovery Functions:</p> <ul> <li><code>phlo.plugins.discover_plugins()</code> - Discover all plugins</li> <li><code>phlo.plugins.get_source_connector(name)</code> - Get source plugin</li> <li><code>phlo.plugins.get_quality_check(name)</code> - Get quality plugin</li> <li><code>phlo.plugins.get_transformation(name)</code> - Get transform plugin</li> </ul> <p>Previous: Part 12 - Production Deployment</p> <p>Series:</p> <ol> <li>Data Lakehouse concepts</li> <li>Getting started</li> <li>Apache Iceberg</li> <li>Project Nessie</li> <li>Data ingestion</li> <li>dbt transformations</li> <li>Dagster orchestration</li> <li>Real-world example</li> <li>Data quality with Pandera</li> <li>Metadata and governance</li> <li>Observability and monitoring</li> <li>Production deployment</li> <li>Plugin system \u2190 You are here</li> </ol> <p>Happy plugin development!</p>"},{"location":"errors/","title":"Phlo Error Documentation","text":"<p>Comprehensive documentation for all Phlo error codes with solutions, examples, and prevention strategies.</p>"},{"location":"errors/#quick-reference","title":"Quick Reference","text":"Error Code Description Severity Category PHLO-001 Asset Not Discovered High Discovery PHLO-002 Schema Mismatch High Configuration PHLO-003 Invalid Cron Expression Medium Configuration PHLO-004 Validation Failed High Validation PHLO-005 Missing Schema High Configuration PHLO-006 Ingestion Failed High Runtime PHLO-007 Table Not Found High Runtime PHLO-008 Infrastructure Error Critical Infrastructure PHLO-200 Schema Conversion Error High Schema PHLO-201 Type Conversion Error Medium Schema PHLO-300 DLT Pipeline Failed High DLT PHLO-301 DLT Source Error High DLT PHLO-400 Iceberg Catalog Error High Iceberg PHLO-401 Iceberg Table Error High Iceberg PHLO-402 Iceberg Write Error High Iceberg"},{"location":"errors/#error-categories","title":"Error Categories","text":""},{"location":"errors/#discovery-and-configuration-errors-phlo-001-to-phlo-099","title":"Discovery and Configuration Errors (PHLO-001 to PHLO-099)","text":"<p>Errors related to asset discovery, configuration, and setup.</p>"},{"location":"errors/#phlo-001-asset-not-discovered","title":"PHLO-001: Asset Not Discovered","text":"<p>Exception: <code>PhloDiscoveryError</code></p> <p>Dagster cannot discover your asset definitions.</p> <p>Common causes:</p> <ul> <li>Asset outside the workflows path</li> <li>Missing decorator</li> <li>Import errors in asset module</li> </ul> <p>Quick fix:</p> <pre><code>from phlo.framework.definitions import defs\nassert len(list(defs.assets or [])) &gt; 0\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-002-schema-mismatch","title":"PHLO-002: Schema Mismatch","text":"<p>Exception: <code>PhloSchemaError</code></p> <p>Mismatch between decorator configuration and Pandera schema.</p> <p>Common causes:</p> <ul> <li>unique_key not in schema</li> <li>Typo in field name</li> <li>Case sensitivity issues</li> </ul> <p>Quick fix:</p> <pre><code># Verify unique_key matches schema field\n@phlo_ingestion(\n    unique_key=\"observation_id\",  # Must match schema field exactly\n    validation_schema=WeatherObservations,\n)\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-003-invalid-cron-expression","title":"PHLO-003: Invalid Cron Expression","text":"<p>Exception: <code>PhloCronError</code></p> <p>Invalid or malformed cron schedule expression.</p> <p>Common causes:</p> <ul> <li>Wrong number of fields (need 5)</li> <li>Invalid field values</li> <li>Incorrect syntax</li> </ul> <p>Quick fix:</p> <pre><code># Use standard 5-field cron format\n@phlo_ingestion(\n    cron=\"0 */1 * * *\",  # minute hour day month weekday\n    ...\n)\n</code></pre> <p>Test your cron: crontab.guru</p> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-004-validation-failed","title":"PHLO-004: Validation Failed","text":"<p>Exception: <code>PhloValidationError</code></p> <p>Data validation failed against Pandera schema.</p> <p>Common causes:</p> <ul> <li>Data doesn't match schema constraints</li> <li>Type mismatches</li> <li>Null values in non-nullable fields</li> <li>Values out of allowed range</li> </ul> <p>Quick fix:</p> <pre><code># Check Pandera validation errors in logs\n# Fix data transformation to match schema constraints\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-005-missing-schema","title":"PHLO-005: Missing Schema","text":"<p>Exception: <code>PhloConfigError</code></p> <p>Validation schema not provided to decorator.</p> <p>Common causes:</p> <ul> <li>validation_schema parameter missing</li> <li>Schema class not imported</li> <li>Typo in schema name</li> </ul> <p>Quick fix:</p> <pre><code>from workflows.schemas.weather import WeatherObservations\n\n@phlo_ingestion(\n    unique_key=\"observation_id\",\n    validation_schema=WeatherObservations,  # \u2705 Required\n)\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#runtime-and-integration-errors-phlo-006-to-phlo-099","title":"Runtime and Integration Errors (PHLO-006 to PHLO-099)","text":"<p>Errors during asset execution and external integrations.</p>"},{"location":"errors/#phlo-006-ingestion-failed","title":"PHLO-006: Ingestion Failed","text":"<p>Exception: <code>PhloIngestionError</code></p> <p>Data ingestion failed during asset execution.</p> <p>Common causes:</p> <ul> <li>API failures</li> <li>Network issues</li> <li>Authentication problems</li> <li>Data processing errors</li> </ul> <p>Quick fix:</p> <pre><code># Add retry logic and error handling\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\nsession = requests.Session()\nretry = Retry(total=3, backoff_factor=1)\nsession.mount(\"http://\", HTTPAdapter(max_retries=retry))\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-007-table-not-found","title":"PHLO-007: Table Not Found","text":"<p>Exception: <code>PhloTableError</code></p> <p>Iceberg table not found in catalog.</p> <p>Common causes:</p> <ul> <li>Table hasn't been created yet</li> <li>Wrong table name</li> <li>Wrong catalog/namespace</li> <li>Permissions issue</li> </ul> <p>Quick fix:</p> <pre><code># Verify table exists in Iceberg catalog\nfrom phlo_iceberg.catalog import get_catalog\n\ncatalog = get_catalog()\ntables = catalog.list_tables(\"bronze\")\nprint(f\"Available tables: {tables}\")\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-008-infrastructure-error","title":"PHLO-008: Infrastructure Error","text":"<p>Exception: <code>PhloInfrastructureError</code></p> <p>Infrastructure services (Dagster, Trino, S3, etc.) are unavailable.</p> <p>Common causes:</p> <ul> <li>Service not running</li> <li>Connection refused</li> <li>Network issues</li> <li>Authentication failed</li> </ul> <p>Quick fix:</p> <pre><code># Check service status\ndocker ps\ndocker logs dagster-webserver\ndocker logs trino\n\n# Restart services if needed\ndocker-compose restart\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#schema-and-type-errors-phlo-200-to-phlo-299","title":"Schema and Type Errors (PHLO-200 to PHLO-299)","text":"<p>Errors related to schema conversion and type handling.</p>"},{"location":"errors/#phlo-200-schema-conversion-error","title":"PHLO-200: Schema Conversion Error","text":"<p>Exception: <code>SchemaConversionError</code></p> <p>Failed to convert Pandera schema to PyIceberg schema.</p> <p>Common causes:</p> <ul> <li>Unsupported Pandera type</li> <li>Custom type without converter</li> <li>Complex nested types</li> </ul> <p>Quick fix:</p> <pre><code># Use supported Pandera types:\n# str, int, float, bool, datetime, date, bytes\n\nclass MySchema(DataFrameModel):\n    id: str  # \u2705 Supported\n    count: int  # \u2705 Supported\n    value: float  # \u2705 Supported\n    timestamp: datetime  # \u2705 Supported\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-201-type-conversion-error","title":"PHLO-201: Type Conversion Error","text":"<p>Exception: <code>PhloError</code></p> <p>Failed to convert data types during processing.</p> <p>Common causes:</p> <ul> <li>Invalid data format</li> <li>Type mismatch</li> <li>Encoding issues</li> </ul> <p>Quick fix:</p> <pre><code># Explicit type conversion\ndf[\"temperature\"] = pd.to_numeric(df[\"temperature\"], errors=\"coerce\")\ndf[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#dlt-errors-phlo-300-to-phlo-399","title":"DLT Errors (PHLO-300 to PHLO-399)","text":"<p>Errors specific to DLT (Data Load Tool) operations.</p>"},{"location":"errors/#phlo-300-dlt-pipeline-failed","title":"PHLO-300: DLT Pipeline Failed","text":"<p>Exception: <code>DLTPipelineError</code></p> <p>DLT pipeline execution failed.</p> <p>Common causes:</p> <ul> <li>Source connection failed</li> <li>Data transformation error</li> <li>Destination write failed</li> <li>Pipeline configuration invalid</li> </ul> <p>Quick fix:</p> <pre><code># Check DLT logs\nimport dlt\n\n# Enable verbose logging\ndlt.config.log_level = \"DEBUG\"\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-301-dlt-source-error","title":"PHLO-301: DLT Source Error","text":"<p>Exception: <code>PhloError</code></p> <p>DLT source connector failed.</p> <p>Common causes:</p> <ul> <li>Source not accessible</li> <li>Invalid credentials</li> <li>Rate limiting</li> <li>Data format unexpected</li> </ul> <p>Quick fix:</p> <pre><code># Test source connectivity\nsource = my_source()\nfor item in source:\n    print(item)\n    break  # Test first item\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#iceberg-errors-phlo-400-to-phlo-499","title":"Iceberg Errors (PHLO-400 to PHLO-499)","text":"<p>Errors related to Apache Iceberg operations.</p>"},{"location":"errors/#phlo-400-iceberg-catalog-error","title":"PHLO-400: Iceberg Catalog Error","text":"<p>Exception: <code>IcebergCatalogError</code></p> <p>Iceberg catalog operations failed.</p> <p>Common causes:</p> <ul> <li>Catalog not initialized</li> <li>Connection to catalog failed</li> <li>Catalog permissions issue</li> <li>S3/backend unavailable</li> </ul> <p>Quick fix:</p> <pre><code># Verify catalog connectivity\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\nprint(f\"Catalog namespaces: {catalog.list_namespaces()}\")\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-401-iceberg-table-error","title":"PHLO-401: Iceberg Table Error","text":"<p>Exception: <code>PhloError</code></p> <p>Iceberg table operations failed.</p> <p>Common causes:</p> <ul> <li>Table not found</li> <li>Schema mismatch</li> <li>Concurrent modification</li> <li>Permissions issue</li> </ul> <p>Quick fix:</p> <pre><code># List available tables\ncatalog = load_catalog(\"default\")\ntables = catalog.list_tables(\"bronze\")\nprint(f\"Tables in bronze: {tables}\")\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#phlo-402-iceberg-write-error","title":"PHLO-402: Iceberg Write Error","text":"<p>Exception: <code>PhloError</code></p> <p>Failed to write data to Iceberg table.</p> <p>Common causes:</p> <ul> <li>Schema mismatch</li> <li>Partition spec invalid</li> <li>S3 write failed</li> <li>Insufficient permissions</li> </ul> <p>Quick fix:</p> <pre><code># Verify schema matches table\ntable_schema = table.schema()\ndf_schema = df.dtypes\n\nprint(\"Table schema:\", table_schema)\nprint(\"DataFrame schema:\", df_schema)\n</code></pre> <p>Full Documentation \u2192</p>"},{"location":"errors/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"errors/#1-use-structured-error-classes","title":"1. Use Structured Error Classes","text":"<p>Always use Phlo exception classes with error codes:</p> <pre><code>from phlo.exceptions import PhloIngestionError\n\nraise PhloIngestionError(\n    message=\"Failed to fetch weather data\",\n    suggestions=[\n        \"Check API connectivity\",\n        \"Verify API credentials\",\n        \"Review rate limits\",\n    ],\n    cause=original_exception\n)\n</code></pre>"},{"location":"errors/#2-provide-actionable-suggestions","title":"2. Provide Actionable Suggestions","text":"<p>Include specific, actionable suggestions:</p> <pre><code># \u274c Bad: Generic suggestion\nsuggestions=[\"Fix the error\"]\n\n# \u2705 Good: Specific actions\nsuggestions=[\n    \"Check that unique_key 'observation_id' exists in WeatherObservations schema\",\n    \"Available fields: id, station_id, temperature, timestamp\",\n    \"Update unique_key to match one of the available fields\",\n]\n</code></pre>"},{"location":"errors/#3-include-context","title":"3. Include Context","text":"<p>Provide relevant context in error messages:</p> <pre><code># \u274c Bad: No context\nraise PhloError(\"Schema mismatch\")\n\n# \u2705 Good: Detailed context\nraise PhloSchemaError(\n    message=f\"unique_key '{unique_key}' not found in {schema.__name__}\",\n    suggestions=suggest_similar_field_names(unique_key, available_fields)\n)\n</code></pre>"},{"location":"errors/#4-log-errors-appropriately","title":"4. Log Errors Appropriately","text":"<p>Use Dagster context for logging:</p> <pre><code>@phlo_ingestion(...)\ndef my_asset(partition: str, context):\n    try:\n        data = fetch_data(partition)\n        context.log.info(f\"\u2705 Fetched {len(data)} records\")\n        return data\n    except Exception as e:\n        context.log.error(f\"\u274c Ingestion failed: {e}\")\n        raise PhloIngestionError(\n            message=f\"Failed to fetch data for partition {partition}\",\n            cause=e\n        )\n</code></pre>"},{"location":"errors/#5-test-error-paths","title":"5. Test Error Paths","text":"<p>Write tests for error conditions:</p> <pre><code>def test_ingestion_handles_api_failure():\n    with pytest.raises(PhloIngestionError) as exc_info:\n        weather_observations(partition=\"invalid\")\n\n    assert \"API returned 404\" in str(exc_info.value)\n    assert exc_info.value.code == PhloErrorCode.INGESTION_FAILED\n</code></pre>"},{"location":"errors/#getting-help","title":"Getting Help","text":"<p>If you encounter an error not covered in this documentation:</p> <ol> <li>Check Error Code: Look up the PHLO-XXX code in the index above</li> <li>Read Full Documentation: Click the link to read detailed documentation</li> <li>Search Issues: Check GitHub Issues</li> <li>Ask for Help: Create a new issue with:</li> <li>Error code and full error message</li> <li>Steps to reproduce</li> <li>Relevant code snippets</li> <li>Environment details (OS, Python version, Phlo version)</li> </ol>"},{"location":"errors/#contributing","title":"Contributing","text":"<p>Found an error not documented? Help us improve:</p> <ol> <li>Fork the repository</li> <li>Add documentation in <code>docs/errors/PHLO-XXX.md</code></li> <li>Update this index</li> <li>Submit a pull request</li> </ol> <p>Template for new error documentation:</p> <pre><code># PHLO-XXX: Error Name\n\n**Error Type:** Category\n**Severity:** High/Medium/Low\n**Exception Class:** `ExceptionClassName`\n\n## Description\n\n[Clear description of when this error occurs]\n\n## Common Causes\n\n[List of common causes]\n\n## Solutions\n\n[Step-by-step solutions]\n\n## Examples\n\n[Code examples showing incorrect and correct usage]\n\n## Related Errors\n\n[Links to related error codes]\n\n## Prevention\n\n[Best practices to avoid this error]\n</code></pre> <p>Documentation Version: 1.0.0 Last Updated: 2024-01-15 Phlo Version: 0.1.0</p>"},{"location":"errors/PHLO-001/","title":"PHLO-001: Asset Not Discovered","text":"<p>Error Type: Discovery and Configuration Error Severity: High Exception Class: <code>PhloDiscoveryError</code></p>"},{"location":"errors/PHLO-001/#description","title":"Description","text":"<p>This error occurs when Dagster cannot discover your asset definitions. Phlo uses Python decorators like <code>@phlo_ingestion</code> to define assets, and Dagster needs to be able to find and load these definitions.</p>"},{"location":"errors/PHLO-001/#common-causes","title":"Common Causes","text":"<ol> <li>Workflow file not in the workflows path</li> <li>Asset lives outside the configured <code>workflows/</code> directory</li> <li> <p><code>PHLO_WORKFLOWS_PATH</code> points to the wrong location</p> </li> <li> <p>Incorrect decorator usage</p> </li> <li>Missing <code>@phlo_ingestion</code> or <code>@phlo_quality</code> decorator</li> <li> <p>Decorator applied to a non-function object</p> </li> <li> <p>Import errors in asset module</p> </li> <li>Syntax errors preventing module import</li> <li>Missing dependencies</li> <li> <p>Circular import issues</p> </li> <li> <p>Dagster not loading <code>phlo.framework.definitions</code></p> </li> <li>Workspace points at the wrong module</li> <li>Dagster is not using <code>phlo.framework.definitions</code> as the entry point</li> </ol>"},{"location":"errors/PHLO-001/#solutions","title":"Solutions","text":""},{"location":"errors/PHLO-001/#solution-1-verify-workflow-placement","title":"Solution 1: Verify workflow placement","text":"<p>Ensure your workflow module is under the configured workflows path (default: <code>workflows/</code>):</p> <pre><code>workflows/\n\u2514\u2500\u2500 ingestion/\n    \u2514\u2500\u2500 weather/\n        \u2514\u2500\u2500 observations.py\n</code></pre> <p>If you use a custom workflows directory, set <code>PHLO_WORKFLOWS_PATH</code> or update <code>phlo.yaml</code>.</p>"},{"location":"errors/PHLO-001/#solution-2-ensure-dagster-loads-phloframeworkdefinitions","title":"Solution 2: Ensure Dagster loads phlo.framework.definitions","text":"<p>Your Dagster workspace should point at <code>phlo.framework.definitions</code>:</p> <pre><code># workspace.yaml\nload_from:\n  - python_module:\n      module_name: phlo.framework.definitions\n</code></pre>"},{"location":"errors/PHLO-001/#solution-3-check-for-import-errors","title":"Solution 3: Check for import errors","text":"<p>Test that your asset module can be imported:</p> <pre><code>python -c \"from workflows.ingestion.weather.observations import weather_observations_asset\"\n</code></pre> <p>If you see an error, fix the import issue first.</p>"},{"location":"errors/PHLO-001/#solution-4-verify-decorator-usage","title":"Solution 4: Verify decorator usage","text":"<p>Ensure you're using the decorator correctly:</p> <pre><code>import phlo\nfrom workflows.schemas.weather import WeatherObservations\n\n@phlo_ingestion(\n    unique_key=\"observation_id\",\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    # Asset implementation\n    pass\n</code></pre>"},{"location":"errors/PHLO-001/#examples","title":"Examples","text":""},{"location":"errors/PHLO-001/#incorrect-asset-outside-workflows-path","title":"\u274c Incorrect: Asset outside workflows path","text":"<pre><code>custom_assets/observations.py  # Not under workflows/\n</code></pre>"},{"location":"errors/PHLO-001/#correct-asset-under-workflows-path","title":"\u2705 Correct: Asset under workflows path","text":"<pre><code>workflows/ingestion/weather/observations.py\n</code></pre>"},{"location":"errors/PHLO-001/#incorrect-missing-decorator","title":"\u274c Incorrect: Missing decorator","text":"<pre><code>def weather_observations(partition: str):\n    # Missing @phlo_ingestion decorator\n    return fetch_weather_data(partition)\n</code></pre>"},{"location":"errors/PHLO-001/#correct-decorator-applied","title":"\u2705 Correct: Decorator applied","text":"<pre><code>@phlo_ingestion(\n    unique_key=\"observation_id\",\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    return fetch_weather_data(partition)\n</code></pre>"},{"location":"errors/PHLO-001/#debugging-steps","title":"Debugging Steps","text":"<ol> <li>Check Dagster UI logs</li> </ol> <p><code>bash    docker logs dagster-webserver</code></p> <ol> <li>List all discovered assets</li> </ol> <p><code>bash    dagster asset list</code></p> <ol> <li>Test asset import directly</li> </ol> <p><code>python    from workflows.ingestion.weather.observations import weather_observations    print(f\"Asset discovered: {weather_observations}\")</code></p> <ol> <li>Check for circular imports <code>bash    python -m py_compile workflows/ingestion/weather/observations.py</code></li> </ol>"},{"location":"errors/PHLO-001/#related-errors","title":"Related Errors","text":"<ul> <li>PHLO-005: Missing Schema - Schema not provided to decorator</li> <li>PHLO-002: Schema Mismatch - unique_key not in schema</li> </ul>"},{"location":"errors/PHLO-001/#prevention","title":"Prevention","text":"<ol> <li>Use consistent workflow placement</li> <li>Keep assets under <code>workflows/</code></li> <li> <p>Follow the domain-based organization structure</p> </li> <li> <p>Test imports in CI/CD</p> </li> </ol> <p><code>python    # tests/test_asset_discovery.py    def test_all_assets_importable():        from phlo.framework.definitions import defs        assert len(defs.assets) &gt; 0</code></p> <ol> <li>Use IDE auto-imports</li> <li>Let your IDE suggest imports automatically</li> <li>This prevents typos in import paths</li> </ol>"},{"location":"errors/PHLO-001/#additional-resources","title":"Additional Resources","text":"<ul> <li>Dagster Asset Discovery</li> <li>Python Import System</li> <li>Phlo Asset Creation Guide</li> </ul>"},{"location":"errors/PHLO-002/","title":"PHLO-002: Schema Mismatch","text":"<p>Error Type: Discovery and Configuration Error Severity: High Exception Class: <code>PhloSchemaError</code></p>"},{"location":"errors/PHLO-002/#description","title":"Description","text":"<p>This error occurs when there's a mismatch between your decorator configuration and your Pandera schema definition. Most commonly, this happens when the <code>unique_key</code> specified in the decorator doesn't match any field in the <code>validation_schema</code>.</p>"},{"location":"errors/PHLO-002/#common-causes","title":"Common Causes","text":"<ol> <li>unique_key not in schema</li> <li>The field specified as <code>unique_key</code> doesn't exist in your Pandera schema</li> <li> <p>Typo in the field name</p> </li> <li> <p>Schema field type mismatch</p> </li> <li>Field exists but has incompatible type</li> <li> <p>Field is nullable when it should be required</p> </li> <li> <p>Schema version mismatch</p> </li> <li>Using old schema definition</li> <li> <p>Schema was updated but decorator config wasn't</p> </li> <li> <p>Case sensitivity issues</p> </li> <li>Field names are case-sensitive</li> <li><code>observation_id</code> \u2260 <code>Observation_ID</code></li> </ol>"},{"location":"errors/PHLO-002/#solutions","title":"Solutions","text":""},{"location":"errors/PHLO-002/#solution-1-verify-unique_key-exists-in-schema","title":"Solution 1: Verify unique_key exists in schema","text":"<p>Check that your <code>unique_key</code> matches a field in your schema:</p> <pre><code># schemas/weather.py\nfrom pandera import DataFrameModel, Field\n\nclass WeatherObservations(DataFrameModel):\n    observation_id: str = Field(nullable=False)  # \u2705 Field exists\n    station_id: str = Field(nullable=False)\n    temperature: float\n    timestamp: datetime\n\n# ingestion/weather.py\n@phlo_ingestion(\n    unique_key=\"observation_id\",  # \u2705 Matches schema field\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    pass\n</code></pre>"},{"location":"errors/PHLO-002/#solution-2-fix-typos-with-fuzzy-matching","title":"Solution 2: Fix typos with fuzzy matching","text":"<p>Phlo provides helpful suggestions when it detects a typo:</p> <pre><code># Error message includes suggestions:\nPhloSchemaError (PHLO-002): unique_key 'observation_idd' not found in schema\n\nSuggested actions:\n  1. Did you mean 'observation_id'?\n  2. Available fields: observation_id, station_id, temperature, timestamp\n</code></pre>"},{"location":"errors/PHLO-002/#solution-3-use-schema-inspection","title":"Solution 3: Use schema inspection","text":"<p>Verify available fields in your schema:</p> <pre><code>from workflows.schemas.weather import WeatherObservations\nimport pandera as pa\n\nschema = WeatherObservations.to_schema()\nprint(\"Available fields:\", list(schema.columns.keys()))\n# Output: ['observation_id', 'station_id', 'temperature', 'timestamp']\n</code></pre>"},{"location":"errors/PHLO-002/#solution-4-check-field-case-sensitivity","title":"Solution 4: Check field case sensitivity","text":"<p>Ensure exact case match:</p> <pre><code># \u274c Wrong case\nunique_key=\"Observation_ID\"\n\n# \u2705 Correct case\nunique_key=\"observation_id\"\n</code></pre>"},{"location":"errors/PHLO-002/#examples","title":"Examples","text":""},{"location":"errors/PHLO-002/#incorrect-typo-in-unique_key","title":"\u274c Incorrect: Typo in unique_key","text":"<pre><code>@phlo_ingestion(\n    unique_key=\"observation_idd\",  # \u274c Typo: extra 'd'\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    pass\n</code></pre>"},{"location":"errors/PHLO-002/#correct-exact-match","title":"\u2705 Correct: Exact match","text":"<pre><code>@phlo_ingestion(\n    unique_key=\"observation_id\",  # \u2705 Matches schema field exactly\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    pass\n</code></pre>"},{"location":"errors/PHLO-002/#incorrect-field-not-in-schema","title":"\u274c Incorrect: Field not in schema","text":"<pre><code>class WeatherObservations(DataFrameModel):\n    station_id: str\n    temperature: float\n    # \u274c No observation_id field\n\n@phlo_ingestion(\n    unique_key=\"observation_id\",  # \u274c Field doesn't exist\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    pass\n</code></pre>"},{"location":"errors/PHLO-002/#correct-all-fields-defined","title":"\u2705 Correct: All fields defined","text":"<pre><code>class WeatherObservations(DataFrameModel):\n    observation_id: str = Field(nullable=False)  # \u2705 Unique key field defined\n    station_id: str\n    temperature: float\n\n@phlo_ingestion(\n    unique_key=\"observation_id\",  # \u2705 Field exists in schema\n    validation_schema=WeatherObservations,\n)\ndef weather_observations(partition: str):\n    pass\n</code></pre>"},{"location":"errors/PHLO-002/#debugging-steps","title":"Debugging Steps","text":"<ol> <li>List schema fields</li> </ol> <p><code>python    from workflows.schemas.weather import WeatherObservations    schema = WeatherObservations.to_schema()    print(list(schema.columns.keys()))</code></p> <ol> <li>Compare unique_key with schema</li> </ol> <p>```python    unique_key = \"observation_id\"    schema_fields = list(WeatherObservations.to_schema().columns.keys())</p> <p>if unique_key in schema_fields:        print(\"\u2705 unique_key found in schema\")    else:        print(f\"\u274c unique_key '{unique_key}' not in schema\")        print(f\"Available: {schema_fields}\")    ```</p> <ol> <li>Check for case sensitivity</li> </ol> <p>```python    # Case-insensitive search    unique_key = \"Observation_ID\"    schema_fields = list(WeatherObservations.to_schema().columns.keys())</p> <p>matches = [f for f in schema_fields if f.lower() == unique_key.lower()]    if matches:        print(f\"Found case-insensitive match: {matches[0]}\")    ```</p> <ol> <li>Validate schema syntax <code>python    try:        schema = WeatherObservations.to_schema()        print(\"\u2705 Schema is valid\")    except Exception as e:        print(f\"\u274c Schema error: {e}\")</code></li> </ol>"},{"location":"errors/PHLO-002/#related-errors","title":"Related Errors","text":"<ul> <li>PHLO-001: Asset Not Discovered - Asset import issues</li> <li>PHLO-005: Missing Schema - Schema not provided</li> <li>PHLO-200: Schema Conversion Error - Pandera \u2192 PyIceberg conversion fails</li> </ul>"},{"location":"errors/PHLO-002/#prevention","title":"Prevention","text":"<ol> <li>Use constants for field names</li> </ol> <p>```python    # schemas/weather.py    class WeatherObservations(DataFrameModel):        observation_id: str = Field(nullable=False)</p> <p>UNIQUE_KEY = \"observation_id\"  # \u2705 Define constant</p> <p># ingestion/weather.py    from workflows.schemas.weather import WeatherObservations, UNIQUE_KEY</p> <p>@phlo_ingestion(        unique_key=UNIQUE_KEY,  # \u2705 Use constant to avoid typos        validation_schema=WeatherObservations,    )    ```</p> <ol> <li>Add schema validation tests</li> </ol> <p>```python    # tests/test_schemas.py    def test_unique_key_in_schema():        from workflows.schemas.weather import WeatherObservations</p> <pre><code>   schema = WeatherObservations.to_schema()\n   assert \"observation_id\" in schema.columns\n</code></pre> <p>```</p> <ol> <li>Use IDE autocomplete</li> <li>Let your IDE suggest field names from the schema</li> <li> <p>Reduces typos</p> </li> <li> <p>Document schema fields</p> </li> </ol> <p>```python    class WeatherObservations(DataFrameModel):        \"\"\"        Schema for weather observations.</p> <pre><code>   Fields:\n       observation_id: Unique identifier (primary key)\n       station_id: Weather station identifier\n       temperature: Temperature in Celsius\n       timestamp: Observation timestamp\n   \"\"\"\n   observation_id: str = Field(nullable=False)\n   station_id: str = Field(nullable=False)\n   temperature: float\n   timestamp: datetime\n</code></pre> <p>```</p>"},{"location":"errors/PHLO-002/#additional-resources","title":"Additional Resources","text":"<ul> <li>Pandera Documentation</li> <li>Phlo Schema Guide</li> <li>Python String Case Methods</li> </ul>"},{"location":"errors/PHLO-003/","title":"PHLO-003: Invalid Cron Expression","text":"<p>Error Type: Discovery and Configuration Error Severity: Medium Exception Class: <code>PhloCronError</code></p>"},{"location":"errors/PHLO-003/#description","title":"Description","text":"<p>This error occurs when the <code>cron</code> schedule expression provided to the <code>@phlo_ingestion</code> decorator is invalid or malformed. Phlo validates cron expressions to ensure assets are scheduled correctly.</p>"},{"location":"errors/PHLO-003/#common-causes","title":"Common Causes","text":"<ol> <li>Invalid cron syntax</li> <li>Wrong number of fields (must be 5 fields)</li> <li>Invalid field values</li> <li> <p>Unsupported special characters</p> </li> <li> <p>Incorrect field order</p> </li> <li>Fields must be: minute hour day_of_month month day_of_week</li> <li> <p>Common mistake: swapping day_of_month and month</p> </li> <li> <p>Invalid ranges</p> </li> <li>Values outside valid ranges (e.g., hour=25)</li> <li> <p>Invalid step values (e.g., */0)</p> </li> <li> <p>Quoting issues</p> </li> <li>Missing quotes around cron expression</li> <li>Extra quotes or escape characters</li> </ol>"},{"location":"errors/PHLO-003/#cron-expression-format","title":"Cron Expression Format","text":"<pre><code> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of week (0 - 6) (Sunday=0)\n \u2502 \u2502 \u2502 \u2502 \u2502\n * * * * *\n</code></pre>"},{"location":"errors/PHLO-003/#solutions","title":"Solutions","text":""},{"location":"errors/PHLO-003/#solution-1-use-standard-cron-syntax","title":"Solution 1: Use standard cron syntax","text":"<p>Ensure your cron expression has 5 fields with valid values:</p> <pre><code>@phlo_ingestion(\n    unique_key=\"observation_id\",\n    validation_schema=WeatherObservations,\n    cron=\"0 */1 * * *\",  # \u2705 Valid: Every hour at minute 0\n)\ndef weather_observations(partition: str):\n    pass\n</code></pre>"},{"location":"errors/PHLO-003/#solution-2-test-on-crontabguru","title":"Solution 2: Test on crontab.guru","text":"<p>Before using a cron expression, test it at crontab.guru:</p> <pre><code># Test this expression: 0 */1 * * *\n# crontab.guru will show: \"At minute 0 past every hour\"\n</code></pre>"},{"location":"errors/PHLO-003/#solution-3-use-common-schedules","title":"Solution 3: Use common schedules","text":"<p>Use these verified common schedules:</p> <pre><code># Every hour\ncron=\"0 */1 * * *\"\n\n# Every day at midnight\ncron=\"0 0 * * *\"\n\n# Every day at 2 AM\ncron=\"0 2 * * *\"\n\n# Every Monday at 9 AM\ncron=\"0 9 * * 1\"\n\n# Every 15 minutes\ncron=\"*/15 * * * *\"\n\n# Twice daily (6 AM and 6 PM)\ncron=\"0 6,18 * * *\"\n</code></pre>"},{"location":"errors/PHLO-003/#solution-4-quote-the-expression","title":"Solution 4: Quote the expression","text":"<p>Always quote your cron expression:</p> <pre><code># \u2705 Correct\ncron=\"0 */1 * * *\"\n\n# \u274c Wrong (no quotes)\ncron=0 */1 * * *  # SyntaxError!\n</code></pre>"},{"location":"errors/PHLO-003/#examples","title":"Examples","text":""},{"location":"errors/PHLO-003/#incorrect-wrong-number-of-fields","title":"\u274c Incorrect: Wrong number of fields","text":"<pre><code>@phlo_ingestion(\n    cron=\"0 */1 * *\",  # \u274c Only 4 fields (need 5)\n    ...\n)\n</code></pre>"},{"location":"errors/PHLO-003/#correct-5-fields","title":"\u2705 Correct: 5 fields","text":"<pre><code>@phlo_ingestion(\n    cron=\"0 */1 * * *\",  # \u2705 All 5 fields present\n    ...\n)\n</code></pre>"},{"location":"errors/PHLO-003/#incorrect-invalid-hour-value","title":"\u274c Incorrect: Invalid hour value","text":"<pre><code>@phlo_ingestion(\n    cron=\"0 25 * * *\",  # \u274c Hour 25 is invalid (max is 23)\n    ...\n)\n</code></pre>"},{"location":"errors/PHLO-003/#correct-valid-hour-value","title":"\u2705 Correct: Valid hour value","text":"<pre><code>@phlo_ingestion(\n    cron=\"0 23 * * *\",  # \u2705 Hour 23 is valid (11 PM)\n    ...\n)\n</code></pre>"},{"location":"errors/PHLO-003/#incorrect-invalid-step-value","title":"\u274c Incorrect: Invalid step value","text":"<pre><code>@phlo_ingestion(\n    cron=\"*/0 * * * *\",  # \u274c Step value 0 is invalid\n    ...\n)\n</code></pre>"},{"location":"errors/PHLO-003/#correct-valid-step-value","title":"\u2705 Correct: Valid step value","text":"<pre><code>@phlo_ingestion(\n    cron=\"*/5 * * * *\",  # \u2705 Every 5 minutes\n    ...\n)\n</code></pre>"},{"location":"errors/PHLO-003/#cron-field-ranges","title":"Cron Field Ranges","text":"Field Valid Values Special Characters minute 0-59 <code>*</code> <code>,</code> <code>-</code> <code>/</code> hour 0-23 <code>*</code> <code>,</code> <code>-</code> <code>/</code> day_of_month 1-31 <code>*</code> <code>,</code> <code>-</code> <code>/</code> <code>?</code> <code>L</code> <code>W</code> month 1-12 <code>*</code> <code>,</code> <code>-</code> <code>/</code> day_of_week 0-6 (Sun=0) <code>*</code> <code>,</code> <code>-</code> <code>/</code> <code>?</code> <code>L</code> <code>#</code>"},{"location":"errors/PHLO-003/#common-patterns","title":"Common Patterns","text":""},{"location":"errors/PHLO-003/#hourly-schedules","title":"Hourly Schedules","text":"<pre><code>cron=\"0 */1 * * *\"     # Every hour\ncron=\"0 */2 * * *\"     # Every 2 hours\ncron=\"0 */6 * * *\"     # Every 6 hours\ncron=\"30 */1 * * *\"    # Every hour at :30\n</code></pre>"},{"location":"errors/PHLO-003/#daily-schedules","title":"Daily Schedules","text":"<pre><code>cron=\"0 0 * * *\"       # Midnight\ncron=\"0 6 * * *\"       # 6 AM\ncron=\"0 12 * * *\"      # Noon\ncron=\"0 18 * * *\"      # 6 PM\n</code></pre>"},{"location":"errors/PHLO-003/#weekly-schedules","title":"Weekly Schedules","text":"<pre><code>cron=\"0 0 * * 0\"       # Sunday midnight\ncron=\"0 9 * * 1\"       # Monday 9 AM\ncron=\"0 0 * * 5\"       # Friday midnight\n</code></pre>"},{"location":"errors/PHLO-003/#monthly-schedules","title":"Monthly Schedules","text":"<pre><code>cron=\"0 0 1 * *\"       # First day of month\ncron=\"0 0 15 * *\"      # 15th of month\ncron=\"0 0 L * *\"       # Last day of month (if supported)\n</code></pre>"},{"location":"errors/PHLO-003/#debugging-steps","title":"Debugging Steps","text":"<ol> <li>Validate cron syntax</li> </ol> <p>```python    from croniter import croniter    from datetime import datetime</p> <p>try:        cron_expr = \"0 /1 * * \"        base = datetime.now()        iter = croniter(cron_expr, base)        print(f\"\u2705 Valid cron: next run at {iter.get_next(datetime)}\")    except Exception as e:        print(f\"\u274c Invalid cron: {e}\")    ```</p> <ol> <li>Test on crontab.guru</li> </ol> <p><code>Visit: https://crontab.guru/    Enter: 0 */1 * * *    Verify: \"At minute 0 past every hour\"</code></p> <ol> <li>Check Dagster schedule</li> </ol> <p><code>bash    dagster schedule list    dagster schedule logs my_schedule</code></p> <ol> <li>Verify schedule in UI</li> <li>Open Dagster UI</li> <li>Navigate to Schedules</li> <li>Check if your schedule appears</li> <li>View next scheduled run time</li> </ol>"},{"location":"errors/PHLO-003/#related-errors","title":"Related Errors","text":"<ul> <li>PHLO-001: Asset Not Discovered - Asset with schedule not found</li> <li>PHLO-005: Missing Schema - Schema required for scheduled assets</li> </ul>"},{"location":"errors/PHLO-003/#prevention","title":"Prevention","text":"<ol> <li>Use constants for common schedules</li> </ol> <p>```python    # config.py    HOURLY = \"0 /1 * * \"    DAILY = \"0 0 * * *\"    WEEKLY = \"0 0 * * 0\"</p> <p># ingestion.py    from config import HOURLY</p> <p>@phlo_ingestion(        cron=HOURLY,  # \u2705 Use constant        ...    )    ```</p> <ol> <li>Add cron validation tests</li> </ol> <p>```python    # tests/test_schedules.py    from croniter import croniter</p> <p>def test_cron_expressions_valid():        schedules = [            \"0 /1 * * \",  # Hourly            \"0 0 * * *\",    # Daily        ]</p> <pre><code>   for cron_expr in schedules:\n       try:\n           croniter(cron_expr)\n       except Exception as e:\n           pytest.fail(f\"Invalid cron '{cron_expr}': {e}\")\n</code></pre> <p>```</p> <ol> <li>Document schedule rationale</li> </ol> <p>```python    @phlo_ingestion(        unique_key=\"observation_id\",        validation_schema=WeatherObservations,        cron=\"0 /1 * * \",  # Run hourly to match API update frequency    )    def weather_observations(partition: str):        \"\"\"        Fetch weather observations every hour.</p> <pre><code>   Schedule: Hourly (0 */1 * * *)\n   Rationale: Weather API updates hourly at :00\n   \"\"\"\n   pass\n</code></pre> <p>```</p> <ol> <li>Use schedule builders</li> </ol> <p>```python    def hourly(minute: int = 0) -&gt; str:        \"\"\"Generate hourly cron expression.\"\"\"        return f\"{minute} /1 * * \"</p> <p>def daily(hour: int = 0, minute: int = 0) -&gt; str:        \"\"\"Generate daily cron expression.\"\"\"        return f\"{minute} {hour} * * *\"</p> <p>@phlo_ingestion(        cron=hourly(minute=0),  # \u2705 Type-safe schedule builder        ...    )    ```</p>"},{"location":"errors/PHLO-003/#additional-resources","title":"Additional Resources","text":"<ul> <li>Crontab Guru - Interactive cron expression tester</li> <li>Cron Wikipedia - Cron format specification</li> <li>Dagster Schedules - Dagster scheduling guide</li> <li>croniter Library - Python cron iterator</li> </ul>"},{"location":"errors/PHLO-006/","title":"PHLO-006: Ingestion Failed","text":"<p>Error Type: Runtime and Integration Error Severity: High Exception Class: <code>PhloIngestionError</code></p>"},{"location":"errors/PHLO-006/#description","title":"Description","text":"<p>This error occurs when data ingestion fails during asset execution. This can happen due to API failures, network issues, authentication problems, or data processing errors.</p>"},{"location":"errors/PHLO-006/#common-causes","title":"Common Causes","text":"<ol> <li>API failures</li> <li>API endpoint unreachable</li> <li>API rate limiting</li> <li>API authentication failed</li> <li> <p>API returned error response</p> </li> <li> <p>Network issues</p> </li> <li>Connection timeout</li> <li>DNS resolution failed</li> <li> <p>Firewall blocking requests</p> </li> <li> <p>Data processing errors</p> </li> <li>Invalid data format from source</li> <li>Data transformation failed</li> <li> <p>Schema validation failed</p> </li> <li> <p>Resource exhaustion</p> </li> <li>Out of memory</li> <li>Disk space full</li> <li>Connection pool exhausted</li> </ol>"},{"location":"errors/PHLO-006/#solutions","title":"Solutions","text":""},{"location":"errors/PHLO-006/#solution-1-check-api-connectivity","title":"Solution 1: Check API connectivity","text":"<p>Verify the API is reachable and responding:</p> <pre><code>import requests\n\ntry:\n    response = requests.get(\"https://api.example.com/status\", timeout=5)\n    response.raise_for_status()\n    print(f\"\u2705 API reachable: {response.status_code}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"\u274c API unreachable: {e}\")\n</code></pre>"},{"location":"errors/PHLO-006/#solution-2-implement-retry-logic","title":"Solution 2: Implement retry logic","text":"<p>Add retry logic for transient failures:</p> <pre><code>from requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\ndef create_session_with_retries():\n    session = requests.Session()\n\n    # Retry on 500, 502, 503, 504\n    retry = Retry(\n        total=3,\n        backoff_factor=1,\n        status_forcelist=[500, 502, 503, 504],\n    )\n\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    return session\n\n# Use in asset\n@phlo_ingestion(...)\ndef weather_observations(partition: str):\n    session = create_session_with_retries()\n    response = session.get(f\"https://api.weather.com/observations/{partition}\")\n    return response.json()\n</code></pre>"},{"location":"errors/PHLO-006/#solution-3-validate-api-authentication","title":"Solution 3: Validate API authentication","text":"<p>Ensure API credentials are valid:</p> <pre><code>import os\n\ndef validate_api_credentials():\n    api_key = os.getenv(\"WEATHER_API_KEY\")\n\n    if not api_key:\n        raise PhloIngestionError(\n            message=\"WEATHER_API_KEY environment variable not set\",\n            suggestions=[\n                \"Set WEATHER_API_KEY environment variable\",\n                \"Check .phlo/.env.local file exists and is loaded\",\n                \"Verify environment variable name spelling\",\n            ]\n        )\n\n    # Test credentials\n    response = requests.get(\n        \"https://api.weather.com/auth/test\",\n        headers={\"Authorization\": f\"Bearer {api_key}\"}\n    )\n\n    if response.status_code == 401:\n        raise PhloIngestionError(\n            message=\"API authentication failed\",\n            suggestions=[\n                \"Verify API key is valid and not expired\",\n                \"Check API key has correct permissions\",\n                \"Generate new API key from provider dashboard\",\n            ]\n        )\n\n@phlo_ingestion(...)\ndef weather_observations(partition: str):\n    validate_api_credentials()\n    # ... fetch data\n</code></pre>"},{"location":"errors/PHLO-006/#solution-4-add-error-handling-and-logging","title":"Solution 4: Add error handling and logging","text":"<p>Wrap data fetching in try/except with detailed logging:</p> <pre><code>@phlo_ingestion(...)\ndef weather_observations(partition: str, context):\n    try:\n        context.log.info(f\"Fetching data for partition: {partition}\")\n\n        response = requests.get(\n            f\"https://api.weather.com/observations/{partition}\",\n            timeout=30\n        )\n\n        context.log.info(f\"API response status: {response.status_code}\")\n\n        response.raise_for_status()\n\n        data = response.json()\n        context.log.info(f\"Fetched {len(data)} records\")\n\n        return data\n\n    except requests.exceptions.Timeout:\n        raise PhloIngestionError(\n            message=f\"API request timed out for partition {partition}\",\n            suggestions=[\n                \"Increase timeout value (currently 30s)\",\n                \"Check API endpoint performance\",\n                \"Verify network connectivity\",\n            ]\n        )\n\n    except requests.exceptions.HTTPError as e:\n        raise PhloIngestionError(\n            message=f\"API returned error: {e.response.status_code}\",\n            suggestions=[\n                f\"Check API documentation for status code {e.response.status_code}\",\n                \"Verify request parameters are correct\",\n                \"Check API rate limits\",\n            ],\n            cause=e\n        )\n\n    except Exception as e:\n        raise PhloIngestionError(\n            message=f\"Unexpected error during ingestion: {type(e).__name__}\",\n            suggestions=[\n                \"Check logs for full stack trace\",\n                \"Verify data format matches expectations\",\n                \"Test with smaller date range\",\n            ],\n            cause=e\n        )\n</code></pre>"},{"location":"errors/PHLO-006/#examples","title":"Examples","text":""},{"location":"errors/PHLO-006/#incorrect-no-error-handling","title":"\u274c Incorrect: No error handling","text":"<pre><code>@phlo_ingestion(...)\ndef weather_observations(partition: str):\n    # \u274c No error handling - will fail silently\n    response = requests.get(f\"https://api.weather.com/obs/{partition}\")\n    return response.json()\n</code></pre>"},{"location":"errors/PHLO-006/#correct-comprehensive-error-handling","title":"\u2705 Correct: Comprehensive error handling","text":"<pre><code>@phlo_ingestion(...)\ndef weather_observations(partition: str, context):\n    try:\n        context.log.info(f\"Fetching data for {partition}\")\n\n        response = requests.get(\n            f\"https://api.weather.com/obs/{partition}\",\n            timeout=30,\n            headers={\"Authorization\": f\"Bearer {os.getenv('API_KEY')}\"}\n        )\n\n        response.raise_for_status()\n\n        data = response.json()\n\n        if not data:\n            raise ValueError(\"API returned empty response\")\n\n        context.log.info(f\"\u2705 Fetched {len(data)} records\")\n        return data\n\n    except Exception as e:\n        context.log.error(f\"\u274c Ingestion failed: {e}\")\n        raise PhloIngestionError(\n            message=f\"Failed to fetch weather data for {partition}\",\n            suggestions=[\n                \"Check API connectivity\",\n                \"Verify API credentials\",\n                \"Review API rate limits\",\n            ],\n            cause=e\n        )\n</code></pre>"},{"location":"errors/PHLO-006/#debugging-steps","title":"Debugging Steps","text":"<ol> <li>Check API status</li> </ol> <p><code>bash    curl -I https://api.example.com/status</code></p> <ol> <li>Test API credentials</li> </ol> <p><code>bash    export API_KEY=\"your-api-key\"    curl -H \"Authorization: Bearer $API_KEY\" https://api.example.com/test</code></p> <ol> <li>Review Dagster logs</li> </ol> <p><code>bash    docker logs dagster-webserver    docker logs dagster-daemon</code></p> <ol> <li>Test asset locally</li> </ol> <p>```python    from workflows.ingestion.weather.observations import weather_observations</p> <p># Test with specific partition    result = weather_observations(partition=\"2024-01-15\")    print(f\"Fetched {len(result)} records\")    ```</p> <ol> <li>Check network connectivity</li> </ol> <p><code>bash    ping api.example.com    nslookup api.example.com    traceroute api.example.com</code></p> <ol> <li>Monitor API rate limits</li> </ol> <p>```python    response = requests.get(url, headers=headers)</p> <p># Check rate limit headers    remaining = response.headers.get('X-RateLimit-Remaining')    reset_time = response.headers.get('X-RateLimit-Reset')</p> <p>print(f\"Rate limit remaining: {remaining}\")    print(f\"Rate limit resets at: {reset_time}\")    ```</p>"},{"location":"errors/PHLO-006/#common-api-error-codes","title":"Common API Error Codes","text":"Status Code Meaning Solution 400 Bad Request Check request parameters 401 Unauthorized Verify API credentials 403 Forbidden Check API permissions 404 Not Found Verify endpoint URL 429 Too Many Requests Implement rate limiting 500 Internal Server Error Retry with backoff 503 Service Unavailable Wait and retry 504 Gateway Timeout Increase timeout"},{"location":"errors/PHLO-006/#related-errors","title":"Related Errors","text":"<ul> <li>PHLO-008: Infrastructure Error - Infrastructure services unavailable</li> <li>PHLO-300: DLT Pipeline Failed - DLT-specific ingestion errors</li> <li>PHLO-004: Validation Failed - Data validation errors</li> </ul>"},{"location":"errors/PHLO-006/#prevention","title":"Prevention","text":"<ol> <li>Implement health checks</li> </ol> <p>```python    def check_api_health():        try:            response = requests.get(\"https://api.example.com/health\", timeout=5)            return response.status_code == 200        except:            return False</p> <p>@phlo_ingestion(...)    def weather_observations(partition: str):        if not check_api_health():            raise PhloIngestionError(                message=\"API health check failed\",                suggestions=[\"Check API status page\", \"Contact API provider\"]            )        # ... proceed with ingestion    ```</p> <ol> <li>Add monitoring and alerting</li> </ol> <p>```python    from dagster import MetadataValue</p> <p>@phlo_ingestion(...)    def weather_observations(partition: str, context):        start_time = time.time()</p> <pre><code>   try:\n       data = fetch_data(partition)\n       duration = time.time() - start_time\n\n       context.log_event(\n           AssetMaterialization(\n               asset_key=context.asset_key,\n               metadata={\n                   \"records_fetched\": len(data),\n                   \"duration_seconds\": duration,\n                   \"partition\": partition,\n               }\n           )\n       )\n\n       return data\n   except Exception as e:\n       # Log failure for monitoring\n       context.log.error(f\"Ingestion failed after {time.time() - start_time}s\")\n       raise\n</code></pre> <p>```</p> <ol> <li>Use circuit breaker pattern</li> </ol> <p>```python    from pybreaker import CircuitBreaker</p> <p>breaker = CircuitBreaker(fail_max=5, timeout_duration=60)</p> <p>@phlo_ingestion(...)    def weather_observations(partition: str):        @breaker        def fetch():            return requests.get(url).json()</p> <pre><code>   return fetch()\n</code></pre> <p>```</p> <ol> <li>Test with mock data</li> </ol> <p>```python    # tests/test_weather_ingestion.py    from phlo_testing import mock_dlt_source</p> <p>def test_weather_ingestion():        mock_data = [            {\"station_id\": \"KSFO\", \"temperature\": 18.5, ...}        ]</p> <pre><code>   with mock_dlt_source(data=mock_data) as source:\n       result = weather_observations(partition=\"2024-01-15\")\n       assert len(result) &gt; 0\n</code></pre> <p>```</p>"},{"location":"errors/PHLO-006/#additional-resources","title":"Additional Resources","text":"<ul> <li>Requests Library Documentation</li> <li>HTTP Status Codes</li> <li>API Rate Limiting Best Practices</li> <li>Circuit Breaker Pattern</li> </ul>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding the fundamental concepts and patterns that make Phlo a modern data lakehouse platform.</p>"},{"location":"getting-started/core-concepts/#what-is-phlo","title":"What is Phlo?","text":"<p>Phlo is a data lakehouse framework that combines best-in-class tools into a cohesive, low-boilerplate platform for data engineering. It provides:</p> <ul> <li>74% less code compared to manual integration</li> <li>Git-like workflows for data versioning and branching</li> <li>Type-safe data quality with automatic validation</li> <li>Production-ready patterns out of the box</li> </ul>"},{"location":"getting-started/core-concepts/#architecture-overview","title":"Architecture Overview","text":""},{"location":"getting-started/core-concepts/#the-stack","title":"The Stack","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Dagster (Orchestration)         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  DLT      \u2502  dbt    \u2502  Pandera          \u2502\n\u2502 (Ingest)  \u2502 (Trans) \u2502  (Quality)        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Trino (Query Engine)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Iceberg (Table Format) | Nessie (Cat.) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  MinIO (S3 Storage) | PostgreSQL        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Storage: MinIO provides S3-compatible object storage for data files and Iceberg metadata.</p> <p>Catalog: Nessie acts as a Git-like catalog for versioning table metadata with branches and tags.</p> <p>Table Format: Apache Iceberg provides ACID transactions, schema evolution, and time travel.</p> <p>Query Engine: Trino executes distributed SQL queries across Iceberg tables.</p> <p>Transformation: dbt handles SQL-based transformations following bronze/silver/gold architecture.</p> <p>Ingestion: DLT (Data Load Tool) handles loading data from external sources.</p> <p>Quality: Pandera provides DataFrame validation with type-safe schemas.</p> <p>Orchestration: Dagster manages the entire workflow with asset-based orchestration.</p>"},{"location":"getting-started/core-concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/core-concepts/#1-write-audit-publish-wap-pattern","title":"1. Write-Audit-Publish (WAP) Pattern","text":"<p>Phlo implements an automated Write-Audit-Publish pattern using Nessie branches:</p> <p>Write Phase</p> <ul> <li>Data lands on isolated branch: <code>pipeline/run-{run_id}</code></li> <li>No impact on production <code>main</code> branch</li> <li>Multiple pipelines can run concurrently</li> </ul> <p>Audit Phase</p> <ul> <li>Quality checks validate data</li> <li>Dagster asset checks execute automatically</li> <li>Failures prevent promotion</li> </ul> <p>Publish Phase</p> <ul> <li>Auto-promotion sensor merges to <code>main</code> when checks pass</li> <li>Atomic commit of all tables</li> <li>Old branches cleaned up after retention period</li> </ul> <p>Implementation</p> <pre><code># Automatic branch creation on job start\n# workflows/sensors/branch_lifecycle.py\n@sensor(name=\"branch_creation_sensor\")\ndef branch_creation_sensor(context):\n    # Creates pipeline/run-{id} branch\n\n# Automatic promotion when checks pass\n@sensor(name=\"auto_promotion_sensor\")\ndef auto_promotion_sensor(context):\n    # Merges to main if all checks pass\n\n# Cleanup old branches\n@sensor(name=\"branch_cleanup_sensor\")\ndef branch_cleanup_sensor(context):\n    # Deletes branches older than retention period\n</code></pre>"},{"location":"getting-started/core-concepts/#2-decorator-driven-development","title":"2. Decorator-Driven Development","text":"<p>Phlo reduces boilerplate through powerful decorators that auto-generate Dagster assets.</p>"},{"location":"getting-started/core-concepts/#phlo_ingestion-decorator","title":"@phlo_ingestion Decorator","text":"<p>Transforms a simple function into a complete ingestion pipeline:</p> <pre><code>@phlo_ingestion(\n    table_name=\"events\",\n    unique_key=\"id\",\n    validation_schema=EventSchema,  # Pandera schema\n    group=\"api\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n    merge_strategy=\"merge\"\n)\ndef api_events(partition_date: str):\n    return rest_api(...)  # DLT source\n</code></pre> <p>What it does:</p> <ol> <li>Creates Dagster asset with partitioning</li> <li>Sets up DLT pipeline with filesystem destination</li> <li>Stages data to Parquet files</li> <li>Auto-generates Iceberg schema from Pandera schema</li> <li>Merges to Iceberg table with deduplication</li> <li>Validates with Pandera schema</li> <li>Handles retries and timeouts</li> <li>Tracks metrics and timing</li> </ol> <p>Without decorator (manual):</p> <pre><code># Would require ~270 lines of boilerplate:\n# - DLT pipeline setup (~50 lines)\n# - Iceberg schema definition (~40 lines)\n# - Merge logic (~60 lines)\n# - Error handling (~40 lines)\n# - Timing/logging (~30 lines)\n# - Dagster asset wrapper (~50 lines)\n</code></pre>"},{"location":"getting-started/core-concepts/#phlo_quality-decorator","title":"@phlo_quality Decorator","text":"<p>Creates data quality checks:</p> <pre><code>@phlo_quality(\n    table=\"bronze.events\",\n    checks=[\n        NullCheck(columns=[\"id\", \"timestamp\"]),\n        RangeCheck(column=\"value\", min_value=0, max_value=100),\n        UniqueCheck(columns=[\"id\"]),\n        FreshnessCheck(column=\"timestamp\", max_age_hours=24)\n    ]\n)\ndef events_quality():\n    pass\n</code></pre> <p>Built-in check types:</p> <ul> <li><code>NullCheck</code>: No null values in columns</li> <li><code>RangeCheck</code>: Numeric values within bounds</li> <li><code>FreshnessCheck</code>: Data recency validation</li> <li><code>UniqueCheck</code>: No duplicate values</li> <li><code>CountCheck</code>: Row count validation</li> <li><code>SchemaCheck</code>: Full Pandera schema validation</li> <li><code>CustomSQLCheck</code>: Arbitrary SQL validation</li> </ul> <p>Quality check contract (for Observatory):</p> <ul> <li>Pandera schema contract checks use the name <code>pandera_contract</code></li> <li>dbt test checks use the name <code>dbt__&lt;test_type&gt;__&lt;target&gt;</code></li> <li>Checks should emit metadata keys: <code>source</code>, <code>partition_key</code> (if applicable), <code>failed_count</code>, <code>total_count</code> (if available), <code>query_or_sql</code> (if applicable), <code>sample</code> (&lt;= 20 rows/ids)</li> <li>Checks may also emit <code>repro_sql</code> (a safe SQL snippet, e.g. with <code>LIMIT</code>, for Trino reproduction).</li> <li>Partitioned runs scope checks to the run partition by default (using <code>_phlo_partition_date</code> unless overridden).</li> </ul>"},{"location":"getting-started/core-concepts/#3-schema-first-development","title":"3. Schema-First Development","text":"<p>Pandera schemas serve as the source of truth for data structure and quality:</p> <pre><code>import pandera as pa\nfrom pandera.typing import Series\n\nclass EventSchema(pa.DataFrameModel):\n    \"\"\"Event data schema.\"\"\"\n\n    id: Series[str] = pa.Field(\n        description=\"Unique event ID\",\n        nullable=False,\n        unique=True\n    )\n\n    timestamp: Series[datetime] = pa.Field(\n        description=\"Event timestamp\",\n        nullable=False\n    )\n\n    value: Series[float] = pa.Field(\n        description=\"Event value\",\n        ge=0,\n        le=100\n    )\n\n    category: Series[str] = pa.Field(\n        description=\"Event category\",\n        isin=[\"A\", \"B\", \"C\"]\n    )\n\n    class Config:\n        strict = True\n        coerce = True\n</code></pre> <p>Benefits:</p> <ul> <li>Type safety at runtime</li> <li>Auto-generated Iceberg schemas</li> <li>Validation enforced automatically</li> <li>Self-documenting data contracts</li> <li>IDE autocomplete support</li> </ul> <p>Schema Conversion (Pandera \u2192 Iceberg):</p> <pre><code># packages/phlo-dlt/src/phlo_dlt/converter.py\nstr \u2192 StringType()\nint \u2192 LongType()\nfloat \u2192 DoubleType()\ndatetime \u2192 TimestamptzType()\nbool \u2192 BooleanType()\n</code></pre>"},{"location":"getting-started/core-concepts/#4-merge-strategies","title":"4. Merge Strategies","text":"<p>Phlo supports flexible merge strategies for handling updates:</p> <p>Append Strategy</p> <pre><code>merge_strategy=\"append\"\n</code></pre> <ul> <li>Insert-only, no deduplication</li> <li>Fastest performance</li> <li>Use for immutable event streams</li> </ul> <p>Merge Strategy</p> <pre><code>merge_strategy=\"merge\"\nmerge_config={\"deduplication_method\": \"last\"}  # or \"first\" or \"hash\"\n</code></pre> <ul> <li>Upsert based on <code>unique_key</code></li> <li>Deduplication strategies:</li> <li><code>first</code>: Keep first occurrence</li> <li><code>last</code>: Keep last occurrence (default)</li> <li><code>hash</code>: Keep based on content hash</li> </ul> <p>Implementation:</p> <pre><code># packages/phlo-dlt/src/phlo_dlt/dlt_helpers.py\ndef merge_to_iceberg(\n    table: Table,\n    new_data: DataFrame,\n    unique_key: str,\n    strategy: str = \"merge\"\n):\n    if strategy == \"append\":\n        # Fast path - just append\n        table.append(new_data)\n    else:\n        # Upsert with deduplication\n        table.merge(\n            new_data,\n            on=unique_key,\n            action=\"upsert\"\n        )\n</code></pre>"},{"location":"getting-started/core-concepts/#5-partition-management","title":"5. Partition Management","text":"<p>Daily partitioning for efficient querying:</p> <pre><code>from dagster import daily_partitioned_config\n\n@daily_partitioned_config(start_date=\"2024-01-01\")\ndef partition_config(start, end):\n    return {\n        \"partition_date\": start.strftime(\"%Y-%m-%d\")\n    }\n</code></pre> <p>Benefits:</p> <ul> <li>Partition pruning for faster queries</li> <li>Incremental processing</li> <li>Time-based data management</li> <li>Backfill support</li> </ul>"},{"location":"getting-started/core-concepts/#6-bronzesilvergold-architecture","title":"6. Bronze/Silver/Gold Architecture","text":"<p>Phlo follows medallion architecture for data transformation:</p> <p>Bronze Layer (Raw)</p> <ul> <li>Ingested data from sources</li> <li>Minimal transformation</li> <li>Schema validated with Pandera</li> <li>Tables: <code>bronze.{table_name}</code></li> </ul> <p>Silver Layer (Cleaned)</p> <ul> <li>Cleaned and conformed data</li> <li>Type conversions, deduplication</li> <li>Business logic applied</li> <li>Tables: <code>silver.{table_name}</code></li> </ul> <p>Gold Layer (Marts)</p> <ul> <li>Aggregated, business-ready data</li> <li>Optimized for BI tools</li> <li>Published to PostgreSQL</li> <li>Tables: <code>marts.{table_name}</code></li> </ul> <p>dbt Implementation:</p> <pre><code>-- models/bronze/stg_events.sql\n{{ config(\n    materialized='incremental',\n    unique_key='id',\n    on_schema_change='append_new_columns'\n) }}\n\nSELECT\n    id,\n    timestamp,\n    value,\n    category\nFROM {{ source('raw', 'events') }}\n\n-- models/silver/events_cleaned.sql\nSELECT\n    id,\n    timestamp,\n    COALESCE(value, 0) as value,\n    UPPER(category) as category\nFROM {{ ref('stg_events') }}\n\n-- models/gold/daily_aggregates.sql\nSELECT\n    DATE(timestamp) as date,\n    category,\n    COUNT(*) as event_count,\n    AVG(value) as avg_value\nFROM {{ ref('events_cleaned') }}\nGROUP BY 1, 2\n</code></pre>"},{"location":"getting-started/core-concepts/#7-branch-aware-operations","title":"7. Branch-Aware Operations","text":"<p>All operations are branch-aware through Nessie:</p> <pre><code># Get current branch from Dagster context\nbranch = get_branch_from_context(context)\n\n# Write to branch-specific reference\ntable.write(\n    data,\n    override_ref=branch  # e.g., \"pipeline/run-abc123\"\n)\n\n# Query from specific branch\ndf = trino.execute(\n    \"SELECT * FROM events\",\n    catalog_options={\"ref\": branch}\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#8-asset-based-orchestration","title":"8. Asset-Based Orchestration","text":"<p>Dagster assets represent data products:</p> <pre><code>@asset(\n    partitions_def=daily_partition,\n    freshness_policy=FreshnessPolicy(\n        maximum_lag_minutes=120\n    ),\n    auto_materialize_policy=AutoMaterializePolicy.eager()\n)\ndef my_asset(context):\n    # Asset implementation\n    pass\n</code></pre> <p>Benefits:</p> <ul> <li>Automatic lineage tracking</li> <li>Partition-aware dependencies</li> <li>Freshness monitoring</li> <li>Smart materialization</li> </ul>"},{"location":"getting-started/core-concepts/#9-quality-gates","title":"9. Quality Gates","text":"<p>Quality checks act as gates in the pipeline:</p> <pre><code># Quality check blocks downstream assets\n@asset_check(asset=bronze_events)\ndef events_quality_check():\n    # Run validation\n    if not valid:\n        raise Exception(\"Quality check failed\")\n    return CheckResult(passed=True)\n\n# Downstream asset only runs if check passes\n@asset(deps=[bronze_events])\ndef silver_events():\n    # Only executes if events_quality_check passed\n    pass\n</code></pre>"},{"location":"getting-started/core-concepts/#10-publishing-pattern","title":"10. Publishing Pattern","text":"<p>Automatic publishing of marts to PostgreSQL for BI:</p> <pre><code># workflows/publishing/trino_to_postgres.py\n@asset(deps=[marts.daily_aggregates])\ndef publish_daily_aggregates(context, trino, postgres):\n    _publish_marts_to_postgres(\n        context, trino,\n        tables_to_publish={\n            \"daily_aggregates\": \"marts.daily_aggregates\"\n        },\n        data_source=\"events\"\n    )\n</code></pre> <p>Process:</p> <ol> <li>Query Iceberg table via Trino</li> <li>Drop existing PostgreSQL table</li> <li>Create new table with inferred schema</li> <li>Batch insert with transactions</li> <li>Return statistics</li> </ol>"},{"location":"getting-started/core-concepts/#data-flow-example","title":"Data Flow Example","text":"<p>Complete end-to-end flow:</p> <pre><code>1. API Source\n   \u2193\n2. @phlo_ingestion decorator\n   \u2193 DLT \u2192 Parquet staging\n   \u2193\n3. Iceberg table (bronze.events)\n   \u2193 on branch: pipeline/run-abc123\n   \u2193\n4. @phlo_quality checks\n   \u2193 validation passes\n   \u2193\n5. Auto-promotion sensor\n   \u2193 merge to main\n   \u2193\n6. dbt transformations\n   \u2193 bronze \u2192 silver \u2192 gold\n   \u2193\n7. Publishing asset\n   \u2193 Iceberg \u2192 PostgreSQL\n   \u2193\n8. Superset dashboards\n</code></pre>"},{"location":"getting-started/core-concepts/#key-files-locations","title":"Key Files &amp; Locations","text":"<p>Ingestion workflows: <code>workflows/ingestion/{domain}/{workflow}.py</code></p> <p>Schemas: <code>workflows/schemas/{domain}.py</code></p> <p>Quality checks: <code>workflows/quality/{domain}.py</code></p> <p>dbt models: <code>workflows/transforms/dbt/models/{layer}/{model}.sql</code></p> <p>Configuration: <code>phlo.config</code>, <code>.phlo/.env.local</code>, <code>phlo.yaml</code></p> <p>Sensors: <code>workflows/sensors/branch_lifecycle.py</code></p>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Build your first pipeline</li> <li>Developer Guide - Deep dive into decorators</li> <li>Architecture Reference - Technical details</li> <li>CLI Reference - Command-line tools</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Complete guide to installing and setting up Phlo on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#required","title":"Required","text":"<ul> <li>Python: 3.11 or later</li> <li>Docker: Version 20.10 or later (for running infrastructure services)</li> <li>Docker Compose: Version 2.0 or later</li> </ul>"},{"location":"getting-started/installation/#recommended","title":"Recommended","text":"<ul> <li>uv: Fast Python package installer (recommended for better performance)</li> <li>8GB RAM: Minimum for running all services</li> <li>20GB disk space: For Docker volumes and data</li> </ul>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"<pre><code># Install Phlo with default services\nuv pip install phlo[defaults]\n\n# Initialize a new project\nphlo init my-project\ncd my-project\n\n# Initialize infrastructure (generates .phlo/.env and .phlo/.env.local)\nphlo services init\n\n# Start services\nphlo services start\n</code></pre>"},{"location":"getting-started/installation/#detailed-installation-steps","title":"Detailed Installation Steps","text":""},{"location":"getting-started/installation/#step-1-install-phlo","title":"Step 1: Install Phlo","text":"<p>Install Phlo and its default services:</p> <pre><code>uv pip install phlo[defaults]\n</code></pre> <p>The <code>[defaults]</code> extra installs these core service packages: - <code>phlo-dagster</code> - Data orchestration platform - <code>phlo-postgres</code> - PostgreSQL database - <code>phlo-trino</code> - Distributed SQL query engine - <code>phlo-nessie</code> - Git-like catalog for Iceberg - <code>phlo-minio</code> - S3-compatible object storage</p> <p>Verify installation:</p> <pre><code>phlo --version\n</code></pre>"},{"location":"getting-started/installation/#step-2-initialize-a-project","title":"Step 2: Initialize a Project","text":"<p>Create a new Phlo project:</p> <pre><code>phlo init my-project\ncd my-project\n</code></pre> <p>This creates:</p> <pre><code>my-project/\n\u251c\u2500\u2500 .env.example         # Local secrets template (.phlo/.env.local)\n\u251c\u2500\u2500 workflows/           # Data ingestion workflows\n\u2502   \u251c\u2500\u2500 ingestion/\n\u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2514\u2500\u2500 transforms/      # dbt transformations\n\u2502       \u2514\u2500\u2500 dbt/\n\u251c\u2500\u2500 tests/               # Test files\n\u2514\u2500\u2500 phlo.yaml           # Project configuration\n</code></pre>"},{"location":"getting-started/installation/#step-3-configure-environment","title":"Step 3: Configure Environment","text":"<p>Configure non-secret defaults in <code>phlo.yaml</code> and secrets in <code>.phlo/.env.local</code>:</p> <pre><code>phlo services init\n</code></pre> <p>Edit <code>phlo.yaml</code> (env:) and <code>.phlo/.env.local</code> with your settings. The defaults work for local development:</p> <pre><code># phlo.yaml (committed)\nenv:\n  POSTGRES_PORT: 10000\n  MINIO_API_PORT: 10001\n  MINIO_CONSOLE_PORT: 10002\n  NESSIE_PORT: 10003\n  TRINO_PORT: 10005\n  DAGSTER_PORT: 10006\n\n# .phlo/.env.local (not committed)\nPOSTGRES_PASSWORD=phlo\nMINIO_ROOT_PASSWORD=minio123\nSUPERSET_ADMIN_PASSWORD=admin\n</code></pre>"},{"location":"getting-started/installation/#step-4-start-services","title":"Step 4: Start Services","text":"<p>Start the infrastructure services:</p> <pre><code>phlo services start\n</code></pre> <p>This automatically: 1. Initializes the <code>.phlo/</code> directory with Docker configurations 2. Starts all service containers:    - PostgreSQL (port 10000)    - MinIO (ports 10001-10002)    - Nessie (port 10003)    - Trino (port 10005)    - Dagster webserver (port 10006)    - Dagster daemon</p> <p>First-time setup: <code>phlo services start</code> will initialize <code>.phlo/</code> if it doesn't exist, but run <code>phlo services init</code> first if you want to edit <code>phlo.yaml</code> and <code>.phlo/.env.local</code> before startup.</p>"},{"location":"getting-started/installation/#step-5-verify-installation","title":"Step 5: Verify Installation","text":"<p>Check service status:</p> <pre><code>phlo services status\n</code></pre> <p>Expected output:</p> <pre><code>SERVICE              STATUS    PORTS\npostgres             running   10000\nminio                running   10001-10002\nnessie               running   10003\ntrino                running   10005\ndagster-webserver    running   10006\ndagster-daemon       running\n</code></pre> <p>Access Dagster UI:</p> <pre><code># Open in browser\nopen http://localhost:10006\n</code></pre>"},{"location":"getting-started/installation/#step-6-create-and-run-your-first-pipeline","title":"Step 6: Create and Run Your First Pipeline","text":"<p>Create a workflow using the interactive wizard:</p> <pre><code>phlo create-workflow\n</code></pre> <p>Or materialize the example assets (if using a template with examples):</p> <pre><code>phlo materialize --all\n</code></pre> <p>You can also use the Dagster UI at http://localhost:10006 to materialize assets.</p>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#minimal-installation","title":"Minimal Installation","text":"<p>Install only the core Phlo framework without service packages:</p> <pre><code>uv pip install phlo\n</code></pre> <p>Then install services individually as needed:</p> <pre><code>uv pip install phlo-dagster phlo-postgres phlo-trino\n</code></pre>"},{"location":"getting-started/installation/#with-optional-services","title":"With Optional Services","text":"<p>Install additional service packages:</p> <pre><code># Business intelligence\nuv pip install phlo-superset\n\n# Observability stack\nuv pip install phlo-prometheus phlo-grafana phlo-loki phlo-alloy\n\n# API layers\nuv pip install phlo-postgrest phlo-hasura\n\n# Data catalog\nuv pip install phlo-openmetadata\n</code></pre> <p>Start with service profiles:</p> <pre><code># With observability (Prometheus, Grafana, Loki)\nphlo services start --profile observability\n\n# With API layer (PostgREST, Hasura)\nphlo services start --profile api\n\n# With data catalog\nphlo services start --profile catalog\n\n# Multiple profiles\nphlo services start --profile observability --profile api\n</code></pre>"},{"location":"getting-started/installation/#native-services-mode","title":"Native Services Mode","text":"<p>Run Observatory and phlo-api as native Python processes instead of Docker containers (useful on ARM Macs or for development):</p> <pre><code>phlo services start --native\n</code></pre> <p>This runs: - All infrastructure services (Postgres, Trino, etc.) in Docker - Observatory UI and phlo-api as Python subprocesses - Better performance on non-Linux systems</p>"},{"location":"getting-started/installation/#development-mode","title":"Development Mode","text":"<p>For developing Phlo itself, mount local source code:</p> <pre><code>phlo services init --dev --phlo-source /path/to/phlo\nphlo services start\n</code></pre> <p>This mounts the phlo monorepo into the Dagster container and installs <code>phlo[defaults]</code> as an editable install, allowing you to modify Phlo's source code and see changes immediately.</p>"},{"location":"getting-started/installation/#production-deployment","title":"Production Deployment","text":"<p>For production deployments, see the Production Deployment Guide.</p>"},{"location":"getting-started/installation/#verify-components","title":"Verify Components","text":""},{"location":"getting-started/installation/#postgresql","title":"PostgreSQL","text":"<pre><code>docker exec -it phlo-postgres-1 psql -U postgres\n</code></pre>"},{"location":"getting-started/installation/#minio","title":"MinIO","text":"<p>Open http://localhost:10001 in browser</p> <ul> <li>Username: minioadmin</li> <li>Password: minioadmin</li> </ul>"},{"location":"getting-started/installation/#nessie","title":"Nessie","text":"<pre><code>curl http://localhost:10003/api/v2/config\n</code></pre>"},{"location":"getting-started/installation/#trino","title":"Trino","text":"<pre><code>docker exec -it phlo-trino-1 trino\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#services-wont-start","title":"Services won't start","text":"<p>Check Docker is running:</p> <pre><code>docker ps\n</code></pre> <p>View logs:</p> <pre><code>phlo services logs -f\n</code></pre>"},{"location":"getting-started/installation/#port-conflicts","title":"Port conflicts","text":"<p>If ports are already in use, edit <code>phlo.yaml</code> (env:) to change port numbers:</p> <pre><code>POSTGRES_PORT=15432\nMINIO_PORT=19000\n# etc.\n</code></pre>"},{"location":"getting-started/installation/#insufficient-resources","title":"Insufficient resources","text":"<p>Ensure Docker has enough resources:</p> <ul> <li>Docker Desktop: Settings \u2192 Resources \u2192 8GB RAM minimum</li> <li>Linux: Check <code>docker info</code> for resource limits</li> </ul>"},{"location":"getting-started/installation/#permission-errors","title":"Permission errors","text":"<p>On Linux, you may need to fix permissions:</p> <pre><code>sudo chown -R $USER:$USER .phlo/\n</code></pre>"},{"location":"getting-started/installation/#uninstall","title":"Uninstall","text":"<p>Stop and remove all services:</p> <pre><code># Stop services\nphlo services stop\n\n# Remove volumes (deletes all data)\nphlo services stop --volumes\n</code></pre> <p>Remove Phlo directory:</p> <pre><code>cd ..\nrm -rf phlo\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Run your first pipeline</li> <li>CLI Reference - Learn CLI commands</li> <li>Configuration Reference - Advanced configuration</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"getting-started/quickstart/","title":"10-Minute Quickstart","text":"<p>Get Phlo running and see your first data pipeline in action in under 10 minutes.</p>"},{"location":"getting-started/quickstart/#what-youll-build","title":"What You'll Build","text":"<p>A simple glucose data ingestion pipeline that:</p> <ol> <li>Fetches data from Nightscout API</li> <li>Validates with Pandera schemas</li> <li>Stores in Apache Iceberg table</li> <li>Views in Dagster UI</li> </ol>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> <li>10 minutes</li> <li>Text editor</li> </ul>"},{"location":"getting-started/quickstart/#step-1-clone-and-setup-2-minutes","title":"Step 1: Clone and Setup (2 minutes)","text":"<pre><code>git clone https://github.com/iamgp/phlo.git\ncd phlo\n\n# Initialize infrastructure (generates .phlo/.env and .phlo/.env.local)\nphlo services init\n# Edit .phlo/.env.local if needed (defaults work for local development)\n\n# Start core services\nmake up-core up-query\n</code></pre> <p>Wait for services to start (~60 seconds).</p>"},{"location":"getting-started/quickstart/#step-2-view-dagster-ui-30-seconds","title":"Step 2: View Dagster UI (30 seconds)","text":"<pre><code>make dagster\n# Opens http://localhost:10006\n</code></pre> <p>You'll see the existing glucose ingestion asset already defined!</p>"},{"location":"getting-started/quickstart/#step-3-understand-the-asset-2-minutes","title":"Step 3: Understand the Asset (2 minutes)","text":"<p>Open <code>workflows/ingestion/nightscout/glucose.py</code>:</p> <pre><code>from dlt.sources.rest_api import rest_api\nimport phlo\nfrom workflows.schemas.glucose import RawGlucoseEntries\n\n@phlo_ingestion(\n    table_name=\"glucose_entries\",\n    unique_key=\"_id\",\n    validation_schema=RawGlucoseEntries,\n    group=\"nightscout\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef glucose_entries(partition_date: str):\n    \"\"\"Ingest Nightscout glucose entries.\"\"\"\n    start_time_iso = f\"{partition_date}T00:00:00.000Z\"\n    end_time_iso = f\"{partition_date}T23:59:59.999Z\"\n\n    source = rest_api({\n        \"client\": {\n            \"base_url\": \"https://gwp-diabetes.fly.dev/api/v1\",\n        },\n        \"resources\": [{\n            \"name\": \"entries\",\n            \"endpoint\": {\n                \"path\": \"entries.json\",\n                \"params\": {\n                    \"count\": 10000,\n                    \"find[dateString][$gte]\": start_time_iso,\n                    \"find[dateString][$lt]\": end_time_iso,\n                },\n            },\n        }],\n    })\n\n    return source\n</code></pre> <p>Notice: Only 60 lines! The <code>@phlo_ingestion</code> decorator handles:</p> <ul> <li>DLT pipeline setup</li> <li>Pandera validation</li> <li>Iceberg table creation</li> <li>Merge with deduplication</li> <li>Timing instrumentation</li> </ul>"},{"location":"getting-started/quickstart/#step-4-materialize-the-asset-1-minute","title":"Step 4: Materialize the Asset (1 minute)","text":"<pre><code># Materialize for today's date\ndocker exec dagster-webserver dagster asset materialize --select glucose_entries\n\n# Or in Dagster UI:\n# Navigate to Assets \u2192 glucose_entries \u2192 Materialize\n</code></pre> <p>Watch the execution in the Dagster UI. You'll see:</p> <ol> <li>DLT fetching data from API</li> <li>Pandera validation</li> <li>Staging to parquet</li> <li>Merge to Iceberg</li> </ol>"},{"location":"getting-started/quickstart/#step-5-query-your-data-2-minutes","title":"Step 5: Query Your Data (2 minutes)","text":""},{"location":"getting-started/quickstart/#option-a-trino-sql","title":"Option A: Trino (SQL)","text":"<pre><code># Connect to Trino\ndocker exec -it trino trino --catalog iceberg_dev --schema raw\n\n# Query your data\nSELECT _id, sgv, dateString\nFROM glucose_entries\nORDER BY dateString DESC\nLIMIT 10;\n</code></pre>"},{"location":"getting-started/quickstart/#option-b-duckdb-local-analysis","title":"Option B: DuckDB (local analysis)","text":"<pre><code>import duckdb\n\nconn = duckdb.connect()\n\n# Install Iceberg extension\nconn.execute(\"INSTALL iceberg;\")\nconn.execute(\"LOAD iceberg;\")\n\n# Query Iceberg table (adapt path to your MinIO endpoint)\nresult = conn.execute(\"\"\"\n    SELECT _id, sgv, dateString\n    FROM iceberg_scan('s3://warehouse/raw/glucose_entries', ...)\n    LIMIT 10\n\"\"\").df()\n\nprint(result)\n</code></pre>"},{"location":"getting-started/quickstart/#what-you-just-did","title":"What You Just Did","text":"<p>In 10 minutes, you:</p> <ol> <li>Started Phlo's lakehouse platform</li> <li>Explored an ingestion asset</li> <li>Materialized data to Iceberg</li> <li>Queried with SQL engines</li> </ol> <p>Key Concepts:</p> <ul> <li>Decorator-driven: Minimal boilerplate with <code>@phlo_ingestion</code></li> <li>Schema-first: Pandera validates data quality</li> <li>Iceberg tables: ACID transactions, time travel, schema evolution</li> <li>Multi-engine: Query with Trino, DuckDB, Spark</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quickstart/#create-your-own-ingestion-asset-15-minutes","title":"Create Your Own Ingestion Asset (15 minutes)","text":"<ol> <li>Define schema in <code>workflows/schemas/mydata.py</code>:</li> </ol> <pre><code>import pandera as pa\nfrom pandera.typing import Series\n\nclass RawWeatherData(pa.DataFrameModel):\n    city_name: Series[str] = pa.Field(description=\"City name\")\n    temperature: Series[float] = pa.Field(ge=-50, le=50)\n    timestamp: Series[str] = pa.Field(description=\"ISO 8601 timestamp\")\n\n    class Config:\n        strict = True\n        coerce = True\n</code></pre> <ol> <li>Create asset in <code>workflows/ingestion/weather/observations.py</code>:</li> </ol> <pre><code>from dlt.sources.rest_api import rest_api\nimport phlo\nfrom workflows.schemas.mydata import RawWeatherData\n\n@phlo_ingestion(\n    table_name=\"weather_observations\",\n    unique_key=\"timestamp\",\n    validation_schema=RawWeatherData,\n    group=\"weather\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef weather_observations(partition_date: str):\n    \"\"\"Ingest weather observations.\"\"\"\n    # TODO: Replace with your API\n    source = rest_api({\n        \"client\": {\n            \"base_url\": \"https://api.openweathermap.org/data/3.0\",\n            \"auth\": {\"token\": \"YOUR_API_KEY\"}\n        },\n        \"resources\": [{\n            \"name\": \"observations\",\n            \"endpoint\": {\n                \"path\": \"onecall/timemachine\",\n                \"params\": {\"dt\": partition_date}\n            }\n        }]\n    })\n    return source\n</code></pre> <ol> <li> <p>No manual registration is needed. Phlo discovers assets under <code>workflows/</code>.</p> </li> <li> <p>Restart Dagster:</p> </li> </ol> <pre><code>docker restart dagster-webserver\n</code></pre> <ol> <li>Materialize in UI!</li> </ol>"},{"location":"getting-started/quickstart/#build-complete-pipeline-60-minutes","title":"Build Complete Pipeline (60 minutes)","text":"<p>Follow the comprehensive tutorial:</p> <ul> <li>Workflow Development Guide (42KB, 10-step tutorial)</li> </ul> <p>This covers:</p> <ul> <li>Bronze/Silver/Gold layers with dbt</li> <li>Data quality checks</li> <li>Publishing to Postgres</li> <li>Scheduling and automation</li> </ul>"},{"location":"getting-started/quickstart/#explore-advanced-features","title":"Explore Advanced Features","text":"<ul> <li>Time Travel: Query historical snapshots</li> <li>Git-like Branching: Nessie for data versioning</li> <li>Data Catalog: OpenMetadata integration</li> <li>Observability: Grafana dashboards</li> </ul>"},{"location":"getting-started/quickstart/#learning-resources","title":"Learning Resources","text":"<ul> <li>Concepts: Core Concepts - Understand lakehouse fundamentals</li> <li>Complete Tutorial: Workflow Development Guide - Build full pipeline</li> <li>Best Practices: Best Practices Guide - Production patterns</li> <li>Architecture: Architecture - System design</li> <li>Troubleshooting: Troubleshooting Guide - Common issues</li> </ul>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":"<p>\"Services won't start\"</p> <pre><code># Check Docker is running\ndocker ps\n\n# Check logs\nmake logs\n\n# Restart services\nmake down\nmake up-core up-query\n</code></pre> <p>\"Asset not showing in UI\"</p> <pre><code># Restart Dagster webserver\ndocker restart dagster-webserver\n\n# Ensure asset file lives under workflows/ and imports cleanly\n</code></pre> <p>\"Validation failed\"</p> <pre><code># Check schema matches your data types\n# Common issue: timestamp as datetime instead of string\n# Review Pandera schema in workflows/schemas/\n</code></pre> <p>\"Permission denied in MinIO\"</p> <pre><code># Check .phlo/.env.local has correct MinIO credentials\n# Default: MINIO_ROOT_USER=minioadmin, MINIO_ROOT_PASSWORD=minioadmin\n</code></pre>"},{"location":"getting-started/quickstart/#why-phlo","title":"Why Phlo?","text":"<p>74% less boilerplate vs manual Dagster/Iceberg/DLT integration:</p> Operation Manual Code With Phlo Reduction DLT setup ~50 lines 0 lines 100% Iceberg schema ~40 lines 0 lines (auto-generated) 100% Merge logic ~60 lines 0 lines 100% Error handling ~40 lines 0 lines 100% Timing/logging ~30 lines 0 lines 100% Total ~270 lines ~60 lines 74% <p>Unique Features:</p> <ul> <li>Git-like branching for data (Nessie)</li> <li>Time travel queries (Iceberg)</li> <li>Schema auto-generation (Pandera \u2192 PyIceberg)</li> <li>Idempotent ingestion (deduplication built-in)</li> <li>Multi-engine analytics (Trino, DuckDB, Spark)</li> </ul>"},{"location":"getting-started/quickstart/#get-help","title":"Get Help","text":"<ul> <li>Documentation: docs/index.md</li> <li>GitHub Issues: Report bugs and request features</li> <li>GitHub Discussions: Ask questions and share ideas</li> </ul> <p>Next: Complete Tutorial | Best Practices</p>"},{"location":"guides/dagster-assets/","title":"Dagster Assets Tutorial","text":""},{"location":"guides/dagster-assets/#mastering-asset-based-orchestration","title":"Mastering Asset-Based Orchestration","text":"<p>This guide teaches you everything about Dagster assets in Phlo - from basics to advanced patterns.</p>"},{"location":"guides/dagster-assets/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Asset Basics</li> <li>Asset Dependencies</li> <li>Resources and Configuration</li> <li>Partitions</li> <li>Asset Checks and Quality</li> <li>Schedules and Sensors</li> <li>Advanced Patterns</li> <li>Troubleshooting</li> </ol>"},{"location":"guides/dagster-assets/#asset-basics","title":"Asset Basics","text":""},{"location":"guides/dagster-assets/#what-is-an-asset","title":"What is an Asset?","text":"<p>An asset is a piece of data that you want to create and maintain.</p> <p>Examples:</p> <ul> <li>A table in Iceberg</li> <li>A file in S3</li> <li>A machine learning model</li> <li>A dashboard</li> </ul> <p>Key concept: You declare what you want (the asset), not how to create it (the task).</p>"},{"location":"guides/dagster-assets/#your-first-asset","title":"Your First Asset","text":"<pre><code>import dagster as dg\n\n@dg.asset\ndef my_first_asset():\n    \"\"\"A simple asset that returns data.\"\"\"\n    return [1, 2, 3, 4, 5]\n</code></pre> <p>That's it! Dagster will:</p> <ul> <li>Track when it was last materialized</li> <li>Let you materialize it on-demand</li> <li>Show it in the UI</li> </ul>"},{"location":"guides/dagster-assets/#materializing-an-asset","title":"Materializing an Asset","text":"<p>In the UI:</p> <ol> <li>Open http://localhost:3000</li> <li>Click \"Assets\"</li> <li>Find your asset</li> <li>Click \"Materialize\"</li> </ol> <p>CLI:</p> <pre><code>dagster asset materialize -m phlo.framework.definitions -a my_first_asset\n</code></pre> <p>Programmatically:</p> <pre><code>from dagster import materialize\n\nresult = materialize([my_first_asset])\n</code></pre>"},{"location":"guides/dagster-assets/#asset-context","title":"Asset Context","text":"<p>Get information about the current run:</p> <pre><code>@dg.asset\ndef asset_with_context(context: dg.AssetExecutionContext):\n    \"\"\"Asset that uses context.\"\"\"\n\n    # Logging\n    context.log.info(\"Starting materialization\")\n    context.log.warning(\"This is a warning\")\n    context.log.error(\"This is an error\")\n\n    # Asset info\n    context.log.info(f\"Asset key: {context.asset_key}\")\n    context.log.info(f\"Run ID: {context.run_id}\")\n\n    # Partition info (if partitioned)\n    if context.has_partition_key:\n        context.log.info(f\"Partition: {context.partition_key}\")\n\n    return \"data\"\n</code></pre>"},{"location":"guides/dagster-assets/#asset-metadata","title":"Asset Metadata","text":"<p>Return metadata about your materialization:</p> <pre><code>@dg.asset\ndef asset_with_metadata(context: dg.AssetExecutionContext) -&gt; dg.MaterializeResult:\n    \"\"\"Asset that returns metadata.\"\"\"\n\n    data = fetch_data()\n\n    return dg.MaterializeResult(\n        metadata={\n            \"num_records\": len(data),\n            \"preview\": dg.MetadataValue.md(data.head().to_markdown()),\n            \"row_count\": dg.MetadataValue.int(len(data)),\n            \"schema\": dg.MetadataValue.json({\"columns\": list(data.columns)}),\n        }\n    )\n</code></pre> <p>Metadata shows up in the Dagster UI!</p>"},{"location":"guides/dagster-assets/#asset-configuration","title":"Asset Configuration","text":"<p>Add configuration to your assets:</p> <pre><code>@dg.asset(\n    name=\"configured_asset\",\n    description=\"This asset has configuration\",\n    compute_kind=\"python\",\n    group_name=\"my_group\",\n    tags={\"owner\": \"data_team\", \"priority\": \"high\"},\n)\ndef configured_asset(context: dg.AssetExecutionContext):\n    \"\"\"Asset with configuration.\"\"\"\n    return \"data\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code>: Asset name (default: function name)</li> <li><code>description</code>: Shows in UI</li> <li><code>compute_kind</code>: Badge showing technology (python, sql, spark, etc.)</li> <li><code>group_name</code>: Organize related assets</li> <li><code>tags</code>: Key-value labels for filtering</li> </ul>"},{"location":"guides/dagster-assets/#asset-dependencies","title":"Asset Dependencies","text":""},{"location":"guides/dagster-assets/#declaring-dependencies","title":"Declaring Dependencies","text":"<p>Method 1: Function parameter</p> <pre><code>@dg.asset\ndef upstream_asset():\n    return [1, 2, 3]\n\n@dg.asset\ndef downstream_asset(upstream_asset):  # Depends on upstream_asset\n    \"\"\"This asset depends on upstream_asset.\"\"\"\n    data = upstream_asset  # Gets the return value\n    return [x * 2 for x in data]\n</code></pre> <p>Dagster automatically:</p> <ul> <li>Knows <code>downstream_asset</code> depends on <code>upstream_asset</code></li> <li>Materializes <code>upstream_asset</code> first</li> <li>Passes its return value to <code>downstream_asset</code></li> </ul> <p>Method 2: deps parameter</p> <pre><code>@dg.asset(\n    deps=[\"upstream_asset\"]  # Depends but doesn't need the data\n)\ndef downstream_asset():\n    \"\"\"This runs after upstream_asset but doesn't use its data.\"\"\"\n    # Fetch data from database instead\n    return query_database()\n</code></pre> <p>Use <code>deps</code> when:</p> <ul> <li>You don't need the upstream asset's return value</li> <li>The upstream asset writes to a database/storage</li> <li>You just need it to run first</li> </ul> <p>Method 3: AssetKey</p> <pre><code>@dg.asset(\n    deps=[dg.AssetKey([\"schema\", \"table_name\"])]\n)\ndef asset_with_key_dep():\n    \"\"\"Depend on asset by key.\"\"\"\n    return \"data\"\n</code></pre>"},{"location":"guides/dagster-assets/#multi-asset-dependencies","title":"Multi-Asset Dependencies","text":"<pre><code>@dg.asset\ndef asset_a():\n    return \"A\"\n\n@dg.asset\ndef asset_b():\n    return \"B\"\n\n@dg.asset\ndef asset_c(asset_a, asset_b):  # Depends on both\n    \"\"\"Combines data from A and B.\"\"\"\n    return f\"{asset_a} + {asset_b}\"\n</code></pre> <p>Graph:</p> <pre><code>asset_a \u2500\u2510\n         \u251c\u2500&gt; asset_c\nasset_b \u2500\u2518\n</code></pre>"},{"location":"guides/dagster-assets/#conditional-dependencies","title":"Conditional Dependencies","text":"<pre><code>from phlo.config import get_config\n\n@dg.asset\ndef optional_upstream():\n    return \"data\"\n\n@dg.asset\ndef conditional_asset():\n    \"\"\"Conditionally uses upstream asset.\"\"\"\n    from phlo.config import get_settings\n    config = get_settings()\n\n    if config.USE_CACHE:\n        # Use upstream asset\n        return load_from_cache()\n    else:\n        # Fetch fresh\n        return fetch_fresh_data()\n</code></pre>"},{"location":"guides/dagster-assets/#resources-and-configuration","title":"Resources and Configuration","text":""},{"location":"guides/dagster-assets/#what-are-resources","title":"What are Resources?","text":"<p>Resources are external services your assets need:</p> <ul> <li>Databases (Trino, Postgres)</li> <li>APIs (Iceberg catalog)</li> <li>File systems (S3)</li> <li>External services</li> </ul>"},{"location":"guides/dagster-assets/#using-resources","title":"Using Resources","text":"<pre><code>from phlo_trino.resource import TrinoResource\nfrom phlo_iceberg.resource import IcebergResource\n\n@dg.asset\ndef asset_with_resources(\n    trino: TrinoResource,\n    iceberg: IcebergResource,\n):\n    \"\"\"Asset that uses resources.\"\"\"\n\n    # Query with Trino\n    results = trino.execute(\"SELECT * FROM iceberg.raw.my_table\")\n\n    # Write to Iceberg\n    iceberg.append_parquet(\"silver.processed_table\", results)\n\n    return len(results)\n</code></pre> <p>Resources are automatically injected by Dagster!</p>"},{"location":"guides/dagster-assets/#creating-custom-resources","title":"Creating Custom Resources","text":"<pre><code>from dagster import ConfigurableResource\nimport requests\n\nclass WeatherAPIResource(ConfigurableResource):\n    \"\"\"Resource for weather API.\"\"\"\n\n    api_key: str\n    base_url: str = \"https://api.openweathermap.org/data/2.5\"\n\n    def get_weather(self, city: str) -&gt; dict:\n        \"\"\"Fetch weather for a city.\"\"\"\n        url = f\"{self.base_url}/weather\"\n        params = {\"q\": city, \"appid\": self.api_key}\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n\n\n# Register resource\ndef build_resources():\n    return dg.Definitions(\n        resources={\n            \"weather_api\": WeatherAPIResource(\n                api_key=get_settings().WEATHER_API_KEY,\n            ),\n        },\n    )\n</code></pre> <p>Use it:</p> <pre><code>@dg.asset\ndef weather_data(weather_api: WeatherAPIResource):\n    \"\"\"Fetch weather data.\"\"\"\n    return weather_api.get_weather(\"London\")\n</code></pre>"},{"location":"guides/dagster-assets/#configuration-from-environment","title":"Configuration from Environment","text":"<pre><code>from phlo.config import get_settings\n\n@dg.asset\ndef configured_asset(context: dg.AssetExecutionContext):\n    \"\"\"Asset that uses config.\"\"\"\n    config = get_settings()\n\n    context.log.info(f\"Using API: {config.API_BASE_URL}\")\n    context.log.info(f\"Batch size: {config.BATCH_SIZE}\")\n\n    # Use configuration\n    return fetch_data(\n        url=config.API_BASE_URL,\n        batch_size=config.BATCH_SIZE,\n    )\n</code></pre>"},{"location":"guides/dagster-assets/#partitions","title":"Partitions","text":""},{"location":"guides/dagster-assets/#what-are-partitions","title":"What are Partitions?","text":"<p>Partitions let you process data incrementally instead of all at once.</p> <p>Example: Instead of processing all historical data every time, process one day at a time.</p>"},{"location":"guides/dagster-assets/#daily-partitions","title":"Daily Partitions","text":"<pre><code>from dagster import DailyPartitionsDefinition\n\ndaily_partition = DailyPartitionsDefinition(start_date=\"2024-01-01\")\n\n@dg.asset(\n    partitions_def=daily_partition,\n)\ndef daily_weather_data(context: dg.AssetExecutionContext):\n    \"\"\"Fetch weather data for one day.\"\"\"\n\n    # Get the partition we're processing\n    date = context.partition_key  # \"2024-11-05\"\n\n    context.log.info(f\"Processing date: {date}\")\n\n    # Fetch only data for this date\n    data = fetch_weather_for_date(date)\n\n    return data\n</code></pre> <p>Benefits:</p> <ul> <li>Process incrementally (one day at a time)</li> <li>Backfill missing dates easily</li> <li>Re-process specific dates if needed</li> <li>Parallel processing of partitions</li> </ul>"},{"location":"guides/dagster-assets/#materializing-partitions","title":"Materializing Partitions","text":"<p>Single partition:</p> <pre><code>dagster asset materialize -m phlo.framework.definitions \\\n  -a daily_weather_data \\\n  --partition 2024-11-05\n</code></pre> <p>Range of partitions:</p> <pre><code># Backfill last 7 days\ndagster asset backfill -m phlo.framework.definitions \\\n  -a daily_weather_data \\\n  --from 2024-11-01 \\\n  --to 2024-11-07\n</code></pre> <p>Latest partition:</p> <pre><code>dagster asset materialize -m phlo.framework.definitions \\\n  -a daily_weather_data \\\n  --partition $(date +%Y-%m-%d)\n</code></pre>"},{"location":"guides/dagster-assets/#hourly-partitions","title":"Hourly Partitions","text":"<pre><code>from dagster import HourlyPartitionsDefinition\n\nhourly_partition = HourlyPartitionsDefinition(start_date=\"2024-01-01-00:00\")\n\n@dg.asset(\n    partitions_def=hourly_partition,\n)\ndef hourly_traffic_data(context: dg.AssetExecutionContext):\n    \"\"\"Fetch traffic data for one hour.\"\"\"\n    hour = context.partition_key  # \"2024-11-05-14:00\"\n    return fetch_traffic_for_hour(hour)\n</code></pre>"},{"location":"guides/dagster-assets/#static-partitions","title":"Static Partitions","text":"<pre><code>from dagster import StaticPartitionsDefinition\n\ncity_partition = StaticPartitionsDefinition(\n    [\"london\", \"new_york\", \"tokyo\", \"sydney\"]\n)\n\n@dg.asset(\n    partitions_def=city_partition,\n)\ndef city_weather_data(context: dg.AssetExecutionContext):\n    \"\"\"Fetch weather for one city.\"\"\"\n    city = context.partition_key  # \"london\"\n    return fetch_weather_for_city(city)\n</code></pre>"},{"location":"guides/dagster-assets/#multi-dimensional-partitions","title":"Multi-Dimensional Partitions","text":"<pre><code>from dagster import MultiPartitionsDefinition\n\nmulti_partition = MultiPartitionsDefinition({\n    \"date\": daily_partition,\n    \"city\": city_partition,\n})\n\n@dg.asset(\n    partitions_def=multi_partition,\n)\ndef multi_partitioned_data(context: dg.AssetExecutionContext):\n    \"\"\"Process by date AND city.\"\"\"\n    partition_key = context.partition_key\n    # partition_key.keys_by_dimension = {\"date\": \"2024-11-05\", \"city\": \"london\"}\n\n    date = partition_key.keys_by_dimension[\"date\"]\n    city = partition_key.keys_by_dimension[\"city\"]\n\n    return fetch_weather(city, date)\n</code></pre>"},{"location":"guides/dagster-assets/#asset-checks-and-quality","title":"Asset Checks and Quality","text":""},{"location":"guides/dagster-assets/#asset-checks","title":"Asset Checks","text":"<p>Asset checks validate data quality without failing the materialization.</p> <pre><code>from dagster import asset_check, AssetCheckResult\n\n@dg.asset\ndef my_data():\n    \"\"\"Create some data.\"\"\"\n    return [{\"value\": x} for x in range(100)]\n\n@asset_check(asset=my_data)\ndef check_data_not_empty(my_data):\n    \"\"\"Check that data is not empty.\"\"\"\n    if len(my_data) == 0:\n        return AssetCheckResult(\n            passed=False,\n            description=\"Data is empty!\",\n        )\n\n    return AssetCheckResult(\n        passed=True,\n        description=f\"Data has {len(my_data)} records\",\n        metadata={\"record_count\": len(my_data)},\n    )\n\n@asset_check(asset=my_data)\ndef check_data_values(my_data):\n    \"\"\"Check data values are in expected range.\"\"\"\n    values = [d[\"value\"] for d in my_data]\n    min_val = min(values)\n    max_val = max(values)\n\n    if min_val &lt; 0 or max_val &gt; 1000:\n        return AssetCheckResult(\n            passed=False,\n            description=f\"Values out of range: {min_val} to {max_val}\",\n        )\n\n    return AssetCheckResult(\n        passed=True,\n        description=\"All values in range\",\n        metadata={\"min\": min_val, \"max\": max_val},\n    )\n</code></pre>"},{"location":"guides/dagster-assets/#freshness-checks","title":"Freshness Checks","text":"<p>Ensure data is up-to-date:</p> <pre><code>from dagster import build_last_update_freshness_checks\nfrom datetime import timedelta\n\n# Automatically check freshness\nfreshness_checks = build_last_update_freshness_checks(\n    assets=[my_data],\n    lower_bound_delta=timedelta(hours=1),  # Must be updated within 1 hour\n)\n</code></pre>"},{"location":"guides/dagster-assets/#data-quality-with-pandera","title":"Data Quality with Pandera","text":"<p>Use Pandera for schema validation:</p> <pre><code>import pandera as pa\nfrom pandera.typing import Series\nfrom dagster_pandera import pandera_schema_to_dagster_type\n\nclass MyDataSchema(pa.DataFrameModel):\n    \"\"\"Schema for my data.\"\"\"\n    id: Series[int] = pa.Field(ge=0)\n    value: Series[float] = pa.Field(ge=0, le=100)\n    name: Series[str]\n\n    class Config:\n        coerce = True\n        strict = True\n\n# Use as type hint\n@dg.asset\ndef validated_data() -&gt; pandera_schema_to_dagster_type(MyDataSchema):\n    \"\"\"Data with automatic validation.\"\"\"\n    import pandas as pd\n\n    df = pd.DataFrame({\n        \"id\": [1, 2, 3],\n        \"value\": [10.5, 20.3, 30.1],\n        \"name\": [\"A\", \"B\", \"C\"],\n    })\n\n    # Validation happens automatically!\n    return df\n</code></pre>"},{"location":"guides/dagster-assets/#schedules-and-sensors","title":"Schedules and Sensors","text":""},{"location":"guides/dagster-assets/#schedules","title":"Schedules","text":"<p>Run assets on a schedule:</p> <pre><code>@dg.schedule(\n    name=\"daily_weather_schedule\",\n    cron_schedule=\"0 2 * * *\",  # Daily at 2 AM\n    job=dg.define_asset_job(\n        name=\"weather_job\",\n        selection=dg.AssetSelection.groups(\"weather\"),\n    ),\n    execution_timezone=\"UTC\",\n    default_status=dg.DefaultScheduleStatus.RUNNING,\n)\ndef daily_weather_schedule():\n    \"\"\"Run weather pipeline daily.\"\"\"\n    return dg.RunRequest()\n</code></pre> <p>Cron examples:</p> <pre><code>\"0 * * * *\"      # Every hour\n\"0 0 * * *\"      # Daily at midnight\n\"0 2 * * *\"      # Daily at 2 AM\n\"0 0 * * 0\"      # Weekly on Sunday\n\"0 0 1 * *\"      # Monthly on 1st\n\"*/15 * * * *\"   # Every 15 minutes\n</code></pre>"},{"location":"guides/dagster-assets/#sensors","title":"Sensors","text":"<p>React to events:</p> <pre><code>@dg.sensor(\n    name=\"file_sensor\",\n    minimum_interval_seconds=60,  # Check every minute\n)\ndef file_arrival_sensor(context: dg.SensorEvaluationContext):\n    \"\"\"Trigger when new file arrives.\"\"\"\n\n    # Check for new files\n    files = list_files_in_bucket(\"s3://my-bucket/incoming/\")\n\n    if not files:\n        return dg.SkipReason(\"No new files\")\n\n    # Trigger job for each file\n    for file in files:\n        yield dg.RunRequest(\n            run_key=file,  # Unique key to avoid duplicates\n            tags={\"file_path\": file},\n        )\n</code></pre>"},{"location":"guides/dagster-assets/#freshness-sensors","title":"Freshness Sensors","text":"<p>Alert on stale data:</p> <pre><code>@dg.sensor(\n    name=\"data_freshness_sensor\",\n    minimum_interval_seconds=300,  # Check every 5 minutes\n    default_status=dg.DefaultSensorStatus.RUNNING,\n)\ndef data_freshness_sensor(context: dg.SensorEvaluationContext):\n    \"\"\"Alert if data is stale.\"\"\"\n\n    asset_key = dg.AssetKey([\"my_important_data\"])\n    last_materialization = context.instance.get_latest_materialization_event(asset_key)\n\n    if not last_materialization:\n        return dg.SkipReason(\"No materialization yet\")\n\n    from datetime import datetime, timedelta\n    age = datetime.now().timestamp() - last_materialization.timestamp\n    max_age = timedelta(hours=2).total_seconds()\n\n    if age &gt; max_age:\n        context.log.error(f\"Data is {age/3600:.1f} hours old!\")\n        # Trigger rematerialization\n        return dg.RunRequest(asset_selection=[asset_key])\n\n    return dg.SkipReason(f\"Data is fresh ({age/3600:.1f} hours)\")\n</code></pre>"},{"location":"guides/dagster-assets/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/dagster-assets/#pattern-1-dynamic-assets","title":"Pattern 1: Dynamic Assets","text":"<p>Generate assets programmatically:</p> <pre><code>CITIES = [\"london\", \"new_york\", \"tokyo\"]\n\ndef build_city_assets():\n    \"\"\"Generate asset for each city.\"\"\"\n    assets = []\n\n    for city in CITIES:\n        @dg.asset(name=f\"weather_{city}\")\n        def city_weather_asset():\n            return fetch_weather(city)\n\n        assets.append(city_weather_asset)\n\n    return assets\n</code></pre>"},{"location":"guides/dagster-assets/#pattern-2-multi-asset","title":"Pattern 2: Multi-Asset","text":"<p>One function creates multiple assets:</p> <pre><code>@dg.multi_asset(\n    outs={\n        \"customers\": dg.AssetOut(),\n        \"orders\": dg.AssetOut(),\n        \"products\": dg.AssetOut(),\n    },\n)\ndef extract_from_api():\n    \"\"\"Extract multiple entities from API.\"\"\"\n\n    api_data = fetch_all_data()\n\n    return (\n        api_data[\"customers\"],  # customers asset\n        api_data[\"orders\"],     # orders asset\n        api_data[\"products\"],   # products asset\n    )\n</code></pre>"},{"location":"guides/dagster-assets/#pattern-3-observable-source-assets","title":"Pattern 3: Observable Source Assets","text":"<p>Track external data you don't create:</p> <pre><code>@dg.observable_source_asset\ndef external_api_data(context: dg.AssetExecutionContext):\n    \"\"\"Track external API without ingesting.\"\"\"\n\n    # Check external source\n    last_updated = get_api_last_updated_time()\n\n    return dg.ObserveResult(\n        metadata={\n            \"last_updated\": last_updated.isoformat(),\n            \"record_count\": get_api_record_count(),\n        }\n    )\n</code></pre>"},{"location":"guides/dagster-assets/#pattern-4-asset-factories","title":"Pattern 4: Asset Factories","text":"<p>Reusable asset patterns:</p> <pre><code>def create_ingestion_asset(source_name: str, table_name: str):\n    \"\"\"Factory function for ingestion assets.\"\"\"\n\n    @dg.asset(\n        name=f\"ingest_{source_name}_{table_name}\",\n        group_name=source_name,\n    )\n    def ingestion_asset(\n        context: dg.AssetExecutionContext,\n        iceberg: IcebergResource,\n    ):\n        \"\"\"Ingest data from source.\"\"\"\n        data = fetch_from_source(source_name, table_name)\n        iceberg.append_parquet(f\"raw.{table_name}\", data)\n        return len(data)\n\n    return ingestion_asset\n\n# Create multiple assets\nassets = [\n    create_ingestion_asset(\"salesforce\", \"accounts\"),\n    create_ingestion_asset(\"salesforce\", \"opportunities\"),\n    create_ingestion_asset(\"stripe\", \"payments\"),\n]\n</code></pre>"},{"location":"guides/dagster-assets/#pattern-5-retry-logic","title":"Pattern 5: Retry Logic","text":"<p>Add automatic retries:</p> <pre><code>@dg.asset(\n    retry_policy=dg.RetryPolicy(\n        max_retries=3,\n        delay=30,  # seconds between retries\n        backoff=dg.Backoff.EXPONENTIAL,  # 30s, 60s, 120s\n    ),\n)\ndef asset_with_retries():\n    \"\"\"Automatically retries on failure.\"\"\"\n    return fetch_unreliable_api()\n</code></pre>"},{"location":"guides/dagster-assets/#pattern-6-asset-versioning","title":"Pattern 6: Asset Versioning","text":"<p>Track versions of your assets:</p> <pre><code>@dg.asset(\n    code_version=\"v2.0\",  # Update when logic changes\n)\ndef versioned_asset():\n    \"\"\"Asset with version tracking.\"\"\"\n    return compute_data_v2()\n</code></pre> <p>Dagster tracks version changes in UI!</p>"},{"location":"guides/dagster-assets/#pattern-7-io-managers","title":"Pattern 7: IO Managers","text":"<p>Custom storage for assets:</p> <pre><code>from dagster import IOManager, io_manager\n\nclass IcebergIOManager(IOManager):\n    \"\"\"Custom IO manager for Iceberg tables.\"\"\"\n\n    def __init__(self, iceberg_resource):\n        self.iceberg = iceberg_resource\n\n    def handle_output(self, context, obj):\n        \"\"\"Save asset to Iceberg.\"\"\"\n        table_name = f\"raw.{context.asset_key.path[-1]}\"\n        self.iceberg.append_parquet(table_name, obj)\n\n    def load_input(self, context):\n        \"\"\"Load asset from Iceberg.\"\"\"\n        table_name = f\"raw.{context.asset_key.path[-1]}\"\n        return self.iceberg.read_table(table_name)\n\n@io_manager\ndef iceberg_io_manager(iceberg: IcebergResource):\n    return IcebergIOManager(iceberg)\n</code></pre>"},{"location":"guides/dagster-assets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dagster-assets/#asset-not-showing-in-ui","title":"Asset Not Showing in UI","text":"<p>Check:</p> <ol> <li>Is it discoverable under workflows/?</li> </ol> <p><code>python    from phlo.framework.definitions import defs    assert defs.get_asset_def(\"my_asset\") is not None</code></p> <ol> <li>Restart Dagster:</li> </ol> <p><code>bash    docker-compose restart dagster-webserver dagster-daemon</code></p> <ol> <li>Check logs:    <code>bash    docker-compose logs dagster-webserver</code></li> </ol>"},{"location":"guides/dagster-assets/#asset-fails-to-materialize","title":"Asset Fails to Materialize","text":"<p>Debug steps:</p> <ol> <li> <p>Check logs in Dagster UI (Runs \u2192 Failed run \u2192 Logs)</p> </li> <li> <p>Run locally:</p> </li> </ol> <p>```python    from phlo.framework.definitions import defs    from dagster import materialize</p> <p>result = materialize([defs.get_asset_def(\"my_asset\")])    print(result)    ```</p> <ol> <li> <p>Check dependencies materialized</p> </li> <li> <p>Verify resources configured</p> </li> </ol>"},{"location":"guides/dagster-assets/#asset-dependencies-not-working","title":"Asset Dependencies Not Working","text":"<p>Common issues:</p> <ol> <li>Typo in asset name:</li> </ol> <p>```python    # Wrong    def downstream(upsteam_asset):  # Typo!        pass</p> <p># Correct    def downstream(upstream_asset):        pass    ```</p> <ol> <li>Asset not in the workflows path:    <code>python    # Ensure both assets live under workflows/ so discovery includes them    from phlo.framework.definitions import defs    assert defs.get_asset_def(\"upstream\") is not None    assert defs.get_asset_def(\"downstream\") is not None</code></li> </ol>"},{"location":"guides/dagster-assets/#partition-errors","title":"Partition Errors","text":"<p>Common issues:</p> <ol> <li>Partition key not found:</li> </ol> <p><code>python    # Check partition exists    context.log.info(f\"Partition: {context.partition_key}\")</code></p> <ol> <li>Partition dependency mismatch:</li> </ol> <p>```python    # Both assets must use same partition definition    @asset(partitions_def=daily_partition)    def upstream(): ...</p> <p>@asset(partitions_def=daily_partition)  # Must match!    def downstream(upstream): ...    ```</p>"},{"location":"guides/dagster-assets/#slow-asset-materialization","title":"Slow Asset Materialization","text":"<p>Optimization:</p> <ol> <li> <p>Add partitioning (process incrementally)</p> </li> <li> <p>Use incremental dbt models</p> </li> <li> <p>Parallelize with multi-asset</p> </li> <li> <p>Optimize queries (add indexes, filters)</p> </li> <li> <p>Increase resources:    <code>yaml    # docker-compose.yml    dagster-webserver:      deploy:        resources:          limits:            cpus: \"2\"            memory: 4G</code></p> </li> </ol>"},{"location":"guides/dagster-assets/#summary","title":"Summary","text":"<p>Key Concepts:</p> <ul> <li>Assets = Data you want to create</li> <li>Dependencies = What depends on what</li> <li>Resources = External services</li> <li>Partitions = Process incrementally</li> <li>Checks = Validate quality</li> <li>Schedules = Time-based triggers</li> <li>Sensors = Event-based triggers</li> </ul> <p>Best Practices:</p> <ul> <li>\u2705 Use meaningful asset names</li> <li>\u2705 Add descriptions and metadata</li> <li>\u2705 Group related assets</li> <li>\u2705 Add data quality checks</li> <li>\u2705 Use partitions for large datasets</li> <li>\u2705 Log important information</li> <li>\u2705 Handle errors gracefully</li> </ul> <p>Next: Troubleshooting Guide - Debug common issues.</p>"},{"location":"guides/data-modeling/","title":"Data Modeling Guide - Bronze, Silver, Gold Architecture","text":""},{"location":"guides/data-modeling/#understanding-the-medallion-architecture","title":"Understanding the Medallion Architecture","text":"<p>This guide explains how to design and organize your data using the Bronze/Silver/Gold pattern (also called the Medallion Architecture).</p>"},{"location":"guides/data-modeling/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is Medallion Architecture?</li> <li>The Bronze Layer</li> <li>The Silver Layer</li> <li>The Gold Layer</li> <li>The Marts Layer</li> <li>Design Principles</li> <li>Common Patterns</li> <li>Schema Evolution</li> <li>Real-World Examples</li> </ol>"},{"location":"guides/data-modeling/#what-is-medallion-architecture","title":"What is Medallion Architecture?","text":""},{"location":"guides/data-modeling/#the-layers","title":"The Layers","text":"<pre><code>RAW \u2192 BRONZE \u2192 SILVER \u2192 GOLD \u2192 MARTS\n</code></pre> <p>Purpose of each layer:</p> Layer Purpose Who Uses It Materialization Raw Exact copy of source data Data engineers debugging Tables Bronze Cleaned, standardized Data engineers Tables/Views Silver Business logic applied Analysts, Data Scientists Tables Gold Aggregated, conformed Business users Tables Marts Published to BI tools Dashboard consumers Tables (Postgres)"},{"location":"guides/data-modeling/#_1","title":"Data Modeling","text":"<p>Why This Pattern?</p> <p>Benefits:</p> <ul> <li>\u2705 Reproducibility - Can always rebuild from raw</li> <li>\u2705 Debuggability - Easy to isolate issues</li> <li>\u2705 Flexibility - Change downstream without reingesting</li> <li>\u2705 Performance - Each layer optimized for its purpose</li> <li>\u2705 Governance - Clear data quality progression</li> </ul> <p>Real-world analogy:</p> <p>Think of it like food processing:</p> <ul> <li>Raw: Fresh ingredients from the farm (messy, varied)</li> <li>Bronze: Washed and sorted ingredients</li> <li>Silver: Prepped and measured ingredients</li> <li>Gold: Cooked dishes</li> <li>Marts: Plated and ready to serve</li> </ul>"},{"location":"guides/data-modeling/#the-bronze-layer","title":"The Bronze Layer","text":""},{"location":"guides/data-modeling/#purpose","title":"Purpose","text":"<p>The Bronze layer cleanses and standardizes raw data without applying business logic.</p>"},{"location":"guides/data-modeling/#what-happens-here","title":"What Happens Here","text":"<p>Type conversions:</p> <pre><code>-- Raw: date stored as bigint (milliseconds)\nCAST(date_ms AS TIMESTAMP) AS date_timestamp\n\n-- Raw: numeric stored as string\nCAST(value_str AS DOUBLE) AS value_numeric\n</code></pre> <p>Standardization:</p> <pre><code>-- Lowercase for consistency\nLOWER(TRIM(category)) AS category\n\n-- Standardize nulls\nNULLIF(TRIM(field), '') AS field  -- Empty strings \u2192 NULL\n</code></pre> <p>Column renaming:</p> <pre><code>-- Make names descriptive and consistent\nsgv AS glucose_value,\nbg AS blood_glucose_mg_dl,\nts AS timestamp_utc\n</code></pre> <p>Data cleaning:</p> <pre><code>-- Remove duplicates\nSELECT DISTINCT * FROM source\n\n-- Filter invalid records\nWHERE created_at IS NOT NULL\n  AND id IS NOT NULL\n</code></pre>"},{"location":"guides/data-modeling/#what-doesnt-happen-here","title":"What DOESN'T Happen Here","text":"<p>\u274c NO business logic (no \"isactive\", \"status_category\", etc.) \u274c NO aggregations (no GROUP BY) \u274c NO enrichments (no calculated fields like \"days_since\") \u274c NO joins (single-source transformations only)</p>"},{"location":"guides/data-modeling/#bronze-layer-best-practices","title":"Bronze Layer Best Practices","text":"<p>1. Keep it close to source</p> <pre><code>-- GOOD: Minimal transformation\nSELECT\n    id,\n    CAST(created_at AS TIMESTAMP) AS created_timestamp,\n    CAST(amount AS DECIMAL(10,2)) AS amount,\n    LOWER(TRIM(status)) AS status\nFROM {{ source('raw', 'orders') }}\n\n-- BAD: Too much logic\nSELECT\n    id,\n    created_at,\n    amount,\n    CASE\n        WHEN status = 'completed' AND amount &gt; 100 THEN 'high_value'\n        ELSE 'standard'\n    END AS order_category  -- This belongs in Silver!\nFROM {{ source('raw', 'orders') }}\n</code></pre> <p>2. Document all transformations</p> <pre><code>-- Good practice: Comment why you're doing something\nSELECT\n    id,\n    -- Convert Unix milliseconds to timestamp\n    CAST(FROM_UNIXTIME(date_ms / 1000) AS TIMESTAMP) AS observation_timestamp,\n\n    -- Standardize missing values: empty strings \u2192 NULL\n    NULLIF(TRIM(notes), '') AS notes,\n\n    -- Remove leading/trailing whitespace and lowercase\n    LOWER(TRIM(city_name)) AS city_name\nFROM {{ source('raw', 'observations') }}\n</code></pre> <p>3. Filter only technical invalids</p> <pre><code>-- GOOD: Filter technical problems\nWHERE id IS NOT NULL  -- Can't process without ID\n  AND created_at IS NOT NULL  -- Can't process without timestamp\n  AND amount &gt;= 0  -- Negative amounts are data errors\n\n-- BAD: Filter business logic\nWHERE status != 'cancelled'  -- This is business logic, belongs in Silver\n</code></pre> <p>4. Naming convention</p> <pre><code>stg_&lt;source&gt;_&lt;entity&gt;\n\nExamples:\n- stg_salesforce_accounts\n- stg_stripe_payments\n- stg_google_analytics_pageviews\n- stg_nightscout_glucose_entries\n</code></pre>"},{"location":"guides/data-modeling/#the-silver-layer","title":"The Silver Layer","text":""},{"location":"guides/data-modeling/#purpose_1","title":"Purpose","text":"<p>The Silver layer adds business logic and context to create analytics-ready fact and dimension tables.</p>"},{"location":"guides/data-modeling/#what-happens-here_1","title":"What Happens Here","text":"<p>Calculated fields:</p> <pre><code>-- Business metrics\namount * tax_rate AS tax_amount,\namount + (amount * tax_rate) AS total_amount,\n\n-- Categorizations\nCASE\n    WHEN amount &lt; 50 THEN 'Low'\n    WHEN amount &lt; 200 THEN 'Medium'\n    ELSE 'High'\nEND AS value_category,\n\n-- Indicators\nCASE\n    WHEN status IN ('completed', 'delivered') THEN TRUE\n    ELSE FALSE\nEND AS is_fulfilled\n</code></pre> <p>Enrichments:</p> <pre><code>-- Date parts for easy filtering\nDATE(created_timestamp) AS created_date,\nYEAR(created_timestamp) AS created_year,\nMONTH(created_timestamp) AS created_month,\nDAY(created_timestamp) AS created_day,\nDAYOFWEEK(created_timestamp) AS day_of_week,\n\n-- Time-based calculations\nDATEDIFF(delivered_date, ordered_date) AS delivery_days,\nAGE(customer_birthdate, current_date) AS customer_age\n</code></pre> <p>Window functions:</p> <pre><code>-- Running totals\nSUM(amount) OVER (\n    PARTITION BY customer_id\n    ORDER BY order_date\n) AS cumulative_spend,\n\n-- Rankings\nROW_NUMBER() OVER (\n    PARTITION BY customer_id\n    ORDER BY order_date\n) AS order_number,\n\n-- Moving averages\nAVG(amount) OVER (\n    ORDER BY order_date\n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n) AS rolling_7day_avg\n</code></pre> <p>Joins (denormalization):</p> <pre><code>-- Join related entities for analysis\nSELECT\n    o.*,\n    c.customer_name,\n    c.customer_segment,\n    p.product_name,\n    p.product_category\nFROM {{ ref('stg_orders') }} o\nLEFT JOIN {{ ref('stg_customers') }} c ON o.customer_id = c.customer_id\nLEFT JOIN {{ ref('stg_products') }} p ON o.product_id = p.product_id\n</code></pre>"},{"location":"guides/data-modeling/#fact-vs-dimension-tables","title":"Fact vs Dimension Tables","text":"<p>Fact Tables (fct_):</p> <ul> <li>Measurements, metrics, transactions</li> <li>Grain: One row per event/transaction</li> <li>Many rows (millions+)</li> <li>Mostly numeric data</li> <li>Examples: <code>fct_orders</code>, <code>fct_glucose_readings</code>, <code>fct_pageviews</code></li> </ul> <pre><code>-- Example fact table\nCREATE TABLE silver.fct_orders AS\nSELECT\n    order_id,\n    customer_id,\n    product_id,\n    order_date,\n    quantity,\n    unit_price,\n    total_amount,\n    tax_amount,\n    -- Calculated metrics\n    quantity * unit_price AS subtotal,\n    -- Categories\n    CASE WHEN total_amount &gt; 100 THEN 'High Value' ELSE 'Standard' END AS value_tier\nFROM {{ ref('stg_orders') }}\n</code></pre> <p>Dimension Tables (dim_):</p> <ul> <li>Descriptive attributes</li> <li>Grain: One row per entity</li> <li>Fewer rows (thousands)</li> <li>Mostly text data</li> <li>Examples: <code>dim_customers</code>, <code>dim_products</code>, <code>dim_date</code></li> </ul> <pre><code>-- Example dimension table\nCREATE TABLE silver.dim_customers AS\nSELECT\n    customer_id,\n    customer_name,\n    email,\n    signup_date,\n    customer_segment,\n    lifetime_value,\n    -- Calculated attributes\n    DATEDIFF(CURRENT_DATE, signup_date) AS days_as_customer,\n    CASE\n        WHEN lifetime_value &gt; 1000 THEN 'VIP'\n        WHEN lifetime_value &gt; 500 THEN 'Premium'\n        ELSE 'Standard'\n    END AS customer_tier\nFROM {{ ref('stg_customers') }}\n</code></pre>"},{"location":"guides/data-modeling/#silver-layer-best-practices","title":"Silver Layer Best Practices","text":"<p>1. Design for analytics</p> <pre><code>-- GOOD: Denormalized, ready for queries\nSELECT\n    order_id,\n    customer_id,\n    customer_name,  -- Denormalized from dim_customers\n    customer_segment,  -- Denormalized\n    product_id,\n    product_name,  -- Denormalized from dim_products\n    order_amount,\n    -- Pre-calculated metrics\n    order_amount / NULLIF(total_lifetime_value, 0) AS pct_of_lifetime_value\nFROM orders_with_customer_and_product_data\n\n-- BAD: Normalized, requires joins for analysis\nSELECT order_id, customer_id, product_id, amount\nFROM orders  -- Analyst must join to get customer/product info\n</code></pre> <p>2. Document business logic</p> <pre><code>-- GOOD: Clear business rule documentation\n-- Business rule (2024-06-01): Orders over $100 get free shipping\nCASE\n    WHEN order_amount &gt; 100 THEN TRUE\n    ELSE FALSE\nEND AS qualifies_for_free_shipping,\n\n-- Business rule: Customer segments defined by marketing team\nCASE\n    WHEN total_orders &gt;= 10 AND lifetime_value &gt; 1000 THEN 'VIP'\n    WHEN total_orders &gt;= 5 OR lifetime_value &gt; 500 THEN 'Loyal'\n    WHEN total_orders &gt;= 2 THEN 'Repeat'\n    ELSE 'New'\nEND AS customer_segment\n</code></pre> <p>3. Naming convention</p> <pre><code>fct_&lt;process&gt;_&lt;event&gt;     -- Fact tables\ndim_&lt;entity&gt;              -- Dimension tables\n\nExamples:\nFact Tables:\n- fct_order_line_items\n- fct_glucose_readings\n- fct_website_sessions\n- fct_payment_transactions\n\nDimension Tables:\n- dim_customers\n- dim_products\n- dim_locations\n- dim_date\n</code></pre>"},{"location":"guides/data-modeling/#the-gold-layer","title":"The Gold Layer","text":""},{"location":"guides/data-modeling/#purpose_2","title":"Purpose","text":"<p>The Gold layer creates aggregated, business-ready datasets optimized for specific use cases.</p>"},{"location":"guides/data-modeling/#what-happens-here_2","title":"What Happens Here","text":"<p>Aggregations:</p> <pre><code>-- Daily aggregations\nSELECT\n    DATE(order_timestamp) AS order_date,\n    customer_segment,\n    COUNT(DISTINCT order_id) AS total_orders,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    SUM(order_amount) AS total_revenue,\n    AVG(order_amount) AS avg_order_value,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY order_amount) AS median_order_value\nFROM {{ ref('fct_orders') }}\nGROUP BY order_date, customer_segment\n</code></pre> <p>Rollups:</p> <pre><code>-- Multi-grain aggregations\nSELECT\n    year,\n    quarter,\n    month,\n    product_category,\n    SUM(revenue) AS total_revenue,\n    SUM(quantity) AS total_quantity,\n    AVG(unit_price) AS avg_unit_price\nFROM {{ ref('fct_orders') }}\nGROUP BY ROLLUP(year, quarter, month, product_category)\n</code></pre> <p>Conformed dimensions:</p> <pre><code>-- Shared dimension across business units\nCREATE TABLE gold.dim_date AS\nSELECT\n    date_key,\n    date_actual,\n    day_of_week,\n    day_name,\n    week_of_year,\n    month_number,\n    month_name,\n    quarter,\n    year,\n    is_weekend,\n    is_holiday,\n    fiscal_year,\n    fiscal_quarter\nFROM date_spine\n</code></pre> <p>Business metrics:</p> <pre><code>-- KPIs calculated once, used everywhere\nSELECT\n    customer_id,\n    -- Recency, Frequency, Monetary (RFM)\n    DATEDIFF(CURRENT_DATE, MAX(order_date)) AS days_since_last_order,\n    COUNT(DISTINCT order_id) AS total_orders,\n    SUM(order_amount) AS lifetime_value,\n    AVG(order_amount) AS avg_order_value,\n    -- Customer status\n    CASE\n        WHEN DATEDIFF(CURRENT_DATE, MAX(order_date)) &lt;= 90 THEN 'Active'\n        WHEN DATEDIFF(CURRENT_DATE, MAX(order_date)) &lt;= 365 THEN 'At Risk'\n        ELSE 'Churned'\n    END AS customer_status\nFROM {{ ref('fct_orders') }}\nGROUP BY customer_id\n</code></pre>"},{"location":"guides/data-modeling/#gold-layer-best-practices","title":"Gold Layer Best Practices","text":"<p>1. Design for specific use cases</p> <pre><code>-- GOOD: Purpose-built for executive dashboard\n-- Gold table: agg_executive_daily_metrics\nSELECT\n    report_date,\n    total_revenue,\n    total_orders,\n    new_customers,\n    returning_customers,\n    avg_order_value,\n    revenue_vs_target,\n    orders_vs_target\nFROM daily_aggregations\nWHERE report_date &gt;= CURRENT_DATE - INTERVAL '90' DAY\n\n-- GOOD: Purpose-built for operations team\n-- Gold table: agg_hourly_fulfillment_metrics\nSELECT\n    fulfillment_hour,\n    warehouse_id,\n    orders_received,\n    orders_processed,\n    orders_shipped,\n    avg_processing_time_minutes,\n    pct_shipped_within_sla\nFROM hourly_warehouse_stats\n</code></pre> <p>2. Pre-calculate expensive computations</p> <pre><code>-- Calculate once in Gold, not every query\nWITH cohorts AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', first_order_date) AS cohort_month\n    FROM {{ ref('dim_customers') }}\n),\nretention AS (\n    SELECT\n        c.cohort_month,\n        DATE_TRUNC('month', o.order_date) AS order_month,\n        COUNT(DISTINCT o.customer_id) AS active_customers\n    FROM cohorts c\n    JOIN {{ ref('fct_orders') }} o ON c.customer_id = o.customer_id\n    GROUP BY c.cohort_month, order_month\n)\nSELECT * FROM retention\n</code></pre> <p>3. Naming convention</p> <pre><code>agg_&lt;grain&gt;_&lt;subject&gt;_&lt;metric&gt;\n\nExamples:\n- agg_daily_revenue_by_segment\n- agg_weekly_customer_retention\n- agg_monthly_product_performance\n- agg_hourly_website_traffic\n</code></pre>"},{"location":"guides/data-modeling/#the-marts-layer","title":"The Marts Layer","text":""},{"location":"guides/data-modeling/#purpose_3","title":"Purpose","text":"<p>The Marts layer publishes data to BI tools and applications, optimized for specific consumers.</p>"},{"location":"guides/data-modeling/#what-happens-here_3","title":"What Happens Here","text":"<p>Simplification:</p> <pre><code>-- Remove unnecessary columns\n-- Add business-friendly names\n-- Pre-filter to relevant data\nSELECT\n    date AS \"Date\",\n    city_name AS \"City\",\n    avg_temp_c AS \"Average Temperature (\u00b0C)\",\n    avg_temp_f AS \"Average Temperature (\u00b0F)\",\n    predominant_weather AS \"Weather Condition\",\n    pct_comfortable AS \"Comfortable Hours (%)\"\nFROM {{ ref('agg_daily_weather_summary') }}\nWHERE date &gt;= CURRENT_DATE - INTERVAL '90' DAY\nORDER BY date DESC\n</code></pre> <p>Denormalization:</p> <pre><code>-- Fully denormalized - no joins needed in BI tool\nSELECT\n    o.order_date,\n    o.order_id,\n    c.customer_name,\n    c.customer_email,\n    c.customer_segment,\n    p.product_name,\n    p.product_category,\n    o.quantity,\n    o.unit_price,\n    o.total_amount,\n    w.warehouse_name,\n    w.warehouse_region\nFROM {{ ref('fct_orders') }} o\nLEFT JOIN {{ ref('dim_customers') }} c ON o.customer_id = c.customer_id\nLEFT JOIN {{ ref('dim_products') }} p ON o.product_id = p.product_id\nLEFT JOIN {{ ref('dim_warehouses') }} w ON o.warehouse_id = w.warehouse_id\n</code></pre> <p>Performance optimization:</p> <pre><code>-- Published to PostgreSQL for fast BI queries\n-- Indexes added on common filter columns\n-- Partitioned by date if large\n-- Only recent data (e.g., last 2 years)\n</code></pre>"},{"location":"guides/data-modeling/#marts-layer-best-practices","title":"Marts Layer Best Practices","text":"<p>1. Design for your BI tool</p> <pre><code>-- GOOD: Tableau-friendly structure\nSELECT\n    date_key,\n    customer_segment,\n    product_category,\n    SUM(revenue) AS revenue,\n    SUM(quantity) AS quantity,\n    COUNT(DISTINCT order_id) AS order_count\nFROM combined_data\nGROUP BY date_key, customer_segment, product_category\n-- Tableau will create visualizations by dragging/dropping dimensions\n\n-- BAD: Requires complex calculations in BI tool\nSELECT * FROM raw_transaction_data  -- User must aggregate themselves\n</code></pre> <p>2. Naming convention</p> <pre><code>mrt_&lt;audience&gt;_&lt;subject&gt;\n\nExamples:\n- mrt_executive_dashboard_daily\n- mrt_sales_team_pipeline\n- mrt_operations_fulfillment_hourly\n- mrt_finance_revenue_summary\n</code></pre>"},{"location":"guides/data-modeling/#design-principles","title":"Design Principles","text":""},{"location":"guides/data-modeling/#principle-1-each-layer-has-a-clear-purpose","title":"Principle 1: Each Layer Has a Clear Purpose","text":"<p>Don't mix concerns:</p> <pre><code>-- BAD: Business logic in Bronze\nSELECT\n    id,\n    CASE WHEN status = 'completed' THEN 'success' ELSE 'other' END AS status_category  -- NO!\nFROM {{ source('raw', 'orders') }}\n\n-- GOOD: Just standardization\nSELECT\n    id,\n    LOWER(TRIM(status)) AS status\nFROM {{ source('raw', 'orders') }}\n</code></pre>"},{"location":"guides/data-modeling/#principle-2-progressive-transformation","title":"Principle 2: Progressive Transformation","text":"<p>Each layer should add value:</p> <pre><code>Raw:    {\"temp\": \"98.6\", \"time\": 1699200000}\nBronze: temp_f = 98.6, time = 2023-11-05 14:00:00\nSilver: temp_f = 98.6, temp_c = 37.0, is_fever = false\nGold:   daily_avg_temp_c = 36.8, daily_max_temp_c = 37.2\nMarts:  \"Average Temperature\": 36.8\u00b0C, \"Status\": \"Normal\"\n</code></pre>"},{"location":"guides/data-modeling/#principle-3-optimize-for-different-users","title":"Principle 3: Optimize for Different Users","text":"Layer Optimized For Query Pattern Bronze Engineers Debugging, investigation Silver Analysts Exploratory analysis Gold Business Users Standard reports Marts Executives Dashboards"},{"location":"guides/data-modeling/#principle-4-schema-flexibility","title":"Principle 4: Schema Flexibility","text":"<p>Design schemas to accommodate change:</p> <pre><code>-- GOOD: Flexible schema\nCREATE TABLE fct_events (\n    event_id STRING,\n    event_type STRING,\n    event_timestamp TIMESTAMP,\n    user_id STRING,\n    properties MAP&lt;STRING, STRING&gt;,  -- Extensible!\n    metadata STRUCT&lt;...&gt;\n)\n\n-- BAD: Rigid schema\nCREATE TABLE fct_events (\n    event_id STRING,\n    event_type STRING,\n    -- Adding new event properties requires schema change\n    property1 STRING,\n    property2 STRING\n)\n</code></pre>"},{"location":"guides/data-modeling/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/data-modeling/#pattern-1-slowly-changing-dimensions-scd","title":"Pattern 1: Slowly Changing Dimensions (SCD)","text":"<p>Type 1: Overwrite</p> <pre><code>-- Just update the dimension\nUPDATE dim_customers\nSET email = 'new@email.com'\nWHERE customer_id = 123\n</code></pre> <p>Type 2: Track history</p> <pre><code>CREATE TABLE dim_customers (\n    customer_key INT,  -- Surrogate key\n    customer_id INT,   -- Natural key\n    customer_name STRING,\n    email STRING,\n    valid_from TIMESTAMP,\n    valid_to TIMESTAMP,\n    is_current BOOLEAN\n)\n\n-- When customer email changes, add new row\nINSERT INTO dim_customers VALUES\n(456, 123, 'John Doe', 'new@email.com', '2024-11-05', NULL, true)\n\n-- Mark old row as historical\nUPDATE dim_customers\nSET valid_to = '2024-11-05', is_current = false\nWHERE customer_key = 123\n</code></pre>"},{"location":"guides/data-modeling/#pattern-2-snapshot-fact-tables","title":"Pattern 2: Snapshot Fact Tables","text":"<p>Capture periodic snapshots:</p> <pre><code>-- Daily inventory snapshot\nSELECT\n    snapshot_date,\n    product_id,\n    quantity_on_hand,\n    quantity_reserved,\n    quantity_available\nFROM inventory_daily_snapshot\nWHERE snapshot_date = CURRENT_DATE\n</code></pre>"},{"location":"guides/data-modeling/#pattern-3-accumulating-snapshot","title":"Pattern 3: Accumulating Snapshot","text":"<p>Track process milestones:</p> <pre><code>-- Order lifecycle\nSELECT\n    order_id,\n    order_date,\n    payment_date,\n    fulfillment_date,\n    shipped_date,\n    delivered_date,\n    -- Calculate durations\n    DATEDIFF(payment_date, order_date) AS days_to_payment,\n    DATEDIFF(shipped_date, fulfillment_date) AS days_to_ship,\n    DATEDIFF(delivered_date, shipped_date) AS days_in_transit\nFROM fct_order_lifecycle\n</code></pre>"},{"location":"guides/data-modeling/#pattern-4-event-sourcing","title":"Pattern 4: Event Sourcing","text":"<p>Keep all events, aggregate as needed:</p> <pre><code>-- Events table (append-only)\nCREATE TABLE fct_user_events (\n    event_id STRING,\n    user_id STRING,\n    event_type STRING,\n    event_timestamp TIMESTAMP,\n    event_data JSON\n)\n\n-- Aggregate to current state\nSELECT\n    user_id,\n    MAX(CASE WHEN event_type = 'profile_updated' THEN event_data['email'] END) AS current_email,\n    MAX(CASE WHEN event_type = 'subscription_changed' THEN event_data['plan'] END) AS current_plan\nFROM fct_user_events\nGROUP BY user_id\n</code></pre>"},{"location":"guides/data-modeling/#schema-evolution","title":"Schema Evolution","text":""},{"location":"guides/data-modeling/#adding-columns-easy","title":"Adding Columns (Easy)","text":"<pre><code>-- Iceberg supports adding columns without rewriting\nALTER TABLE silver.fct_orders\nADD COLUMN discount_amount DOUBLE;\n\n-- In dbt, just add to SELECT\nSELECT\n    *,\n    order_amount * discount_rate AS discount_amount  -- New column\nFROM {{ ref('stg_orders') }}\n</code></pre>"},{"location":"guides/data-modeling/#changing-column-types-medium","title":"Changing Column Types (Medium)","text":"<pre><code>-- Iceberg can evolve types (widening)\nALTER TABLE silver.fct_orders\nALTER COLUMN quantity TYPE BIGINT;  -- INT \u2192 BIGINT ok\n\n-- For narrowing, recreate table\nCREATE TABLE silver.fct_orders_v2 AS\nSELECT\n    order_id,\n    CAST(quantity AS INT) AS quantity  -- If you need to narrow\nFROM silver.fct_orders;\n</code></pre>"},{"location":"guides/data-modeling/#renaming-columns-easy","title":"Renaming Columns (Easy)","text":"<pre><code>-- Iceberg supports renaming\nALTER TABLE silver.fct_orders\nRENAME COLUMN old_name TO new_name;\n</code></pre>"},{"location":"guides/data-modeling/#removing-columns-easy","title":"Removing Columns (Easy)","text":"<pre><code>-- Iceberg supports dropping columns\nALTER TABLE silver.fct_orders\nDROP COLUMN old_column;\n</code></pre>"},{"location":"guides/data-modeling/#real-world-examples","title":"Real-World Examples","text":""},{"location":"guides/data-modeling/#example-1-e-commerce-orders","title":"Example 1: E-Commerce Orders","text":"<p>Raw \u2192 Bronze:</p> <pre><code>-- Bronze: Clean and standardize\nSELECT\n    order_id,\n    CAST(order_timestamp AS TIMESTAMP) AS order_timestamp,\n    CAST(customer_id AS STRING) AS customer_id,\n    CAST(total_amount AS DECIMAL(10,2)) AS total_amount,\n    LOWER(TRIM(status)) AS status\nFROM {{ source('raw', 'orders') }}\nWHERE order_id IS NOT NULL\n</code></pre> <p>Bronze \u2192 Silver:</p> <pre><code>-- Silver: Add business logic\nSELECT\n    order_id,\n    order_timestamp,\n    customer_id,\n    total_amount,\n    status,\n    -- Calculated fields\n    CASE WHEN status IN ('completed', 'delivered') THEN TRUE ELSE FALSE END AS is_fulfilled,\n    CASE\n        WHEN total_amount &lt; 50 THEN 'Low'\n        WHEN total_amount &lt; 200 THEN 'Medium'\n        ELSE 'High'\n    END AS value_tier,\n    -- Date parts\n    DATE(order_timestamp) AS order_date,\n    YEAR(order_timestamp) AS order_year,\n    MONTH(order_timestamp) AS order_month\nFROM {{ ref('stg_orders') }}\n</code></pre> <p>Silver \u2192 Gold:</p> <pre><code>-- Gold: Aggregate\nSELECT\n    order_date,\n    value_tier,\n    COUNT(DISTINCT order_id) AS total_orders,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    SUM(total_amount) AS total_revenue,\n    AVG(total_amount) AS avg_order_value,\n    SUM(CASE WHEN is_fulfilled THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS fulfillment_rate\nFROM {{ ref('fct_orders') }}\nGROUP BY order_date, value_tier\n</code></pre> <p>Gold \u2192 Marts:</p> <pre><code>-- Marts: Business-friendly\nSELECT\n    order_date AS \"Date\",\n    value_tier AS \"Order Value\",\n    total_orders AS \"Orders\",\n    unique_customers AS \"Customers\",\n    total_revenue AS \"Revenue\",\n    avg_order_value AS \"Avg Order Value\",\n    fulfillment_rate AS \"Fulfillment Rate (%)\"\nFROM {{ ref('agg_daily_orders') }}\nWHERE order_date &gt;= CURRENT_DATE - INTERVAL '90' DAY\nORDER BY order_date DESC\n</code></pre>"},{"location":"guides/data-modeling/#summary","title":"Summary","text":"<p>Bronze Layer:</p> <ul> <li>Clean and standardize</li> <li>Type conversions, column renaming</li> <li>Remove technical invalids</li> <li>No business logic</li> </ul> <p>Silver Layer:</p> <ul> <li>Add business logic</li> <li>Calculated fields, categorizations</li> <li>Fact and dimension tables</li> <li>Join related entities</li> </ul> <p>Gold Layer:</p> <ul> <li>Aggregate and rollup</li> <li>Business metrics</li> <li>Purpose-built datasets</li> <li>Conformed dimensions</li> </ul> <p>Marts Layer:</p> <ul> <li>Publish to BI tools</li> <li>Denormalized, optimized</li> <li>Business-friendly names</li> <li>Performance-tuned</li> </ul> <p>Remember: Each layer adds value and serves a specific purpose. Don't skip layers or mix concerns!</p> <p>Next: dbt Development Guide - Learn advanced dbt techniques for implementing these patterns.</p>"},{"location":"guides/dbt-development/","title":"dbt Development Guide","text":""},{"location":"guides/dbt-development/#sql-transformations-made-easy","title":"SQL Transformations Made Easy","text":"<p>This guide teaches you how to use dbt effectively in Phlo for data transformations.</p>"},{"location":"guides/dbt-development/#table-of-contents","title":"Table of Contents","text":"<ol> <li>dbt Basics</li> <li>Project Structure</li> <li>Sources</li> <li>Models</li> <li>Tests</li> <li>Documentation</li> <li>Macros and Functions</li> <li>Incremental Models</li> <li>Best Practices</li> </ol>"},{"location":"guides/dbt-development/#dbt-basics","title":"dbt Basics","text":""},{"location":"guides/dbt-development/#what-is-dbt","title":"What is dbt?","text":"<p>dbt (data build tool) transforms data in your warehouse using SQL SELECT statements.</p> <p>Philosophy:</p> <ul> <li>Transformations are SELECT statements (not INSERT/UPDATE)</li> <li>Version control your SQL</li> <li>Test data quality automatically</li> <li>Generate documentation automatically</li> <li>Manage dependencies automatically</li> </ul>"},{"location":"guides/dbt-development/#how-dbt-works","title":"How dbt Works","text":"<pre><code>1. You write: models/my_model.sql\n   SELECT * FROM source_table WHERE active = true\n\n2. dbt generates: CREATE TABLE my_schema.my_model AS\n   SELECT * FROM source_table WHERE active = true\n\n3. dbt runs it in your warehouse (Trino)\n\n4. Result: Table created!\n</code></pre>"},{"location":"guides/dbt-development/#dbt-in-phlo","title":"dbt in Phlo","text":"<p>Location: <code>workflows/transforms/dbt/</code></p> <p>Configuration: <code>workflows/transforms/dbt/dbt_project.yml</code></p> <p>Profile: <code>workflows/transforms/dbt/profiles/profiles.yml</code> (Trino connection)</p> <p>Models: <code>workflows/transforms/dbt/models/</code></p> <p>Integration: Dag ster runs dbt and tracks lineage automatically</p>"},{"location":"guides/dbt-development/#project-structure","title":"Project Structure","text":"<pre><code>workflows/transforms/dbt/\n\u251c\u2500\u2500 dbt_project.yml          # Project configuration\n\u251c\u2500\u2500 profiles/\n\u2502   \u2514\u2500\u2500 profiles.yml          # Connection settings\n\u2502\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 sources/              # Source definitions\n\u2502   \u2502   \u251c\u2500\u2500 sources.yml\n\u2502   \u2502   \u2514\u2500\u2500 sources_weather.yml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 bronze/               # Staging models\n\u2502   \u2502   \u251c\u2500\u2500 stg_glucose_entries.sql\n\u2502   \u2502   \u251c\u2500\u2500 stg_weather_observations.sql\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml        # Tests and documentation\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 silver/               # Fact/dimension models\n\u2502   \u2502   \u251c\u2500\u2500 fct_glucose_readings.sql\n\u2502   \u2502   \u251c\u2500\u2500 fct_weather_readings.sql\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 gold/                 # Aggregations\n\u2502   \u2502   \u251c\u2500\u2500 agg_daily_weather_summary.sql\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 marts_postgres/       # Published marts\n\u2502       \u251c\u2500\u2500 mrt_glucose_overview.sql\n\u2502       \u2514\u2500\u2500 schema.yml\n\u2502\n\u251c\u2500\u2500 macros/                   # Reusable SQL functions\n\u2502   \u2514\u2500\u2500 generate_schema_name.sql\n\u2502\n\u251c\u2500\u2500 tests/                    # Custom data tests\n\u2502   \u2514\u2500\u2500 assert_positive_values.sql\n\u2502\n\u2514\u2500\u2500 target/                   # Generated files (gitignored)\n    \u251c\u2500\u2500 manifest.json         # Dependency graph\n    \u251c\u2500\u2500 run_results.json      # Test results\n    \u2514\u2500\u2500 compiled/             # Compiled SQL\n</code></pre>"},{"location":"guides/dbt-development/#sources","title":"Sources","text":""},{"location":"guides/dbt-development/#what-are-sources","title":"What are Sources?","text":"<p>Sources are raw tables (not created by dbt) that you want to transform.</p>"},{"location":"guides/dbt-development/#defining-sources","title":"Defining Sources","text":"<p>Create: <code>models/sources/sources_weather.yml</code></p> <pre><code>version: 2\n\nsources:\n  - name: raw\n    description: \"Raw data layer\"\n    schema: raw # Iceberg schema\n    database: iceberg # Trino catalog\n\n    tables:\n      - name: weather_observations\n        description: \"Raw weather data from OpenWeather API\"\n        columns:\n          - name: city_name\n            description: \"City name\"\n            tests:\n              - not_null\n\n          - name: temperature\n            description: \"Temperature in Celsius\"\n            tests:\n              - not_null\n\n          - name: observation_time\n            description: \"When observation was recorded\"\n            tests:\n              - not_null\n              - dbt_utils.at_least_one\n\n      - name: glucose_entries\n        description: \"Raw glucose readings from Nightscout\"\n        # ... columns\n</code></pre>"},{"location":"guides/dbt-development/#referencing-sources","title":"Referencing Sources","text":"<p>In your models:</p> <pre><code>-- Use source() function\nSELECT *\nFROM {{ source('raw', 'weather_observations') }}\n\n-- dbt generates:\n-- SELECT * FROM iceberg.raw.weather_observations\n</code></pre> <p>Benefits:</p> <ul> <li>dbt knows dependencies</li> <li>Tests run on source data</li> <li>Documentation auto-generated</li> <li>Source freshness checks</li> </ul>"},{"location":"guides/dbt-development/#source-freshness","title":"Source Freshness","text":"<p>Check if source data is stale:</p> <pre><code>sources:\n  - name: raw\n    freshness:\n      warn_after: { count: 2, period: hour }\n      error_after: { count: 24, period: hour }\n\n    tables:\n      - name: weather_observations\n        loaded_at_field: observation_time\n        freshness:\n          warn_after: { count: 1, period: hour }\n</code></pre> <p>Check freshness:</p> <pre><code>dbt source freshness --project-dir /app/workflows/transforms/dbt\n</code></pre>"},{"location":"guides/dbt-development/#models","title":"Models","text":""},{"location":"guides/dbt-development/#what-are-models","title":"What are Models?","text":"<p>Models are SELECT statements that create tables/views.</p>"},{"location":"guides/dbt-development/#basic-model","title":"Basic Model","text":"<p>Create: <code>models/bronze/stg_weather_observations.sql</code></p> <pre><code>SELECT\n    city_name,\n    CAST(temperature AS DOUBLE) AS temperature_c,\n    CAST(observation_time AS TIMESTAMP) AS observation_timestamp\nFROM {{ source('raw', 'weather_observations') }}\nWHERE temperature IS NOT NULL\n</code></pre>"},{"location":"guides/dbt-development/#model-configuration","title":"Model Configuration","text":"<p>In-file config:</p> <pre><code>{{\n    config(\n        materialized='table',      # table, view, incremental, ephemeral\n        schema='bronze',            # Target schema\n        tags=['weather', 'bronze'], # Tags for selection\n        alias='weather_staging',    # Override table name\n    )\n}}\n\nSELECT * FROM {{ source('raw', 'weather_observations') }}\n</code></pre> <p>In dbt_project.yml:</p> <pre><code>models:\n  phlo:\n    # All models default to table\n    materialized: table\n\n    bronze:\n      # Bronze models are views (fast, always fresh)\n      materialized: view\n      schema: bronze\n\n    silver:\n      # Silver models are tables (fast queries)\n      materialized: table\n      schema: silver\n\n    gold:\n      materialized: table\n      schema: gold\n</code></pre>"},{"location":"guides/dbt-development/#materialization-types","title":"Materialization Types","text":"<p>1. Table (default)</p> <pre><code>{{ config(materialized='table') }}\n\n-- Creates: CREATE TABLE silver.my_model AS SELECT ...\n-- Pros: Fast queries\n-- Cons: Takes time to rebuild\n</code></pre> <p>2. View</p> <pre><code>{{ config(materialized='view') }}\n\n-- Creates: CREATE VIEW bronze.my_model AS SELECT ...\n-- Pros: Always fresh, fast to \"build\"\n-- Cons: Slow queries (re-runs SELECT every time)\n</code></pre> <p>3. Incremental</p> <pre><code>{{ config(materialized='incremental', unique_key='id') }}\n\nSELECT * FROM {{ source('raw', 'events') }}\n\n{% if is_incremental() %}\n    -- Only new records since last run\n    WHERE created_at &gt; (SELECT MAX(created_at) FROM {{ this }})\n{% endif %}\n\n-- Pros: Fast builds (only new data)\n-- Cons: More complex logic\n</code></pre> <p>4. Ephemeral</p> <pre><code>{{ config(materialized='ephemeral') }}\n\n-- Not materialized - just a CTE in downstream models\n-- Pros: No table created\n-- Cons: Can't query directly\n</code></pre>"},{"location":"guides/dbt-development/#referencing-models","title":"Referencing Models","text":"<pre><code>-- Use ref() function\nSELECT *\nFROM {{ ref('stg_weather_observations') }}\n\n-- dbt generates:\n-- SELECT * FROM iceberg.bronze.stg_weather_observations\n\n-- And tracks the dependency!\n</code></pre> <p>Dependency graph automatically built:</p> <pre><code>raw.weather_observations (source)\n    \u2193\nstg_weather_observations (ref)\n    \u2193\nfct_weather_readings (ref)\n</code></pre>"},{"location":"guides/dbt-development/#model-selection","title":"Model Selection","text":"<p>Run specific model:</p> <pre><code>dbt run --select stg_weather_observations\n</code></pre> <p>Run model and upstream:</p> <pre><code>dbt run --select +stg_weather_observations\n</code></pre> <p>Run model and downstream:</p> <pre><code>dbt run --select stg_weather_observations+\n</code></pre> <p>Run by tag:</p> <pre><code>dbt run --select tag:weather\n</code></pre> <p>Run by directory:</p> <pre><code>dbt run --select bronze.*\n</code></pre> <p>Exclude models:</p> <pre><code>dbt run --exclude tag:deprecated\n</code></pre>"},{"location":"guides/dbt-development/#tests","title":"Tests","text":""},{"location":"guides/dbt-development/#built-in-tests","title":"Built-in Tests","text":"<p>Add to <code>schema.yml</code>:</p> <pre><code>version: 2\n\nmodels:\n  - name: fct_weather_readings\n    description: \"Weather readings with metrics\"\n    columns:\n      - name: city_name\n        description: \"City name\"\n        tests:\n          - not_null # No NULL values\n          - unique # No duplicates\n\n      - name: temperature_c\n        description: \"Temperature in Celsius\"\n        tests:\n          - not_null\n          - accepted_values:\n              values: [-50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50]\n              quote: false\n          - dbt_utils.accepted_range:\n              min_value: -50\n              max_value: 60\n\n      - name: temp_category\n        tests:\n          - accepted_values:\n              values: [\"Freezing\", \"Cold\", \"Mild\", \"Warm\", \"Hot\"]\n\n      - name: observation_timestamp\n        tests:\n          - not_null\n          - dbt_utils.recency:\n              datepart: hour\n              field: observation_timestamp\n              interval: 24\n</code></pre>"},{"location":"guides/dbt-development/#custom-sql-tests","title":"Custom SQL Tests","text":"<p>Create: <code>tests/assert_reasonable_temperatures.sql</code></p> <pre><code>-- Test passes if query returns zero rows\n-- Test fails if query returns any rows\n\nSELECT\n    city_name,\n    temperature_c,\n    observation_timestamp\nFROM {{ ref('fct_weather_readings') }}\nWHERE\n    temperature_c &lt; -60  -- Unreasonably cold\n    OR temperature_c &gt; 60  -- Unreasonably hot\n</code></pre>"},{"location":"guides/dbt-development/#relationship-tests","title":"Relationship Tests","text":"<p>Ensure foreign keys are valid:</p> <pre><code>models:\n  - name: fct_orders\n    columns:\n      - name: customer_id\n        tests:\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n</code></pre>"},{"location":"guides/dbt-development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\ndbt test --project-dir /app/workflows/transforms/dbt\n\n# Test specific model\ndbt test --select fct_weather_readings\n\n# Test specific column\ndbt test --select fct_weather_readings,column:temperature_c\n\n# Run only data tests (not schema tests)\ndbt test --data\n\n# Run only schema tests\ndbt test --schema\n</code></pre>"},{"location":"guides/dbt-development/#test-severity","title":"Test Severity","text":"<pre><code>columns:\n  - name: temperature_c\n    tests:\n      - not_null:\n          severity: error # Fails build (default)\n\n      - accepted_values:\n          values: [...]\n          severity: warn # Warning only, doesn't fail\n</code></pre>"},{"location":"guides/dbt-development/#documentation","title":"Documentation","text":""},{"location":"guides/dbt-development/#column-documentation","title":"Column Documentation","text":"<pre><code>models:\n  - name: fct_weather_readings\n    description: |\n      Weather readings with calculated metrics and categorizations.\n\n      This model:\n      - Converts temperature to Fahrenheit\n      - Categorizes temperature (Freezing/Cold/Mild/Warm/Hot)\n      - Calculates comfort level\n      - Determines daytime vs nighttime\n\n    columns:\n      - name: city_name\n        description: \"Name of the city\"\n\n      - name: temperature_c\n        description: |\n          Temperature in Celsius as reported by the weather service.\n          Negative values indicate below freezing.\n\n      - name: temp_category\n        description: |\n          Temperature category based on Celsius value:\n          - Freezing: &lt; 0\u00b0C\n          - Cold: 0-10\u00b0C\n          - Mild: 10-20\u00b0C\n          - Warm: 20-30\u00b0C\n          - Hot: &gt; 30\u00b0C\n</code></pre>"},{"location":"guides/dbt-development/#generating-documentation","title":"Generating Documentation","text":"<pre><code># Generate docs\ndbt docs generate --project-dir /app/workflows/transforms/dbt\n\n# Serve docs locally\ndbt docs serve --project-dir /app/workflows/transforms/dbt --port 8080\n\n# Open browser to http://localhost:8080\n</code></pre>"},{"location":"guides/dbt-development/#documentation-features","title":"Documentation Features","text":"<ul> <li>Lineage graph - Visual dependency diagram</li> <li>Column details - Descriptions, types, tests</li> <li>Model code - SQL source code</li> <li>Test results - Pass/fail status</li> <li>Source freshness - Last updated times</li> </ul>"},{"location":"guides/dbt-development/#macros-and-functions","title":"Macros and Functions","text":""},{"location":"guides/dbt-development/#what-are-macros","title":"What are Macros?","text":"<p>Macros are reusable SQL snippets (like functions).</p>"},{"location":"guides/dbt-development/#creating-macros","title":"Creating Macros","text":"<p>Create: <code>macros/temperature_category.sql</code></p> <pre><code>{% macro temperature_category(temp_column) %}\n    CASE\n        WHEN {{ temp_column }} &lt; 0 THEN 'Freezing'\n        WHEN {{ temp_column }} &lt; 10 THEN 'Cold'\n        WHEN {{ temp_column }} &lt; 20 THEN 'Mild'\n        WHEN {{ temp_column }} &lt; 30 THEN 'Warm'\n        ELSE 'Hot'\n    END\n{% endmacro %}\n</code></pre>"},{"location":"guides/dbt-development/#using-macros","title":"Using Macros","text":"<pre><code>SELECT\n    city_name,\n    temperature_c,\n    {{ temperature_category('temperature_c') }} AS temp_category\nFROM {{ ref('stg_weather_observations') }}\n\n-- Compiles to:\n-- CASE\n--     WHEN temperature_c &lt; 0 THEN 'Freezing'\n--     ...\n-- END AS temp_category\n</code></pre>"},{"location":"guides/dbt-development/#built-in-dbt-macros","title":"Built-in dbt Macros","text":"<pre><code>-- Current timestamp\n{{ dbt_utils.current_timestamp() }}\n\n-- Generate surrogate key\n{{ dbt_utils.generate_surrogate_key(['city_name', 'observation_time']) }}\n\n-- Date spine (generate date series)\n{{ dbt_utils.date_spine(\n    datepart=\"day\",\n    start_date=\"cast('2024-01-01' as date)\",\n    end_date=\"cast('2024-12-31' as date)\"\n) }}\n\n-- Union tables\n{{ dbt_utils.union_relations(\n    relations=[ref('table1'), ref('table2')]\n) }}\n</code></pre>"},{"location":"guides/dbt-development/#jinja-control-flow","title":"Jinja Control Flow","text":"<pre><code>{% set cities = ['London', 'New York', 'Tokyo'] %}\n\nSELECT *\nFROM {{ ref('weather_observations') }}\nWHERE city_name IN (\n    {% for city in cities %}\n        '{{ city }}'{% if not loop.last %},{% endif %}\n    {% endfor %}\n)\n\n-- Compiles to:\n-- WHERE city_name IN ('London', 'New York', 'Tokyo')\n</code></pre>"},{"location":"guides/dbt-development/#conditional-logic","title":"Conditional Logic","text":"<pre><code>{% if target.name == 'prod' %}\n    -- Production-only logic\n    WHERE is_verified = true\n{% else %}\n    -- Development: all data\n    WHERE 1=1\n{% endif %}\n</code></pre>"},{"location":"guides/dbt-development/#incremental-models","title":"Incremental Models","text":""},{"location":"guides/dbt-development/#what-are-incremental-models","title":"What are Incremental Models?","text":"<p>Process only new data since last run (not full refresh).</p>"},{"location":"guides/dbt-development/#basic-incremental-model","title":"Basic Incremental Model","text":"<pre><code>{{\n    config(\n        materialized='incremental',\n        unique_key='id',\n    )\n}}\n\nSELECT\n    id,\n    city_name,\n    temperature_c,\n    observation_timestamp\nFROM {{ ref('stg_weather_observations') }}\n\n{% if is_incremental() %}\n    -- Only new records\n    WHERE observation_timestamp &gt; (\n        SELECT MAX(observation_timestamp)\n        FROM {{ this }}  -- Reference to current table\n    )\n{% endif %}\n</code></pre> <p>First run: Full table created</p> <p>Subsequent runs: Only new rows added</p>"},{"location":"guides/dbt-development/#incremental-strategies","title":"Incremental Strategies","text":"<p>1. Append (default)</p> <pre><code>{{ config(\n    materialized='incremental',\n    incremental_strategy='append',\n) }}\n\n-- Just adds new rows\n-- Fast but can create duplicates if not careful\n</code></pre> <p>2. Merge (upsert)</p> <pre><code>{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key='id',\n) }}\n\n-- Updates existing rows, inserts new rows\n-- Slower but handles updates correctly\n</code></pre> <p>3. Delete+Insert</p> <pre><code>{{ config(\n    materialized='incremental',\n    incremental_strategy='delete+insert',\n    unique_key='id',\n) }}\n\n-- Deletes matching rows, then inserts\n-- Good for partitioned data\n</code></pre>"},{"location":"guides/dbt-development/#partitioned-incremental","title":"Partitioned Incremental","text":"<pre><code>{{\n    config(\n        materialized='incremental',\n        unique_key='id',\n        partition_by={\n            'field': 'observation_date',\n            'data_type': 'date',\n            'granularity': 'day'\n        }\n    )\n}}\n\nSELECT * FROM {{ source('raw', 'observations') }}\n\n{% if is_incremental() %}\n    WHERE observation_date &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 3 DAY)\n{% endif %}\n\n-- Only processes last 3 days (handles late arrivals)\n</code></pre>"},{"location":"guides/dbt-development/#full-refresh","title":"Full Refresh","text":"<p>Force full rebuild:</p> <pre><code># Rebuild one model\ndbt run --select my_incremental_model --full-refresh\n\n# Rebuild all incremental models\ndbt run --full-refresh\n</code></pre>"},{"location":"guides/dbt-development/#best-practices","title":"Best Practices","text":""},{"location":"guides/dbt-development/#1-naming-conventions","title":"1. Naming Conventions","text":"<pre><code>Sources:     raw.table_name\nBronze:      stg_source_entity\nSilver Fact: fct_subject_event\nSilver Dim:  dim_entity\nGold:        agg_grain_subject\nMarts:       mrt_audience_subject\n\nExamples:\nraw.nightscout_entries\nstg_nightscout_glucose_entries\nfct_glucose_readings\ndim_date\nagg_daily_glucose_summary\nmrt_patient_glucose_overview\n</code></pre>"},{"location":"guides/dbt-development/#2-dry-dont-repeat-yourself","title":"2. DRY (Don't Repeat Yourself)","text":"<p>Bad:</p> <pre><code>-- orders_2023.sql\nSELECT * FROM raw.orders WHERE YEAR(order_date) = 2023\n\n-- orders_2024.sql\nSELECT * FROM raw.orders WHERE YEAR(order_date) = 2024\n</code></pre> <p>Good:</p> <pre><code>-- macros/filter_by_year.sql\n{% macro filter_by_year(year) %}\n    WHERE YEAR(order_date) = {{ year }}\n{% endmacro %}\n\n-- orders.sql\nSELECT * FROM raw.orders {{ filter_by_year(var('year')) }}\n</code></pre>"},{"location":"guides/dbt-development/#3-ctes-for-readability","title":"3. CTEs for Readability","text":"<pre><code>-- Good: Clear, readable\nWITH source AS (\n    SELECT * FROM {{ source('raw', 'orders') }}\n),\n\nfiltered AS (\n    SELECT *\n    FROM source\n    WHERE order_date &gt;= '2024-01-01'\n),\n\nwith_metrics AS (\n    SELECT\n        *,\n        amount * 0.1 AS tax,\n        amount * 1.1 AS total\n    FROM filtered\n)\n\nSELECT * FROM with_metrics\n\n-- Bad: Hard to read\nSELECT\n    *,\n    amount * 0.1 AS tax,\n    amount * 1.1 AS total\nFROM (\n    SELECT *\n    FROM {{ source('raw', 'orders') }}\n    WHERE order_date &gt;= '2024-01-01'\n) filtered\n</code></pre>"},{"location":"guides/dbt-development/#4-comment-your-sql","title":"4. Comment Your SQL","text":"<pre><code>WITH base AS (\n    SELECT * FROM {{ ref('stg_orders') }}\n),\n\n-- Business rule (2024-06): Exclude cancelled orders per product team\nactive_orders AS (\n    SELECT *\n    FROM base\n    WHERE status != 'cancelled'\n),\n\n-- Calculate lifetime value per customer\n-- Uses SUM(amount) not COUNT(*) per analytics team decision\ncustomer_ltv AS (\n    SELECT\n        customer_id,\n        SUM(amount) AS lifetime_value\n    FROM active_orders\n    GROUP BY customer_id\n)\n\nSELECT * FROM customer_ltv\n</code></pre>"},{"location":"guides/dbt-development/#5-keep-models-focused","title":"5. Keep Models Focused","text":"<p>Bad: One huge model</p> <pre><code>-- orders_with_everything.sql (1000 lines)\nWITH orders AS (...),\n     customers AS (...),\n     products AS (...),\n     inventory AS (...),\n     shipping AS (...),\n     payments AS (...),\n     -- Too many concerns!\n</code></pre> <p>Good: Separate models</p> <pre><code>stg_orders.sql\nstg_customers.sql\nfct_orders.sql  -- Joins orders + customers\nfct_inventory.sql  -- Separate concern\n</code></pre>"},{"location":"guides/dbt-development/#6-test-everything-important","title":"6. Test Everything Important","text":"<pre><code>models:\n  - name: fct_orders\n    tests:\n      # Row-level tests\n      - dbt_utils.at_least_one\n\n    columns:\n      # Key columns\n      - name: order_id\n        tests:\n          - not_null\n          - unique\n\n      # Foreign keys\n      - name: customer_id\n        tests:\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n\n      # Business logic\n      - name: total_amount\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"&gt;= 0\" # No negative amounts\n</code></pre>"},{"location":"guides/dbt-development/#7-document-important-logic","title":"7. Document Important Logic","text":"<pre><code>models:\n  - name: fct_orders\n    description: |\n      Orders with calculated metrics and business logic applied.\n\n      **Business Rules:**\n      - Excludes cancelled orders (status != 'cancelled')\n      - Applies tax rate of 10% (0.1)\n      - Free shipping for orders &gt; $100\n      - Discount codes applied before tax\n\n      **Data Quality:**\n      - Deduplicates on order_id + order_timestamp\n      - Filters out test orders (customer_id != 'TEST')\n      - Removes orders with negative amounts\n\n      **Refresh Schedule:**\n      - Full refresh: Weekly on Sunday\n      - Incremental: Hourly\n</code></pre>"},{"location":"guides/dbt-development/#8-use-variables-for-configuration","title":"8. Use Variables for Configuration","text":"<p>dbt_project.yml:</p> <pre><code>vars:\n  start_date: \"2024-01-01\"\n  tax_rate: 0.1\n  free_shipping_threshold: 100\n</code></pre> <p>In models:</p> <pre><code>SELECT\n    *,\n    amount * {{ var('tax_rate') }} AS tax_amount\nFROM orders\nWHERE order_date &gt;= '{{ var('start_date') }}'\n</code></pre>"},{"location":"guides/dbt-development/#9-leverage-tags","title":"9. Leverage Tags","text":"<pre><code>models:\n  bronze:\n    +tags: [\"bronze\", \"staging\"]\n\n  silver:\n    fct_orders:\n      +tags: [\"silver\", \"fact\", \"revenue\", \"daily\"]\n\n    dim_customers:\n      +tags: [\"silver\", \"dimension\", \"customer\"]\n</code></pre> <p>Run by tag:</p> <pre><code>dbt run --select tag:revenue\ndbt test --select tag:critical\n</code></pre>"},{"location":"guides/dbt-development/#10-version-control-everything","title":"10. Version Control Everything","text":"<p>.gitignore:</p> <pre><code>target/\ndbt_packages/\nlogs/\n.phlo/.env.local\n</code></pre> <p>Commit:</p> <ul> <li>\u2705 Models (.sql)</li> <li>\u2705 Tests (.yml, .sql)</li> <li>\u2705 Macros (.sql)</li> <li>\u2705 Documentation (.yml, .md)</li> <li>\u2705 Configuration (.yml)</li> <li>\u274c Compiled SQL (target/)</li> <li>\u274c Packages (dbt_packages/)</li> </ul>"},{"location":"guides/dbt-development/#summary","title":"Summary","text":"<p>Key Concepts:</p> <ul> <li>Sources = External tables</li> <li>Models = SELECT statements</li> <li>Refs = Dependencies</li> <li>Tests = Data quality</li> <li>Docs = Auto-generated</li> <li>Macros = Reusable SQL</li> <li>Incremental = Process only new data</li> </ul> <p>Common Commands:</p> <pre><code># Compile (check for errors)\ndbt compile\n\n# Run transformations\ndbt run\n\n# Run specific model\ndbt run --select my_model\n\n# Test data quality\ndbt test\n\n# Generate documentation\ndbt docs generate\ndbt docs serve\n</code></pre> <p>Best Practices:</p> <ul> <li>\u2705 Use naming conventions</li> <li>\u2705 Keep models focused</li> <li>\u2705 Use CTEs for readability</li> <li>\u2705 Comment business logic</li> <li>\u2705 Test important columns</li> <li>\u2705 Document everything</li> <li>\u2705 Use incremental models for large data</li> <li>\u2705 Version control all code</li> </ul> <p>Next: Troubleshooting Guide - Debug common dbt issues.</p>"},{"location":"guides/developer-guide/","title":"Developer Guide","text":"<p>Complete guide to building data pipelines with Phlo's decorator-driven framework.</p>"},{"location":"guides/developer-guide/#overview","title":"Overview","text":"<p>Phlo provides powerful decorators that transform simple functions into complete data pipelines. This guide covers:</p> <ul> <li>Using <code>@phlo_ingestion</code> for data ingestion</li> <li>Using <code>@phlo_quality</code> for data quality checks</li> <li>Schema definition with Pandera</li> <li>Integration with dbt</li> <li>Publishing to BI tools</li> <li>Advanced patterns and best practices</li> </ul>"},{"location":"guides/developer-guide/#quick-example","title":"Quick Example","text":"<p>A complete ingestion pipeline in ~30 lines:</p> <pre><code># workflows/schemas/api.py\nimport pandera as pa\nfrom pandera.typing import Series\n\nclass EventSchema(pa.DataFrameModel):\n    id: Series[str] = pa.Field(nullable=False, unique=True)\n    timestamp: Series[datetime] = pa.Field(nullable=False)\n    value: Series[float] = pa.Field(ge=0, le=100)\n\n# workflows/ingestion/api/events.py\nfrom dlt.sources.rest_api import rest_api\nfrom phlo_dlt import phlo_ingestion\nfrom workflows.schemas.api import EventSchema\n\n@phlo_ingestion(\n    table_name=\"events\",\n    unique_key=\"id\",\n    validation_schema=EventSchema,\n    group=\"api\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef api_events(partition_date: str):\n    return rest_api({\n        \"client\": {\"base_url\": \"https://api.example.com\"},\n        \"resources\": [{\n            \"name\": \"events\",\n            \"endpoint\": {\"path\": f\"/events?date={partition_date}\"}\n        }]\n    })\n\n# workflows/quality/api.py\nfrom phlo_quality import phlo_quality, NullCheck, RangeCheck, UniqueCheck\n\n@phlo_quality(\n    table=\"bronze.events\",\n    checks=[\n        NullCheck(columns=[\"id\", \"timestamp\"]),\n        RangeCheck(column=\"value\", min_value=0, max_value=100),\n        UniqueCheck(columns=[\"id\"])\n    ]\n)\ndef events_quality():\n    pass\n</code></pre> <p>That's it! You get:</p> <ul> <li>Automatic DLT pipeline setup</li> <li>Iceberg table creation from Pandera schema</li> <li>Merge with deduplication</li> <li>Validation enforcement</li> <li>Quality checks with detailed reporting</li> <li>Branch-aware writes</li> <li>Retry handling</li> <li>Metrics tracking</li> </ul>"},{"location":"guides/developer-guide/#phlo_ingestion-decorator","title":"@phlo_ingestion Decorator","text":""},{"location":"guides/developer-guide/#basic-usage","title":"Basic Usage","text":"<pre><code>from phlo_dlt import phlo_ingestion\nfrom dlt.sources.rest_api import rest_api\n\n@phlo_ingestion(\n    table_name=\"my_table\",\n    unique_key=\"id\",\n    validation_schema=MySchema,\n    group=\"my_group\",\n)\ndef my_ingestion(partition_date: str):\n    # Return a DLT source\n    return rest_api(...)\n</code></pre>"},{"location":"guides/developer-guide/#parameters","title":"Parameters","text":"<p>Required:</p> <p><code>table_name</code> (str): Name of target Iceberg table</p> <pre><code>table_name=\"events\"  # Creates bronze.events\n</code></pre> <p><code>unique_key</code> (str): Column used for deduplication</p> <pre><code>unique_key=\"id\"  # Primary key column\n</code></pre> <p><code>validation_schema</code> (pa.DataFrameModel): Pandera schema for validation</p> <pre><code>validation_schema=EventSchema  # Must be a Pandera DataFrameModel\n</code></pre> <p><code>group</code> (str): Logical grouping for organization</p> <pre><code>group=\"api\"  # Groups assets in Dagster UI\n</code></pre> <p>Optional:</p> <p><code>cron</code> (str): Cron schedule expression</p> <pre><code>cron=\"0 */1 * * *\"  # Every hour\ncron=\"0 0 * * *\"    # Daily at midnight\n</code></pre> <p><code>freshness_hours</code> (tuple): Freshness policy (warn, error)</p> <pre><code>freshness_hours=(1, 24)  # Warn after 1h, error after 24h\n</code></pre> <p><code>merge_strategy</code> (str): How to handle updates</p> <pre><code>merge_strategy=\"merge\"   # Upsert (default)\nmerge_strategy=\"append\"  # Insert-only\n</code></pre> <p><code>merge_config</code> (dict): Merge and deduplication configuration</p> <pre><code>merge_config={\"deduplication_method\": \"last\"}   # Keep last occurrence (default)\nmerge_config={\"deduplication_method\": \"first\"}  # Keep first occurrence\nmerge_config={\"deduplication_method\": \"hash\"}   # Keep based on content hash\n</code></pre> <p><code>max_retries</code> (int): Number of retry attempts (default: 3)</p> <pre><code>max_retries=3\n</code></pre> <p><code>retry_delay_seconds</code> (int): Delay between retries in seconds (default: 30)</p> <pre><code>retry_delay_seconds=30\n</code></pre> <p><code>max_runtime_seconds</code> (int): Execution timeout in seconds (default: 300)</p> <pre><code>max_runtime_seconds=3600  # 1 hour\n</code></pre> <p><code>validate</code> (bool): Enable validation (default: True)</p> <pre><code>validate=True\n</code></pre> <p><code>strict_validation</code> (bool): Fail on validation errors (default: True)</p> <pre><code>strict_validation=True\n</code></pre> <p><code>add_metadata_columns</code> (bool): Add <code>_phlo_*</code> metadata columns (default: True)</p> <pre><code>add_metadata_columns=True\n</code></pre>"},{"location":"guides/developer-guide/#dlt-source-integration","title":"DLT Source Integration","text":"<p>Phlo works with any DLT source. Common patterns:</p> <p>REST API Source:</p> <pre><code>from dlt.sources.rest_api import rest_api\n\n@phlo_ingestion(...)\ndef api_data(partition_date: str):\n    return rest_api({\n        \"client\": {\n            \"base_url\": \"https://api.example.com\",\n            \"auth\": {\n                \"type\": \"bearer\",\n                \"token\": os.getenv(\"API_TOKEN\")\n            }\n        },\n        \"resources\": [{\n            \"name\": \"events\",\n            \"endpoint\": {\n                \"path\": \"events\",\n                \"params\": {\n                    \"date\": partition_date,\n                    \"limit\": 1000\n                }\n            },\n            \"write_disposition\": \"replace\"\n        }]\n    })\n</code></pre> <p>Custom Python Source:</p> <pre><code>import dlt\n\n@dlt.source\ndef my_source(start_date: str):\n    @dlt.resource(write_disposition=\"append\")\n    def events():\n        # Custom logic to yield records\n        for record in fetch_data(start_date):\n            yield record\n    return events\n\n@phlo_ingestion(...)\ndef custom_data(partition_date: str):\n    return my_source(start_date=partition_date)\n</code></pre> <p>File Source:</p> <pre><code>from dlt.sources.filesystem import filesystem\n\n@phlo_ingestion(...)\ndef file_data(partition_date: str):\n    return filesystem(\n        bucket_url=f\"s3://bucket/data/{partition_date}\",\n        file_glob=\"*.csv\"\n    )\n</code></pre> <p>SQL Source:</p> <pre><code>import dlt\nfrom sqlalchemy import create_engine\n\n@phlo_ingestion(...)\ndef sql_data(partition_date: str):\n    @dlt.resource\n    def query():\n        engine = create_engine(os.getenv(\"DATABASE_URL\"))\n        return pd.read_sql(\n            f\"SELECT * FROM events WHERE date = '{partition_date}'\",\n            engine\n        ).to_dict('records')\n    return query\n</code></pre>"},{"location":"guides/developer-guide/#merge-strategies","title":"Merge Strategies","text":"<p>Append Strategy (fastest, no deduplication):</p> <pre><code>@phlo_ingestion(\n    table_name=\"logs\",\n    unique_key=\"id\",\n    merge_strategy=\"append\",  # Insert-only\n    ...\n)\ndef logs(partition_date: str):\n    # Good for: immutable event streams, logs\n    return source\n</code></pre> <p>Merge Strategy (upsert with deduplication):</p> <pre><code>@phlo_ingestion(\n    table_name=\"users\",\n    unique_key=\"user_id\",\n    merge_strategy=\"merge\",\n    merge_config={\"deduplication_method\": \"last\"},  # Keep most recent\n    ...\n)\ndef users(partition_date: str):\n    # Good for: dimension tables, user profiles\n    return source\n</code></pre> <p>Deduplication Strategies:</p> <p><code>last</code> (default): Keep last occurrence by partition</p> <pre><code>merge_config={\"deduplication_method\": \"last\"}\n# If same ID appears twice, keep the one with latest timestamp\n</code></pre> <p><code>first</code>: Keep first occurrence</p> <pre><code>merge_config={\"deduplication_method\": \"first\"}\n# If same ID appears twice, keep the one with earliest timestamp\n</code></pre> <p><code>hash</code>: Keep based on content hash</p> <pre><code>merge_config={\"deduplication_method\": \"hash\"}\n# If same ID appears twice, keep the one with different content\n</code></pre>"},{"location":"guides/developer-guide/#partition-handling","title":"Partition Handling","text":"<p>Phlo uses daily partitioning by default:</p> <pre><code>@phlo_ingestion(...)\ndef my_data(partition_date: str):\n    # partition_date is automatically provided by Dagster\n    # Format: \"YYYY-MM-DD\"\n    start_time = f\"{partition_date}T00:00:00Z\"\n    end_time = f\"{partition_date}T23:59:59Z\"\n\n    return rest_api({\n        \"resources\": [{\n            \"endpoint\": {\n                \"params\": {\n                    \"start\": start_time,\n                    \"end\": end_time\n                }\n            }\n        }]\n    })\n</code></pre> <p>Backfills:</p> <pre><code># Backfill specific date\nphlo materialize my_data --partition 2025-01-15\n\n# Backfill date range (in Dagster UI)\n# Select partitions \u2192 2025-01-01 to 2025-01-31 \u2192 Materialize\n</code></pre>"},{"location":"guides/developer-guide/#pandera-schemas","title":"Pandera Schemas","text":"<p>Schemas serve as the source of truth for data structure and validation.</p>"},{"location":"guides/developer-guide/#basic-schema","title":"Basic Schema","text":"<pre><code>import pandera as pa\nfrom pandera.typing import Series\nfrom datetime import datetime\n\nclass MySchema(pa.DataFrameModel):\n    \"\"\"My data schema.\"\"\"\n\n    # Basic types\n    id: Series[str]\n    count: Series[int]\n    amount: Series[float]\n    timestamp: Series[datetime]\n    is_active: Series[bool]\n\n    class Config:\n        strict = True  # Reject unknown columns\n        coerce = True  # Coerce types automatically\n</code></pre>"},{"location":"guides/developer-guide/#field-constraints","title":"Field Constraints","text":"<pre><code>class AdvancedSchema(pa.DataFrameModel):\n    # Not null\n    id: Series[str] = pa.Field(nullable=False)\n\n    # Unique values\n    email: Series[str] = pa.Field(unique=True)\n\n    # Range validation\n    age: Series[int] = pa.Field(ge=0, le=150)\n    temperature: Series[float] = pa.Field(ge=-50.0, le=50.0)\n\n    # String patterns\n    postal_code: Series[str] = pa.Field(regex=r\"^\\d{5}$\")\n\n    # Allowed values\n    status: Series[str] = pa.Field(isin=[\"active\", \"inactive\", \"pending\"])\n\n    # String length\n    name: Series[str] = pa.Field(str_length={\"min_value\": 1, \"max_value\": 100})\n\n    # Custom checks\n    email: Series[str] = pa.Field(str_contains=\"@\")\n\n    # Descriptions (for documentation)\n    user_id: Series[str] = pa.Field(\n        description=\"Unique user identifier\",\n        nullable=False\n    )\n</code></pre>"},{"location":"guides/developer-guide/#optional-fields","title":"Optional Fields","text":"<pre><code>class SchemaWithOptional(pa.DataFrameModel):\n    # Required field\n    id: Series[str] = pa.Field(nullable=False)\n\n    # Optional field (allows None)\n    notes: Series[str] | None = pa.Field(nullable=True)\n\n    # Optional with default\n    status: Series[str] = pa.Field(\n        nullable=True,\n        default=\"pending\"\n    )\n</code></pre>"},{"location":"guides/developer-guide/#custom-validators","title":"Custom Validators","text":"<pre><code>import pandera as pa\nfrom pandera import check\n\nclass CustomSchema(pa.DataFrameModel):\n    value: Series[float]\n\n    @check(\"value\")\n    def value_is_positive(cls, value):\n        return value &gt; 0\n\n    @check(\"value\")\n    def value_is_reasonable(cls, value):\n        return value &lt; 1000000\n\n# Multi-column check\nclass MultiColumnSchema(pa.DataFrameModel):\n    start_date: Series[datetime]\n    end_date: Series[datetime]\n\n    @pa.check(\"end_date\")\n    def end_after_start(cls, series):\n        return series &gt;= cls.start_date\n</code></pre>"},{"location":"guides/developer-guide/#schema-conversion-to-iceberg","title":"Schema Conversion to Iceberg","text":"<p>Pandera types automatically convert to Iceberg types:</p> <pre><code># Pandera \u2192 Iceberg mapping:\nstr \u2192 StringType()\nint \u2192 LongType()\nfloat \u2192 DoubleType()\ndatetime \u2192 TimestamptzType()\nbool \u2192 BooleanType()\n\n# Example:\nclass MySchema(pa.DataFrameModel):\n    id: Series[str]         # \u2192 StringType()\n    count: Series[int]      # \u2192 LongType()\n    amount: Series[float]   # \u2192 DoubleType()\n    timestamp: Series[datetime]  # \u2192 TimestamptzType()\n\n# Results in Iceberg schema:\nSchema(\n    NestedField(1, \"id\", StringType(), required=True),\n    NestedField(2, \"count\", LongType(), required=True),\n    NestedField(3, \"amount\", DoubleType(), required=True),\n    NestedField(4, \"timestamp\", TimestamptzType(), required=True),\n    # DLT metadata fields added automatically:\n    NestedField(100, \"_dlt_load_id\", StringType(), required=False),\n    NestedField(101, \"_dlt_id\", StringType(), required=False),\n    NestedField(102, \"_cascade_ingested_at\", TimestamptzType(), required=False),\n)\n</code></pre>"},{"location":"guides/developer-guide/#phlo_quality-decorator","title":"@phlo_quality Decorator","text":""},{"location":"guides/developer-guide/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from phlo_quality import phlo_quality\nfrom phlo_quality.checks import NullCheck, RangeCheck\n\n@phlo_quality(\n    table=\"bronze.events\",\n    checks=[\n        NullCheck(columns=[\"id\", \"timestamp\"]),\n        RangeCheck(column=\"value\", min_value=0, max_value=100)\n    ]\n)\ndef events_quality():\n    pass\n</code></pre>"},{"location":"guides/developer-guide/#built-in-checks","title":"Built-in Checks","text":"<p>NullCheck: Ensure no null values</p> <pre><code>NullCheck(columns=[\"id\", \"email\", \"timestamp\"])\n</code></pre> <p>RangeCheck: Numeric values within bounds</p> <pre><code>RangeCheck(column=\"age\", min_value=0, max_value=150)\nRangeCheck(column=\"temperature\", min_value=-50.0, max_value=50.0)\n</code></pre> <p>FreshnessCheck: Data recency</p> <pre><code>FreshnessCheck(\n    column=\"timestamp\",\n    max_age_hours=24  # Error if data older than 24h\n)\n</code></pre> <p>UniqueCheck: No duplicate values</p> <pre><code>UniqueCheck(columns=[\"id\"])\nUniqueCheck(columns=[\"user_id\", \"timestamp\"])  # Composite key\n</code></pre> <p>CountCheck: Row count validation</p> <pre><code>CountCheck(min_count=1)  # At least 1 row\nCountCheck(max_count=1000000)  # At most 1M rows\nCountCheck(min_count=100, max_count=10000)  # Between 100-10k\n</code></pre> <p>SchemaCheck: Full Pandera schema validation</p> <pre><code>from workflows.schemas.api import EventSchema\n\nSchemaCheck(schema=EventSchema)\n</code></pre> <p>CustomSQLCheck: Arbitrary SQL validation</p> <pre><code>CustomSQLCheck(\n    query=\"SELECT COUNT(*) FROM bronze.events WHERE value &lt; 0\",\n    expected_result=0,\n    description=\"No negative values\"\n)\n</code></pre>"},{"location":"guides/developer-guide/#reconciliation-checks-cross-table","title":"Reconciliation Checks (Cross-table)","text":"<p>Reconciliation checks live in <code>phlo_quality.reconciliation</code> and use the Trino resource from the Dagster context to query source tables.</p> <p>ReconciliationCheck: Row count parity / coverage between source and target</p> <ul> <li><code>check_type=\"rowcount_parity\"</code>: target and source counts must match (within tolerance)</li> <li><code>check_type=\"rowcount_gte\"</code>: target must be &gt;= source (within tolerance)</li> </ul> <pre><code>from phlo_quality.reconciliation import ReconciliationCheck\n\nReconciliationCheck(\n    source_table=\"silver.stg_github_events\",\n    partition_column=\"_phlo_partition_date\",\n    check_type=\"rowcount_parity\",\n    tolerance=0.02,  # 2% allowed difference\n    absolute_tolerance=50,  # Optional absolute row difference\n)\n</code></pre> <p>AggregateConsistencyCheck: Compare target aggregates to source aggregates</p> <pre><code>from phlo_quality.reconciliation import AggregateConsistencyCheck\n\nAggregateConsistencyCheck(\n    source_table=\"silver.stg_github_events\",\n    aggregate_column=\"total_events\",\n    source_expression=\"COUNT(*)\",\n    group_by=[\"activity_date\"],\n    partition_column=\"_phlo_partition_date\",\n    tolerance=0.0,\n    absolute_tolerance=5,\n)\n</code></pre> <p>KeyParityCheck: Ensure keys match between source and target</p> <pre><code>from phlo_quality.reconciliation import KeyParityCheck\n\nKeyParityCheck(\n    source_table=\"silver.stg_github_events\",\n    key_columns=[\"event_id\"],\n    partition_column=\"_phlo_partition_date\",\n    tolerance=0.0,\n)\n</code></pre> <p>MultiAggregateConsistencyCheck: Compare multiple aggregates in one check</p> <pre><code>from phlo_quality.reconciliation import AggregateSpec, MultiAggregateConsistencyCheck\n\nMultiAggregateConsistencyCheck(\n    source_table=\"silver.stg_github_events\",\n    aggregates=[\n        AggregateSpec(name=\"row_count\", expression=\"COUNT(*)\", target_column=\"total_events\"),\n        AggregateSpec(name=\"total_amount\", expression=\"SUM(amount)\", target_column=\"amount_total\"),\n    ],\n    group_by=[\"activity_date\"],\n    partition_column=\"_phlo_partition_date\",\n    tolerance=0.0,\n    absolute_tolerance=5,\n)\n</code></pre> <p>ChecksumReconciliationCheck: Compare row-level hashes across tables</p> <pre><code>from phlo_quality.reconciliation import ChecksumReconciliationCheck\n\nChecksumReconciliationCheck(\n    source_table=\"silver.stg_github_events\",\n    target_table=\"gold.fct_github_events\",\n    key_columns=[\"event_id\"],\n    columns=[\"event_type\", \"actor_id\", \"repo_id\"],\n    partition_column=\"_phlo_partition_date\",\n    tolerance=0.0,\n    absolute_tolerance=10,\n    hash_algorithm=\"xxhash64\",\n)\n</code></pre> <p>Common reconciliation gaps (use CustomSQLCheck or @asset_check):</p> <ul> <li>Multi-source or multi-target reconciliation in one check</li> <li>Distribution drift checks (percentiles/histograms vs source)</li> <li>Row-level checksum with engine-specific normalization rules</li> </ul>"},{"location":"guides/developer-guide/#advanced-quality-checks","title":"Advanced Quality Checks","text":"<p>Multiple tables:</p> <pre><code>@phlo_quality(\n    table=\"bronze.events\",\n    checks=[\n        CustomSQLCheck(\n            query=\"\"\"\n                SELECT COUNT(*)\n                FROM bronze.events e\n                LEFT JOIN bronze.users u ON e.user_id = u.id\n                WHERE u.id IS NULL\n            \"\"\",\n            expected_result=0,\n            description=\"All events have valid user_id\"\n        )\n    ]\n)\ndef referential_integrity():\n    pass\n</code></pre> <p>Conditional checks:</p> <pre><code>import pandas as pd\nfrom datetime import datetime\nfrom phlo_quality.checks import QualityCheck, QualityCheckResult\n\nclass ConditionalCheck(QualityCheck):\n    def execute(self, df: pd.DataFrame, context) -&gt; QualityCheckResult:\n        # Only run check on weekdays\n        if datetime.now().weekday() &gt;= 5:\n            return QualityCheckResult(\n                passed=True,\n                failed=False,\n                message=\"Skipped on weekend\"\n            )\n\n        # Run validation\n        passed = len(df) &gt; 0  # Example validation\n        return QualityCheckResult(\n            passed=passed,\n            failed=not passed,\n            message=f\"Validated {len(df)} rows\"\n        )\n\n@phlo_quality(\n    table=\"bronze.events\",\n    checks=[ConditionalCheck()]\n)\ndef conditional_quality():\n    pass\n</code></pre>"},{"location":"guides/developer-guide/#quality-check-results","title":"Quality Check Results","text":"<p>Check results include rich metadata:</p> <pre><code>{\n    \"passed\": True,\n    \"check_name\": \"NullCheck\",\n    \"table\": \"bronze.events\",\n    \"columns\": [\"id\", \"timestamp\"],\n    \"row_count\": 1000,\n    \"null_count\": 0,\n    \"execution_time_seconds\": 0.5\n}\n</code></pre> <p>Displayed in Dagster UI with:</p> <ul> <li>Pass/fail status</li> <li>Detailed metrics table</li> <li>Execution timing</li> <li>Error messages (if failed)</li> </ul>"},{"location":"guides/developer-guide/#dbt-integration","title":"dbt Integration","text":"<p>Phlo automatically integrates with dbt for transformations.</p>"},{"location":"guides/developer-guide/#setup","title":"Setup","text":"<pre><code># dbt project structure\nworkflows/transforms/dbt/\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 bronze/      # Staging models\n\u2502   \u251c\u2500\u2500 silver/      # Cleaned models\n\u2502   \u2514\u2500\u2500 gold/        # Marts\n\u251c\u2500\u2500 tests/\n\u2514\u2500\u2500 macros/\n</code></pre>"},{"location":"guides/developer-guide/#source-configuration","title":"Source Configuration","text":"<p>Define Iceberg tables as dbt sources:</p> <pre><code># models/bronze/sources.yml\nversion: 2\n\nsources:\n  - name: raw\n    description: Raw ingested data\n    tables:\n      - name: events\n        description: Event data from API\n        meta:\n          dagster_asset_key: \"dlt_events\"\n</code></pre>"},{"location":"guides/developer-guide/#model-development","title":"Model Development","text":"<p>Bronze (staging):</p> <pre><code>-- models/bronze/stg_events.sql\n{{\n    config(\n        materialized='incremental',\n        unique_key='id',\n        on_schema_change='append_new_columns'\n    )\n}}\n\nSELECT\n    id,\n    timestamp,\n    value,\n    category,\n    _dlt_load_id,\n    CURRENT_TIMESTAMP() as _transformed_at\nFROM {{ source('raw', 'events') }}\n\n{% if is_incremental() %}\nWHERE timestamp &gt; (SELECT MAX(timestamp) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Silver (cleaned):</p> <pre><code>-- models/silver/events_cleaned.sql\n{{\n    config(\n        materialized='incremental',\n        unique_key='id'\n    )\n}}\n\nSELECT\n    id,\n    timestamp,\n    COALESCE(value, 0) as value,\n    UPPER(category) as category,\n    _dlt_load_id\nFROM {{ ref('stg_events') }}\nWHERE value IS NOT NULL\n\n{% if is_incremental() %}\nAND timestamp &gt; (SELECT MAX(timestamp) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Gold (marts):</p> <pre><code>-- models/gold/daily_aggregates.sql\n{{\n    config(\n        materialized='table'\n    )\n}}\n\nSELECT\n    DATE(timestamp) as date,\n    category,\n    COUNT(*) as event_count,\n    AVG(value) as avg_value,\n    MIN(value) as min_value,\n    MAX(value) as max_value,\n    STDDEV(value) as stddev_value\nFROM {{ ref('events_cleaned') }}\nGROUP BY 1, 2\n</code></pre>"},{"location":"guides/developer-guide/#dagster-integration","title":"Dagster Integration","text":"<p>dbt models automatically become Dagster assets:</p> <pre><code># workflows/transform/dbt.py\nfrom dagster_dbt import DbtCliResource, dbt_assets\nfrom phlo_dbt.translator import CustomDbtTranslator\n\n@dbt_assets(\n    manifest=DBT_PROJECT_DIR / \"target\" / \"manifest.json\",\n    dagster_dbt_translator=CustomDbtTranslator(),\n    partitions_def=daily_partition,\n)\ndef all_dbt_assets(context, dbt: DbtCliResource):\n    yield from dbt.cli([\"build\"], context=context).stream()\n</code></pre> <p>Custom Translator maps dbt sources to Dagster assets:</p> <ul> <li><code>dlt_{table}</code> convention for ingestion assets</li> <li>Group inference from folder structure</li> <li>Partition support</li> </ul>"},{"location":"guides/developer-guide/#running-dbt","title":"Running dbt","text":"<p>Via Dagster UI:</p> <ul> <li>Navigate to asset in UI</li> <li>Click \"Materialize\"</li> </ul> <p>Via CLI:</p> <pre><code># Materialize specific model\nphlo materialize stg_events\n\n# Materialize with dependencies\nphlo materialize stg_events+\n\n# All dbt models\nphlo materialize --select \"tag:dbt\"\n</code></pre>"},{"location":"guides/developer-guide/#publishing-to-bi-tools","title":"Publishing to BI Tools","text":"<p>Automatically publish Iceberg marts to PostgreSQL for BI tools.</p>"},{"location":"guides/developer-guide/#publishing-asset","title":"Publishing Asset","text":"<pre><code># workflows/publishing/events.py\nfrom dagster import asset\nfrom phlo.publishing import publish_marts_to_postgres\n\n@asset(\n    deps=[\"marts__daily_aggregates\"],  # Depends on dbt mart\n    group=\"publishing\"\n)\ndef publish_daily_aggregates(context, trino, postgres):\n    \"\"\"Publish daily aggregates to PostgreSQL.\"\"\"\n    return publish_marts_to_postgres(\n        context=context,\n        trino=trino,\n        postgres=postgres,\n        tables_to_publish={\n            \"daily_aggregates\": \"marts.daily_aggregates\"\n        },\n        data_source=\"events\"\n    )\n</code></pre>"},{"location":"guides/developer-guide/#generic-publisher","title":"Generic Publisher","text":"<p>The <code>publish_marts_to_postgres</code> function:</p> <ol> <li>Queries Iceberg table via Trino</li> <li>Drops existing PostgreSQL table</li> <li>Creates new table with inferred schema</li> <li>Batch inserts with transactions</li> <li>Returns statistics</li> </ol> <pre><code># Usage example:\npublish_marts_to_postgres(\n    context=context,\n    trino=trino,\n    postgres=postgres,\n    tables_to_publish={\n        \"table1\": \"marts.fct_table1\",\n        \"table2\": \"marts.dim_table2\",\n    },\n    data_source=\"my_domain\"\n)\n</code></pre>"},{"location":"guides/developer-guide/#superset-integration","title":"Superset Integration","text":"<p>Connect Superset to PostgreSQL:</p> <ol> <li>Add database connection</li> <li>Create datasets from published tables</li> <li>Build dashboards</li> </ol>"},{"location":"guides/developer-guide/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/developer-guide/#custom-resource","title":"Custom Resource","text":"<p>Create custom Dagster resources:</p> <pre><code># workflows/resources/custom.py\nfrom dagster import ConfigurableResource\n\nclass MyAPIResource(ConfigurableResource):\n    api_key: str\n    base_url: str\n\n    def fetch_data(self, endpoint: str):\n        # Custom logic\n        pass\n\n# Usage in asset:\n@phlo_ingestion(...)\ndef my_data(context, my_api: MyAPIResource):\n    data = my_api.fetch_data(\"/events\")\n    return data\n</code></pre>"},{"location":"guides/developer-guide/#sensors","title":"Sensors","text":"<p>Create custom sensors for automation:</p> <pre><code># workflows/sensors/custom.py\nfrom dagster import sensor, RunRequest\n\n@sensor(job=my_job)\ndef file_sensor(context):\n    # Check for new files\n    new_files = check_for_files()\n\n    for file in new_files:\n        yield RunRequest(\n            run_key=file,\n            run_config={\"file_path\": file}\n        )\n</code></pre>"},{"location":"guides/developer-guide/#conditional-execution","title":"Conditional Execution","text":"<pre><code>@phlo_ingestion(...)\ndef conditional_data(context):\n    # Skip on weekends\n    if datetime.now().weekday() &gt;= 5:\n        context.log.info(\"Skipping weekend execution\")\n        return None\n\n    return rest_api(...)\n</code></pre>"},{"location":"guides/developer-guide/#best-practices","title":"Best Practices","text":""},{"location":"guides/developer-guide/#1-schema-first-development","title":"1. Schema-First Development","text":"<p>Always define Pandera schemas before writing ingestion code.</p>"},{"location":"guides/developer-guide/#2-incremental-loading","title":"2. Incremental Loading","text":"<p>Use partition-aware queries to load only new data:</p> <pre><code>def my_data(partition_date: str):\n    return rest_api({\n        \"params\": {\"date\": partition_date}  # Only fetch partition data\n    })\n</code></pre>"},{"location":"guides/developer-guide/#3-error-handling","title":"3. Error Handling","text":"<p>Let Phlo handle retries, but add custom handling where needed:</p> <pre><code>@phlo_ingestion(\n    max_retries=3,\n    retry_delay_seconds=30,\n    max_runtime_seconds=3600\n)\ndef robust_data(partition_date: str):\n    try:\n        return fetch_data(partition_date)\n    except SpecificError as e:\n        context.log.error(f\"Custom handling: {e}\")\n        raise  # Re-raise for Dagster retry\n</code></pre>"},{"location":"guides/developer-guide/#4-testing","title":"4. Testing","text":"<p>Write tests for schemas and workflows:</p> <pre><code># tests/test_schemas.py\ndef test_event_schema():\n    df = pd.DataFrame({\n        \"id\": [\"1\", \"2\"],\n        \"value\": [10.0, 20.0]\n    })\n    EventSchema.validate(df)  # Should not raise\n\n# tests/test_ingestion.py\ndef test_api_events():\n    result = api_events(\"2025-01-15\")\n    assert result is not None\n</code></pre>"},{"location":"guides/developer-guide/#5-documentation","title":"5. Documentation","text":"<p>Document schemas and workflows:</p> <pre><code>class EventSchema(pa.DataFrameModel):\n    \"\"\"Event data from API.\n\n    This schema validates incoming event data from the external API.\n    All events must have a unique ID and valid timestamp.\n    \"\"\"\n\n    id: Series[str] = pa.Field(\n        description=\"Unique event identifier from source system\",\n        nullable=False\n    )\n</code></pre>"},{"location":"guides/developer-guide/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - Command-line tools</li> <li>Configuration Reference - Advanced configuration</li> <li>Testing Guide - Testing strategies</li> <li>Best Practices - Production patterns</li> </ul>"},{"location":"guides/plugin-development/","title":"Plugin Development Guide","text":"<p>Complete guide to developing custom Phlo plugins.</p>"},{"location":"guides/plugin-development/#overview","title":"Overview","text":"<p>Phlo's plugin system allows you to extend the platform with custom functionality:</p> <ul> <li>Service Plugins: Add infrastructure services (databases, query engines, etc.)</li> <li>Source Connectors: Fetch data from external systems</li> <li>Quality Checks: Implement custom validation rules</li> <li>Transformations: Create reusable data transformation logic</li> <li>Dagster Extensions: Add custom resources, sensors, or schedules</li> <li>CLI Extensions: Add custom CLI commands</li> <li>Hook Plugins: Subscribe to pipeline events without direct dependencies</li> </ul> <p>This guide walks through creating each type of plugin.</p>"},{"location":"guides/plugin-development/#plugin-system-basics","title":"Plugin System Basics","text":""},{"location":"guides/plugin-development/#how-plugin-discovery-works","title":"How Plugin Discovery Works","text":"<p>Phlo uses Python entry points to automatically discover plugins:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Python Environment               \u2502\n\u2502  - phlo (core framework)             \u2502\n\u2502  - phlo-dagster (service plugin)     \u2502\n\u2502  - phlo-custom-source (your plugin)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Entry Point Discovery              \u2502\n\u2502   (importlib.metadata)               \u2502\n\u2502                                      \u2502\n\u2502   Groups scanned:                    \u2502\n\u2502     \u2022 phlo.plugins.services          \u2502\n\u2502     \u2022 phlo.plugins.sources           \u2502\n\u2502     \u2022 phlo.plugins.quality           \u2502\n\u2502     \u2022 phlo.plugins.transforms        \u2502\n\u2502     \u2022 phlo.plugins.dagster           \u2502\n\u2502     \u2022 phlo.plugins.cli               \u2502\n\u2502     \u2022 phlo.plugins.hooks             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Plugin Registry                    \u2502\n\u2502   - Installed plugins cached         \u2502\n\u2502   - Available via CLI commands       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits: - No manual registration required - Install a package, restart Phlo, plugin is available - Plugins can be distributed independently - Failed plugins don't crash the system</p>"},{"location":"guides/plugin-development/#quick-start-create-a-plugin-scaffold","title":"Quick Start: Create a Plugin Scaffold","text":"<pre><code># Create source connector plugin\nphlo plugin create my-api-source --type source\n\n# Create quality check plugin\nphlo plugin create my-validation --type quality\n\n# Create transformation plugin\nphlo plugin create my-transform --type transform\n\n# Create service plugin\nphlo plugin create my-database --type service\n\n# Create hook plugin\nphlo plugin create my-hooks --type hook\n</code></pre> <p>This creates a complete package structure ready for development.</p>"},{"location":"guides/plugin-development/#developing-a-hook-plugin","title":"Developing a Hook Plugin","text":"<p>Hook plugins subscribe to the Hook Bus and react to pipeline events without importing other capability packages directly.</p>"},{"location":"guides/plugin-development/#hook-bus-events","title":"Hook Bus Events","text":"<ul> <li><code>service.pre_start</code> / <code>service.post_start</code> / <code>service.pre_stop</code> / <code>service.post_stop</code></li> <li><code>ingestion.start</code> / <code>ingestion.end</code></li> <li><code>transform.start</code> / <code>transform.end</code></li> <li><code>publish.start</code> / <code>publish.end</code></li> <li><code>quality.result</code></li> <li><code>lineage.edges</code></li> <li><code>telemetry.metric</code> / <code>telemetry.log</code></li> </ul>"},{"location":"guides/plugin-development/#example-hook-plugin","title":"Example Hook Plugin","text":"<pre><code>from phlo.hooks import QualityResultEvent\nfrom phlo.plugins import HookFilter, HookPlugin, PluginMetadata\nfrom phlo.plugins.hooks import HookRegistration\n\n\nclass MyHookPlugin(HookPlugin):\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"my-hooks\",\n            version=\"0.1.0\",\n            description=\"Custom hook handlers\",\n        )\n\n    def get_hooks(self):\n        return [\n            HookRegistration(\n                hook_name=\"quality_alerts\",\n                handler=self.handle_quality,\n                filters=HookFilter(event_types={\"quality.result\"}),\n            )\n        ]\n\n    def handle_quality(self, event: QualityResultEvent) -&gt; None:\n        if not event.passed:\n            # Handle failures here\n            pass\n</code></pre>"},{"location":"guides/plugin-development/#notes","title":"Notes","text":"<ul> <li>Hooks execute synchronously in-process; keep handlers fast and offload heavy work.</li> <li>Use <code>HookFilter</code> to scope by event types, asset keys, and tags.</li> <li>Use <code>failure_policy</code> on <code>HookRegistration</code> to control error behavior.</li> </ul>"},{"location":"guides/plugin-development/#semantic-layer-providers","title":"Semantic Layer Providers","text":"<p>Semantic layer providers expose standardized models for downstream tooling.</p> <pre><code>from phlo.plugins import SemanticLayerProvider, SemanticModel\n\n\nclass MySemanticLayer(SemanticLayerProvider):\n    def list_models(self):\n        return [\n            SemanticModel(\n                name=\"revenue_daily\",\n                description=\"Daily revenue rollup\",\n                sql=\"SELECT ...\",\n            )\n        ]\n\n    def get_model(self, name: str) -&gt; SemanticModel | None:\n        return next((m for m in self.list_models() if m.name == name), None)\n</code></pre> <p>Recommended: subscribe to <code>publish.end</code> events to refresh semantic models when marts update.</p>"},{"location":"guides/plugin-development/#developing-a-source-connector-plugin","title":"Developing a Source Connector Plugin","text":"<p>Source connector plugins fetch data from external systems.</p>"},{"location":"guides/plugin-development/#base-class","title":"Base Class","text":"<p>All source connectors inherit from <code>SourceConnectorPlugin</code>:</p> <pre><code>from phlo.plugins.base import SourceConnectorPlugin, PluginMetadata\nfrom typing import Any, Iterator\n\nclass MyAPISource(SourceConnectorPlugin):\n    \"\"\"Custom API data source.\"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        \"\"\"Return plugin metadata for discovery.\"\"\"\n        return PluginMetadata(\n            name=\"my_api\",\n            version=\"1.0.0\",\n            description=\"Fetch data from My API\",\n            author=\"Your Name\",\n            homepage=\"https://github.com/yourorg/phlo-plugin-my-api\",\n            tags=[\"api\", \"custom\"],\n            license=\"MIT\",\n        )\n\n    def fetch_data(self, config: dict[str, Any]) -&gt; Iterator[dict[str, Any]]:\n        \"\"\"\n        Fetch data from the source.\n\n        Args:\n            config: Configuration dictionary with source-specific settings\n\n        Yields:\n            Records as dictionaries\n        \"\"\"\n        # Implementation here\n        pass\n\n    def get_schema(self, config: dict[str, Any]) -&gt; dict[str, str] | None:\n        \"\"\"\n        Get expected schema for the data.\n\n        Returns:\n            Dictionary mapping column names to types, or None if schema is dynamic\n        \"\"\"\n        return {\n            \"id\": \"int\",\n            \"name\": \"string\",\n            \"created_at\": \"timestamp\",\n        }\n\n    def test_connection(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Test if connection to source is successful.\"\"\"\n        try:\n            # Test connection logic\n            return True\n        except Exception:\n            return False\n\n    def validate_config(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Validate configuration dictionary.\"\"\"\n        required_keys = [\"api_key\", \"base_url\"]\n        return all(k in config for k in required_keys)\n</code></pre>"},{"location":"guides/plugin-development/#complete-example-rest-api-source","title":"Complete Example: REST API Source","text":"<pre><code>\"\"\"REST API source connector plugin.\"\"\"\n\nimport requests\nfrom typing import Any, Iterator\nfrom phlo.plugins.base import SourceConnectorPlugin, PluginMetadata\n\n\nclass RESTAPISource(SourceConnectorPlugin):\n    \"\"\"\n    Generic REST API source connector.\n\n    Supports pagination, authentication, and custom headers.\n    \"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"rest_api_advanced\",\n            version=\"1.0.0\",\n            description=\"Advanced REST API connector with pagination support\",\n            author=\"Data Team\",\n            tags=[\"api\", \"rest\", \"http\"],\n            license=\"MIT\",\n        )\n\n    def fetch_data(self, config: dict[str, Any]) -&gt; Iterator[dict[str, Any]]:\n        \"\"\"\n        Fetch data from REST API with pagination.\n\n        Config keys:\n            - base_url: API base URL\n            - endpoint: API endpoint path\n            - headers: Optional headers dict\n            - auth_token: Optional bearer token\n            - pagination: Optional pagination config\n        \"\"\"\n        if not self.validate_config(config):\n            raise ValueError(\"Invalid configuration\")\n\n        base_url = config[\"base_url\"].rstrip(\"/\")\n        endpoint = config[\"endpoint\"].lstrip(\"/\")\n        url = f\"{base_url}/{endpoint}\"\n\n        # Prepare headers\n        headers = config.get(\"headers\", {})\n        if auth_token := config.get(\"auth_token\"):\n            headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n\n        # Handle pagination\n        pagination = config.get(\"pagination\", {})\n        page = pagination.get(\"start_page\", 1)\n        page_size = pagination.get(\"page_size\", 100)\n        max_pages = pagination.get(\"max_pages\", 0)  # 0 = unlimited\n\n        pages_fetched = 0\n        while True:\n            # Build request params\n            params = {\n                pagination.get(\"page_param\", \"page\"): page,\n                pagination.get(\"size_param\", \"limit\"): page_size,\n            }\n\n            # Add custom params\n            params.update(config.get(\"params\", {}))\n\n            try:\n                response = requests.get(url, headers=headers, params=params, timeout=30)\n                response.raise_for_status()\n\n                data = response.json()\n\n                # Handle different response structures\n                results_key = pagination.get(\"results_key\", \"results\")\n                items = data.get(results_key, data if isinstance(data, list) else [])\n\n                if not items:\n                    break  # No more data\n\n                for item in items:\n                    yield item\n\n                pages_fetched += 1\n\n                # Check if we should continue\n                if max_pages &gt; 0 and pages_fetched &gt;= max_pages:\n                    break\n\n                # Check if there are more pages\n                has_next = data.get(pagination.get(\"has_next_key\", \"has_next\"), False)\n                if not has_next and results_key in data:\n                    break\n\n                page += 1\n\n            except requests.RequestException as e:\n                raise RuntimeError(f\"Failed to fetch from {url}: {e}\")\n\n    def get_schema(self, config: dict[str, Any]) -&gt; dict[str, str] | None:\n        \"\"\"Schema is dynamic based on API response.\"\"\"\n        return None\n\n    def test_connection(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Test API connectivity.\"\"\"\n        try:\n            base_url = config[\"base_url\"].rstrip(\"/\")\n            endpoint = config[\"endpoint\"].lstrip(\"/\")\n            url = f\"{base_url}/{endpoint}\"\n\n            headers = config.get(\"headers\", {})\n            if auth_token := config.get(\"auth_token\"):\n                headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n\n            response = requests.get(url, headers=headers, timeout=10)\n            return response.status_code == 200\n        except Exception:\n            return False\n\n    def validate_config(self, config: dict[str, Any]) -&gt; bool:\n        \"\"\"Validate required configuration.\"\"\"\n        return \"base_url\" in config and \"endpoint\" in config\n</code></pre>"},{"location":"guides/plugin-development/#register-the-plugin","title":"Register the Plugin","text":"<p>In <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"phlo.plugins.sources\"]\nrest_api_advanced = \"phlo_rest_api.plugin:RESTAPISource\"\n</code></pre>"},{"location":"guides/plugin-development/#use-the-plugin","title":"Use the Plugin","text":"<pre><code>from phlo.plugins import get_source_connector\n\n# Get the plugin\nsource = get_source_connector(\"rest_api_advanced\")\n\n# Configure and fetch data\nconfig = {\n    \"base_url\": \"https://api.example.com\",\n    \"endpoint\": \"/v1/users\",\n    \"auth_token\": \"your-token-here\",\n    \"pagination\": {\n        \"page_param\": \"page\",\n        \"size_param\": \"per_page\",\n        \"page_size\": 100,\n        \"results_key\": \"data\",\n    },\n}\n\nfor record in source.fetch_data(config):\n    print(record)\n</code></pre>"},{"location":"guides/plugin-development/#developing-a-quality-check-plugin","title":"Developing a Quality Check Plugin","text":"<p>Quality check plugins implement custom validation logic.</p>"},{"location":"guides/plugin-development/#base-class_1","title":"Base Class","text":"<pre><code>from phlo.plugins.base import QualityCheckPlugin, PluginMetadata\nfrom typing import Any\nimport pandas as pd\n\n\nclass MyQualityCheck(QualityCheckPlugin):\n    \"\"\"Custom quality check plugin.\"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"my_check\",\n            version=\"1.0.0\",\n            description=\"Custom quality validation\",\n            author=\"Your Name\",\n        )\n\n    def create_check(self, **kwargs) -&gt; \"CheckInstance\":\n        \"\"\"\n        Create a check instance with specific parameters.\n\n        Returns:\n            Check instance that can execute validation\n        \"\"\"\n        return CheckInstance(**kwargs)\n\n\nclass CheckInstance:\n    \"\"\"Instance of the quality check with specific parameters.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Store check parameters\n        self.params = kwargs\n\n    def execute(self, df: pd.DataFrame, context: Any = None) -&gt; dict:\n        \"\"\"\n        Execute the quality check.\n\n        Returns:\n            Dictionary with check results:\n            {\n                \"passed\": bool,\n                \"violations\": int,\n                \"total\": int,\n                \"violation_rate\": float,\n                \"details\": Any,  # Optional additional info\n            }\n        \"\"\"\n        # Implement validation logic\n        pass\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return descriptive check name.\"\"\"\n        return \"my_check\"\n</code></pre>"},{"location":"guides/plugin-development/#complete-example-business-rule-check","title":"Complete Example: Business Rule Check","text":"<pre><code>\"\"\"Business rule validation plugin.\"\"\"\n\nimport pandas as pd\nfrom typing import Any, Callable\nfrom phlo.plugins.base import QualityCheckPlugin, PluginMetadata\n\n\nclass BusinessRuleCheckPlugin(QualityCheckPlugin):\n    \"\"\"\n    Quality check for custom business rules.\n\n    Allows arbitrary Python functions as validation rules.\n    \"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"business_rule\",\n            version=\"1.0.0\",\n            description=\"Validate custom business rules\",\n            author=\"Data Team\",\n            tags=[\"validation\", \"business-logic\"],\n        )\n\n    def create_check(self, **kwargs) -&gt; \"BusinessRuleCheck\":\n        return BusinessRuleCheck(**kwargs)\n\n\nclass BusinessRuleCheck:\n    \"\"\"Business rule validation check.\"\"\"\n\n    def __init__(\n        self,\n        rule: Callable[[pd.Series], pd.Series],\n        columns: list[str],\n        name: str,\n        description: str = \"\",\n        tolerance: float = 0.0,\n    ):\n        \"\"\"\n        Initialize business rule check.\n\n        Args:\n            rule: Function that takes row data and returns True if valid\n            columns: Columns required by the rule\n            name: Name of the rule\n            description: Human-readable description\n            tolerance: Fraction of rows allowed to fail (0.0-1.0)\n        \"\"\"\n        self.rule = rule\n        self.columns = columns\n        self.rule_name = name\n        self.description = description\n        self.tolerance = max(0.0, min(1.0, tolerance))\n\n    def execute(self, df: pd.DataFrame, context: Any = None) -&gt; dict:\n        \"\"\"Execute the business rule validation.\"\"\"\n        # Validate required columns exist\n        missing_columns = set(self.columns) - set(df.columns)\n        if missing_columns:\n            return {\n                \"passed\": False,\n                \"violations\": len(df),\n                \"total\": len(df),\n                \"violation_rate\": 1.0,\n                \"error\": f\"Missing columns: {missing_columns}\",\n            }\n\n        # Apply rule to each row\n        try:\n            valid_mask = df[self.columns].apply(self.rule, axis=1)\n            violations = (~valid_mask).sum()\n            total = len(df)\n            violation_rate = violations / total if total &gt; 0 else 0.0\n\n            passed = violation_rate &lt;= self.tolerance\n\n            return {\n                \"passed\": passed,\n                \"violations\": int(violations),\n                \"total\": total,\n                \"violation_rate\": violation_rate,\n                \"details\": {\n                    \"rule\": self.rule_name,\n                    \"description\": self.description,\n                    \"failed_rows\": df[~valid_mask].index.tolist()[:100],  # First 100\n                },\n            }\n        except Exception as e:\n            return {\n                \"passed\": False,\n                \"violations\": len(df),\n                \"total\": len(df),\n                \"violation_rate\": 1.0,\n                \"error\": f\"Rule execution failed: {e}\",\n            }\n\n    @property\n    def name(self) -&gt; str:\n        return f\"business_rule({self.rule_name})\"\n</code></pre>"},{"location":"guides/plugin-development/#use-the-plugin_1","title":"Use the Plugin","text":"<pre><code>from phlo.plugins import get_quality_check\nimport pandas as pd\n\n# Get the plugin\nplugin = get_quality_check(\"business_rule\")\n\n# Define a business rule\ndef revenue_exceeds_cost(row):\n    \"\"\"Revenue must be greater than cost.\"\"\"\n    return row[\"revenue\"] &gt; row[\"cost\"]\n\n# Create check\ncheck = plugin.create_check(\n    rule=revenue_exceeds_cost,\n    columns=[\"revenue\", \"cost\"],\n    name=\"revenue_exceeds_cost\",\n    description=\"Revenue must exceed cost\",\n    tolerance=0.05,  # Allow 5% violations\n)\n\n# Execute\ndf = pd.DataFrame({\n    \"revenue\": [100, 200, 50],\n    \"cost\": [80, 150, 60],\n})\n\nresult = check.execute(df)\nprint(f\"Passed: {result['passed']}\")\nprint(f\"Violations: {result['violations']}/{result['total']}\")\n</code></pre>"},{"location":"guides/plugin-development/#developing-a-service-plugin","title":"Developing a Service Plugin","text":"<p>Service plugins package infrastructure services (databases, query engines, etc.) as installable Python packages.</p>"},{"location":"guides/plugin-development/#structure","title":"Structure","text":"<pre><code>phlo-mydb/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 phlo_mydb/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 plugin.py\n\u2502       \u2514\u2500\u2500 service.yaml       # Docker Compose service definition\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_plugin.py\n</code></pre>"},{"location":"guides/plugin-development/#service-plugin-class","title":"Service Plugin Class","text":"<pre><code>\"\"\"MyDB service plugin.\"\"\"\n\nfrom phlo.plugins.base import ServicePlugin, PluginMetadata\nimport importlib.resources\n\n\nclass MyDBServicePlugin(ServicePlugin):\n    \"\"\"Service plugin for MyDB.\"\"\"\n\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"mydb\",\n            version=\"1.0.0\",\n            description=\"MyDB distributed database\",\n            author=\"Data Team\",\n            tags=[\"database\", \"distributed\"],\n        )\n\n    def get_service_definition(self) -&gt; dict:\n        \"\"\"Load service.yaml definition.\"\"\"\n        import yaml\n\n        # Load bundled service.yaml\n        yaml_content = importlib.resources.read_text(\n            \"phlo_mydb\", \"service.yaml\"\n        )\n        return yaml.safe_load(yaml_content)\n</code></pre>"},{"location":"guides/plugin-development/#serviceyaml","title":"service.yaml","text":"<pre><code>name: mydb\ndescription: MyDB distributed database\ncontainer_name: phlo-mydb\nimage: mydb/mydb:latest\nports:\n  - \"9000:9000\"\nenvironment:\n  MYDB_ROOT_PASSWORD: ${MYDB_ROOT_PASSWORD:-mydb}\n  MYDB_LOG_LEVEL: ${MYDB_LOG_LEVEL:-INFO}\nvolumes:\n  - mydb-data:/var/lib/mydb\ndepends_on:\n  - postgres\nhealthcheck:\n  test: [\"CMD\", \"mydb-health-check\"]\n  interval: 10s\n  timeout: 5s\n  retries: 5\nprofiles:\n  - core\n</code></pre>"},{"location":"guides/plugin-development/#register-the-plugin_1","title":"Register the Plugin","text":"<pre><code>[project.entry-points.\"phlo.plugins.services\"]\nmydb = \"phlo_mydb.plugin:MyDBServicePlugin\"\n</code></pre>"},{"location":"guides/plugin-development/#usage","title":"Usage","text":"<p>Once installed, the service is automatically available:</p> <pre><code># Install the plugin\npip install phlo-mydb\n\n# List services (mydb will appear)\nphlo services list\n\n# Start services (includes mydb)\nphlo services start\n</code></pre>"},{"location":"guides/plugin-development/#best-practices","title":"Best Practices","text":""},{"location":"guides/plugin-development/#1-comprehensive-metadata","title":"1. Comprehensive Metadata","text":"<pre><code>@property\ndef metadata(self) -&gt; PluginMetadata:\n    return PluginMetadata(\n        name=\"my_plugin\",\n        version=\"2.1.0\",\n        description=\"Detailed description of what this plugin does\",\n        author=\"Data Platform Team\",\n        homepage=\"https://github.com/yourorg/phlo-plugin-my-plugin\",\n        documentation_url=\"https://docs.yourorg.com/plugins/my-plugin\",\n        tags=[\"api\", \"external\", \"production\"],\n        license=\"MIT\",\n    )\n</code></pre>"},{"location":"guides/plugin-development/#2-robust-error-handling","title":"2. Robust Error Handling","text":"<pre><code>def fetch_data(self, config: dict) -&gt; Iterator[dict]:\n    try:\n        response = self.client.get(url)\n        response.raise_for_status()\n        yield from response.json()\n    except httpx.HTTPStatusError as e:\n        raise PluginError(\n            f\"API returned {e.response.status_code}: {e.response.text}\"\n        )\n    except httpx.RequestError as e:\n        raise PluginError(f\"Network error: {e}\")\n    except Exception as e:\n        raise PluginError(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"guides/plugin-development/#3-configuration-validation","title":"3. Configuration Validation","text":"<pre><code>def validate_config(self, config: dict[str, Any]) -&gt; bool:\n    \"\"\"Validate configuration with clear error messages.\"\"\"\n    required_keys = [\"api_key\", \"base_url\", \"endpoint\"]\n\n    for key in required_keys:\n        if key not in config:\n            raise ValueError(f\"Missing required config key: {key}\")\n\n    if not isinstance(config[\"api_key\"], str):\n        raise TypeError(\"api_key must be a string\")\n\n    if not config[\"base_url\"].startswith(\"https://\"):\n        raise ValueError(\"base_url must use HTTPS\")\n\n    return True\n</code></pre>"},{"location":"guides/plugin-development/#4-comprehensive-testing","title":"4. Comprehensive Testing","text":"<pre><code>\"\"\"Test suite for source connector.\"\"\"\n\nimport pytest\nfrom phlo_my_api.plugin import MyAPISource\n\n\ndef test_metadata():\n    \"\"\"Test plugin metadata is correct.\"\"\"\n    source = MyAPISource()\n    assert source.metadata.name == \"my_api\"\n    assert source.metadata.version == \"1.0.0\"\n\n\ndef test_fetch_data_returns_records():\n    \"\"\"Test data fetching.\"\"\"\n    source = MyAPISource()\n    config = {\n        \"base_url\": \"https://api.example.com\",\n        \"api_key\": \"test-key\",\n    }\n\n    records = list(source.fetch_data(config))\n\n    assert len(records) &gt; 0\n    assert all(\"id\" in r for r in records)\n\n\ndef test_connection_check():\n    \"\"\"Test connection validation.\"\"\"\n    source = MyAPISource()\n    assert source.test_connection({\"api_key\": \"valid\"}) is True\n    assert source.test_connection({}) is False\n\n\ndef test_invalid_config_raises():\n    \"\"\"Test invalid config is rejected.\"\"\"\n    source = MyAPISource()\n    with pytest.raises(ValueError):\n        source.validate_config({})\n</code></pre>"},{"location":"guides/plugin-development/#5-semantic-versioning","title":"5. Semantic Versioning","text":"<pre><code>[project]\nversion = \"2.0.0\"  # Breaking change: changed config schema\nversion = \"1.1.0\"  # Feature: added new resource type\nversion = \"1.0.1\"  # Fix: handled edge case\n</code></pre>"},{"location":"guides/plugin-development/#6-documentation","title":"6. Documentation","text":"<p>Include comprehensive README.md:</p> <pre><code># phlo-plugin-my-api\n\nCustom API source connector for Phlo.\n\n## Installation\n\n```bash\npip install phlo-plugin-my-api\n</code></pre>"},{"location":"guides/plugin-development/#usage_1","title":"Usage","text":"<pre><code>from phlo.plugins import get_source_connector\n\nsource = get_source_connector(\"my_api\")\nconfig = {\n    \"base_url\": \"https://api.example.com\",\n    \"api_key\": \"your-key\",\n}\n\nfor record in source.fetch_data(config):\n    print(record)\n</code></pre>"},{"location":"guides/plugin-development/#configuration","title":"Configuration","text":"<ul> <li><code>base_url</code> (required): API base URL</li> <li><code>api_key</code> (required): API authentication key</li> <li><code>timeout</code> (optional): Request timeout in seconds (default: 30)</li> </ul>"},{"location":"guides/plugin-development/#license","title":"License","text":"<p>MIT</p> <pre><code>\n## Publishing Plugins\n\n### To PyPI\n\n```bash\n# Build distribution\npython -m build\n\n# Upload to PyPI\ntwine upload dist/*\n</code></pre>"},{"location":"guides/plugin-development/#to-internal-registry","title":"To Internal Registry","text":"<pre><code># Configure internal PyPI server\npip config set global.index-url https://pypi.yourcompany.com/simple\n\n# Publish\ntwine upload --repository-url https://pypi.yourcompany.com dist/*\n</code></pre>"},{"location":"guides/plugin-development/#plugin-security","title":"Plugin Security","text":""},{"location":"guides/plugin-development/#for-plugin-users","title":"For Plugin Users","text":"<ul> <li>Only install plugins from trusted sources</li> <li>Review plugin source code before installation</li> <li>Use <code>plugins_whitelist</code> to restrict allowed plugins</li> </ul>"},{"location":"guides/plugin-development/#for-plugin-developers","title":"For Plugin Developers","text":"<ul> <li>Never include secrets in code</li> <li>Validate all user inputs</li> <li>Use parameterized queries (avoid SQL injection)</li> <li>Follow security best practices</li> </ul>"},{"location":"guides/plugin-development/#next-steps","title":"Next Steps","text":"<ul> <li>Review Blog Post 13: Plugin System for examples</li> <li>Check ADR 0030: Unified Plugin System</li> <li>Explore existing plugins in <code>packages/phlo-core-plugins/</code></li> <li>Join the community to share your plugins</li> </ul>"},{"location":"guides/plugin-development/#support","title":"Support","text":"<ul> <li>GitHub Issues: https://github.com/iamgp/phlo/issues</li> <li>Documentation: https://docs.phlo.io</li> <li>Community Discord: https://discord.gg/phlo</li> </ul>"},{"location":"guides/service-packages/","title":"Service Packages","text":"<p>Phlo services are distributed as Python packages. Each service package ships its own <code>service.yaml</code> definition and registers a <code>phlo.plugins.services</code> entry point for CLI discovery.</p>"},{"location":"guides/service-packages/#architecture","title":"Architecture","text":"<p>Phlo services are organized into core and package services:</p>"},{"location":"guides/service-packages/#core-services","title":"Core Services","text":"<p>Core services are bundled with <code>pip install phlo</code> and cannot be removed:</p> <ul> <li>Observatory - Data platform UI for visibility and lineage</li> <li>phlo-api - Backend API exposing phlo internals to Observatory</li> </ul>"},{"location":"guides/service-packages/#package-services","title":"Package Services","text":"<p>Package services are installed separately and can be swapped for alternatives:</p> <pre><code># Install default services (recommended)\npip install phlo[defaults]\n\n# Or install individually\npip install phlo-dagster phlo-postgres phlo-trino\n</code></pre> <p>Default package services:</p> <ul> <li><code>phlo-dagster</code> - Data orchestration platform</li> <li><code>phlo-postgres</code> - PostgreSQL for Dagster metadata</li> <li><code>phlo-minio</code> - S3-compatible object storage</li> <li><code>phlo-nessie</code> - Git-like catalog for Iceberg</li> <li><code>phlo-trino</code> - Distributed SQL query engine</li> </ul> <p>Optional packages:</p> <ul> <li><code>phlo-superset</code> - Business intelligence</li> <li><code>phlo-pgweb</code> - PostgreSQL web admin</li> <li><code>phlo-postgrest</code> - Auto-generated REST API</li> <li><code>phlo-hasura</code> - GraphQL API</li> <li><code>phlo-prometheus</code> - Metrics [observability]</li> <li><code>phlo-grafana</code> - Dashboards [observability]</li> <li><code>phlo-loki</code> - Log aggregation [observability]</li> <li><code>phlo-alloy</code> - Log shipping [observability]</li> </ul>"},{"location":"guides/service-packages/#customizing-services","title":"Customizing Services","text":"<p>Override service settings in your <code>phlo.yaml</code>:</p> <pre><code>name: my-lakehouse\n\nservices:\n  # Override a package service\n  observatory:\n    ports:\n      - \"8080:3000\"\n    environment:\n      DEBUG: \"true\"\n\n  # Disable a default service\n  superset:\n    enabled: false\n\n  # Add a custom inline service\n  custom-api:\n    type: inline\n    image: my-registry/api:latest\n    ports:\n      - \"4000:4000\"\n    depends_on:\n      - trino\n</code></pre>"},{"location":"guides/service-packages/#override-behavior","title":"Override Behavior","text":"Setting Behavior <code>ports</code> Replaces package defaults <code>environment</code> Merges (user values override) <code>volumes</code> Appends to package defaults <code>depends_on</code> Replaces package defaults <code>command</code> Replaces package defaults <code>enabled: false</code> Excludes service entirely"},{"location":"guides/service-packages/#discovering-services","title":"Discovering Services","text":"<pre><code># List installed services with runtime status\nphlo services list\n\n# Show all including optional profiles\nphlo services list --all\n\n# JSON output with status details\nphlo services list --json\n</code></pre> <p>Example output:</p> <pre><code>Package Services (installed):\n  \u2713 dagster            Running    :3000   Data orchestration platform for workflows and pipelines\n  \u2713 postgres           Running    :5432   PostgreSQL database for metadata and operational storage\n  \u2713 trino              Running    :8080   Distributed SQL query engine for the data lake\n  \u2713 minio              Running    :9001   S3-compatible object storage for data lake\n  \u2713 nessie             Running    :19120  Git-like catalog for Iceberg tables\n  \u2717 superset           Disabled           Business intelligence platform (disabled in phlo.yaml)\n\nCustom Services (phlo.yaml):\n  \u2713 custom-api         Running    :4000   Custom API backend (inline)\n</code></pre> <p>The enhanced output shows:</p> <ul> <li>Status marker: \u2713 (running/enabled), \u2717 (disabled), or blank (stopped)</li> <li>Running state: Running, Stopped, or Disabled</li> <li>Exposed ports: First exposed external port (e.g., :3000)</li> <li>Service description: From package or phlo.yaml</li> <li>Configuration notes: (disabled in phlo.yaml), (inline), etc.</li> </ul>"},{"location":"guides/service-packages/#development-mode","title":"Development Mode","text":"<p>Mount local package sources into containers for live development:</p> <pre><code>phlo services init --dev --phlo-source /path/to/phlo\nphlo services start\n</code></pre> <p>Dev mode uses the <code>dev</code> section in each service's <code>service.yaml</code> to override commands, volumes, and environment.</p>"},{"location":"guides/testing-strategy/","title":"Phlo Comprehensive Test Strategy Matrix","text":"<p>This document outlines the testing requirements for all components in the Phlo ecosystem. It defines where tests should live and what they should verify at each level of the Test Pyramid.</p>"},{"location":"guides/testing-strategy/#test-levels-definition","title":"Test Levels Definition","text":"<ol> <li>Level 1: Unit Tests (Mocked, Fast, Logic-focused)</li> <li>Level 2: Functional/Integration Tests (Real I/O, Containerized Single-Service)</li> <li>Level 3: System/E2E Tests (Full Platform, Cross-Service flows)</li> </ol>"},{"location":"guides/testing-strategy/#1-core-framework","title":"1. Core Framework","text":"<p>Location: <code>tests/</code> (root)</p> Integration Location Scope System E2E <code>tests/test_system_e2e.py</code> Golden Path: Ingest (DLT) -&gt; Store (Iceberg/Nessie) -&gt; Transform (DBT) -&gt; Monitor (Quality/Metrics).Goal: Verify the entire platform works as a cohesive unit. Workflow Discovery <code>tests/test_framework_integration.py</code> Project Loading: Verify user project structures/files are correctly parsed and loaded into Dagster definitions. Configuration <code>tests/test_config.py</code> Resolution: Verify environment variable overrides, <code>.phlo/.env(.local)</code> loading, and default value merging. Plugin System <code>tests/test_plugin_system.py</code> Lifecycle: Verify plugin discovery, registration, conflict resolution, and metadata validation. Hook Bus <code>tests/test_hook_bus.py</code> Event Propagation: Verify events (Ingestion, Quality, Lineage) are correctly routed to all listeners with priority/ordering. Services <code>tests/test_services_discovery.py</code> Dependency Injection: Verify core services (Trino, MinIO) are correctly initialized and injected into asset contexts. Publishing <code>tests/test_publishing.py</code> Data Sync: Verify logic for syncing Trino tables to Postgres for serving (incremental/full refresh)."},{"location":"guides/testing-strategy/#2-ingestion-transformation-packages","title":"2. Ingestion &amp; Transformation Packages","text":"<p>Focus: Data movement and logic application.</p> Package Test Location Level 1 (Unit) Level 2 (Functional) phlo-dlt <code>packages/phlo-dlt/tests/</code> Decorator logic, configuration parsing. Real Ingestion: Run a pipeline writing to a local/minio Iceberg table. Verify rows in storage. phlo-dbt <code>packages/phlo-dbt/tests/</code> Manifest parsing, lineage generation logic, translator adapters. Real Transformation: generate a <code>dbt_project</code>, run <code>dbt build</code> against DuckDB/Postgres. Verify tables exist. phlo-iceberg <code>packages/phlo-iceberg/tests/</code> Schema conversion (Pandera&lt;-&gt;Iceberg), Partition spec generation. Catalog Operations: Create/Drop tables in a real Catalog (File/Rest). Verify metadata files."},{"location":"guides/testing-strategy/#3-storage-catalog-packages","title":"3. Storage &amp; Catalog Packages","text":"<p>Focus: Infrastructure orchestration and connectivity.</p> Package Test Location Level 1 (Unit) Level 2 (Functional) phlo-nessie <code>packages/phlo-nessie/tests/</code> Configuration validation. Branching: Create branches, commit changes, merge branches via Nessie API. phlo-minio <code>packages/phlo-minio/tests/</code> Bucket policy generation. Storage: Create buckets, upload/download files, verify policy enforcement. phlo-trino <code>packages/phlo-trino/tests/</code> Query generation helpers. Query Execution: Execute simple SQL (<code>SELECT 1</code>) against a Trino container. phlo-postgres <code>packages/phlo-postgres/tests/</code> Connection string building. DB Ops: Create user, create DB, verify connectivity."},{"location":"guides/testing-strategy/#4-orchestration-api","title":"4. Orchestration &amp; API","text":"<p>Focus: Job control and external access.</p> Package Test Location Level 1 (Unit) Level 2 (Functional) phlo-dagster <code>packages/phlo-dagster/tests/</code> Resource definition helpers. Job Execution: Load a <code>Definitions</code> object, execute a dummy job, check Run status. phlo-api <code>packages/phlo-api/tests/</code> Route definitions, Pydantic models. API Endpoints: Spin up FastAPI test client, hit endpoints, verify 200 OK. phlo-core-plugins <code>packages/phlo-core-plugins/tests/</code> Plugin loading mechanism. Plugin Load: Install a dummy plugin and verify it appears in the registry."},{"location":"guides/testing-strategy/#5-observability-quality","title":"5. Observability &amp; Quality","text":"<p>Focus: Metadata, metrics, and contracts.</p> Package Test Location Level 1 (Unit) Level 2 (Functional) phlo-quality <code>packages/phlo-quality/tests/</code> Check generation logic. Check Execution: Run a check against a real dataset (Pandas/DuckDB). Verify pass/fail. phlo-metrics <code>packages/phlo-metrics/tests/</code> Metric formatting (Prometheus). Endpoint Scrape: Expose metrics endpoint, scrape with test client. phlo-alerting <code>packages/phlo-alerting/tests/</code> Alert template rendering. Notification: \"Send\" an alert to a mock sink/webhook receiver. phlo-lineage <code>packages/phlo-lineage/tests/</code> Graph construction. Store/Retrieve: Write lineage events to DB, query graph back. phlo-observatory <code>packages/phlo-observatory/tests/</code> UI component rendering logic (if applicable). Integration: Verify it can connect to Trino/Postgres and fetch summary stats."},{"location":"guides/testing-strategy/#6-infrastructure-services","title":"6. Infrastructure Services","text":"<p>Focus: Third-party tool management/provisioning.</p> Package Test Location Level 1 (Unit) Level 2 (Functional) phlo-superset <code>packages/phlo-superset/tests/</code> Config generation. Provisioning: Verify container spin-up (if applicable) or API health check. phlo-openmetadata <code>packages/phlo-openmetadata/tests/</code> Metadata mapping logic. Sync: Push sample metadata to OM server (mock or container). phlo-grafana <code>packages/phlo-grafana/tests/</code> Dashboard JSON generation. Datasource Config: Verify datasource provisioning via API. phlo-loki <code>packages/phlo-loki/tests/</code> Config validation. Health: Verify service reachable. phlo-prometheus <code>packages/phlo-prometheus/tests/</code> Config validation. Health: Verify service reachable. phlo-hasura <code>packages/phlo-hasura/tests/</code> GraphQL generation. Metadata Apply: Push metadata to Hasura container. phlo-postgrest <code>packages/phlo-postgrest/tests/</code> Config validation. Health: Verify service reachable. phlo-pgweb <code>packages/phlo-pgweb/tests/</code> Config validation. Health: Verify service reachable. phlo-alloy <code>packages/phlo-alloy/tests/</code> Config validation. Health: Verify service reachable."},{"location":"guides/testing-strategy/#implementation-priority","title":"Implementation Priority","text":"<ol> <li>Core Data Flow: <code>phlo-dlt</code> (Ingest), <code>phlo-iceberg</code> (Store), <code>phlo-dbt</code> (Transform).</li> <li>Orchestration: <code>phlo-dagster</code> (Run).</li> <li>Core Infrastructure: <code>phlo-minio</code>, <code>phlo-nessie</code> (if used in Golden Path).</li> <li>Observability: <code>phlo-quality</code>, <code>phlo-metrics</code>.</li> <li>Extended Services: Everything else.</li> </ol>"},{"location":"guides/testing-strategy/#7-shared-test-infrastructure","title":"7. Shared Test Infrastructure","text":"<p>Location: <code>conftest.py</code> (root) &amp; <code>tests/fixtures/</code></p> <p>To avoid code duplication and ensure consistent test environments, the following shared fixtures are available to all tests:</p>"},{"location":"guides/testing-strategy/#storage-compute","title":"storage &amp; Compute","text":"Fixture Scope Description <code>minio_service</code> Session Spins up an ephemeral MinIO container (mock S3). Skips if Docker is unavailable. <code>iceberg_catalog</code> Function Provides a <code>pyiceberg</code> catalog client pre-configured to talk to the shared MinIO service with a fresh namespace. <code>duckdb_connection</code> Function Provides an in-memory DuckDB connection with <code>httpfs</code> and <code>iceberg</code> extensions pre-installed/configured for the MinIO service. <code>trino_service</code> Session (Optional) Spins up a Trino container for heavy SQL validation. Used primarily in E2E tests. <code>nessie_service</code> Session (Optional) Spins up a Nessie catalog for testing branching/versioning workflows."},{"location":"guides/testing-strategy/#orchestration-context","title":"Orchestration &amp; Context","text":"Fixture Scope Description <code>dagster_instance</code> Function Provides an isolated <code>DagsterInstance</code> for executing runs in-process without side effects. <code>mock_hook_bus</code> Function A pre-configured <code>MockHookBus</code> for verifying plugin event emission (Unit/Integration level). <code>sample_project</code> Session Generates a standard Phlo project structure in a temp dir for verifying loading/CLI commands. <code>reset_test_env</code> Autouse Automatically resets environment variables (<code>PHLO_ENV</code>, <code>PHLO_LOG_LEVEL</code>) before every test to prevent pollution."},{"location":"guides/testing-strategy/#implementation-guide","title":"Implementation Guide","text":"<ol> <li>Level 1 (Unit): Use <code>mock_hook_bus</code> and <code>reset_test_env</code>. Avoid containers.</li> <li>Level 2 (Functional): Use <code>iceberg_catalog</code> and <code>duckdb_connection</code> for fast data verification.</li> <li>Level 3 (E2E): Use <code>minio_service</code>, <code>trino_service</code>, <code>dagster_instance</code> for full system emulation.</li> </ol>"},{"location":"guides/workflow-development/","title":"Workflow Development Guide","text":""},{"location":"guides/workflow-development/#building-your-first-data-pipeline-in-phlo","title":"Building Your First Data Pipeline in Phlo","text":"<p>This guide walks you through creating a complete data pipeline from scratch. We'll build a pipeline that ingests weather data from an API, transforms it through Bronze/Silver/Gold layers, and publishes it for analytics.</p>"},{"location":"guides/workflow-development/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Pipeline Overview</li> <li>Prerequisites</li> <li>Step 1: Define Your Data Schema</li> <li>Step 2: Create the Ingestion Asset</li> <li>Step 3: Create Bronze Layer (Staging)</li> <li>Step 4: Create Silver Layer (Facts)</li> <li>Step 5: Create Gold Layer (Aggregations)</li> <li>Step 6: Create Marts for BI</li> <li>Step 7: Add Data Quality Checks</li> <li>Step 8: Configure Publishing</li> <li>Step 9: Add Scheduling</li> <li>Step 10: Test and Deploy</li> <li>Advanced Patterns</li> </ol>"},{"location":"guides/workflow-development/#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"guides/workflow-development/#what-were-building","title":"What We're Building","text":"<p>A weather data pipeline that:</p> <ol> <li>Fetches weather data from OpenWeather API</li> <li>Stores raw data in Iceberg (Bronze)</li> <li>Cleans and standardizes data (Silver)</li> <li>Calculates daily statistics (Gold)</li> <li>Publishes to PostgreSQL for dashboards (Marts)</li> <li>Runs automatically every hour</li> </ol>"},{"location":"guides/workflow-development/#architecture","title":"Architecture","text":"<pre><code>OpenWeather API\n    \u2193\n[Dagster Asset: weather_data]\n    \u2193\nIceberg: raw.weather_observations\n    \u2193\n[dbt: stg_weather_observations]\n    \u2193\nIceberg: bronze.stg_weather_observations\n    \u2193\n[dbt: fct_weather_readings]\n    \u2193\nIceberg: silver.fct_weather_readings\n    \u2193\n[dbt: mrt_daily_weather_summary]\n    \u2193\nIceberg: marts.mrt_daily_weather_summary\n    \u2193\n[Dagster Asset: publish_weather_marts]\n    \u2193\nPostgreSQL: marts.mrt_daily_weather_summary\n    \u2193\nSuperset Dashboard\n</code></pre>"},{"location":"guides/workflow-development/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have:</p> <ol> <li>\u2705 Phlo running (<code>make up-core up-query</code>)</li> <li>\u2705 Basic understanding of SQL</li> <li>\u2705 OpenWeather API key (free at https://openweathermap.org/api)</li> <li>\u2705 Text editor or IDE</li> </ol>"},{"location":"guides/workflow-development/#step-1-define-your-data-schema","title":"Step 1: Define Your Data Schema","text":"<p>First, let's define what our data looks like.</p>"},{"location":"guides/workflow-development/#11-create-schema-definition","title":"1.1 Create Schema Definition","text":"<p>Create a new file: <code>workflows/schemas/weather.py</code></p> <pre><code>\"\"\"Schema definitions for weather data.\"\"\"\n\nimport pandera as pa\nfrom pandera.typing import Series\nfrom datetime import datetime\n\nclass WeatherObservationSchema(pa.DataFrameModel):\n    \"\"\"Schema for raw weather observations from OpenWeather API.\"\"\"\n\n    # Identifiers\n    city_name: Series[str] = pa.Field(description=\"City name\")\n    country: Series[str] = pa.Field(description=\"Country code\")\n    latitude: Series[float] = pa.Field(ge=-90, le=90, description=\"Latitude\")\n    longitude: Series[float] = pa.Field(ge=-180, le=180, description=\"Longitude\")\n\n    # Weather data\n    temperature: Series[float] = pa.Field(description=\"Temperature in Celsius\")\n    feels_like: Series[float] = pa.Field(description=\"Feels like temperature\")\n    humidity: Series[int] = pa.Field(ge=0, le=100, description=\"Humidity percentage\")\n    pressure: Series[int] = pa.Field(description=\"Atmospheric pressure in hPa\")\n    wind_speed: Series[float] = pa.Field(ge=0, description=\"Wind speed in m/s\")\n\n    # Weather conditions\n    weather_main: Series[str] = pa.Field(description=\"Main weather condition (Rain, Clear, etc.)\")\n    weather_description: Series[str] = pa.Field(description=\"Detailed description\")\n\n    # Timestamps\n    observation_time: Series[datetime] = pa.Field(description=\"Time of observation\")\n    sunrise_time: Series[datetime] = pa.Field(description=\"Sunrise time\")\n    sunset_time: Series[datetime] = pa.Field(description=\"Sunset time\")\n\n    class Config:\n        \"\"\"Pandera configuration.\"\"\"\n        coerce = True  # Coerce types automatically\n        strict = False  # Allow extra columns\n\n\nclass WeatherReadingSchema(pa.DataFrameModel):\n    \"\"\"Schema for transformed weather readings (silver layer).\"\"\"\n\n    # All fields from raw, plus calculated fields\n    city_name: Series[str]\n    temperature: Series[float]\n    humidity: Series[int]\n    observation_time: Series[datetime]\n\n    # Calculated fields\n    temperature_f: Series[float] = pa.Field(description=\"Temperature in Fahrenheit\")\n    temp_category: Series[str] = pa.Field(\n        isin=[\"Freezing\", \"Cold\", \"Mild\", \"Warm\", \"Hot\"],\n        description=\"Temperature category\"\n    )\n    is_daytime: Series[bool] = pa.Field(description=\"True if during daylight\")\n\n    class Config:\n        coerce = True\n        strict = False\n</code></pre> <p>What this does:</p> <ul> <li>Defines the structure of our data</li> <li>Validates data types</li> <li>Adds constraints (e.g., humidity 0-100%)</li> <li>Documents each field</li> <li>Enables automatic testing</li> </ul>"},{"location":"guides/workflow-development/#12-create-iceberg-table-schema-optional","title":"1.2 Create Iceberg Table Schema (Optional)","text":"<p>Create: <code>workflows/schemas/iceberg_weather.py</code></p> <pre><code>\"\"\"Iceberg table schemas for weather data.\"\"\"\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import (\n    NestedField,\n    StringType,\n    FloatType,\n    IntegerType,\n    TimestampType,\n    BooleanType,\n)\n\nWEATHER_OBSERVATION_SCHEMA = Schema(\n    NestedField(1, \"city_name\", StringType(), required=True),\n    NestedField(2, \"country\", StringType(), required=True),\n    NestedField(3, \"latitude\", FloatType(), required=True),\n    NestedField(4, \"longitude\", FloatType(), required=True),\n    NestedField(5, \"temperature\", FloatType(), required=True),\n    NestedField(6, \"feels_like\", FloatType(), required=True),\n    NestedField(7, \"humidity\", IntegerType(), required=True),\n    NestedField(8, \"pressure\", IntegerType(), required=True),\n    NestedField(9, \"wind_speed\", FloatType(), required=True),\n    NestedField(10, \"weather_main\", StringType(), required=True),\n    NestedField(11, \"weather_description\", StringType(), required=True),\n    NestedField(12, \"observation_time\", TimestampType(), required=True),\n    NestedField(13, \"sunrise_time\", TimestampType(), required=True),\n    NestedField(14, \"sunset_time\", TimestampType(), required=True),\n)\n\nWEATHER_READING_SCHEMA = Schema(\n    # Raw fields\n    NestedField(1, \"city_name\", StringType(), required=True),\n    NestedField(2, \"country\", StringType(), required=True),\n    NestedField(3, \"temperature\", FloatType(), required=True),\n    NestedField(4, \"humidity\", IntegerType(), required=True),\n    NestedField(5, \"observation_time\", TimestampType(), required=True),\n\n    # Calculated fields\n    NestedField(6, \"temperature_f\", FloatType(), required=False),\n    NestedField(7, \"temp_category\", StringType(), required=False),\n    NestedField(8, \"is_daytime\", BooleanType(), required=False),\n)\n</code></pre>"},{"location":"guides/workflow-development/#step-2-create-the-ingestion-asset","title":"Step 2: Create the Ingestion Asset","text":"<p>Now let's fetch data from the OpenWeather API and store it in Iceberg.</p>"},{"location":"guides/workflow-development/#21-add-api-configuration","title":"2.1 Add API Configuration","text":"<p>Add your API configuration to <code>.phlo/.env.local</code>:</p> <pre><code># Weather API Configuration\nOPENWEATHER_API_KEY=your_api_key_here\nOPENWEATHER_CITIES=London,GB;New York,US;Tokyo,JP;Sydney,AU\n</code></pre>"},{"location":"guides/workflow-development/#22-create-the-ingestion-asset","title":"2.2 Create the Ingestion Asset","text":"<p>Create: <code>workflows/ingestion/weather_assets.py</code></p> <p>Important: We use DLT (Data Load Tool) for ingestion, following Phlo's established pattern. DLT handles:</p> <ul> <li>Robust data loading with retries</li> <li>Schema inference and validation</li> <li>Parquet file staging</li> <li>State management</li> </ul> <pre><code>\"\"\"Weather data ingestion assets using DLT.\"\"\"\n\nfrom __future__ import annotations\n\nimport time\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any\n\nimport dagster as dg\nimport dlt\nimport requests\n\nfrom phlo.config import config\nfrom phlo_iceberg.resource import IcebergResource\nfrom workflows.schemas.weather import WeatherObservationSchema\nfrom workflows.schemas.iceberg_weather import WEATHER_OBSERVATION_SCHEMA\n\n\n@dg.asset(\n    name=\"dlt_weather_data\",\n    group_name=\"weather\",\n    compute_kind=\"dlt+pyiceberg\",\n    description=\"Fetches current weather data from OpenWeather API using DLT\",\n    retry_policy=dg.RetryPolicy(max_retries=3, delay=30),\n)\ndef weather_data(\n    context: dg.AssetExecutionContext,\n    iceberg: IcebergResource,\n) -&gt; dg.MaterializeResult:\n    \"\"\"\n    Ingest weather data using two-step DLT pattern (like glucose example):\n\n    1. Fetch data from OpenWeather API\n    2. DLT stages data to parquet files\n    3. PyIceberg registers/appends to Iceberg table\n\n    Why DLT?\n    - Handles schema evolution automatically\n    - Robust error handling and retries\n    - Consistent with other ingestion assets (dlt_glucose_entries)\n    - State management for incremental loads\n    \"\"\"\n    table_name = f\"{config.iceberg_default_namespace}.weather_observations\"\n    pipeline_name = \"weather_openweathermap\"\n\n    # Setup DLT directories\n    pipelines_base_dir = Path.home() / \".dlt\" / \"pipelines\" / \"weather\"\n    pipelines_base_dir.mkdir(parents=True, exist_ok=True)\n\n    context.log.info(f\"Starting weather data ingestion\")\n    context.log.info(f\"Target table: {table_name}\")\n\n    try:\n        # Step 1: Fetch data from OpenWeather API\n        context.log.info(\"Fetching data from OpenWeather API...\")\n\n        cities = [\n            city.strip().split(\",\")\n            for city in config.openweather_cities.split(\";\")\n        ]\n\n        weather_records = []\n\n        for city_name, country in cities:\n            context.log.info(f\"Fetching weather for {city_name}, {country}\")\n\n            url = \"https://api.openweathermap.org/data/2.5/weather\"\n            params = {\n                \"q\": f\"{city_name},{country}\",\n                \"appid\": config.openweather_api_key,\n                \"units\": \"metric\",  # Celsius\n            }\n\n            try:\n                response = requests.get(url, params=params, timeout=10)\n                response.raise_for_status()\n                data = response.json()\n\n                # Extract and structure data\n                record = {\n                    \"city_name\": data[\"name\"],\n                    \"country\": data[\"sys\"][\"country\"],\n                    \"latitude\": data[\"coord\"][\"lat\"],\n                    \"longitude\": data[\"coord\"][\"lon\"],\n                    \"temperature\": data[\"main\"][\"temp\"],\n                    \"feels_like\": data[\"main\"][\"feels_like\"],\n                    \"humidity\": data[\"main\"][\"humidity\"],\n                    \"pressure\": data[\"main\"][\"pressure\"],\n                    \"wind_speed\": data[\"wind\"][\"speed\"],\n                    \"weather_main\": data[\"weather\"][0][\"main\"],\n                    \"weather_description\": data[\"weather\"][0][\"description\"],\n                    \"observation_time\": datetime.fromtimestamp(data[\"dt\"], tz=timezone.utc),\n                    \"sunrise_time\": datetime.fromtimestamp(data[\"sys\"][\"sunrise\"], tz=timezone.utc),\n                    \"sunset_time\": datetime.fromtimestamp(data[\"sys\"][\"sunset\"], tz=timezone.utc),\n                }\n\n                weather_records.append(record)\n\n            except requests.RequestException as e:\n                context.log.error(f\"Failed to fetch {city_name}: {e}\")\n                continue\n\n        if not weather_records:\n            context.log.info(\"No weather data fetched, skipping\")\n            return dg.MaterializeResult(\n                metadata={\n                    \"rows_loaded\": dg.MetadataValue.int(0),\n                    \"status\": dg.MetadataValue.text(\"no_data\"),\n                }\n            )\n\n        context.log.info(f\"Successfully fetched {len(weather_records)} weather observations\")\n\n        # Add phlo ingestion timestamp\n        ingestion_timestamp = datetime.now(timezone.utc)\n        for record in weather_records:\n            record[\"_cascade_ingested_at\"] = ingestion_timestamp\n\n        # Step 2: Stage to parquet using DLT\n        context.log.info(\"Staging data to parquet via DLT...\")\n        start_time_ts = time.time()\n\n        local_staging_root = (pipelines_base_dir / pipeline_name / \"stage\").resolve()\n        local_staging_root.mkdir(parents=True, exist_ok=True)\n\n        # Create DLT pipeline with filesystem destination targeting local staging\n        filesystem_destination = dlt.destinations.filesystem(\n            bucket_url=local_staging_root.as_uri(),\n        )\n\n        pipeline = dlt.pipeline(\n            pipeline_name=pipeline_name,\n            destination=filesystem_destination,\n            dataset_name=\"weather\",\n            pipelines_dir=str(pipelines_base_dir),\n        )\n\n        # Define DLT resource\n        @dlt.resource(name=\"weather_observations\", write_disposition=\"replace\")\n        def provide_weather() -&gt; Any:\n            yield weather_records\n\n        # Run DLT pipeline to stage parquet files\n        info = pipeline.run(\n            provide_weather(),\n            loader_file_format=\"parquet\",\n        )\n\n        if not info.load_packages:\n            raise RuntimeError(\"DLT pipeline produced no load packages\")\n\n        # Get parquet file path\n        load_package = info.load_packages[0]\n        completed_jobs = load_package.jobs.get(\"completed_jobs\") or []\n\n        # Filter for actual parquet files (exclude pipeline state and other files)\n        parquet_files = [job for job in completed_jobs if job.file_path.endswith('.parquet')]\n\n        if not parquet_files:\n            raise RuntimeError(\"DLT pipeline completed without producing parquet files\")\n\n        parquet_path = Path(parquet_files[0].file_path)\n        if not parquet_path.is_absolute():\n            parquet_path = (local_staging_root / parquet_path).resolve()\n\n        dlt_elapsed = time.time() - start_time_ts\n        context.log.info(f\"DLT staging completed in {dlt_elapsed:.2f}s\")\n\n        # Step 3: Ensure Iceberg table exists\n        context.log.info(f\"Ensuring Iceberg table {table_name} exists...\")\n        iceberg.ensure_table(\n            table_name=table_name,\n            schema=WEATHER_OBSERVATION_SCHEMA,\n            partition_spec=None,\n        )\n\n        # Step 4: Append to Iceberg table\n        context.log.info(\"Appending data to Iceberg table...\")\n        iceberg.append_parquet(\n            table_name=table_name,\n            data_path=str(parquet_path),\n        )\n\n        total_elapsed = time.time() - start_time_ts\n        rows_loaded = len(weather_records)\n        context.log.info(f\"Ingestion completed successfully in {total_elapsed:.2f}s\")\n        context.log.info(f\"Loaded {rows_loaded} rows to {table_name}\")\n\n        return dg.MaterializeResult(\n            metadata={\n                \"rows_loaded\": dg.MetadataValue.int(rows_loaded),\n                \"cities\": dg.MetadataValue.text(\", \".join([r[\"city_name\"] for r in weather_records])),\n                \"table_name\": dg.MetadataValue.text(table_name),\n                \"dlt_elapsed_seconds\": dg.MetadataValue.float(dlt_elapsed),\n                \"total_elapsed_seconds\": dg.MetadataValue.float(total_elapsed),\n            }\n        )\n\n    except requests.RequestException as e:\n        context.log.error(f\"API request failed: {e}\")\n        raise RuntimeError(f\"Failed to fetch data from OpenWeather API\") from e\n\n    except Exception as e:\n        context.log.error(f\"Ingestion failed: {e}\")\n        raise RuntimeError(f\"Weather data ingestion failed: {e}\") from e\n\n\ndef build_weather_ingestion_defs() -&gt; dg.Definitions:\n    \"\"\"Build definitions for weather ingestion assets.\"\"\"\n    return dg.Definitions(\n        assets=[weather_data],\n    )\n</code></pre>"},{"location":"guides/workflow-development/#24-registration","title":"2.4 Registration","text":"<p>No manual registration is needed. Phlo discovers all Python files under <code>workflows/</code> automatically.</p>"},{"location":"guides/workflow-development/#25-test-the-ingestion","title":"2.5 Test the Ingestion","text":"<pre><code># Reload Dagster\ndocker-compose restart dagster-webserver dagster-daemon\n\n# Open Dagster UI\n# Navigate to Assets \u2192 dlt_weather_data\n# Click \"Materialize\"\n\n# Or use CLI\ndagster asset materialize -m phlo.framework.definitions -a dlt_weather_data\n</code></pre> <p>What just happened?</p> <ol> <li>Fetched weather data from the OpenWeather API</li> <li>DLT staged the data to local parquet files with schema validation</li> <li>PyIceberg appended the parquet to the Iceberg table</li> <li>Nessie catalog updated with the new snapshot</li> </ol> <p>Why DLT?</p> <ul> <li>Consistent with the glucose example pattern (<code>dlt_glucose_entries</code>)</li> <li>Handles schema evolution automatically</li> <li>Robust parquet file generation with proper typing</li> <li>State management for incremental loads</li> <li>Better separation of concerns (fetch \u2192 DLT stage \u2192 PyIceberg register)</li> <li>Matches established Phlo pattern for all ingestion assets</li> </ul> <p>Verify:</p> <pre><code>-- Connect to Trino\ndocker-compose exec trino trino\n\n-- Query the data\nSELECT * FROM iceberg.raw.weather_observations;\n</code></pre>"},{"location":"guides/workflow-development/#step-3-create-bronze-layer-staging","title":"Step 3: Create Bronze Layer (Staging)","text":"<p>The Bronze layer cleans and standardizes raw data.</p>"},{"location":"guides/workflow-development/#31-define-the-source","title":"3.1 Define the Source","text":"<p>Create: <code>workflows/transforms/dbt/models/sources/sources_weather.yml</code></p> <pre><code>version: 2\n\nsources:\n  - name: raw\n    description: \"Raw data layer - data as ingested from sources\"\n    schema: raw\n\n    tables:\n      - name: weather_observations\n        description: \"Raw weather observations from OpenWeather API\"\n        columns:\n          - name: city_name\n            description: \"City name\"\n            tests:\n              - not_null\n\n          - name: observation_time\n            description: \"When the observation was recorded\"\n            tests:\n              - not_null\n\n          - name: temperature\n            description: \"Temperature in Celsius\"\n            tests:\n              - not_null\n</code></pre>"},{"location":"guides/workflow-development/#32-create-the-staging-model","title":"3.2 Create the Staging Model","text":"<p>Create: <code>workflows/transforms/dbt/models/bronze/stg_weather_observations.sql</code></p> <pre><code>{{\n    config(\n        materialized='table',\n        schema='bronze',\n        tags=['weather', 'bronze']\n    )\n}}\n\nWITH source AS (\n    SELECT * FROM {{ source('raw', 'weather_observations') }}\n),\n\ncleaned AS (\n    SELECT\n        -- Identifiers\n        city_name,\n        country,\n        ROUND(CAST(latitude AS DOUBLE), 4) AS latitude,\n        ROUND(CAST(longitude AS DOUBLE), 4) AS longitude,\n\n        -- Weather measurements\n        ROUND(CAST(temperature AS DOUBLE), 2) AS temperature_c,\n        ROUND(CAST(feels_like AS DOUBLE), 2) AS feels_like_c,\n        CAST(humidity AS INTEGER) AS humidity_pct,\n        CAST(pressure AS INTEGER) AS pressure_hpa,\n        ROUND(CAST(wind_speed AS DOUBLE), 2) AS wind_speed_ms,\n\n        -- Conditions\n        LOWER(TRIM(weather_main)) AS weather_main,\n        LOWER(TRIM(weather_description)) AS weather_description,\n\n        -- Timestamps\n        CAST(observation_time AS TIMESTAMP) AS observation_timestamp,\n        CAST(sunrise_time AS TIMESTAMP) AS sunrise_timestamp,\n        CAST(sunset_time AS TIMESTAMP) AS sunset_timestamp,\n\n        -- Metadata\n        DATE(observation_time) AS observation_date,\n        HOUR(observation_time) AS observation_hour\n\n    FROM source\n    WHERE\n        -- Data quality filters\n        temperature IS NOT NULL\n        AND observation_time IS NOT NULL\n        AND city_name IS NOT NULL\n        -- Remove outliers\n        AND temperature BETWEEN -50 AND 60  -- Reasonable temp range\n        AND humidity BETWEEN 0 AND 100\n        AND wind_speed &gt;= 0\n)\n\nSELECT * FROM cleaned\n</code></pre> <p>What this does:</p> <ul> <li>Standardizes column names (e.g., <code>temperature</code> \u2192 <code>temperature_c</code>)</li> <li>Converts types explicitly</li> <li>Rounds numeric values</li> <li>Standardizes text (lowercase, trim)</li> <li>Adds derived date/hour fields</li> <li>Filters out invalid data</li> <li>Removes outliers</li> </ul>"},{"location":"guides/workflow-development/#33-add-tests","title":"3.3 Add Tests","text":"<p>Create: <code>workflows/transforms/dbt/models/bronze/schema.yml</code></p> <pre><code>version: 2\n\nmodels:\n  - name: stg_weather_observations\n    description: \"Cleaned and standardized weather observations\"\n    columns:\n      - name: city_name\n        description: \"City name\"\n        tests:\n          - not_null\n\n      - name: temperature_c\n        description: \"Temperature in Celsius\"\n        tests:\n          - not_null\n          - dbt_utils.accepted_range:\n              min_value: -50\n              max_value: 60\n\n      - name: humidity_pct\n        description: \"Humidity percentage\"\n        tests:\n          - not_null\n          - dbt_utils.accepted_range:\n              min_value: 0\n              max_value: 100\n\n      - name: observation_timestamp\n        description: \"Observation timestamp\"\n        tests:\n          - not_null\n\n      - name: observation_date\n        description: \"Observation date\"\n        tests:\n          - not_null\n</code></pre>"},{"location":"guides/workflow-development/#step-4-create-silver-layer-facts","title":"Step 4: Create Silver Layer (Facts)","text":"<p>The Silver layer adds business logic and calculated fields.</p>"},{"location":"guides/workflow-development/#41-create-the-fact-table","title":"4.1 Create the Fact Table","text":"<p>Create: <code>workflows/transforms/dbt/models/silver/fct_weather_readings.sql</code></p> <pre><code>{{\n    config(\n        materialized='table',\n        schema='silver',\n        tags=['weather', 'silver', 'fact']\n    )\n}}\n\nWITH staged AS (\n    SELECT * FROM {{ ref('stg_weather_observations') }}\n),\n\nenriched AS (\n    SELECT\n        -- All original fields\n        *,\n\n        -- Temperature conversions\n        ROUND((temperature_c * 9.0 / 5.0) + 32, 2) AS temperature_f,\n        ROUND((feels_like_c * 9.0 / 5.0) + 32, 2) AS feels_like_f,\n\n        -- Temperature category\n        CASE\n            WHEN temperature_c &lt; 0 THEN 'Freezing'\n            WHEN temperature_c &lt; 10 THEN 'Cold'\n            WHEN temperature_c &lt; 20 THEN 'Mild'\n            WHEN temperature_c &lt; 30 THEN 'Warm'\n            ELSE 'Hot'\n        END AS temp_category,\n\n        -- Comfort index (simplified)\n        CASE\n            WHEN humidity_pct &gt; 80 AND temperature_c &gt; 25 THEN 'Uncomfortable'\n            WHEN humidity_pct &lt; 30 AND temperature_c &lt; 10 THEN 'Dry-Cold'\n            ELSE 'Comfortable'\n        END AS comfort_level,\n\n        -- Wind category (Beaufort scale simplified)\n        CASE\n            WHEN wind_speed_ms &lt; 0.3 THEN 'Calm'\n            WHEN wind_speed_ms &lt; 3.3 THEN 'Light'\n            WHEN wind_speed_ms &lt; 7.9 THEN 'Moderate'\n            WHEN wind_speed_ms &lt; 13.8 THEN 'Fresh'\n            ELSE 'Strong'\n        END AS wind_category,\n\n        -- Daylight indicator\n        CASE\n            WHEN observation_timestamp BETWEEN sunrise_timestamp AND sunset_timestamp\n            THEN TRUE\n            ELSE FALSE\n        END AS is_daytime,\n\n        -- Daylight duration (hours)\n        ROUND(\n            CAST(sunset_timestamp AS BIGINT) - CAST(sunrise_timestamp AS BIGINT)\n        ) / 3600.0 AS daylight_hours,\n\n        -- Weather condition grouping\n        CASE\n            WHEN weather_main IN ('rain', 'drizzle', 'thunderstorm') THEN 'Precipitation'\n            WHEN weather_main IN ('snow', 'sleet') THEN 'Winter Precipitation'\n            WHEN weather_main IN ('clear', 'clouds') THEN 'Dry'\n            WHEN weather_main IN ('fog', 'mist', 'haze') THEN 'Low Visibility'\n            ELSE 'Other'\n        END AS weather_category,\n\n        -- Location identifier (for grouping)\n        city_name || ', ' || country AS location_key\n\n    FROM staged\n)\n\nSELECT * FROM enriched\n</code></pre> <p>What this does:</p> <ul> <li>Converts Celsius to Fahrenheit</li> <li>Categorizes temperature (Freezing/Cold/Mild/Warm/Hot)</li> <li>Calculates comfort level</li> <li>Categorizes wind speed</li> <li>Determines if daytime or nighttime</li> <li>Calculates daylight duration</li> <li>Groups weather conditions</li> <li>Creates location key for grouping</li> </ul>"},{"location":"guides/workflow-development/#42-add-tests","title":"4.2 Add Tests","text":"<p>Create: <code>workflows/transforms/dbt/models/silver/schema.yml</code></p> <pre><code>version: 2\n\nmodels:\n  - name: fct_weather_readings\n    description: \"Weather readings with calculated metrics and categorizations\"\n    columns:\n      - name: location_key\n        description: \"Unique location identifier\"\n        tests:\n          - not_null\n\n      - name: temperature_f\n        description: \"Temperature in Fahrenheit\"\n        tests:\n          - not_null\n\n      - name: temp_category\n        description: \"Temperature category\"\n        tests:\n          - not_null\n          - accepted_values:\n              values: [\"Freezing\", \"Cold\", \"Mild\", \"Warm\", \"Hot\"]\n\n      - name: is_daytime\n        description: \"True if observation during daylight\"\n        tests:\n          - not_null\n</code></pre>"},{"location":"guides/workflow-development/#step-5-create-gold-layer-aggregations","title":"Step 5: Create Gold Layer (Aggregations)","text":"<p>The Gold layer creates aggregated, business-ready datasets.</p>"},{"location":"guides/workflow-development/#51-create-daily-summary","title":"5.1 Create Daily Summary","text":"<p>Create: <code>workflows/transforms/dbt/models/gold/agg_daily_weather_summary.sql</code></p> <pre><code>{{\n    config(\n        materialized='table',\n        schema='gold',\n        tags=['weather', 'gold', 'aggregation']\n    )\n}}\n\nWITH weather_readings AS (\n    SELECT * FROM {{ ref('fct_weather_readings') }}\n),\n\ndaily_stats AS (\n    SELECT\n        observation_date,\n        location_key,\n        city_name,\n        country,\n\n        -- Temperature statistics\n        COUNT(*) AS reading_count,\n        ROUND(AVG(temperature_c), 2) AS avg_temp_c,\n        ROUND(MIN(temperature_c), 2) AS min_temp_c,\n        ROUND(MAX(temperature_c), 2) AS max_temp_c,\n        ROUND(AVG(temperature_f), 2) AS avg_temp_f,\n\n        -- Feels like temperature\n        ROUND(AVG(feels_like_c), 2) AS avg_feels_like_c,\n\n        -- Humidity statistics\n        ROUND(AVG(humidity_pct), 2) AS avg_humidity_pct,\n        MIN(humidity_pct) AS min_humidity_pct,\n        MAX(humidity_pct) AS max_humidity_pct,\n\n        -- Wind statistics\n        ROUND(AVG(wind_speed_ms), 2) AS avg_wind_speed_ms,\n        ROUND(MAX(wind_speed_ms), 2) AS max_wind_speed_ms,\n\n        -- Pressure statistics\n        ROUND(AVG(pressure_hpa), 2) AS avg_pressure_hpa,\n\n        -- Daylight info (should be consistent per day)\n        AVG(daylight_hours) AS daylight_hours,\n\n        -- Weather conditions (most common)\n        MODE() WITHIN GROUP (ORDER BY weather_main) AS predominant_weather,\n        MODE() WITHIN GROUP (ORDER BY temp_category) AS predominant_temp_category,\n\n        -- Comfort analysis\n        SUM(CASE WHEN comfort_level = 'Comfortable' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_comfortable,\n\n        -- Precipitation indicator\n        MAX(CASE WHEN weather_category = 'Precipitation' THEN 1 ELSE 0 END) AS had_precipitation\n\n    FROM weather_readings\n    GROUP BY\n        observation_date,\n        location_key,\n        city_name,\n        country\n)\n\nSELECT * FROM daily_stats\nORDER BY observation_date DESC, location_key\n</code></pre>"},{"location":"guides/workflow-development/#52-create-location-dimension","title":"5.2 Create Location Dimension","text":"<p>Create: <code>workflows/transforms/dbt/models/gold/dim_location.sql</code></p> <pre><code>{{\n    config(\n        materialized='table',\n        schema='gold',\n        tags=['weather', 'gold', 'dimension']\n    )\n}}\n\nWITH locations AS (\n    SELECT DISTINCT\n        location_key,\n        city_name,\n        country,\n        latitude,\n        longitude\n    FROM {{ ref('stg_weather_observations') }}\n)\n\nSELECT\n    ROW_NUMBER() OVER (ORDER BY location_key) AS location_id,\n    location_key,\n    city_name,\n    country,\n    latitude,\n    longitude,\n    -- Add region/continent based on country (simplified)\n    CASE\n        WHEN country IN ('US', 'CA', 'MX') THEN 'North America'\n        WHEN country IN ('GB', 'FR', 'DE', 'IT', 'ES') THEN 'Europe'\n        WHEN country IN ('JP', 'CN', 'IN', 'KR') THEN 'Asia'\n        WHEN country IN ('AU', 'NZ') THEN 'Oceania'\n        ELSE 'Other'\n    END AS region\nFROM locations\n</code></pre>"},{"location":"guides/workflow-development/#step-6-create-marts-for-bi","title":"Step 6: Create Marts for BI","text":"<p>Marts are optimized for BI tools and end-user queries.</p>"},{"location":"guides/workflow-development/#61-create-weather-overview-mart","title":"6.1 Create Weather Overview Mart","text":"<p>Create: <code>workflows/transforms/dbt/models/marts_postgres/mrt_weather_overview.sql</code></p> <pre><code>{{\n    config(\n        materialized='table',\n        schema='marts',\n        tags=['weather', 'marts', 'postgres']\n    )\n}}\n\nWITH daily_summary AS (\n    SELECT * FROM {{ ref('agg_daily_weather_summary') }}\n),\n\nlocation_dim AS (\n    SELECT * FROM {{ ref('dim_location') }}\n),\n\nenriched AS (\n    SELECT\n        d.observation_date,\n        d.location_key,\n        d.city_name,\n        d.country,\n        l.region,\n\n        -- Temperature metrics\n        d.avg_temp_c,\n        d.min_temp_c,\n        d.max_temp_c,\n        d.avg_temp_f,\n        d.avg_feels_like_c,\n        d.max_temp_c - d.min_temp_c AS temp_range_c,\n\n        -- Other metrics\n        d.avg_humidity_pct,\n        d.avg_wind_speed_ms,\n        d.avg_pressure_hpa,\n        d.daylight_hours,\n\n        -- Weather conditions\n        d.predominant_weather,\n        d.predominant_temp_category,\n        d.pct_comfortable,\n        d.had_precipitation,\n\n        -- Metadata\n        d.reading_count,\n        CURRENT_TIMESTAMP AS last_updated\n\n    FROM daily_summary d\n    LEFT JOIN location_dim l ON d.location_key = l.location_key\n)\n\nSELECT * FROM enriched\nORDER BY observation_date DESC, city_name\n</code></pre>"},{"location":"guides/workflow-development/#62-create-recent-readings-mart","title":"6.2 Create Recent Readings Mart","text":"<p>Create: <code>workflows/transforms/dbt/models/marts_postgres/mrt_recent_weather.sql</code></p> <pre><code>{{\n    config(\n        materialized='table',\n        schema='marts',\n        tags=['weather', 'marts', 'postgres']\n    )\n}}\n\nWITH recent_readings AS (\n    SELECT\n        location_key,\n        city_name,\n        country,\n        temperature_c,\n        temperature_f,\n        feels_like_c,\n        humidity_pct,\n        wind_speed_ms,\n        weather_main,\n        weather_description,\n        temp_category,\n        comfort_level,\n        is_daytime,\n        observation_timestamp,\n\n        -- Rank by recency per location\n        ROW_NUMBER() OVER (\n            PARTITION BY location_key\n            ORDER BY observation_timestamp DESC\n        ) AS recency_rank\n\n    FROM {{ ref('fct_weather_readings') }}\n    WHERE observation_timestamp &gt;= CURRENT_TIMESTAMP - INTERVAL '24' HOUR\n)\n\nSELECT\n    location_key,\n    city_name,\n    country,\n    temperature_c,\n    temperature_f,\n    feels_like_c,\n    humidity_pct,\n    wind_speed_ms,\n    weather_main,\n    weather_description,\n    temp_category,\n    comfort_level,\n    is_daytime,\n    observation_timestamp,\n    CURRENT_TIMESTAMP AS refreshed_at\nFROM recent_readings\nWHERE recency_rank = 1  -- Most recent reading per location\nORDER BY city_name\n</code></pre>"},{"location":"guides/workflow-development/#step-7-add-data-quality-checks","title":"Step 7: Add Data Quality Checks","text":"<p>Quality checks ensure your data meets expectations.</p>"},{"location":"guides/workflow-development/#71-create-quality-check-asset","title":"7.1 Create Quality Check Asset","text":"<p>Create: <code>workflows/quality/weather.py</code></p> <pre><code>\"\"\"Data quality checks for weather pipeline.\"\"\"\n\nimport dagster as dg\nimport pandas as pd\nfrom dagster_pandera import pandera_schema_to_dagster_type\n\nfrom workflows.schemas.weather import WeatherObservationSchema, WeatherReadingSchema\nfrom phlo_trino.resource import TrinoResource\n\n\n@dg.asset(\n    name=\"weather_quality_check_raw\",\n    group_name=\"weather_quality\",\n    deps=[\"weather_data\"],\n    description=\"Quality checks for raw weather data\",\n)\ndef check_raw_weather_quality(\n    context: dg.AssetExecutionContext,\n    trino: TrinoResource,\n) -&gt; dg.MaterializeResult:\n    \"\"\"\n    Perform quality checks on raw weather data.\n\n    Checks:\n    - Record count\n    - Null values\n    - Value ranges\n    - Duplicate detection\n    \"\"\"\n    query = \"\"\"\n    SELECT\n        COUNT(*) as total_records,\n        COUNT(DISTINCT city_name) as unique_cities,\n        SUM(CASE WHEN temperature IS NULL THEN 1 ELSE 0 END) as null_temperatures,\n        MIN(temperature) as min_temp,\n        MAX(temperature) as max_temp,\n        MIN(observation_time) as earliest_observation,\n        MAX(observation_time) as latest_observation\n    FROM iceberg.raw.weather_observations\n    \"\"\"\n\n    result = trino.execute(query)\n    row = result[0]\n\n    # Assertions\n    assert row[\"total_records\"] &gt; 0, \"No records found\"\n    assert row[\"unique_cities\"] &gt; 0, \"No cities found\"\n    assert row[\"null_temperatures\"] == 0, f\"Found {row['null_temperatures']} null temperatures\"\n    assert -50 &lt;= row[\"min_temp\"] &lt;= 60, f\"Temperature out of range: {row['min_temp']}\"\n    assert -50 &lt;= row[\"max_temp\"] &lt;= 60, f\"Temperature out of range: {row['max_temp']}\"\n\n    context.log.info(f\"\u2713 Quality checks passed\")\n    context.log.info(f\"  Total records: {row['total_records']}\")\n    context.log.info(f\"  Unique cities: {row['unique_cities']}\")\n    context.log.info(f\"  Temperature range: {row['min_temp']}\u00b0C to {row['max_temp']}\u00b0C\")\n\n    return dg.MaterializeResult(\n        metadata={\n            \"total_records\": row[\"total_records\"],\n            \"unique_cities\": row[\"unique_cities\"],\n            \"min_temp\": row[\"min_temp\"],\n            \"max_temp\": row[\"max_temp\"],\n        }\n    )\n\n\n@dg.asset(\n    name=\"weather_quality_check_silver\",\n    group_name=\"weather_quality\",\n    deps=[\"dbt:fct_weather_readings\"],\n    description=\"Quality checks for silver layer weather data\",\n)\ndef check_silver_weather_quality(\n    context: dg.AssetExecutionContext,\n    trino: TrinoResource,\n) -&gt; dg.MaterializeResult:\n    \"\"\"\n    Perform quality checks on silver layer weather data.\n\n    Checks:\n    - All calculated fields present\n    - Categories valid\n    - No nulls in required fields\n    \"\"\"\n    query = \"\"\"\n    SELECT\n        COUNT(*) as total_records,\n        SUM(CASE WHEN temp_category IS NULL THEN 1 ELSE 0 END) as null_categories,\n        SUM(CASE WHEN temperature_f IS NULL THEN 1 ELSE 0 END) as null_temp_f,\n        COUNT(DISTINCT temp_category) as distinct_categories\n    FROM iceberg.silver.fct_weather_readings\n    \"\"\"\n\n    result = trino.execute(query)\n    row = result[0]\n\n    # Assertions\n    assert row[\"null_categories\"] == 0, f\"Found {row['null_categories']} null categories\"\n    assert row[\"null_temp_f\"] == 0, f\"Found {row['null_temp_f']} null Fahrenheit temps\"\n    assert row[\"distinct_categories\"] == 5, f\"Expected 5 categories, found {row['distinct_categories']}\"\n\n    context.log.info(f\"\u2713 Silver layer quality checks passed\")\n\n    return dg.MaterializeResult(\n        metadata={\n            \"total_records\": row[\"total_records\"],\n            \"distinct_categories\": row[\"distinct_categories\"],\n        }\n    )\n\n\ndef build_weather_quality_defs() -&gt; dg.Definitions:\n    \"\"\"Build quality check definitions.\"\"\"\n    return dg.Definitions(\n        assets=[\n            check_raw_weather_quality,\n            check_silver_weather_quality,\n        ],\n    )\n</code></pre>"},{"location":"guides/workflow-development/#72-quality-check-discovery","title":"7.2 Quality Check Discovery","text":"<p>No manual registration is needed. Phlo discovers quality workflows under <code>workflows/quality/</code>.</p>"},{"location":"guides/workflow-development/#step-8-configure-publishing","title":"Step 8: Configure Publishing","text":"<p>Configure which tables get published to PostgreSQL for BI tools.</p>"},{"location":"guides/workflow-development/#81-update-publishing-config","title":"8.1 Update Publishing Config","text":"<p>Edit: <code>workflows/publishing/config.yaml</code></p> <pre><code># Existing configurations...\n\nweather:\n  description: \"Weather data marts for BI dashboards\"\n  enabled: true\n  tables:\n    - iceberg_table: \"marts.mrt_weather_overview\"\n      postgres_table: \"mrt_weather_overview\"\n      postgres_schema: \"marts\"\n      description: \"Daily weather summary by location\"\n      mode: \"replace\" # Options: replace, append, upsert\n\n    - iceberg_table: \"marts.mrt_recent_weather\"\n      postgres_table: \"mrt_recent_weather\"\n      postgres_schema: \"marts\"\n      description: \"Most recent weather reading per location\"\n      mode: \"replace\"\n</code></pre>"},{"location":"guides/workflow-development/#82-create-publishing-asset","title":"8.2 Create Publishing Asset","text":"<p>The publishing asset is already generic and will pick up your config automatically. Just ensure it runs after your dbt models.</p> <p>Test it:</p> <pre><code>dagster asset materialize -m phlo.framework.definitions -a publish_postgres_marts\n</code></pre>"},{"location":"guides/workflow-development/#step-9-add-scheduling","title":"Step 9: Add Scheduling","text":"<p>Schedule your pipeline to run automatically.</p>"},{"location":"guides/workflow-development/#91-create-schedule","title":"9.1 Create Schedule","text":"<p>Edit: <code>workflows/schedules/schedules.py</code></p> <pre><code># Add to existing schedules\n\n@dg.schedule(\n    name=\"weather_pipeline_schedule\",\n    cron_schedule=\"0 * * * *\",  # Every hour\n    job=dg.define_asset_job(\n        name=\"weather_pipeline_job\",\n        selection=dg.AssetSelection.groups(\"weather\"),  # All weather assets\n    ),\n    default_status=dg.DefaultScheduleStatus.RUNNING,  # Auto-start\n    execution_timezone=\"UTC\",\n)\ndef weather_pipeline_schedule():\n    \"\"\"Run weather pipeline every hour.\"\"\"\n    return dg.RunRequest()\n</code></pre>"},{"location":"guides/workflow-development/#92-add-sensor-for-freshness","title":"9.2 Add Sensor for Freshness","text":"<p>Create a sensor that alerts if data gets stale:</p> <pre><code>@dg.sensor(\n    name=\"weather_freshness_sensor\",\n    minimum_interval_seconds=300,  # Check every 5 minutes\n    default_status=dg.DefaultSensorStatus.RUNNING,\n)\ndef weather_freshness_sensor(context: dg.SensorEvaluationContext):\n    \"\"\"Alert if weather data is stale (&gt; 2 hours old).\"\"\"\n\n    # Check last materialization time\n    weather_asset_key = dg.AssetKey([\"weather_data\"])\n    latest_materialization = context.instance.get_latest_materialization_event(weather_asset_key)\n\n    if not latest_materialization:\n        context.log.warning(\"No materialization found for weather_data\")\n        return dg.SkipReason(\"No materialization yet\")\n\n    from datetime import datetime, timedelta\n\n    last_update = latest_materialization.timestamp\n    age = datetime.now().timestamp() - last_update\n    age_hours = age / 3600\n\n    if age_hours &gt; 2:\n        context.log.error(f\"Weather data is {age_hours:.1f} hours old!\")\n        # Trigger re-materialization\n        return dg.RunRequest(\n            asset_selection=[weather_asset_key],\n        )\n\n    return dg.SkipReason(f\"Data is fresh ({age_hours:.1f} hours old)\")\n</code></pre>"},{"location":"guides/workflow-development/#step-10-test-and-deploy","title":"Step 10: Test and Deploy","text":""},{"location":"guides/workflow-development/#101-run-complete-pipeline","title":"10.1 Run Complete Pipeline","text":"<pre><code># Materialize all weather assets\ndagster asset materialize -m phlo.framework.definitions \\\n  --select \"tag:weather\"\n\n# Or use Dagster UI:\n# 1. Open http://localhost:3000\n# 2. Assets \u2192 Filter by tag: weather\n# 3. Select all \u2192 Materialize\n</code></pre>"},{"location":"guides/workflow-development/#102-verify-each-layer","title":"10.2 Verify Each Layer","text":"<pre><code>-- Check raw data\nSELECT COUNT(*), MIN(observation_time), MAX(observation_time)\nFROM iceberg.raw.weather_observations;\n\n-- Check bronze data\nSELECT COUNT(*) FROM iceberg.bronze.stg_weather_observations;\n\n-- Check silver data\nSELECT\n    city_name,\n    temperature_c,\n    temp_category,\n    comfort_level\nFROM iceberg.silver.fct_weather_readings\nORDER BY observation_timestamp DESC\nLIMIT 10;\n\n-- Check gold data\nSELECT * FROM iceberg.gold.agg_daily_weather_summary\nORDER BY observation_date DESC;\n\n-- Check marts (in PostgreSQL)\nSELECT * FROM marts.mrt_weather_overview\nORDER BY observation_date DESC;\n</code></pre>"},{"location":"guides/workflow-development/#103-run-dbt-tests","title":"10.3 Run dbt Tests","text":"<pre><code>docker-compose exec dagster-webserver \\\n  dbt test --project-dir /app/workflows/transforms/dbt \\\n  --select tag:weather\n</code></pre>"},{"location":"guides/workflow-development/#104-create-superset-dashboard","title":"10.4 Create Superset Dashboard","text":"<ol> <li>Open Superset: http://localhost:10008</li> <li>Add PostgreSQL database connection</li> <li>Create dataset from <code>marts.mrt_weather_overview</code></li> <li>Create charts:</li> <li>Line chart: Temperature trends by city</li> <li>Bar chart: Current temperatures</li> <li>Table: Recent readings</li> <li>Add to dashboard</li> </ol>"},{"location":"guides/workflow-development/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/workflow-development/#pattern-1-incremental-processing","title":"Pattern 1: Incremental Processing","text":"<p>Instead of reprocessing all data, process only new data:</p> <pre><code>{{\n    config(\n        materialized='incremental',\n        unique_key='observation_timestamp',\n        schema='silver'\n    )\n}}\n\nSELECT * FROM {{ ref('stg_weather_observations') }}\n\n{% if is_incremental() %}\n    -- Only new data since last run\n    WHERE observation_timestamp &gt; (SELECT MAX(observation_timestamp) FROM {{ this }})\n{% endif %}\n</code></pre>"},{"location":"guides/workflow-development/#pattern-2-partitioned-assets","title":"Pattern 2: Partitioned Assets","text":"<p>Partition by date for better performance:</p> <pre><code>from dagster import DailyPartitionsDefinition\n\ndaily_partition = DailyPartitionsDefinition(start_date=\"2024-01-01\")\n\n@dg.asset(\n    partitions_def=daily_partition,\n    name=\"weather_data_partitioned\",\n)\ndef fetch_weather_data_partitioned(context: dg.AssetExecutionContext):\n    partition_date = context.partition_key  # \"2024-11-05\"\n    # Fetch only data for this date\n    ...\n</code></pre>"},{"location":"guides/workflow-development/#pattern-3-dynamic-cities","title":"Pattern 3: Dynamic Cities","text":"<p>Fetch list of cities from a config table instead of hardcoding:</p> <pre><code>@dg.asset\ndef weather_cities_config(trino: TrinoResource):\n    \"\"\"Fetch cities to monitor from config table.\"\"\"\n    query = \"SELECT city_name, country FROM config.weather_cities WHERE active = true\"\n    return trino.execute(query)\n\n@dg.asset\ndef weather_data(weather_cities_config):\n    \"\"\"Fetch weather for dynamic list of cities.\"\"\"\n    cities = weather_cities_config\n    # Fetch for each city...\n</code></pre>"},{"location":"guides/workflow-development/#pattern-4-error-handling","title":"Pattern 4: Error Handling","text":"<p>Add retry logic and error notifications:</p> <pre><code>@dg.asset(\n    retry_policy=dg.RetryPolicy(\n        max_retries=3,\n        delay=30,  # seconds\n    ),\n    op_tags={\"notify_on_failure\": \"true\"},\n)\ndef weather_data_with_retry(...):\n    \"\"\"Fetch with automatic retries.\"\"\"\n    ...\n</code></pre>"},{"location":"guides/workflow-development/#pattern-5-caching","title":"Pattern 5: Caching","text":"<p>Cache expensive operations:</p> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_api_client():\n    \"\"\"Cached API client.\"\"\"\n    return OpenWeatherClient(api_key=config.openweather_api_key)\n</code></pre>"},{"location":"guides/workflow-development/#next-steps","title":"Next Steps","text":"<p>\ud83c\udf89 Congratulations! You've built a complete data pipeline from scratch.</p> <p>What you've learned:</p> <ul> <li>\u2705 Define schemas</li> <li>\u2705 Create ingestion assets</li> <li>\u2705 Build Bronze/Silver/Gold layers with dbt</li> <li>\u2705 Add data quality checks</li> <li>\u2705 Publish to PostgreSQL</li> <li>\u2705 Schedule automated runs</li> <li>\u2705 Create BI dashboards</li> </ul> <p>Continue learning:</p> <ul> <li>Data Modeling Guide - Best practices for schema design</li> <li>Dagster Assets Tutorial - Advanced orchestration patterns</li> <li>dbt Development Guide - Advanced SQL techniques</li> <li>Troubleshooting Guide - Debug common issues</li> </ul> <p>Try these challenges:</p> <ol> <li>Add more cities to monitor</li> <li>Create hourly aggregations</li> <li>Add weather alerts (e.g., temperature &gt; 35\u00b0C)</li> <li>Integrate with another API</li> <li>Create predictive features for ML</li> </ol> <p>Happy data engineering! \ud83c\udf24\ufe0f</p>"},{"location":"operations/best-practices/","title":"Best Practices Guide","text":""},{"location":"operations/best-practices/#building-production-ready-data-pipelines","title":"Building Production-Ready Data Pipelines","text":"<p>This guide contains battle-tested best practices for building reliable, maintainable, and scalable data pipelines in Phlo.</p>"},{"location":"operations/best-practices/#table-of-contents","title":"Table of Contents","text":"<ol> <li>General Principles</li> <li>Code Organization</li> <li>Data Quality</li> <li>Performance</li> <li>Security</li> <li>Monitoring and Observability</li> <li>Testing</li> <li>Documentation</li> <li>Git Workflow</li> <li>Production Deployment</li> </ol>"},{"location":"operations/best-practices/#general-principles","title":"General Principles","text":""},{"location":"operations/best-practices/#1-idempotency","title":"1. Idempotency","text":"<p>Make operations repeatable without side effects.</p> <pre><code># BAD: Not idempotent\n@dg.asset\ndef append_data():\n    data = fetch_data()\n    append_to_table(data)  # Running twice appends twice!\n\n# GOOD: Idempotent\n@dg.asset\ndef upsert_data():\n    data = fetch_data()\n    upsert_to_table(data, unique_key='id')  # Running twice has same result\n</code></pre> <p>Benefits:</p> <ul> <li>Safe to retry on failure</li> <li>Can backfill without duplicates</li> <li>Predictable behavior</li> </ul>"},{"location":"operations/best-practices/#2-immutability","title":"2. Immutability","text":"<p>Don't modify existing data, create new versions.</p> <pre><code>-- BAD: Updates in place\nUPDATE orders SET status = 'shipped' WHERE order_id = 123\n\n-- GOOD: Append new record with updated status\nINSERT INTO orders_history\nSELECT *, CURRENT_TIMESTAMP as updated_at\nFROM orders WHERE order_id = 123\n</code></pre> <p>Benefits:</p> <ul> <li>Time travel (see historical state)</li> <li>Audit trail (who changed what when)</li> <li>Easier to debug issues</li> </ul>"},{"location":"operations/best-practices/#3-fail-fast","title":"3. Fail Fast","text":"<p>Detect problems early, fail clearly.</p> <pre><code># GOOD: Validate inputs immediately\n@dg.asset\ndef process_data():\n    data = fetch_data()\n\n    # Fail fast if data is bad\n    assert len(data) &gt; 0, \"No data fetched\"\n    assert 'id' in data.columns, \"Missing required column: id\"\n    assert data['id'].is_unique, \"Duplicate IDs found\"\n\n    # Now process with confidence\n    return transform(data)\n</code></pre>"},{"location":"operations/best-practices/#4-single-responsibility","title":"4. Single Responsibility","text":"<p>Each asset/model should do one thing well.</p> <pre><code># BAD: Does too much\n@dg.asset\ndef fetch_transform_and_load_everything():\n    # Fetches from 5 APIs\n    # Transforms data\n    # Loads to 3 destinations\n    # Too complex!\n\n# GOOD: Separate responsibilities\n@dg.asset\ndef fetch_orders():\n    return fetch_from_api('orders')\n\n@dg.asset\ndef transform_orders(fetch_orders):\n    return transform(fetch_orders)\n\n@dg.asset\ndef load_orders(transform_orders):\n    load_to_warehouse(transform_orders)\n</code></pre>"},{"location":"operations/best-practices/#5-explicit-dependencies","title":"5. Explicit Dependencies","text":"<p>Make dependencies clear and explicit.</p> <pre><code># BAD: Hidden dependency\n@dg.asset\ndef downstream():\n    # Implicitly depends on upstream table existing\n    return query(\"SELECT * FROM upstream_table\")\n\n# GOOD: Explicit dependency\n@dg.asset\ndef downstream(upstream):  # Clear dependency\n    return transform(upstream)\n</code></pre>"},{"location":"operations/best-practices/#code-organization","title":"Code Organization","text":""},{"location":"operations/best-practices/#file-structure","title":"File Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 phlo.yaml                    # Project + infra config\n\u251c\u2500\u2500 workflows/                   # Dagster assets discovered by phlo.framework.definitions\n\u2502   \u251c\u2500\u2500 ingestion/               # Ingestion assets\n\u2502   \u251c\u2500\u2500 quality/                 # Quality checks and assets\n\u2502   \u251c\u2500\u2500 schemas/                 # Pandera schemas\n\u2502   \u2514\u2500\u2500 transforms/              # dbt models\n\u2502       \u2514\u2500\u2500 dbt/\n\u2514\u2500\u2500 tests/                       # Project tests\n</code></pre>"},{"location":"operations/best-practices/#naming-conventions","title":"Naming Conventions","text":"<p>Assets:</p> <pre><code>&lt;action&gt;_&lt;subject&gt;_&lt;detail&gt;\n\nExamples:\n- fetch_orders_api\n- transform_orders_silver\n- aggregate_orders_daily\n- publish_orders_postgres\n</code></pre> <p>dbt Models:</p> <pre><code>stg_&lt;source&gt;_&lt;entity&gt;           # Bronze\nfct_&lt;subject&gt;_&lt;grain&gt;           # Silver facts\ndim_&lt;entity&gt;                    # Silver dimensions\nagg_&lt;grain&gt;_&lt;subject&gt;           # Gold aggregations\nmrt_&lt;audience&gt;_&lt;subject&gt;        # Marts\n</code></pre> <p>Python Functions:</p> <pre><code># Use verbs for functions\ndef fetch_data()\ndef transform_records()\ndef calculate_metrics()\n\n# Use nouns for classes\nclass TrinoResource\nclass OrderSchema\nclass DateUtils\n</code></pre>"},{"location":"operations/best-practices/#configuration-management","title":"Configuration Management","text":"<p>Use environment variables:</p> <pre><code># config.py\nfrom pydantic_settings import BaseSettings\n\nclass Config(BaseSettings):\n    # Database\n    POSTGRES_HOST: str = \"localhost\"\n    POSTGRES_PORT: int = 5432\n\n    # API\n    API_KEY: str\n    API_BASE_URL: str = \"https://api.example.com\"\n\n    # Feature flags\n    ENABLE_NOTIFICATIONS: bool = False\n    ENABLE_CACHING: bool = True\n\n    class Config:\n        env_file = (\".phlo/.env\", \".phlo/.env.local\")\n</code></pre> <p>Benefits:</p> <ul> <li>One place to change configuration</li> <li>Easy to switch between environments</li> <li>Secrets not in code</li> </ul>"},{"location":"operations/best-practices/#data-quality","title":"Data Quality","text":""},{"location":"operations/best-practices/#add-schema-validation","title":"Add Schema Validation","text":"<p>Use Pandera:</p> <pre><code>import pandera as pa\n\nclass OrderSchema(pa.DataFrameModel):\n    order_id: Series[str] = pa.Field(unique=True)\n    customer_id: Series[str] = pa.Field()\n    amount: Series[float] = pa.Field(ge=0)  # &gt;= 0\n    order_date: Series[datetime] = pa.Field()\n\n    @pa.check(\"amount\")\n    def check_reasonable_amount(cls, amount: Series[float]) -&gt; Series[bool]:\n        \"\"\"Amounts should be reasonable (&lt; $10,000).\"\"\"\n        return amount &lt; 10000\n\n@dg.asset\ndef validated_orders() -&gt; pandera_schema_to_dagster_type(OrderSchema):\n    data = fetch_orders()\n    return data  # Automatically validated!\n</code></pre>"},{"location":"operations/best-practices/#add-dbt-tests","title":"Add dbt Tests","text":"<p>Test everything important:</p> <pre><code>models:\n  - name: fct_orders\n    tests:\n      - dbt_utils.at_least_one # Table not empty\n\n    columns:\n      - name: order_id\n        tests:\n          - not_null\n          - unique\n\n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n\n      - name: amount\n        tests:\n          - not_null\n          - dbt_utils.accepted_range:\n              min_value: 0\n              max_value: 1000000\n</code></pre>"},{"location":"operations/best-practices/#add-asset-checks","title":"Add Asset Checks","text":"<pre><code>from dagster import asset_check, AssetCheckResult\n\n@asset_check(asset=orders)\ndef check_orders_recent(orders):\n    \"\"\"Ensure we have recent orders.\"\"\"\n    max_date = orders['order_date'].max()\n    age_hours = (datetime.now() - max_date).total_seconds() / 3600\n\n    if age_hours &gt; 24:\n        return AssetCheckResult(\n            passed=False,\n            description=f\"Most recent order is {age_hours:.1f} hours old\"\n        )\n\n    return AssetCheckResult(passed=True)\n</code></pre>"},{"location":"operations/best-practices/#implement-data-contracts","title":"Implement Data Contracts","text":"<p>Define expectations:</p> <pre><code># data_contracts/orders.yml\ntable: raw.orders\nowner: data-team@company.com\ndescription: Customer orders from e-commerce platform\n\nschema:\n  - name: order_id\n    type: string\n    required: true\n    unique: true\n\n  - name: amount\n    type: decimal(10,2)\n    required: true\n    constraints:\n      - min: 0\n      - max: 1000000\n\nfreshness:\n  warn_threshold: 2h\n  error_threshold: 24h\n</code></pre>"},{"location":"operations/best-practices/#performance","title":"Performance","text":""},{"location":"operations/best-practices/#partition-large-datasets","title":"Partition Large Datasets","text":"<pre><code>from dagster import DailyPartitionsDefinition\n\ndaily = DailyPartitionsDefinition(start_date=\"2024-01-01\")\n\n@dg.asset(partitions_def=daily)\ndef daily_orders(context):\n    date = context.partition_key\n    # Process only one day\n    return fetch_orders_for_date(date)\n</code></pre>"},{"location":"operations/best-practices/#use-incremental-dbt-models","title":"Use Incremental dbt Models","text":"<pre><code>{{\n    config(\n        materialized='incremental',\n        unique_key='order_id',\n    )\n}}\n\nSELECT * FROM {{ ref('stg_orders') }}\n\n{% if is_incremental() %}\n    WHERE updated_at &gt; (SELECT MAX(updated_at) FROM {{ this }})\n{% endif %}\n</code></pre>"},{"location":"operations/best-practices/#optimize-queries","title":"Optimize Queries","text":"<pre><code>-- BAD: Full table scan\nSELECT *\nFROM orders\nWHERE YEAR(order_date) = 2024\n\n-- GOOD: Partition pruning\nSELECT *\nFROM orders\nWHERE order_date &gt;= '2024-01-01'\n  AND order_date &lt; '2025-01-01'\n\n-- BETTER: Add LIMIT for exploration\nSELECT *\nFROM orders\nWHERE order_date &gt;= '2024-01-01'\nLIMIT 10000\n</code></pre>"},{"location":"operations/best-practices/#cache-expensive-operations","title":"Cache Expensive Operations","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_api_client():\n    \"\"\"Cached API client (created once).\"\"\"\n    return ExpensiveAPIClient()\n\n@dg.asset\ndef fetch_data():\n    client = get_api_client()  # Reuses cached client\n    return client.fetch()\n</code></pre>"},{"location":"operations/best-practices/#parallelize-when-possible","title":"Parallelize When Possible","text":"<pre><code># BAD: Sequential\n@dg.asset\ndef process_all_cities():\n    results = []\n    for city in cities:\n        results.append(process_city(city))  # One at a time\n    return results\n\n# GOOD: Parallel (with static partitions)\ncity_partition = StaticPartitionsDefinition(cities)\n\n@dg.asset(partitions_def=city_partition)\ndef process_city(context):\n    city = context.partition_key\n    return process_city(city)  # Each city processes in parallel!\n</code></pre>"},{"location":"operations/best-practices/#security","title":"Security","text":""},{"location":"operations/best-practices/#never-commit-secrets","title":"Never Commit Secrets","text":"<p>.gitignore:</p> <pre><code>.phlo/.env.local\n*.key\n*.pem\nsecrets/\ncredentials.json\n</code></pre> <p>Use environment variables:</p> <pre><code># GOOD\nAPI_KEY = os.getenv('API_KEY')\n\n# BAD\nAPI_KEY = 'abc123'  # Don't hardcode!\n</code></pre>"},{"location":"operations/best-practices/#use-strong-authentication","title":"Use Strong Authentication","text":"<pre><code>from phlo.config import get_settings\n\n@dg.asset\ndef fetch_from_api():\n    config = get_settings()\n\n    # Use API key from environment\n    headers = {\n        'Authorization': f'Bearer {config.API_KEY}'\n    }\n\n    response = requests.get(config.API_URL, headers=headers)\n    return response.json()\n</code></pre>"},{"location":"operations/best-practices/#limit-access","title":"Limit Access","text":"<pre><code>-- Create read-only user for analysts\nCREATE USER analyst_user WITH PASSWORD 'secure_password';\nGRANT USAGE ON SCHEMA marts TO analyst_user;\nGRANT SELECT ON ALL TABLES IN SCHEMA marts TO analyst_user;\n\n-- Don't grant access to raw data\nREVOKE ALL ON SCHEMA raw FROM analyst_user;\n</code></pre>"},{"location":"operations/best-practices/#encrypt-sensitive-data","title":"Encrypt Sensitive Data","text":"<pre><code>from cryptography.fernet import Fernet\n\ndef encrypt_sensitive_field(value: str, key: bytes) -&gt; str:\n    \"\"\"Encrypt sensitive data before storing.\"\"\"\n    f = Fernet(key)\n    return f.encrypt(value.encode()).decode()\n\n@dg.asset\ndef process_customer_data():\n    data = fetch_customers()\n\n    # Encrypt email addresses\n    encryption_key = get_settings().ENCRYPTION_KEY\n    data['email_encrypted'] = data['email'].apply(\n        lambda x: encrypt_sensitive_field(x, encryption_key)\n    )\n\n    # Drop plaintext email\n    data = data.drop(columns=['email'])\n\n    return data\n</code></pre>"},{"location":"operations/best-practices/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"operations/best-practices/#add-logging","title":"Add Logging","text":"<pre><code>@dg.asset\ndef process_orders(context: dg.AssetExecutionContext):\n    context.log.info(\"Starting order processing\")\n\n    orders = fetch_orders()\n    context.log.info(f\"Fetched {len(orders)} orders\")\n\n    # Log important metrics\n    total_amount = orders['amount'].sum()\n    context.log.info(f\"Total order value: ${total_amount:,.2f}\")\n\n    # Log warnings\n    late_orders = orders[orders['is_late']]\n    if len(late_orders) &gt; 10:\n        context.log.warning(f\"{len(late_orders)} orders are late!\")\n\n    return orders\n</code></pre>"},{"location":"operations/best-practices/#return-metadata","title":"Return Metadata","text":"<pre><code>@dg.asset\ndef process_orders(context) -&gt; dg.MaterializeResult:\n    orders = fetch_orders()\n\n    return dg.MaterializeResult(\n        metadata={\n            \"num_records\": len(orders),\n            \"date_range\": f\"{orders['date'].min()} to {orders['date'].max()}\",\n            \"total_amount\": dg.MetadataValue.float(orders['amount'].sum()),\n            \"preview\": dg.MetadataValue.md(orders.head().to_markdown()),\n        }\n    )\n</code></pre>"},{"location":"operations/best-practices/#set-up-alerts","title":"Set Up Alerts","text":"<p>Dagster sensors:</p> <pre><code>@dg.sensor(\n    name=\"alert_on_failure\",\n    minimum_interval_seconds=60,\n)\ndef failure_alert_sensor(context):\n    \"\"\"Alert on asset failures.\"\"\"\n    runs = context.instance.get_runs(\n        filters=dg.RunsFilter(\n            statuses=[dg.DagsterRunStatus.FAILURE],\n            created_after=datetime.now() - timedelta(minutes=5),\n        )\n    )\n\n    for run in runs:\n        send_alert(f\"Run {run.run_id} failed: {run.pipeline_name}\")\n</code></pre> <p>dbt tests as monitors:</p> <pre><code>models:\n  - name: fct_orders\n    tests:\n      - dbt_utils.recency:\n          datepart: hour\n          field: created_at\n          interval: 2\n          # Alerts if no orders in last 2 hours\n</code></pre>"},{"location":"operations/best-practices/#use-grafana-dashboards","title":"Use Grafana Dashboards","text":"<p>Monitor key metrics:</p> <ul> <li>Asset materialization rates</li> <li>Test pass/fail rates</li> <li>Query performance</li> <li>Resource usage</li> <li>Data volumes</li> </ul>"},{"location":"operations/best-practices/#testing","title":"Testing","text":""},{"location":"operations/best-practices/#unit-tests","title":"Unit Tests","text":"<pre><code># tests/test_transformations.py\nimport pytest\nfrom phlo.utils.transformations import calculate_tax\n\ndef test_calculate_tax():\n    assert calculate_tax(100, 0.1) == 10\n    assert calculate_tax(0, 0.1) == 0\n    assert calculate_tax(100, 0) == 0\n\ndef test_calculate_tax_negative():\n    with pytest.raises(ValueError):\n        calculate_tax(-100, 0.1)\n</code></pre>"},{"location":"operations/best-practices/#integration-tests","title":"Integration Tests","text":"<pre><code># tests/test_pipeline.py\nfrom dagster import materialize\nfrom phlo.framework.definitions import defs\n\ndef test_orders_pipeline():\n    \"\"\"Test complete orders pipeline.\"\"\"\n\n    # Materialize all orders assets\n    result = materialize(\n        defs.get_asset_graph().get_all_asset_keys(),\n        selection=dg.AssetSelection.groups(\"orders\"),\n    )\n\n    assert result.success\n</code></pre>"},{"location":"operations/best-practices/#dbt-tests","title":"dbt Tests","text":"<pre><code>-- tests/assert_no_negative_amounts.sql\nSELECT *\nFROM {{ ref('fct_orders') }}\nWHERE amount &lt; 0\n</code></pre>"},{"location":"operations/best-practices/#test-in-cicd","title":"Test in CI/CD","text":"<pre><code># .github/workflows/test.yml\nname: Test\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Run tests\n        run: pytest tests/\n\n      - name: Run dbt tests\n        run: dbt test --project-dir workflows/transforms/dbt\n</code></pre>"},{"location":"operations/best-practices/#documentation","title":"Documentation","text":""},{"location":"operations/best-practices/#document-assets","title":"Document Assets","text":"<pre><code>@dg.asset(\n    name=\"customer_lifetime_value\",\n    description=\"\"\"\n    Calculates customer lifetime value (LTV) based on historical orders.\n\n    **Calculation:**\n    - LTV = SUM(order_amount) for all completed orders\n\n    **Business Rules:**\n    - Only includes orders with status = 'completed'\n    - Excludes returns and refunds\n    - Updated daily\n\n    **Depends On:**\n    - fct_orders (silver layer)\n    - dim_customers (silver layer)\n\n    **Used By:**\n    - mrt_customer_segmentation\n    - Executive dashboard (Superset)\n\n    **Owner:** Data Team (data-team@company.com)\n    \"\"\",\n    group_name=\"customer_analytics\",\n    compute_kind=\"python\",\n)\ndef customer_lifetime_value(fct_orders, dim_customers):\n    ...\n</code></pre>"},{"location":"operations/best-practices/#document-dbt-models","title":"Document dbt Models","text":"<pre><code># models/silver/schema.yml\nmodels:\n  - name: fct_orders\n    description: |\n      Orders with calculated metrics and business logic.\n\n      ## Business Rules\n      - Excludes cancelled orders\n      - Applies 10% tax rate\n      - Free shipping for orders &gt; $100\n\n      ## Data Quality\n      - Deduplicated on order_id\n      - Late-arriving data handled via incremental logic\n\n      ## Refresh Schedule\n      - Incremental: Hourly\n      - Full refresh: Weekly (Sunday 2 AM)\n\n      ## Owner\n      Data Engineering Team\n\n    columns:\n      - name: order_id\n        description: Unique order identifier (UUID)\n        tests:\n          - not_null\n          - unique\n</code></pre>"},{"location":"operations/best-practices/#generate-documentation","title":"Generate Documentation","text":"<pre><code># dbt documentation\ndbt docs generate\ndbt docs serve\n\n# Dagster documentation\n# Available in UI automatically!\n</code></pre>"},{"location":"operations/best-practices/#maintain-readme","title":"Maintain README","text":"<pre><code># Phlo Data Platform\n\n## Quick Start\n\n...\n\n## Architecture\n\n...\n\n## Common Workflows\n\n...\n\n## Troubleshooting\n\n...\n\n## Contact\n\nFor help: data-team@company.com\n</code></pre>"},{"location":"operations/best-practices/#git-workflow","title":"Git Workflow","text":""},{"location":"operations/best-practices/#branching-strategy","title":"Branching Strategy","text":"<pre><code>main (production)\n  \u251c\u2500 dev (staging)\n  \u2502   \u251c\u2500 feature/add-customer-segmentation\n  \u2502   \u251c\u2500 fix/orders-duplicate-bug\n  \u2502   \u2514\u2500 refactor/optimize-queries\n</code></pre>"},{"location":"operations/best-practices/#commit-messages","title":"Commit Messages","text":"<pre><code># Good commit messages\ngit commit -m \"feat: add customer lifetime value calculation\"\ngit commit -m \"fix: resolve duplicate orders in fct_orders\"\ngit commit -m \"docs: update data modeling guide\"\ngit commit -m \"perf: optimize daily aggregation query\"\ngit commit -m \"test: add validation for negative amounts\"\n\n# Bad commit messages\ngit commit -m \"updates\"\ngit commit -m \"fix bug\"\ngit commit -m \"wip\"\n</code></pre>"},{"location":"operations/best-practices/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create feature branch</li> </ol> <p><code>bash    git checkout -b feature/my-feature</code></p> <ol> <li>Make changes and commit</li> </ol> <p><code>bash    git add .    git commit -m \"feat: add new feature\"</code></p> <ol> <li>Push and create PR</li> </ol> <p><code>bash    git push -u origin feature/my-feature</code></p> <ol> <li>PR template:</li> </ol> <p>```markdown    ## What</p> <p>Brief description of changes</p> <p>## Why</p> <p>Why are these changes needed?</p> <p>## Testing</p> <ul> <li>[ ] Unit tests pass</li> <li>[ ] dbt tests pass</li> <li>[ ] Manually tested in dev</li> </ul> <p>## Screenshots</p> <p>(if applicable)    ```</p> <ol> <li> <p>Code review</p> </li> <li> <p>Merge to dev, test, then merge to main</p> </li> </ol>"},{"location":"operations/best-practices/#production-deployment","title":"Production Deployment","text":""},{"location":"operations/best-practices/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":"<ul> <li>[ ] All tests passing</li> <li>[ ] Code reviewed</li> <li>[ ] Documentation updated</li> <li>[ ] Secrets configured</li> <li>[ ] Monitoring set up</li> <li>[ ] Rollback plan documented</li> </ul>"},{"location":"operations/best-practices/#deployment-strategy","title":"Deployment Strategy","text":"<p>1. Test in dev environment</p> <pre><code># Deploy to dev\ngit checkout dev\ngit pull origin main\ngit push origin dev\n\n# Monitor in Dagster dev environment\n</code></pre> <p>2. Smoke test</p> <pre><code># Materialize critical assets\ndagster asset materialize -a critical_asset_1\ndagster asset materialize -a critical_asset_2\n\n# Run dbt tests\ndbt test --select tag:critical\n</code></pre> <p>3. Deploy to production</p> <pre><code># Merge to main\ngit checkout main\ngit merge dev\ngit push origin main\n\n# Deployment happens automatically (CI/CD)\n</code></pre> <p>4. Monitor</p> <ul> <li>Check Grafana dashboards</li> <li>Watch Dagster runs</li> <li>Check alert channels</li> </ul>"},{"location":"operations/best-practices/#rollback-plan","title":"Rollback Plan","text":"<pre><code># If something goes wrong, revert\ngit revert &lt;commit-hash&gt;\ngit push origin main\n\n# Or roll back to previous version\ngit reset --hard &lt;previous-commit&gt;\ngit push --force origin main\n\n# Restore data from Nessie/Iceberg\n# Time travel to before deployment\nSELECT * FROM iceberg.silver.fct_orders\nFOR SYSTEM_TIME AS OF TIMESTAMP '2024-11-05 10:00:00';\n</code></pre>"},{"location":"operations/best-practices/#summary","title":"Summary","text":"<p>Key Principles:</p> <ul> <li>\u2705 Idempotency</li> <li>\u2705 Immutability</li> <li>\u2705 Fail fast</li> <li>\u2705 Single responsibility</li> <li>\u2705 Explicit dependencies</li> </ul> <p>Code Quality:</p> <ul> <li>\u2705 Clear naming</li> <li>\u2705 Modular structure</li> <li>\u2705 Configuration management</li> <li>\u2705 Error handling</li> </ul> <p>Data Quality:</p> <ul> <li>\u2705 Schema validation</li> <li>\u2705 dbt tests</li> <li>\u2705 Asset checks</li> <li>\u2705 Data contracts</li> </ul> <p>Operations:</p> <ul> <li>\u2705 Monitoring</li> <li>\u2705 Logging</li> <li>\u2705 Alerting</li> <li>\u2705 Testing</li> <li>\u2705 Documentation</li> </ul> <p>Security:</p> <ul> <li>\u2705 No secrets in code</li> <li>\u2705 Strong authentication</li> <li>\u2705 Least privilege access</li> <li>\u2705 Encrypt sensitive data</li> </ul> <p>Deployment:</p> <ul> <li>\u2705 Version control</li> <li>\u2705 Code review</li> <li>\u2705 Testing in dev</li> <li>\u2705 Gradual rollout</li> <li>\u2705 Monitoring</li> <li>\u2705 Rollback plan</li> </ul> <p>Remember: Good practices prevent problems before they happen. Invest time upfront to save time later!</p>"},{"location":"operations/operations-guide/","title":"Operations Guide","text":"<p>Production operations guide for running and maintaining Phlo.</p>"},{"location":"operations/operations-guide/#daily-operations","title":"Daily Operations","text":""},{"location":"operations/operations-guide/#monitoring-services","title":"Monitoring Services","text":"<p>Check all services are running:</p> <pre><code>phlo services status\n</code></pre> <p>Expected output:</p> <pre><code>SERVICE              STATUS    PORTS\npostgres             running   10000\nminio                running   10001-10002\nnessie               running   10003\ntrino                running   10005\ndagster-webserver    running   10006\ndagster-daemon       running\n</code></pre>"},{"location":"operations/operations-guide/#viewing-logs","title":"Viewing Logs","text":"<p>Monitor service logs:</p> <pre><code># All services\nphlo services logs -f\n\n# Specific service\nphlo services logs -f dagster-webserver\n\n# Last 100 lines\nphlo services logs --tail 100 dagster-daemon\n</code></pre>"},{"location":"operations/operations-guide/#asset-status","title":"Asset Status","text":"<p>Check asset health and freshness:</p> <pre><code># All assets\nphlo status\n\n# Only stale assets\nphlo status --stale\n\n# Specific group\nphlo status --group nightscout\n</code></pre>"},{"location":"operations/operations-guide/#manual-materialization","title":"Manual Materialization","text":"<p>Trigger asset runs manually:</p> <pre><code># Single asset\nphlo materialize dlt_glucose_entries\n\n# With downstream\nphlo materialize dlt_glucose_entries+\n\n# Specific partition\nphlo materialize dlt_glucose_entries --partition 2025-01-15\n\n# By tag\nphlo materialize --select \"tag:nightscout\"\n</code></pre>"},{"location":"operations/operations-guide/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"operations/operations-guide/#database-backups","title":"Database Backups","text":"<p>PostgreSQL:</p> <pre><code># Backup\ndocker exec phlo-postgres-1 pg_dump -U postgres cascade &gt; backup.sql\n\n# Backup with compression\ndocker exec phlo-postgres-1 pg_dump -U postgres cascade | gzip &gt; backup.sql.gz\n\n# Restore\ncat backup.sql | docker exec -i phlo-postgres-1 psql -U postgres cascade\n\n# Restore from compressed\ngunzip -c backup.sql.gz | docker exec -i phlo-postgres-1 psql -U postgres cascade\n</code></pre> <p>Automated backups:</p> <pre><code># Add to crontab\n0 2 * * * /path/to/backup-postgres.sh\n</code></pre> <pre><code>#!/bin/bash\n# backup-postgres.sh\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=\"/backups/postgres\"\nmkdir -p $BACKUP_DIR\ndocker exec phlo-postgres-1 pg_dump -U postgres cascade | \\\n  gzip &gt; $BACKUP_DIR/cascade_$DATE.sql.gz\n\n# Keep only last 30 days\nfind $BACKUP_DIR -name \"*.sql.gz\" -mtime +30 -delete\n</code></pre>"},{"location":"operations/operations-guide/#object-storage-backups","title":"Object Storage Backups","text":"<p>MinIO/S3:</p> <pre><code># Install mc (MinIO client)\nbrew install minio/stable/mc\n\n# Configure\nmc alias set local http://localhost:10001 minioadmin minioadmin\n\n# Backup bucket\nmc mirror local/lake /backups/minio/lake\n\n# Restore\nmc mirror /backups/minio/lake local/lake\n</code></pre> <p>Automated S3 sync:</p> <pre><code># Using AWS CLI or rclone\nrclone sync /backups/minio/lake s3://backup-bucket/lake\n</code></pre>"},{"location":"operations/operations-guide/#nessie-catalog-backups","title":"Nessie Catalog Backups","text":"<p>Nessie state is stored in PostgreSQL, so backing up Postgres includes catalog metadata.</p> <p>Export specific branches:</p> <pre><code># List branches\nphlo branch list &gt; branches_backup.txt\n\n# Export branch commits\ncurl http://localhost:10003/api/v1/trees/main &gt; main_branch.json\n</code></pre>"},{"location":"operations/operations-guide/#branch-management","title":"Branch Management","text":""},{"location":"operations/operations-guide/#creating-branches","title":"Creating Branches","text":"<pre><code># Development branch\nphlo branch create dev\n\n# Feature branch from specific ref\nphlo branch create feature-xyz --from main\n\n# With description\nphlo branch create experiment --description \"Testing new ingestion\"\n</code></pre>"},{"location":"operations/operations-guide/#merging-branches","title":"Merging Branches","text":"<pre><code># Merge dev to main\nphlo branch merge dev main\n\n# Force merge commit\nphlo branch merge dev main --no-ff\n</code></pre>"},{"location":"operations/operations-guide/#cleanup-old-branches","title":"Cleanup Old Branches","text":"<pre><code># List all branches\nphlo branch list\n\n# Delete specific branch\nphlo branch delete old-feature\n\n# Automated cleanup (configure in .phlo/.env.local)\nBRANCH_CLEANUP_ENABLED=true\nBRANCH_RETENTION_DAYS=7\nBRANCH_RETENTION_DAYS_FAILED=2\n</code></pre> <p>Manual cleanup script:</p> <pre><code>from phlo_nessie.resource import BranchManagerResource\nfrom datetime import datetime, timedelta\n\nbranch_manager = BranchManagerResource()\n\n# Get all pipeline branches\nbranches = branch_manager.get_all_pipeline_branches()\n\nretention_days = 7\ncutoff_date = datetime.now() - timedelta(days=retention_days)\n\nfor branch in branches:\n    if branch.created_at &lt; cutoff_date:\n        print(f\"Deleting old branch: {branch.name}\")\n        branch_manager.cleanup_branch(branch.name)\n</code></pre>"},{"location":"operations/operations-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"operations/operations-guide/#trino-query-optimization","title":"Trino Query Optimization","text":"<p>Enable query profiling:</p> <pre><code>-- In Trino CLI\nEXPLAIN ANALYZE SELECT * FROM bronze.events WHERE date = '2025-01-15';\n</code></pre> <p>Partition pruning:</p> <pre><code>-- Good: uses partition pruning\nSELECT * FROM bronze.events WHERE partition_date = '2025-01-15';\n\n-- Bad: full table scan\nSELECT * FROM bronze.events WHERE timestamp &gt; '2025-01-15';\n</code></pre> <p>Table statistics:</p> <pre><code>-- Analyze table\nANALYZE iceberg_dev.bronze.events;\n\n-- Show stats\nSHOW STATS FOR bronze.events;\n</code></pre>"},{"location":"operations/operations-guide/#iceberg-maintenance","title":"Iceberg Maintenance","text":"<p>Optimize files:</p> <pre><code>from phlo_iceberg import get_iceberg_table\n\ntable = get_iceberg_table(\"bronze.events\")\n\n# Compact small files\ntable.optimize.compact()\n\n# Expire old snapshots\ntable.expire_snapshots(older_than=30)  # days\n</code></pre> <p>Automated maintenance:</p> <pre><code># workflows/maintenance/iceberg.py\nfrom dagster import asset, schedule\n\n@asset\ndef optimize_iceberg_tables():\n    tables = [\"bronze.events\", \"silver.events_cleaned\"]\n    for table_name in tables:\n        table = get_iceberg_table(table_name)\n        table.optimize.compact()\n        table.expire_snapshots(older_than=30)\n\n@schedule(cron_schedule=\"0 2 * * 0\", job_name=\"weekly_maintenance\")\ndef weekly_iceberg_maintenance():\n    return RunRequest()\n</code></pre>"},{"location":"operations/operations-guide/#dagster-performance","title":"Dagster Performance","text":"<p>Use multiprocess executor for production:</p> <pre><code># .phlo/.env.local\nDAGSTER_EXECUTOR=multiprocess\n</code></pre> <p>Configure resource limits:</p> <pre><code># dagster.yaml\nexecution:\n  multiprocess:\n    max_concurrent: 4\n    retries:\n      enabled: true\n      max_retries: 3\n</code></pre>"},{"location":"operations/operations-guide/#scaling","title":"Scaling","text":""},{"location":"operations/operations-guide/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Trino workers:</p> <p>Add workers in <code>docker-compose.yml</code>:</p> <pre><code>services:\n  trino-worker-1:\n    image: trinodb/trino:461\n    environment:\n      - TRINO_DISCOVERY_URI=http://trino:10005\n    depends_on:\n      - trino\n\n  trino-worker-2:\n    image: trinodb/trino:461\n    environment:\n      - TRINO_DISCOVERY_URI=http://trino:10005\n    depends_on:\n      - trino\n</code></pre> <p>Dagster daemon replicas:</p> <pre><code>services:\n  dagster-daemon-1:\n    # ... configuration\n\n  dagster-daemon-2:\n    # ... configuration\n</code></pre>"},{"location":"operations/operations-guide/#vertical-scaling","title":"Vertical Scaling","text":"<p>Resource limits in <code>docker-compose.yml</code>:</p> <pre><code>services:\n  trino:\n    deploy:\n      resources:\n        limits:\n          cpus: \"4\"\n          memory: 8G\n        reservations:\n          cpus: \"2\"\n          memory: 4G\n\n  postgres:\n    deploy:\n      resources:\n        limits:\n          cpus: \"2\"\n          memory: 4G\n</code></pre>"},{"location":"operations/operations-guide/#storage-scaling","title":"Storage Scaling","text":"<p>MinIO distributed mode:</p> <pre><code>services:\n  minio-1:\n    image: minio/minio\n    command: server http://minio-{1...4}/data{1...2}\n\n  minio-2:\n    image: minio/minio\n    command: server http://minio-{1...4}/data{1...2}\n\n  minio-3:\n    image: minio/minio\n    command: server http://minio-{1...4}/data{1...2}\n\n  minio-4:\n    image: minio/minio\n    command: server http://minio-{1...4}/data{1...2}\n</code></pre>"},{"location":"operations/operations-guide/#security","title":"Security","text":""},{"location":"operations/operations-guide/#access-control","title":"Access Control","text":"<p>PostgreSQL roles:</p> <pre><code>-- Read-only role for BI\nCREATE ROLE bi_readonly WITH LOGIN PASSWORD 'secure-password';\nGRANT CONNECT ON DATABASE cascade TO bi_readonly;\nGRANT USAGE ON SCHEMA marts TO bi_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA marts TO bi_readonly;\nALTER DEFAULT PRIVILEGES IN SCHEMA marts GRANT SELECT ON TABLES TO bi_readonly;\n\n-- Application role with limited write\nCREATE ROLE app_writer WITH LOGIN PASSWORD 'secure-password';\nGRANT CONNECT ON DATABASE cascade TO app_writer;\nGRANT USAGE ON SCHEMA bronze TO app_writer;\nGRANT INSERT, UPDATE ON ALL TABLES IN SCHEMA bronze TO app_writer;\n</code></pre> <p>MinIO policies:</p> <pre><code># Create read-only policy\nmc admin policy create local readonly-policy policy.json\n\n# policy.json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": [\"arn:aws:s3:::lake/*\"]\n    }\n  ]\n}\n\n# Create user and assign policy\nmc admin user add local readonly secure-password\nmc admin policy attach local readonly-policy --user readonly\n</code></pre>"},{"location":"operations/operations-guide/#network-security","title":"Network Security","text":"<p>Docker network isolation:</p> <pre><code># docker-compose.yml\nnetworks:\n  backend:\n    driver: bridge\n  frontend:\n    driver: bridge\n\nservices:\n  postgres:\n    networks:\n      - backend\n\n  dagster-webserver:\n    networks:\n      - backend\n      - frontend\n    ports:\n      - \"10006:10006\" # Only expose webserver\n</code></pre> <p>Firewall rules:</p> <pre><code># Allow only specific IPs to access services\niptables -A INPUT -p tcp --dport 10006 -s 10.0.0.0/8 -j ACCEPT\niptables -A INPUT -p tcp --dport 10006 -j DROP\n</code></pre>"},{"location":"operations/operations-guide/#secrets-management","title":"Secrets Management","text":"<p>Use secret managers:</p> <pre><code># AWS Secrets Manager\nexport POSTGRES_PASSWORD=$(aws secretsmanager get-secret-value \\\n  --secret-id phlo/postgres/password \\\n  --query SecretString --output text)\n\n# HashiCorp Vault\nexport POSTGRES_PASSWORD=$(vault kv get -field=password secret/phlo/postgres)\n</code></pre> <p>Docker secrets:</p> <pre><code># docker-compose.yml\nsecrets:\n  postgres_password:\n    external: true\n\nservices:\n  postgres:\n    secrets:\n      - postgres_password\n    environment:\n      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password\n</code></pre>"},{"location":"operations/operations-guide/#monitoring","title":"Monitoring","text":""},{"location":"operations/operations-guide/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Enable Prometheus in Dagster:</p> <pre><code># dagster.yaml\ntelemetry:\n  enabled: true\n  prometheus:\n    enabled: true\n    port: 9090\n</code></pre> <p>Key metrics to monitor:</p> <pre><code># Asset materialization success rate\nrate(dagster_asset_materializations_total[5m])\n\n# Asset materialization duration\nhistogram_quantile(0.95, rate(dagster_asset_materialization_duration_seconds_bucket[5m]))\n\n# Failed materializations\nrate(dagster_asset_materializations_failed_total[5m])\n\n# Trino query duration\ntrino_query_execution_time_seconds\n\n# MinIO storage usage\nminio_disk_storage_used_bytes\n</code></pre>"},{"location":"operations/operations-guide/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Import dashboards:</p> <ol> <li>Start with observability profile:</li> </ol> <pre><code>phlo services start --profile observability\n</code></pre> <ol> <li> <p>Access Grafana: http://localhost:3000</p> </li> <li> <p>Import pre-built dashboards:</p> </li> <li>Dagster metrics</li> <li>Trino performance</li> <li>MinIO storage</li> <li>PostgreSQL queries</li> </ol>"},{"location":"operations/operations-guide/#alerting","title":"Alerting","text":"<p>Configure Prometheus alerts:</p> <pre><code># prometheus/alerts.yml\ngroups:\n  - name: phlo_alerts\n    rules:\n      - alert: AssetMaterializationFailed\n        expr: rate(dagster_asset_materializations_failed_total[5m]) &gt; 0\n        for: 5m\n        annotations:\n          summary: \"Asset materialization failures detected\"\n\n      - alert: HighQueryLatency\n        expr: histogram_quantile(0.95, rate(trino_query_execution_time_seconds_bucket[5m])) &gt; 30\n        for: 10m\n        annotations:\n          summary: \"High Trino query latency\"\n\n      - alert: LowStorageSpace\n        expr: (minio_disk_storage_free_bytes / minio_disk_storage_total_bytes) &lt; 0.1\n        for: 5m\n        annotations:\n          summary: \"Low MinIO storage space\"\n</code></pre> <p>Slack integration:</p> <pre><code># .phlo/.env.local\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\nSLACK_CHANNEL=#data-alerts\n</code></pre>"},{"location":"operations/operations-guide/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"operations/operations-guide/#recovery-plan","title":"Recovery Plan","text":"<p>RTO/RPO targets:</p> <ul> <li>RTO (Recovery Time Objective): 4 hours</li> <li>RPO (Recovery Point Objective): 24 hours</li> </ul> <p>Recovery steps:</p> <ol> <li>Restore PostgreSQL:</li> </ol> <pre><code># Stop services\nphlo services stop\n\n# Restore database\ngunzip -c backup.sql.gz | docker exec -i phlo-postgres-1 psql -U postgres cascade\n\n# Start services\nphlo services start\n</code></pre> <ol> <li>Restore MinIO:</li> </ol> <pre><code># Sync from backup\nmc mirror /backups/minio/lake local/lake\n</code></pre> <ol> <li>Verify Nessie catalog:</li> </ol> <pre><code># Check branches\nphlo branch list\n\n# Verify table metadata\ncurl http://localhost:10003/api/v1/trees/main\n</code></pre> <ol> <li>Re-materialize recent partitions:</li> </ol> <pre><code># Last 7 days\nfor i in {0..6}; do\n  date=$(date -d \"$i days ago\" +%Y-%m-%d)\n  phlo materialize --partition $date\ndone\n</code></pre>"},{"location":"operations/operations-guide/#testing-recovery","title":"Testing Recovery","text":"<p>Regular DR drills:</p> <pre><code>#!/bin/bash\n# dr-test.sh\n\n# 1. Backup current state\n./backup-all.sh\n\n# 2. Destroy services\nphlo services stop --volumes\n\n# 3. Restore from backup\n./restore-all.sh\n\n# 4. Verify services\nphlo services status\n\n# 5. Test asset materialization\nphlo materialize --select \"tag:critical\" --partition $(date -d \"yesterday\" +%Y-%m-%d)\n\n# 6. Validate data\n./validate-data.sh\n</code></pre>"},{"location":"operations/operations-guide/#release-management","title":"Release Management","text":"<p>Phlo uses Release Please to manage versions and changelogs on <code>main</code>.</p>"},{"location":"operations/operations-guide/#release-please-configuration","title":"Release Please Configuration","text":"<ul> <li>Workflow: <code>.github/workflows/release-please.yml</code></li> <li>Config: <code>.github/release-please-config.json</code></li> <li>Manifest: <code>.github/release-please-manifest.json</code></li> </ul> <p>The manifest tracks the current release version used to generate tags and changelogs.</p>"},{"location":"operations/operations-guide/#local-validation-dry-run","title":"Local Validation (Dry Run)","text":"<p>Use the Release Please CLI with a GitHub token to preview the next release.</p> <pre><code>export RELEASE_PLEASE_TOKEN=\"ghp_your_token\"\nnpx release-please manifest-pr \\\n  --config-file .github/release-please-config.json \\\n  --manifest-file .github/release-please-manifest.json \\\n  --dry-run\n</code></pre> <p>If the output shows the next version incrementing as expected and the compare link targets the previous tag, the configuration is working as expected.</p>"},{"location":"operations/operations-guide/#maintenance-windows","title":"Maintenance Windows","text":""},{"location":"operations/operations-guide/#planned-downtime","title":"Planned Downtime","text":"<p>Communication:</p> <pre><code># Announce maintenance\ncurl -X POST $SLACK_WEBHOOK_URL \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"channel\": \"#data-alerts\",\n    \"text\": \"Scheduled maintenance: Phlo will be down 2025-01-15 02:00-04:00 UTC\"\n  }'\n</code></pre> <p>Maintenance tasks:</p> <pre><code>#!/bin/bash\n# maintenance.sh\n\n# 1. Stop Dagster daemon (prevent new runs)\ndocker stop phlo-dagster-daemon-1\n\n# 2. Wait for running jobs to complete\nwhile [ $(dagster job list --running) -gt 0 ]; do\n  sleep 60\ndone\n\n# 3. Backup databases\n./backup-postgres.sh\n./backup-minio.sh\n\n# 4. Perform maintenance\ndocker exec phlo-postgres-1 vacuumdb -U postgres -z cascade\n\n# 5. Optimize Iceberg tables\npython -m phlo.maintenance.optimize_tables\n\n# 6. Restart services\nphlo services stop\nphlo services start\n\n# 7. Verify health\n./health-check.sh\n\n# 8. Announce completion\ncurl -X POST $SLACK_WEBHOOK_URL \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"channel\": \"#data-alerts\",\n    \"text\": \"Maintenance complete. Phlo is back online.\"\n  }'\n</code></pre>"},{"location":"operations/operations-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting Guide - Common issues and solutions</li> <li>Configuration Reference - Detailed configuration</li> <li>Best Practices - Production patterns</li> </ul>"},{"location":"operations/testing/","title":"Testing Guide","text":"<p>A comprehensive guide to testing Phlo workflows.</p>"},{"location":"operations/testing/#overview","title":"Overview","text":"<p>This guide covers:</p> <ul> <li>Unit Testing: Test schemas and business logic (fast, no Docker)</li> <li>Integration Testing: Test full pipelines (slower, requires Docker)</li> <li>Best Practices: Patterns and recommendations</li> </ul> <p>Time to write first test: 5-10 minutes</p>"},{"location":"operations/testing/#quick-start","title":"Quick Start","text":""},{"location":"operations/testing/#1-create-a-test-file","title":"1. Create a Test File","text":"<pre><code># Option A: scaffold a workflow (creates schema + tests)\nphlo create-workflow --type ingestion --domain my_domain --table my_table --unique-key id\n\n# Option B: create a test file if you already have a schema\ntouch tests/test_my_workflow.py\n</code></pre>"},{"location":"operations/testing/#2-run-your-tests","title":"2. Run Your Tests","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_my_workflow.py -v\n\n# Run specific test\npytest tests/test_my_workflow.py::TestSchemaValidation::test_valid_data_passes_validation -v\n</code></pre>"},{"location":"operations/testing/#unit-testing","title":"Unit Testing","text":"<p>Unit tests are fast (&lt; 1 second), require no Docker, and test individual components.</p>"},{"location":"operations/testing/#testing-pandera-schemas","title":"Testing Pandera Schemas","text":"<p>Test that your schema validates data correctly:</p> <pre><code>import pytest\nimport pandas as pd\nfrom workflows.schemas.weather import RawWeatherData\n\ndef test_valid_weather_data_passes_validation():\n    \"\"\"Test that valid weather data passes schema validation.\"\"\"\n\n    test_data = pd.DataFrame([\n        {\n            \"city_name\": \"London\",\n            \"temperature\": 15.5,\n            \"humidity\": 65,\n            \"timestamp\": \"2024-01-15T12:00:00Z\",\n        },\n    ])\n\n    # Should not raise\n    validated = RawWeatherData.validate(test_data)\n    assert len(validated) == 1\n\n\ndef test_invalid_temperature_fails_validation():\n    \"\"\"Test that invalid temperature fails validation.\"\"\"\n\n    test_data = pd.DataFrame([\n        {\n            \"city_name\": \"London\",\n            \"temperature\": -200.0,  # Invalid: should be &gt;= -100\n            \"humidity\": 65,\n            \"timestamp\": \"2024-01-15T12:00:00Z\",\n        },\n    ])\n\n    # Should raise SchemaError\n    with pytest.raises(Exception):\n        RawWeatherData.validate(test_data)\n</code></pre>"},{"location":"operations/testing/#testing-schema-configuration","title":"Testing Schema Configuration","text":"<p>Verify that your schema includes required fields:</p> <pre><code>def test_schema_has_unique_key():\n    \"\"\"Test that unique_key field exists in schema.\"\"\"\n\n    schema_fields = RawWeatherData.to_schema().columns.keys()\n\n    # Ensure unique_key from decorator exists in schema\n    assert \"timestamp\" in schema_fields\n\n\ndef test_schema_has_required_fields():\n    \"\"\"Test that schema includes all required fields.\"\"\"\n\n    schema_fields = RawWeatherData.to_schema().columns.keys()\n\n    required_fields = [\"city_name\", \"temperature\", \"humidity\", \"timestamp\"]\n\n    for field in required_fields:\n        assert field in schema_fields, f\"Missing field: {field}\"\n</code></pre>"},{"location":"operations/testing/#testing-asset-configuration","title":"Testing Asset Configuration","text":"<p>Verify decorator parameters:</p> <pre><code>import inspect\nfrom workflows.ingestion.weather.observations import weather_observations\n\ndef test_asset_has_correct_table_name():\n    \"\"\"Test that asset has correct table name.\"\"\"\n\n    # Asset op name should be prefixed with 'dlt_'\n    assert weather_observations.op.name == \"dlt_weather_observations\"\n\n\ndef test_asset_accepts_partition_parameter():\n    \"\"\"Test that asset function accepts partition_date.\"\"\"\n\n    sig = inspect.signature(weather_observations.op.compute_fn)\n    params = sig.parameters\n\n    assert \"partition_date\" in params\n</code></pre>"},{"location":"operations/testing/#testing-business-logic","title":"Testing Business Logic","text":"<p>Test any data transformations in your asset:</p> <pre><code>def test_date_formatting():\n    \"\"\"Test that partition_date is formatted correctly.\"\"\"\n\n    partition_date = \"2024-01-15\"\n\n    # Your formatting logic\n    start_time = f\"{partition_date}T00:00:00.000Z\"\n    end_time = f\"{partition_date}T23:59:59.999Z\"\n\n    assert start_time == \"2024-01-15T00:00:00.000Z\"\n    assert end_time == \"2024-01-15T23:59:59.999Z\"\n\n\ndef test_data_filtering():\n    \"\"\"Test custom data filtering logic.\"\"\"\n\n    sample_data = pd.DataFrame([\n        {\"value\": 10, \"status\": \"active\"},\n        {\"value\": 20, \"status\": \"inactive\"},\n        {\"value\": 30, \"status\": \"active\"},\n    ])\n\n    # Your filtering logic\n    filtered = sample_data[sample_data[\"status\"] == \"active\"]\n\n    assert len(filtered) == 2\n    assert filtered[\"value\"].tolist() == [10, 30]\n</code></pre>"},{"location":"operations/testing/#advanced-testing-utilities-new","title":"Advanced Testing Utilities \u2728 NEW","text":"<p>Phlo provides powerful testing utilities for fast, local testing without Docker.</p>"},{"location":"operations/testing/#mockicebergcatalog","title":"MockIcebergCatalog","text":"<p>Test Iceberg table operations in-memory using DuckDB backend.</p> <p>Features:</p> <ul> <li>\u26a1 &lt; 5 second execution (vs 30-60 sec with Docker)</li> <li>\ud83e\uddea In-memory DuckDB backend (no persistence)</li> <li>\u2705 PyIceberg-compatible API</li> <li>\ud83d\udd04 Append, scan, filter operations</li> <li>\ud83d\udcca Convert to Pandas or Arrow</li> </ul> <p>Example:</p> <pre><code>from phlo_testing import mock_iceberg_catalog\nfrom phlo_dlt.converter import pandera_to_iceberg\nfrom workflows.schemas.weather import RawWeatherData\nimport pandas as pd\n\ndef test_iceberg_operations():\n    \"\"\"Test Iceberg table operations locally.\"\"\"\n\n    # Convert Pandera schema to Iceberg\n    iceberg_schema = pandera_to_iceberg(RawWeatherData)\n\n    # Create mock catalog\n    with mock_iceberg_catalog() as catalog:\n        # Create table\n        table = catalog.create_table(\"raw.weather\", schema=iceberg_schema)\n\n        # Prepare test data\n        test_data = pd.DataFrame([\n            {\"city\": \"London\", \"temp\": 15.5, \"timestamp\": \"2024-01-15\"},\n            {\"city\": \"Paris\", \"temp\": 12.3, \"timestamp\": \"2024-01-15\"},\n        ])\n\n        # Validate with Pandera\n        validated = RawWeatherData.validate(test_data)\n\n        # Append to table\n        table.append(validated)\n\n        # Query entire table\n        result = table.scan().to_pandas()\n        assert len(result) == 2\n\n        # Query with filter\n        london = table.scan().filter(\"city = 'London'\").to_pandas()\n        assert len(london) == 1\n        assert london[\"temp\"].iloc[0] == 15.5\n\n        # Query with limit\n        first_row = table.scan().limit(1).to_pandas()\n        assert len(first_row) == 1\n</code></pre> <p>API Reference:</p> <pre><code># Create catalog\nwith mock_iceberg_catalog() as catalog:\n    # Create table\n    table = catalog.create_table(\"namespace.table_name\", schema=iceberg_schema)\n\n    # Load existing table\n    table = catalog.load_table(\"namespace.table_name\")\n\n    # List tables\n    tables = catalog.list_tables()\n\n    # Drop table\n    catalog.drop_table(\"namespace.table_name\")\n\n# Table operations\ntable.append(df)                          # Append DataFrame\ntable.append(arrow_table)                 # Append Arrow Table\nresult = table.scan().to_pandas()         # Scan to DataFrame\nresult = table.scan().to_arrow()          # Scan to Arrow Table\nfiltered = table.scan().filter(\"city = 'London'\").to_pandas()\nlimited = table.scan().limit(10).to_pandas()\ncount = table.count()                     # Row count\ntable.delete_all()                        # Delete all rows\ntable.drop()                              # Drop table\n</code></pre>"},{"location":"operations/testing/#test_asset_execution","title":"test_asset_execution","text":"<p>Test asset functions directly without Dagster or Docker.</p> <p>Features:</p> <ul> <li>\ud83d\ude80 Execute asset logic directly</li> <li>\ud83e\uddea Mock data support</li> <li>\u2705 Schema validation</li> <li>\ud83d\udcca Returns TestAssetResult with success/failure</li> </ul> <p>Example:</p> <pre><code>from phlo_testing import test_asset_execution\nfrom workflows.schemas.weather import RawWeatherData\n\ndef test_weather_asset():\n    \"\"\"Test asset with mock data.\"\"\"\n\n    # Mock API response data\n    test_data = [\n        {\n            \"city\": \"London\",\n            \"temp\": 15.5,\n            \"humidity\": 65,\n            \"timestamp\": \"2024-01-15T12:00:00Z\",\n        },\n    ]\n\n    # Test asset execution\n    result = test_asset_execution(\n        asset_fn=weather_observations_logic,  # Your asset function (not decorator)\n        partition=\"2024-01-15\",\n        mock_data=test_data,\n        validation_schema=RawWeatherData,\n    )\n\n    # Assertions\n    assert result.success, f\"Execution failed: {result.error}\"\n    assert len(result.data) == 1\n    assert result.data[\"city\"].iloc[0] == \"London\"\n    assert result.data[\"temp\"].iloc[0] == 15.5\n    assert result.metadata[\"validation\"] == \"passed\"\n</code></pre> <p>Without Mock Data (tests actual API):</p> <pre><code>def test_weather_asset_real_api():\n    \"\"\"Test with real API call.\"\"\"\n\n    result = test_asset_execution(\n        asset_fn=weather_observations_logic,\n        partition=\"2024-01-15\",\n        validation_schema=RawWeatherData,\n        # No mock_data = makes real API call\n    )\n\n    assert result.success\n    assert len(result.data) &gt; 0\n</code></pre> <p>API Reference:</p> <pre><code>result = test_asset_execution(\n    asset_fn=my_asset_function,           # Asset function (NOT decorated)\n    partition=\"2024-01-15\",                # Partition date\n    mock_data=[...],                       # Optional: mock data (list/DataFrame)\n    validation_schema=MyPanderaSchema,     # Optional: validate with schema\n)\n\n# Result attributes\nresult.success      # bool: True if successful\nresult.data         # DataFrame: output data\nresult.error        # Exception: error if failed\nresult.metadata     # dict: metadata (validation status, etc.)\n</code></pre>"},{"location":"operations/testing/#combining-mockicebergcatalog-test_asset_execution","title":"Combining MockIcebergCatalog + test_asset_execution","text":"<p>Full end-to-end test without Docker:</p> <pre><code>from phlo_testing import test_asset_execution, mock_iceberg_catalog\nfrom phlo_dlt.converter import pandera_to_iceberg\nfrom workflows.schemas.weather import RawWeatherData\n\ndef test_full_pipeline_local():\n    \"\"\"Test complete ingestion \u2192 Iceberg pipeline locally.\"\"\"\n\n    # Step 1: Execute asset with mock data\n    test_data = [\n        {\"city\": \"London\", \"temp\": 15.5, \"timestamp\": \"2024-01-15T12:00:00Z\"},\n    ]\n\n    result = test_asset_execution(\n        asset_fn=weather_observations_logic,\n        partition=\"2024-01-15\",\n        mock_data=test_data,\n        validation_schema=RawWeatherData,\n    )\n\n    assert result.success\n\n    # Step 2: Write to mock Iceberg catalog\n    iceberg_schema = pandera_to_iceberg(RawWeatherData)\n\n    with mock_iceberg_catalog() as catalog:\n        table = catalog.create_table(\"raw.weather\", schema=iceberg_schema)\n        table.append(result.data)\n\n        # Step 3: Query from Iceberg\n        queried = table.scan().to_pandas()\n        assert len(queried) == 1\n        assert queried[\"city\"].iloc[0] == \"London\"\n</code></pre> <p>Performance Comparison: | Approach | Setup Time | Execution | Total | |----------|------------|-----------|-------| | Docker | 30-60s | 30-60s | ~60-120s | | MockIcebergCatalog | 0s | &lt; 5s | &lt; 5s |</p>"},{"location":"operations/testing/#integration-testing","title":"Integration Testing","text":"<p>Integration tests require the full Docker stack and test end-to-end pipelines.</p>"},{"location":"operations/testing/#core-service-auto-configure-validation","title":"Core Service Auto-Configure Validation","text":"<p>Use this checklist after a fresh install or when core service definitions change.</p> <pre><code>phlo services start\ndocker compose ps\ndocker compose logs postgres\ndocker compose logs minio\ndocker compose logs nessie\ndocker compose logs trino\ndocker compose logs dagster\n</code></pre> <p>Edge cases to watch: - <code>SUPERSET_ADMIN_PASSWORD</code> is injected into Dagster even if Superset is not installed; set it only when enabling Superset. - <code>PHLO_VERSION</code> is optional (blank uses latest) for the Dagster image build. - <code>PHLO_HOST_PLATFORM</code> should be set for macOS hosts that need explicit Linux/amd64 builds.</p>"},{"location":"operations/testing/#prerequisites","title":"Prerequisites","text":"<pre><code># Start Docker services\nmake up-core up-query\n\n# Wait for services to be healthy\ndocker compose ps\n</code></pre>"},{"location":"operations/testing/#testing-asset-materialization","title":"Testing Asset Materialization","text":"<p>Test full asset execution:</p> <pre><code>import pytest\n\n@pytest.mark.integration  # Mark as integration test\ndef test_weather_asset_materialization():\n    \"\"\"\n    Test full weather asset materialization.\n\n    Requires: Docker services running\n    \"\"\"\n\n    # Materialize via Docker exec\n    import subprocess\n\n    result = subprocess.run([\n        \"docker\", \"exec\", \"dagster-webserver\",\n        \"dagster\", \"asset\", \"materialize\",\n        \"--select\", \"weather_observations\",\n        \"--partition\", \"2024-01-15\",\n    ], capture_output=True, text=True)\n\n    assert result.returncode == 0, f\"Materialization failed: {result.stderr}\"\n\n\n@pytest.mark.integration\ndef test_data_written_to_iceberg():\n    \"\"\"Test that data was written to Iceberg table.\"\"\"\n\n    import trino\n\n    conn = trino.dbapi.connect(\n        host='localhost',\n        port=8080,\n        user='trino',\n        catalog='iceberg_dev',\n        schema='raw',\n    )\n\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM weather_observations\")\n    count = cursor.fetchone()[0]\n\n    assert count &gt; 0, \"No data found in Iceberg table\"\n</code></pre>"},{"location":"operations/testing/#running-integration-tests","title":"Running Integration Tests","text":"<pre><code># Run only integration tests\npytest tests/ -m integration -v\n\n# Run only unit tests (exclude integration)\npytest tests/ -m \"not integration\" -v\n\n# Run with verbose output\npytest tests/ -v -s\n</code></pre>"},{"location":"operations/testing/#cli-commands-new","title":"CLI Commands \u2728 NEW","text":"<p>Phlo now provides CLI commands for testing and workflow management.</p>"},{"location":"operations/testing/#phlo-test","title":"phlo test","text":"<p>Run tests with various options:</p> <pre><code># Run all tests\nphlo test\n\n# Run tests for specific asset\nphlo test weather_observations\n\n# Run unit tests only (skip Docker integration tests)\nphlo test --local\n\n# Run with coverage\nphlo test --coverage\n\n# Run specific marker\nphlo test -m integration\n</code></pre>"},{"location":"operations/testing/#phlo-materialize","title":"phlo materialize","text":"<p>Materialize assets via Docker:</p> <pre><code># Materialize asset\nphlo materialize weather_observations\n\n# Materialize with partition\nphlo materialize weather_observations --partition 2024-01-15\n\n# Dry run (show command without executing)\nphlo materialize weather_observations --dry-run\n</code></pre>"},{"location":"operations/testing/#phlo-create-workflow","title":"phlo create-workflow","text":"<p>Interactive workflow scaffolding:</p> <pre><code># Interactive prompts\nphlo create-workflow\n\n# With options\nphlo create-workflow --type ingestion --domain weather --table observations\n</code></pre> <p>See <code>phlo --help</code> for full documentation.</p>"},{"location":"operations/testing/#test-organization","title":"Test Organization","text":""},{"location":"operations/testing/#recommended-structure","title":"Recommended Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                     # Pytest configuration and shared fixtures\n\u251c\u2500\u2500 fixtures/                       # Test data fixtures\n\u2502   \u251c\u2500\u2500 weather_data.json\n\u2502   \u2514\u2500\u2500 github_events.json\n\u251c\u2500\u2500 unit/                          # Fast tests (no Docker)\n\u2502   \u251c\u2500\u2500 test_schemas.py           # Schema validation tests\n\u2502   \u251c\u2500\u2500 test_asset_config.py      # Decorator configuration tests\n\u2502   \u2514\u2500\u2500 test_business_logic.py    # Data transformation tests\n\u2514\u2500\u2500 integration/                   # Slow tests (require Docker)\n    \u251c\u2500\u2500 test_weather_pipeline.py  # Full pipeline tests\n    \u2514\u2500\u2500 test_github_pipeline.py\n</code></pre>"},{"location":"operations/testing/#example-conftestpy","title":"Example conftest.py","text":"<pre><code>\"\"\"Pytest configuration and shared fixtures.\"\"\"\n\nimport pytest\nimport pandas as pd\n\n\ndef pytest_configure(config):\n    \"\"\"Configure pytest markers.\"\"\"\n    config.addinivalue_line(\n        \"markers\",\n        \"integration: mark test as integration test (requires Docker)\"\n    )\n\n\n@pytest.fixture\ndef sample_weather_data():\n    \"\"\"Fixture providing sample weather data.\"\"\"\n    return pd.DataFrame([\n        {\n            \"city_name\": \"London\",\n            \"temperature\": 15.5,\n            \"humidity\": 65,\n            \"timestamp\": \"2024-01-15T12:00:00Z\",\n        },\n        {\n            \"city_name\": \"Paris\",\n            \"temperature\": 12.3,\n            \"humidity\": 70,\n            \"timestamp\": \"2024-01-15T12:00:00Z\",\n        },\n    ])\n\n\n@pytest.fixture\ndef sample_invalid_data():\n    \"\"\"Fixture providing invalid weather data.\"\"\"\n    return pd.DataFrame([\n        {\n            \"city_name\": \"London\",\n            \"temperature\": -200.0,  # Invalid\n            \"humidity\": 150,  # Invalid\n            \"timestamp\": \"2024-01-15T12:00:00Z\",\n        },\n    ])\n</code></pre>"},{"location":"operations/testing/#best-practices","title":"Best Practices","text":""},{"location":"operations/testing/#1-test-schema-first","title":"1. Test Schema First","text":"<p>Always test your Pandera schema before testing the full pipeline:</p> <pre><code>def test_schema_validates_good_data():\n    \"\"\"Schema should accept valid data.\"\"\"\n    # Test with known good data\n    pass\n\n\ndef test_schema_rejects_bad_data():\n    \"\"\"Schema should reject invalid data.\"\"\"\n    # Test constraint violations\n    pass\n</code></pre> <p>Why: Schema errors are the most common issue. Catching them early saves time.</p>"},{"location":"operations/testing/#2-use-descriptive-test-names","title":"2. Use Descriptive Test Names","text":"<pre><code># Good\ndef test_temperature_constraint_rejects_values_below_minus_100():\n    pass\n\n# Bad\ndef test_temperature():\n    pass\n</code></pre> <p>Why: Clear names make failures easier to diagnose.</p>"},{"location":"operations/testing/#3-one-assertion-concept-per-test","title":"3. One Assertion Concept Per Test","text":"<pre><code># Good\ndef test_temperature_field_exists():\n    assert \"temperature\" in schema_fields\n\ndef test_temperature_has_range_constraint():\n    assert schema.fields[\"temperature\"].checks[\"ge\"] == -100\n\n# Bad\ndef test_temperature_field():\n    assert \"temperature\" in schema_fields\n    assert schema.fields[\"temperature\"].checks[\"ge\"] == -100\n    assert schema.fields[\"temperature\"].type == float\n</code></pre> <p>Why: Specific failures are easier to debug.</p>"},{"location":"operations/testing/#4-use-fixtures-for-test-data","title":"4. Use Fixtures for Test Data","text":"<pre><code>@pytest.fixture\ndef valid_weather_data():\n    return pd.DataFrame([...])\n\n\ndef test_schema_validation(valid_weather_data):\n    result = MySchema.validate(valid_weather_data)\n    assert len(result) == 2\n</code></pre> <p>Why: Reduces duplication, makes tests more maintainable.</p>"},{"location":"operations/testing/#5-mark-integration-tests","title":"5. Mark Integration Tests","text":"<pre><code>@pytest.mark.integration\ndef test_full_pipeline():\n    \"\"\"Requires Docker services.\"\"\"\n    pass\n</code></pre> <p>Why: Enables running fast tests separately from slow tests.</p>"},{"location":"operations/testing/#6-test-error-cases","title":"6. Test Error Cases","text":"<pre><code>def test_handles_missing_required_field():\n    \"\"\"Test that missing required field raises error.\"\"\"\n    incomplete_data = pd.DataFrame([{\"city_name\": \"London\"}])  # Missing temperature\n\n    with pytest.raises(Exception):\n        MySchema.validate(incomplete_data)\n</code></pre> <p>Why: Error handling is as important as happy paths.</p>"},{"location":"operations/testing/#common-testing-patterns","title":"Common Testing Patterns","text":""},{"location":"operations/testing/#pattern-1-schema-validation-test","title":"Pattern 1: Schema Validation Test","text":"<pre><code>def test_my_schema_validation():\n    \"\"\"Test schema validates expected data.\"\"\"\n\n    # Arrange\n    test_data = pd.DataFrame([{...}])\n\n    # Act\n    result = MySchema.validate(test_data)\n\n    # Assert\n    assert len(result) == expected_count\n    assert result[\"field\"].dtype == expected_type\n</code></pre>"},{"location":"operations/testing/#pattern-2-constraint-violation-test","title":"Pattern 2: Constraint Violation Test","text":"<pre><code>def test_schema_rejects_negative_values():\n    \"\"\"Test that negative values are rejected.\"\"\"\n\n    invalid_data = pd.DataFrame([{\"value\": -10}])\n\n    with pytest.raises(Exception) as exc_info:\n        MySchema.validate(invalid_data)\n\n    # Optionally check error message\n    assert \"value\" in str(exc_info.value).lower()\n</code></pre>"},{"location":"operations/testing/#pattern-3-field-existence-test","title":"Pattern 3: Field Existence Test","text":"<pre><code>def test_required_fields_exist():\n    \"\"\"Test that all required fields are present in schema.\"\"\"\n\n    schema_fields = MySchema.to_schema().columns.keys()\n    required = [\"id\", \"timestamp\", \"value\"]\n\n    for field in required:\n        assert field in schema_fields, f\"Missing: {field}\"\n</code></pre>"},{"location":"operations/testing/#pattern-4-asset-configuration-test","title":"Pattern 4: Asset Configuration Test","text":"<pre><code>def test_asset_decorator_configuration():\n    \"\"\"Test that asset decorator is configured correctly.\"\"\"\n\n    from my_asset import my_ingestion_asset\n\n    # Check op name\n    assert my_ingestion_asset.op.name == \"dlt_my_table\"\n\n    # Check function signature\n    import inspect\n    sig = inspect.signature(my_ingestion_asset.op.compute_fn)\n    assert \"partition_date\" in sig.parameters\n</code></pre>"},{"location":"operations/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/testing/#test-discovery-issues","title":"Test Discovery Issues","text":"<p>Problem: <code>pytest</code> doesn't find your tests</p> <p>Solution:</p> <ul> <li>Ensure test files start with <code>test_</code></li> <li>Ensure test functions start with <code>test_</code></li> <li>Ensure test classes start with <code>Test</code></li> <li>Check <code>pytest.ini</code> configuration</li> </ul>"},{"location":"operations/testing/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError</code> when running tests</p> <p>Solution:</p> <pre><code># Install phlo in editable mode\npip install -e .\n\n# Or set PYTHONPATH\nexport PYTHONPATH=/home/user/phlo/src:$PYTHONPATH\n</code></pre>"},{"location":"operations/testing/#schema-validation-failures","title":"Schema Validation Failures","text":"<p>Problem: Valid data fails schema validation</p> <p>Solution:</p> <ul> <li>Check field types match (str vs datetime)</li> <li>Check nullable settings</li> <li>Check constraint values (ge, le, etc.)</li> <li>Use <code>coerce=True</code> in schema Config</li> </ul>"},{"location":"operations/testing/#integration-test-failures","title":"Integration Test Failures","text":"<p>Problem: Integration tests fail with connection errors</p> <p>Solution:</p> <pre><code># Check Docker services are running\ndocker compose ps\n\n# Check service health\ndocker logs dagster-webserver\ndocker logs nessie\ndocker logs minio\n\n# Restart services if needed\nmake down\nmake up-core up-query\n</code></pre>"},{"location":"operations/testing/#running-tests-in-cicd","title":"Running Tests in CI/CD","text":""},{"location":"operations/testing/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  unit-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n      - name: Install dependencies\n        run: |\n          pip install -e \".[dev]\"\n      - name: Run unit tests\n        run: |\n          pytest tests/ -m \"not integration\" -v --cov=phlo\n\n  integration-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Start Docker services\n        run: |\n          make up-core up-query\n          sleep 30  # Wait for services\n      - name: Run integration tests\n        run: |\n          pytest tests/ -m integration -v\n</code></pre>"},{"location":"operations/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"operations/testing/#measuring-coverage","title":"Measuring Coverage","text":"<pre><code># Run tests with coverage\npytest tests/ --cov=phlo --cov-report=html\n\n# Open coverage report\nopen htmlcov/index.html\n</code></pre>"},{"location":"operations/testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Schema validation: 100% (easy to test)</li> <li>Decorator configuration: 100% (easy to test)</li> <li>Business logic: 90%+ (important to test)</li> <li>Integration paths: 70%+ (harder to test)</li> </ul>"},{"location":"operations/testing/#next-steps","title":"Next Steps","text":"<ol> <li>Start with schema tests: Use the example in this guide or the scaffolded test file</li> <li>Add business logic tests: Test any custom transformations</li> <li>Add integration tests: Test full pipelines when ready</li> <li>Set up CI/CD: Run tests on every commit</li> <li>Track coverage: Aim for 80%+ overall coverage</li> </ol>"},{"location":"operations/testing/#additional-resources","title":"Additional Resources","text":"<ul> <li>Pandera Docs: https://pandera.readthedocs.io/</li> <li>Pytest Docs: https://docs.pytest.org/</li> <li>Troubleshooting: Troubleshooting Guide</li> </ul> <p>Have questions? Open a GitHub Discussion</p>"},{"location":"operations/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"operations/troubleshooting/#debugging-common-issues-in-phlo","title":"Debugging Common Issues in Phlo","text":"<p>This guide helps you debug and fix common problems in Phlo.</p>"},{"location":"operations/troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Services Won't Start</li> <li>Dagster Issues</li> <li>dbt Issues</li> <li>Trino/Query Issues</li> <li>Iceberg/Nessie Issues</li> <li>Data Quality Issues</li> <li>Performance Issues</li> <li>Debugging Techniques</li> </ol>"},{"location":"operations/troubleshooting/#services-wont-start","title":"Services Won't Start","text":""},{"location":"operations/troubleshooting/#docker-compose-fails","title":"Docker Compose Fails","text":"<p>Symptom: <code>docker-compose up</code> fails or services crash</p> <p>Check 1: Port conflicts</p> <pre><code># See what's using ports\nlsof -i :3000  # Dagster\nlsof -i :10011 # Trino\nlsof -i :9000  # MinIO\n\n# Kill conflicting process\nkill -9 &lt;PID&gt;\n</code></pre> <p>Check 2: Insufficient resources</p> <pre><code># Check Docker resources\ndocker stats\n\n# Increase in Docker Desktop:\n# Settings \u2192 Resources \u2192 increase CPU/Memory\n</code></pre> <p>Check 3: Check logs</p> <pre><code># View logs for specific service\ndocker-compose logs postgres\ndocker-compose logs dagster-webserver\ndocker-compose logs trino\n\n# Follow logs in real-time\ndocker-compose logs -f dagster-webserver\n</code></pre> <p>Check 4: Clean start</p> <pre><code># Stop everything\ndocker-compose down\n\n# Remove volumes (CAUTION: deletes data!)\ndocker-compose down -v\n\n# Rebuild and start\ndocker-compose up --build\n</code></pre>"},{"location":"operations/troubleshooting/#postgres-wont-start","title":"Postgres Won't Start","text":"<p>Symptom: <code>connection refused</code> or postgres container crashes</p> <p>Solution 1: Check data directory permissions</p> <pre><code># Ensure postgres data directory writable\nsudo chown -R 999:999 ./postgres-data\n\n# Or remove and recreate\ndocker-compose down\nrm -rf ./postgres-data\ndocker-compose up postgres\n</code></pre> <p>Solution 2: Check for corruption</p> <pre><code># View postgres logs\ndocker-compose logs postgres\n\n# If corrupted, remove data\ndocker-compose down\ndocker volume rm cascade_postgres-data\ndocker-compose up postgres\n</code></pre>"},{"location":"operations/troubleshooting/#minio-wont-start","title":"MinIO Won't Start","text":"<p>Symptom: <code>cannot write to data directory</code></p> <p>Solution:</p> <pre><code># Fix permissions\nsudo chown -R 1000:1000 ./minio-data\n\n# Or recreate\ndocker-compose down\nrm -rf ./minio-data\ndocker-compose up minio\n</code></pre>"},{"location":"operations/troubleshooting/#trino-wont-start","title":"Trino Won't Start","text":"<p>Symptom: Trino crashes on startup</p> <p>Check 1: Memory</p> <pre><code># Trino needs at least 2GB\ndocker stats cascade_trino\n\n# Increase in docker-compose.yml:\ntrino:\n  deploy:\n    resources:\n      limits:\n        memory: 4G\n</code></pre> <p>Check 2: Nessie connection</p> <pre><code># Ensure Nessie is running\ndocker-compose ps nessie\n\n# Check Nessie is healthy\ncurl http://localhost:10003/api/v1/trees\n</code></pre> <p>Check 3: Configuration</p> <pre><code># View Trino logs\ndocker-compose logs trino | grep ERROR\n</code></pre>"},{"location":"operations/troubleshooting/#dagster-issues","title":"Dagster Issues","text":""},{"location":"operations/troubleshooting/#assets-not-showing-in-ui","title":"Assets Not Showing in UI","text":"<p>Solution 1: Restart services</p> <pre><code>docker-compose restart dagster-webserver dagster-daemon\n</code></pre> <p>Solution 2: Check asset registration</p> <pre><code># Your assets should be discoverable via phlo.framework.definitions\n# (assets registered in workflows/ via decorators and plugins)\nfrom phlo.framework.definitions import defs\n</code></pre> <p>Solution 3: Check for syntax errors</p> <pre><code># Test definitions load\ndocker-compose exec dagster-webserver python -c \"from phlo.framework.definitions import defs; print(defs)\"\n</code></pre>"},{"location":"operations/troubleshooting/#asset-materialization-fails","title":"Asset Materialization Fails","text":"<p>Debug steps:</p> <p>1. Check logs in UI</p> <ul> <li>Go to Runs \u2192 Find failed run \u2192 Click run \u2192 View logs</li> <li>Look for error message at bottom</li> </ul> <p>2. Check dependencies</p> <pre><code># Ensure upstream assets materialized\ndagster asset materialize -m phlo.framework.definitions -a upstream_asset\n</code></pre> <p>3. Test locally</p> <pre><code># In Python shell\nfrom phlo.framework.definitions import defs\nfrom dagster import materialize\n\nasset_def = defs.get_asset_def(\"my_asset\")\nresult = materialize([asset_def])\nprint(result)\n</code></pre> <p>4. Check resources</p> <pre><code># Are resources configured?\nfrom phlo.config import get_settings\nconfig = get_settings()\nprint(config.trino_host)  # Should print value, not error\n</code></pre>"},{"location":"operations/troubleshooting/#dagster-daemon-not-running","title":"Dagster Daemon Not Running","text":"<p>Symptom: Schedules/sensors not triggering</p> <p>Solution:</p> <pre><code># Check daemon status\ndocker-compose ps dagster-daemon\n\n# Restart daemon\ndocker-compose restart dagster-daemon\n\n# View daemon logs\ndocker-compose logs dagster-daemon\n</code></pre>"},{"location":"operations/troubleshooting/#slow-ui","title":"Slow UI","text":"<p>Solution 1: Clear instance cache</p> <pre><code>docker-compose exec dagster-webserver rm -rf /tmp/dagster-*\ndocker-compose restart dagster-webserver\n</code></pre> <p>Solution 2: Increase resources</p> <pre><code># docker-compose.yml\ndagster-webserver:\n  deploy:\n    resources:\n      limits:\n        cpus: \"2\"\n        memory: 2G\n</code></pre>"},{"location":"operations/troubleshooting/#dbt-issues","title":"dbt Issues","text":""},{"location":"operations/troubleshooting/#dbt-models-wont-run","title":"dbt Models Won't Run","text":"<p>Symptom: <code>dbt run</code> fails</p> <p>Check 1: Connection</p> <pre><code># Test Trino connection\ndocker-compose exec dagster-webserver \\\n  dbt debug --project-dir /app/workflows/transforms/dbt\n\n# Should show all checks passing\n</code></pre> <p>Check 2: Syntax errors</p> <pre><code># Compile first (catches syntax errors)\ndocker-compose exec dagster-webserver \\\n  dbt compile --project-dir /app/workflows/transforms/dbt\n</code></pre> <p>Check 3: Dependencies</p> <pre><code># Check source exists\ndocker-compose exec trino trino --execute \\\n  \"SHOW TABLES IN iceberg.raw\"\n\n# Should list your source tables\n</code></pre>"},{"location":"operations/troubleshooting/#compilation-errors","title":"Compilation Errors","text":"<p>Symptom: <code>Compilation Error in model my_model</code></p> <p>Debug:</p> <pre><code># View compiled SQL\ndocker-compose exec dagster-webserver \\\n  dbt compile --project-dir /app/workflows/transforms/dbt\n\n# Check: workflows/transforms/dbt/target/compiled/phlo/models/your_model.sql\n# This shows the actual SQL generated\n</code></pre> <p>Common issues:</p> <p>1. Undefined variable</p> <pre><code>-- Error: {{ ref('typo_table_name') }}\n-- Fix: {{ ref('correct_table_name') }}\n</code></pre> <p>2. Missing source</p> <pre><code>-- Error: {{ source('raw', 'nonexistent') }}\n-- Fix: Define source in sources.yml first\n</code></pre> <p>3. SQL syntax error</p> <pre><code>-- Error: Missing comma\nSELECT\n    col1\n    col2  -- Missing comma!\nFROM table\n\n-- Fix: Add comma\nSELECT\n    col1,\n    col2\nFROM table\n</code></pre>"},{"location":"operations/troubleshooting/#dbt-tests-fail","title":"dbt Tests Fail","text":"<p>Debug failing test:</p> <pre><code># Run single test\ndocker-compose exec dagster-webserver \\\n  dbt test --project-dir /app/workflows/transforms/dbt \\\n  --select fct_orders,column:customer_id,test_name:not_null\n\n# View failed rows\ndocker-compose exec trino trino --execute \\\n  \"SELECT * FROM iceberg.silver.fct_orders WHERE customer_id IS NULL\"\n</code></pre> <p>Common test failures:</p> <p>1. not_null fails</p> <pre><code>-- Find NULL values\nSELECT * FROM table WHERE column_name IS NULL\n\n-- Fix in model:\nSELECT\n    COALESCE(column_name, 'UNKNOWN') AS column_name\nFROM source\n</code></pre> <p>2. unique fails</p> <pre><code>-- Find duplicates\nSELECT\n    column_name,\n    COUNT(*) as count\nFROM table\nGROUP BY column_name\nHAVING COUNT(*) &gt; 1\n\n-- Fix: Add DISTINCT or GROUP BY\nSELECT DISTINCT column_name\nFROM source\n</code></pre> <p>3. relationships fails</p> <pre><code>-- Find orphaned records\nSELECT o.*\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.customer_id\nWHERE c.customer_id IS NULL\n\n-- Fix: Add join or filter\nWHERE customer_id IN (SELECT customer_id FROM customers)\n</code></pre>"},{"location":"operations/troubleshooting/#incremental-model-issues","title":"Incremental Model Issues","text":"<p>Symptom: Incremental model not updating</p> <p>Check 1: is_incremental() logic</p> <pre><code>SELECT * FROM source\n\n{% if is_incremental() %}\n    -- Is this condition correct?\n    WHERE updated_at &gt; (SELECT MAX(updated_at) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Check 2: Force full refresh</p> <pre><code># Rebuild from scratch\ndbt run --select my_incremental_model --full-refresh\n</code></pre> <p>Check 3: Check unique_key</p> <pre><code>{{ config(\n    materialized='incremental',\n    unique_key='id',  # Does this column exist and is it unique?\n) }}\n</code></pre>"},{"location":"operations/troubleshooting/#trinoquery-issues","title":"Trino/Query Issues","text":""},{"location":"operations/troubleshooting/#query-fails","title":"Query Fails","text":"<p>Symptom: <code>Query failed: ...</code></p> <p>Common errors:</p> <p>1. Table not found</p> <pre><code>Error: Table 'iceberg.raw.my_table' does not exist\n\nSolutions:\n- Check table actually exists: SHOW TABLES IN iceberg.raw\n- Check spelling/capitalization\n- Ensure asset materialized first\n</code></pre> <p>2. Column not found</p> <pre><code>Error: Column 'colum_name' cannot be resolved\n\nSolutions:\n- Check column exists: DESCRIBE iceberg.raw.my_table\n- Check for typo\n- Ensure using correct table version\n</code></pre> <p>3. Type mismatch</p> <pre><code>Error: Cannot cast VARCHAR to INTEGER\n\nSolutions:\n- Use explicit CAST: CAST(column AS INTEGER)\n- Handle NULLs: CAST(NULLIF(column, '') AS INTEGER)\n- Use TRY_CAST: TRY_CAST(column AS INTEGER)\n</code></pre> <p>4. Out of memory</p> <pre><code>Error: Query exceeded per-node user memory limit\n\nSolutions:\n- Add LIMIT to query\n- Filter data earlier: WHERE date &gt;= CURRENT_DATE - INTERVAL '7' DAY\n- Increase Trino memory in docker-compose.yml\n</code></pre>"},{"location":"operations/troubleshooting/#slow-queries","title":"Slow Queries","text":"<p>Debug:</p> <pre><code>-- Check query plan\nEXPLAIN SELECT * FROM large_table\n\n-- Check table stats\nSHOW STATS FOR iceberg.silver.fct_orders\n</code></pre> <p>Optimizations:</p> <p>1. Add filters</p> <pre><code>-- Bad: Full scan\nSELECT * FROM fct_orders\n\n-- Good: Filtered\nSELECT * FROM fct_orders\nWHERE order_date &gt;= CURRENT_DATE - INTERVAL '30' DAY\n</code></pre> <p>2. Limit results</p> <pre><code>-- Add LIMIT for exploration\nSELECT * FROM fct_orders LIMIT 1000\n</code></pre> <p>3. Use columnar format (Parquet)</p> <ul> <li>Iceberg already uses Parquet \u2705</li> </ul> <p>4. Partition data</p> <pre><code># Partition Iceberg table by date\nschema = Schema(\n    ...,\n    partition_spec=PartitionSpec(\n        PartitionField(\"order_date\", \"day\")\n    )\n)\n</code></pre>"},{"location":"operations/troubleshooting/#connection-issues","title":"Connection Issues","text":"<p>Symptom: <code>Connection refused</code> or <code>Timeout</code></p> <p>Solution:</p> <pre><code># Check Trino is running\ndocker-compose ps trino\n\n# Check Trino is healthy\ncurl http://localhost:10011/v1/info\n\n# Check firewall isn't blocking\ntelnet localhost 10011\n\n# Restart Trino\ndocker-compose restart trino\n</code></pre>"},{"location":"operations/troubleshooting/#icebergnessie-issues","title":"Iceberg/Nessie Issues","text":""},{"location":"operations/troubleshooting/#table-not-found","title":"Table Not Found","text":"<p>Symptom: <code>Table 'iceberg.raw.my_table' does not exist</code></p> <p>Check 1: List tables</p> <pre><code># Connect to Trino\ndocker-compose exec trino trino\n\n# List schemas\nSHOW SCHEMAS IN iceberg;\n\n# List tables in schema\nSHOW TABLES IN iceberg.raw;\n</code></pre> <p>Check 2: Check branch</p> <pre><code># Are you on the right branch?\n# To query dev branch, use iceberg_dev catalog\nSELECT * FROM iceberg_dev.raw.my_table\n</code></pre> <p>Check 3: Ensure asset materialized</p> <pre><code># Materialize the ingestion asset first\ndagster asset materialize -m phlo.framework.definitions -a my_ingestion_asset\n</code></pre>"},{"location":"operations/troubleshooting/#nessie-api-errors","title":"Nessie API Errors","text":"<p>Symptom: <code>Failed to connect to Nessie</code></p> <p>Solution:</p> <pre><code># Check Nessie is running\ndocker-compose ps nessie\n\n# Test Nessie API\ncurl http://localhost:10003/api/v1/trees\n\n# Should return: {\"name\": \"main\", ...}\n\n# Restart Nessie\ndocker-compose restart nessie\n</code></pre>"},{"location":"operations/troubleshooting/#branch-issues","title":"Branch Issues","text":"<p>List branches:</p> <pre><code>curl http://localhost:10003/api/v1/trees\n</code></pre> <p>Create branch:</p> <pre><code>curl -X POST http://localhost:10003/api/v1/trees/branch/dev \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"sourceRefName\": \"main\"}'\n</code></pre> <p>Switch branch in Trino:</p> <pre><code>-- Use iceberg catalog (main branch)\nSELECT * FROM iceberg.raw.my_table;\n\n-- Use iceberg_dev catalog (dev branch)\nSELECT * FROM iceberg_dev.raw.my_table;\n</code></pre>"},{"location":"operations/troubleshooting/#data-quality-issues","title":"Data Quality Issues","text":""},{"location":"operations/troubleshooting/#missing-data","title":"Missing Data","text":"<p>Debug steps:</p> <p>1. Check source</p> <pre><code>-- Does raw data exist?\nSELECT COUNT(*), MIN(date), MAX(date)\nFROM iceberg.raw.my_source_table\n</code></pre> <p>2. Check transformations</p> <pre><code>-- Are rows being filtered out?\n-- Check each layer\nSELECT COUNT(*) FROM iceberg.raw.my_table;          -- 1000 rows\nSELECT COUNT(*) FROM iceberg.bronze.stg_my_table;   -- 950 rows (where did 50 go?)\nSELECT COUNT(*) FROM iceberg.silver.fct_my_table;   -- 900 rows (where did 50 go?)\n</code></pre> <p>3. Check for filters</p> <pre><code>-- Review model SQL\n-- Look for WHERE clauses that might be too aggressive\nWHERE date &gt;= '2024-01-01'  -- Are you excluding earlier data?\n  AND status = 'active'      -- Are you excluding inactive records?\n</code></pre>"},{"location":"operations/troubleshooting/#duplicate-data","title":"Duplicate Data","text":"<p>Debug:</p> <pre><code>-- Find duplicates\nSELECT\n    id,\n    COUNT(*) as count\nFROM iceberg.silver.fct_my_table\nGROUP BY id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>Solutions:</p> <p>1. Add DISTINCT</p> <pre><code>SELECT DISTINCT * FROM source\n</code></pre> <p>2. Use GROUP BY</p> <pre><code>SELECT\n    id,\n    MAX(updated_at) AS updated_at,\n    ...\nFROM source\nGROUP BY id\n</code></pre> <p>3. Use ROW_NUMBER()</p> <pre><code>WITH ranked AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY id\n            ORDER BY updated_at DESC\n        ) AS rn\n    FROM source\n)\nSELECT * FROM ranked WHERE rn = 1\n</code></pre>"},{"location":"operations/troubleshooting/#incorrect-values","title":"Incorrect Values","text":"<p>Debug:</p> <pre><code>-- Check value distributions\nSELECT\n    column_name,\n    COUNT(*) as count,\n    MIN(value) as min_val,\n    MAX(value) as max_val,\n    AVG(value) as avg_val\nFROM table\nGROUP BY column_name\n</code></pre> <p>Check transformations:</p> <pre><code>-- Review calculated fields\n-- Is the formula correct?\namount * 1.1 AS total  -- Should this be amount * 1.1 or amount + (amount * 0.1)?\n</code></pre>"},{"location":"operations/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"operations/troubleshooting/#slow-asset-materialization","title":"Slow Asset Materialization","text":"<p>Debug:</p> <p>1. Check logs for bottlenecks</p> <pre><code># Find slow steps\ndocker-compose logs dagster-webserver | grep \"Execution time\"\n</code></pre> <p>2. Profile query</p> <pre><code>-- In Trino, check query runtime\nEXPLAIN ANALYZE SELECT * FROM expensive_query\n</code></pre> <p>Solutions:</p> <p>1. Add partitions</p> <pre><code>@dg.asset(partitions_def=daily_partition)\ndef partitioned_asset(context):\n    date = context.partition_key\n    # Process only one day\n</code></pre> <p>2. Use incremental dbt models</p> <pre><code>{{ config(materialized='incremental') }}\n</code></pre> <p>3. Optimize queries</p> <pre><code>-- Bad: Full table scan\nSELECT * FROM large_table\n\n-- Good: Filtered and limited\nSELECT *\nFROM large_table\nWHERE date &gt;= CURRENT_DATE - INTERVAL '7' DAY\nLIMIT 10000\n</code></pre> <p>4. Increase resources</p> <pre><code># docker-compose.yml\ndagster-webserver:\n  deploy:\n    resources:\n      limits:\n        cpus: \"4\"\n        memory: 8G\n</code></pre>"},{"location":"operations/troubleshooting/#slow-dashboard","title":"Slow Dashboard","text":"<p>Solutions:</p> <p>1. Pre-aggregate data</p> <pre><code>-- Instead of aggregating in dashboard,\n-- pre-aggregate in gold layer\nSELECT\n    date,\n    SUM(amount) as total_amount,\n    COUNT(*) as total_orders\nFROM fct_orders\nGROUP BY date\n</code></pre> <p>2. Publish to PostgreSQL</p> <ul> <li>Marts published to PostgreSQL are much faster than querying Iceberg</li> </ul> <p>3. Add indexes (in PostgreSQL marts)</p> <pre><code>CREATE INDEX idx_mrt_orders_date ON marts.mrt_orders(order_date);\n</code></pre>"},{"location":"operations/troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"operations/troubleshooting/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<p>Dagster:</p> <pre><code>@dg.asset\ndef my_asset(context: dg.AssetExecutionContext):\n    context.log.set_level(logging.DEBUG)\n    context.log.debug(\"Detailed debug info\")\n</code></pre> <p>dbt:</p> <pre><code>dbt run --debug\n</code></pre> <p>Trino:</p> <pre><code># Enable query logging\ndocker-compose exec trino trino --debug\n</code></pre>"},{"location":"operations/troubleshooting/#inspect-data-at-each-step","title":"Inspect Data at Each Step","text":"<pre><code>-- Check raw\nSELECT * FROM iceberg.raw.my_table LIMIT 10;\n\n-- Check bronze\nSELECT * FROM iceberg.bronze.stg_my_table LIMIT 10;\n\n-- Check silver\nSELECT * FROM iceberg.silver.fct_my_table LIMIT 10;\n\n-- Compare counts\nSELECT 'raw' as layer, COUNT(*) FROM iceberg.raw.my_table\nUNION ALL\nSELECT 'bronze', COUNT(*) FROM iceberg.bronze.stg_my_table\nUNION ALL\nSELECT 'silver', COUNT(*) FROM iceberg.silver.fct_my_table;\n</code></pre>"},{"location":"operations/troubleshooting/#use-dagster-playground","title":"Use Dagster Playground","text":"<p>Test assets interactively:</p> <ol> <li>Open Dagster UI</li> <li>Assets \u2192 Click asset \u2192 \"Launchpad\" tab</li> <li>Modify config</li> <li>Click \"Materialize\"</li> </ol>"},{"location":"operations/troubleshooting/#query-compiled-sql","title":"Query Compiled SQL","text":"<p>For dbt issues:</p> <pre><code># Compile models\ndbt compile --project-dir /app/workflows/transforms/dbt\n\n# View compiled SQL\ncat workflows/transforms/dbt/target/compiled/phlo/models/silver/fct_my_model.sql\n\n# Run compiled SQL directly in Trino to debug\ndocker-compose exec trino trino &lt; compiled.sql\n</code></pre>"},{"location":"operations/troubleshooting/#check-metadata-tables","title":"Check Metadata Tables","text":"<p>Iceberg metadata:</p> <pre><code>-- View snapshots\nSELECT * FROM iceberg.raw.\"my_table$snapshots\";\n\n-- View manifests\nSELECT * FROM iceberg.raw.\"my_table$manifests\";\n\n-- View files\nSELECT * FROM iceberg.raw.\"my_table$files\";\n</code></pre> <p>Nessie metadata:</p> <pre><code># View commit log\ncurl http://localhost:10003/api/v1/trees/branch/main/log\n\n# View specific table history\ncurl http://localhost:10003/api/v1/trees/branch/main/contents/raw.my_table\n</code></pre>"},{"location":"operations/troubleshooting/#isolate-the-problem","title":"Isolate the Problem","text":"<p>Binary search:</p> <ol> <li>Does the source data exist? \u2192 Yes</li> <li>Does bronze model work? \u2192 Yes</li> <li>Does silver model work? \u2192 No \u2190 Problem is here</li> </ol> <p>Test in isolation:</p> <pre><code>-- Run transformation logic manually\nSELECT\n    id,\n    -- Your transformation\n    CASE WHEN value &lt; 0 THEN 0 ELSE value END AS clean_value\nFROM iceberg.bronze.stg_my_table\nLIMIT 10\n</code></pre>"},{"location":"operations/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"operations/troubleshooting/#check-logs","title":"Check Logs","text":"<p>Always check logs first:</p> <pre><code># All logs\ndocker-compose logs\n\n# Specific service\ndocker-compose logs dagster-webserver\ndocker-compose logs trino\ndocker-compose logs postgres\n\n# Follow logs\ndocker-compose logs -f dagster-webserver\n\n# Last 100 lines\ndocker-compose logs --tail=100 dagster-webserver\n</code></pre>"},{"location":"operations/troubleshooting/#search-documentation","title":"Search Documentation","text":"<ul> <li>Dagster Docs: https://docs.dagster.io</li> <li>dbt Docs: https://docs.getdbt.com</li> <li>Trino Docs: https://trino.io/docs</li> <li>Iceberg Docs: https://iceberg.apache.org/docs</li> <li>Nessie Docs: https://projectnessie.org/docs</li> </ul>"},{"location":"operations/troubleshooting/#ask-for-help","title":"Ask for Help","text":"<p>When asking for help, include:</p> <ol> <li>What you're trying to do</li> <li>What you expected to happen</li> <li>What actually happened</li> <li>Error messages (full stack trace)</li> <li>Logs (relevant portions)</li> <li>Code (the asset/model causing issues)</li> <li>What you've tried (troubleshooting steps)</li> </ol>"},{"location":"operations/troubleshooting/#summary","title":"Summary","text":"<p>Debugging Process:</p> <ol> <li>Read the error message carefully</li> <li>Check logs for details</li> <li>Isolate the problem (which component?)</li> <li>Test in isolation (can you reproduce?)</li> <li>Check dependencies (are upstream steps ok?)</li> <li>Verify configuration (is everything configured?)</li> <li>Search documentation (is this a known issue?)</li> <li>Ask for help (provide details!)</li> </ol> <p>Common Solutions:</p> <ul> <li>Restart services</li> <li>Check logs</li> <li>Verify configuration</li> <li>Test connections</li> <li>Check dependencies</li> <li>Increase resources</li> <li>Clear caches</li> </ul> <p>Prevention:</p> <ul> <li>\u2705 Add tests</li> <li>\u2705 Add logging</li> <li>\u2705 Add error handling</li> <li>\u2705 Monitor performance</li> <li>\u2705 Document assumptions</li> <li>\u2705 Version control everything</li> </ul> <p>Next: Best Practices Guide - Build better pipelines.</p>"},{"location":"packages/","title":"Phlo Packages Reference","text":"<p>Phlo is organized as a monorepo with individual packages that provide specific functionality. Each package can be installed independently or as part of a complete installation.</p>"},{"location":"packages/#package-categories","title":"Package Categories","text":""},{"location":"packages/#core-framework","title":"Core Framework","text":"Package Description phlo Core framework with CLI, plugin system, configuration, and hooks"},{"location":"packages/#data-processing","title":"Data Processing","text":"Package Description phlo-dagster Data orchestration platform for scheduling and monitoring pipelines phlo-dbt dbt integration for SQL transformations phlo-dlt Data Load Tool integration for ingestion phlo-iceberg Apache Iceberg catalog and table format support phlo-quality Data quality validation and checks phlo-lineage Data lineage tracking and visualization"},{"location":"packages/#infrastructure-services","title":"Infrastructure Services","text":"Package Description phlo-postgres PostgreSQL database service phlo-nessie Git-like catalog for Iceberg tables phlo-trino Distributed SQL query engine phlo-minio S3-compatible object storage"},{"location":"packages/#observability","title":"Observability","text":"Package Description phlo-grafana Metrics visualization and dashboards phlo-prometheus Metrics collection and alerting phlo-loki Log aggregation phlo-alloy OpenTelemetry collector phlo-alerting Alert management and routing phlo-metrics Custom metrics collection"},{"location":"packages/#api-layer","title":"API Layer","text":"Package Description phlo-api FastAPI REST endpoints for Phlo phlo-postgrest Auto-generated REST API from PostgreSQL phlo-hasura GraphQL API with real-time subscriptions"},{"location":"packages/#data-catalog-governance","title":"Data Catalog &amp; Governance","text":"Package Description phlo-openmetadata Data catalog and governance platform"},{"location":"packages/#user-interface","title":"User Interface","text":"Package Description phlo-observatory Web UI for exploring data and monitoring pipelines phlo-superset Business intelligence and visualization phlo-pgweb PostgreSQL web interface"},{"location":"packages/#testing-development","title":"Testing &amp; Development","text":"Package Description phlo-testing Testing utilities and fixtures phlo-core-plugins Built-in plugins for common use cases"},{"location":"packages/#installation","title":"Installation","text":""},{"location":"packages/#full-installation-recommended","title":"Full Installation (Recommended)","text":"<p>Install Phlo with all default services:</p> <pre><code>uv pip install phlo[defaults]\n</code></pre> <p>This includes core data processing packages and infrastructure services.</p>"},{"location":"packages/#minimal-installation","title":"Minimal Installation","text":"<p>Install only the core framework:</p> <pre><code>uv pip install phlo\n</code></pre> <p>Then add packages as needed:</p> <pre><code>uv pip install phlo-dagster phlo-postgres phlo-trino\n</code></pre>"},{"location":"packages/#with-optional-profiles","title":"With Optional Profiles","text":"<pre><code># With observability stack\nuv pip install phlo[defaults,observability]\n\n# With API layer\nuv pip install phlo[defaults,api]\n\n# With data catalog\nuv pip install phlo[defaults,catalog]\n</code></pre>"},{"location":"packages/#plugin-system","title":"Plugin System","text":"<p>All packages integrate through Phlo's unified plugin system using Python entry points. When a package is installed, its plugins are automatically discovered and registered.</p>"},{"location":"packages/#entry-point-groups","title":"Entry Point Groups","text":"Entry Point Description <code>phlo.plugins.services</code> Infrastructure service definitions <code>phlo.plugins.dagster</code> Dagster resources and definitions <code>phlo.plugins.sources</code> Data source connectors <code>phlo.plugins.quality</code> Quality check implementations <code>phlo.plugins.transforms</code> Data transformation plugins <code>phlo.plugins.cli</code> CLI command extensions <code>phlo.plugins.hooks</code> Event hook handlers <code>phlo.plugins.trino_catalogs</code> Trino catalog configurations"},{"location":"packages/#discovering-installed-plugins","title":"Discovering Installed Plugins","text":"<pre><code># List all installed plugins\nphlo plugin list\n\n# List by type\nphlo plugin list --type services\n\n# Get plugin details\nphlo plugin info dagster\n</code></pre>"},{"location":"packages/#package-versioning","title":"Package Versioning","text":"<p>All packages in the monorepo share a synchronized version number. When installing, ensure all Phlo packages are at the same version to avoid compatibility issues.</p> <p>Check installed versions:</p> <pre><code>pip list | grep phlo\n</code></pre>"},{"location":"packages/#creating-custom-packages","title":"Creating Custom Packages","text":"<p>See the Plugin Development Guide for creating custom packages that integrate with Phlo.</p>"},{"location":"packages/#next-steps","title":"Next Steps","text":"<ul> <li>Plugin Development Guide - Build custom plugins</li> <li>Installation Guide - Complete installation instructions</li> <li>CLI Reference - CLI commands for managing packages</li> </ul>"},{"location":"packages/phlo-alerting/","title":"phlo-alerting","text":"<p>Alert routing and notification plugin for Phlo.</p>"},{"location":"packages/phlo-alerting/#overview","title":"Overview","text":"<p><code>phlo-alerting</code> routes alerts from quality check failures, telemetry events, and pipeline errors to configured destinations like Slack, PagerDuty, and Email.</p>"},{"location":"packages/phlo-alerting/#installation","title":"Installation","text":"<pre><code>pip install phlo-alerting\n# or\nphlo plugin install alerting\n</code></pre>"},{"location":"packages/phlo-alerting/#configuration","title":"Configuration","text":"Variable Default Description <code>PHLO_ALERT_SLACK_WEBHOOK</code> - Slack incoming webhook URL <code>PHLO_ALERT_SLACK_CHANNEL</code> - Default Slack channel <code>PHLO_ALERT_PAGERDUTY_KEY</code> - PagerDuty Events API v2 key <code>PHLO_ALERT_EMAIL_SMTP_HOST</code> - SMTP server hostname <code>PHLO_ALERT_EMAIL_SMTP_PORT</code> <code>587</code> SMTP server port <code>PHLO_ALERT_EMAIL_SMTP_USER</code> - SMTP username <code>PHLO_ALERT_EMAIL_SMTP_PASSWORD</code> - SMTP password <code>PHLO_ALERT_EMAIL_RECIPIENTS</code> <code>[]</code> Comma-separated list: <code>a@x.com,b@y.com</code>"},{"location":"packages/phlo-alerting/#features","title":"Features","text":""},{"location":"packages/phlo-alerting/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Hook Registration Automatically registers as a hook plugin via entry points Event Handling Listens for <code>quality.result</code> and <code>telemetry.*</code> events Severity Mapping Maps event severities to alert levels (critical, warning, info)"},{"location":"packages/phlo-alerting/#supported-events","title":"Supported Events","text":"<ul> <li><code>quality.result</code> - Receives quality check results and sends alerts on failures</li> <li><code>telemetry.*</code> - Receives telemetry events and forwards to configured destinations</li> </ul>"},{"location":"packages/phlo-alerting/#severity-levels","title":"Severity Levels","text":"Level Description Default Destinations <code>critical</code> Immediate action required PagerDuty, Slack <code>warning</code> Attention needed Slack <code>info</code> Informational Email (optional)"},{"location":"packages/phlo-alerting/#usage","title":"Usage","text":""},{"location":"packages/phlo-alerting/#cli-commands","title":"CLI Commands","text":"<pre><code># Send a test alert\nphlo alerts test --destination slack\n\n# List configured destinations\nphlo alerts list\n\n# Silence alerts for a table\nphlo alerts silence bronze.problematic_table --duration 1h\n</code></pre>"},{"location":"packages/phlo-alerting/#programmatic","title":"Programmatic","text":"<pre><code>from phlo_alerting.manager import AlertManager\n\nmanager = AlertManager()\nmanager.send_alert(\n    title=\"Quality Check Failed\",\n    message=\"Null check failed on table users\",\n    severity=\"critical\",\n    metadata={\n        \"table\": \"bronze.users\",\n        \"check\": \"null_check\",\n        \"failed_count\": 150\n    }\n)\n</code></pre>"},{"location":"packages/phlo-alerting/#slack-configuration","title":"Slack Configuration","text":"<ol> <li>Create a Slack Incoming Webhook</li> <li>Set <code>PHLO_ALERT_SLACK_WEBHOOK</code> environment variable</li> <li>Optionally set <code>PHLO_ALERT_SLACK_CHANNEL</code></li> </ol>"},{"location":"packages/phlo-alerting/#pagerduty-configuration","title":"PagerDuty Configuration","text":"<ol> <li>Create a PagerDuty Events API v2 integration</li> <li>Set <code>PHLO_ALERT_PAGERDUTY_KEY</code> to the integration key</li> </ol>"},{"location":"packages/phlo-alerting/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.cli</code> <code>alerts</code> CLI command <code>phlo.plugins.hooks</code> <code>AlertingHookPlugin</code> for event handling"},{"location":"packages/phlo-alerting/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-quality - Quality checks</li> <li>phlo-prometheus - Metrics alerting</li> <li>phlo-grafana - Alert visualization</li> </ul>"},{"location":"packages/phlo-alerting/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Setup - Complete monitoring</li> <li>Operations Guide - Alert best practices</li> </ul>"},{"location":"packages/phlo-alloy/","title":"phlo-alloy","text":"<p>Grafana Alloy log collector for Phlo.</p>"},{"location":"packages/phlo-alloy/#overview","title":"Overview","text":"<p><code>phlo-alloy</code> collects logs from all Docker containers and ships them to Loki for aggregation and querying.</p>"},{"location":"packages/phlo-alloy/#installation","title":"Installation","text":"<pre><code>pip install phlo-alloy\n# or\nphlo plugin install alloy\n</code></pre>"},{"location":"packages/phlo-alloy/#profile","title":"Profile","text":"<p>Part of the <code>observability</code> profile.</p>"},{"location":"packages/phlo-alloy/#configuration","title":"Configuration","text":"Variable Default Description <code>ALLOY_PORT</code> <code>12345</code> Alloy HTTP port"},{"location":"packages/phlo-alloy/#features","title":"Features","text":""},{"location":"packages/phlo-alloy/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Container Discovery Auto-discovers all Docker containers via Docker socket Log Collection Collects stdout/stderr from all containers Loki Shipping Ships logs to Loki for storage and querying Metrics Labels Exposes Alloy metrics for Prometheus"},{"location":"packages/phlo-alloy/#docker-socket-access","title":"Docker Socket Access","text":"<p>Alloy mounts the Docker socket to discover and collect logs from all containers:</p> <pre><code>volumes:\n  - /var/run/docker.sock:/var/run/docker.sock:ro\n</code></pre>"},{"location":"packages/phlo-alloy/#usage","title":"Usage","text":""},{"location":"packages/phlo-alloy/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start with observability profile\nphlo services start --profile observability\n\n# Or start individually\nphlo services start --service alloy\n</code></pre>"},{"location":"packages/phlo-alloy/#endpoints","title":"Endpoints","text":"Endpoint URL HTTP <code>http://localhost:12345</code> Ready <code>http://localhost:12345/-/ready</code>"},{"location":"packages/phlo-alloy/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>AlloyServicePlugin</code>"},{"location":"packages/phlo-alloy/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-loki - Log storage</li> <li>phlo-grafana - Log visualization</li> <li>phlo-prometheus - Metrics collection</li> </ul>"},{"location":"packages/phlo-alloy/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Setup - Complete monitoring setup</li> <li>Troubleshooting Guide - Debug with logs</li> </ul>"},{"location":"packages/phlo-api/","title":"phlo-api","text":"<p>Backend API service for Phlo Observatory.</p>"},{"location":"packages/phlo-api/#overview","title":"Overview","text":"<p><code>phlo-api</code> is a FastAPI-based backend service that exposes Phlo internals to the Observatory UI. It provides endpoints for lineage, quality checks, assets, branches, and metadata.</p>"},{"location":"packages/phlo-api/#installation","title":"Installation","text":"<pre><code>pip install phlo-api\n# or\nphlo plugin install api\n</code></pre>"},{"location":"packages/phlo-api/#configuration","title":"Configuration","text":"Variable Default Description <code>PHLO_API_PORT</code> <code>4000</code> API server port <code>HOST</code> <code>0.0.0.0</code> API server host"},{"location":"packages/phlo-api/#features","title":"Features","text":""},{"location":"packages/phlo-api/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Metrics Labels Exposes Prometheus metrics at <code>/metrics</code> Service Discovery Automatically scraped by Prometheus Health Check Provides <code>/health</code> endpoint for container orchestration"},{"location":"packages/phlo-api/#usage","title":"Usage","text":""},{"location":"packages/phlo-api/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start the API service\nphlo services start --service phlo-api\n\n# Or run in native mode (better for ARM Macs)\nphlo services start --native\n</code></pre>"},{"location":"packages/phlo-api/#api-routes","title":"API Routes","text":"Route Method Description <code>/health</code> GET Health check <code>/api/config</code> GET Project configuration <code>/api/plugins</code> GET List all plugins <code>/api/plugins/{type}</code> GET List plugins by type <code>/api/plugins/{type}/{name}</code> GET Get plugin details <code>/api/services</code> GET List all services <code>/api/services/{name}</code> GET Get service details <code>/api/registry</code> GET Plugin registry <code>/api/lineage/*</code> GET Data lineage queries <code>/api/quality/*</code> GET Quality check results <code>/api/dagster/*</code> GET Dagster asset information <code>/api/nessie/*</code> GET Nessie branch management <code>/api/iceberg/*</code> GET Iceberg table operations <code>/api/trino/*</code> GET/POST Query execution <code>/api/loki/*</code> GET Log queries <code>/api/maintenance/*</code> GET Maintenance operations <code>/api/search/*</code> GET Unified search"},{"location":"packages/phlo-api/#example-requests","title":"Example Requests","text":"<pre><code># Health check\ncurl http://localhost:4000/health\n\n# Get lineage for a table\ncurl \"http://localhost:4000/api/lineage?table=bronze.users\"\n\n# Get quality check results\ncurl \"http://localhost:4000/api/quality?asset=bronze.users\"\n\n# List Nessie branches\ncurl http://localhost:4000/api/nessie/branches\n\n# Execute a query\ncurl -X POST http://localhost:4000/api/trino/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"sql\": \"SELECT * FROM bronze.users LIMIT 10\"}'\n</code></pre>"},{"location":"packages/phlo-api/#endpoints","title":"Endpoints","text":"Endpoint URL API Base <code>http://localhost:4000</code> Health <code>http://localhost:4000/health</code> Metrics <code>http://localhost:4000/metrics</code> OpenAPI Docs <code>http://localhost:4000/docs</code> ReDoc <code>http://localhost:4000/redoc</code>"},{"location":"packages/phlo-api/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>PhloApiServicePlugin</code>"},{"location":"packages/phlo-api/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-observatory - Frontend UI</li> <li>phlo-dagster - Asset information</li> <li>phlo-nessie - Branch management</li> <li>phlo-lineage - Lineage data</li> </ul>"},{"location":"packages/phlo-api/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Full API documentation</li> <li>Observability Setup - API monitoring</li> </ul>"},{"location":"packages/phlo-core-plugins/","title":"phlo-core-plugins","text":"<p>Core plugins for Phlo (quality checks and source connectors).</p>"},{"location":"packages/phlo-core-plugins/#overview","title":"Overview","text":"<p><code>phlo-core-plugins</code> provides built-in quality check plugins and source connectors. These are registered via entry points and auto-discovered at runtime.</p>"},{"location":"packages/phlo-core-plugins/#installation","title":"Installation","text":"<pre><code>pip install phlo-core-plugins\n# or included with phlo by default\n</code></pre>"},{"location":"packages/phlo-core-plugins/#features","title":"Features","text":""},{"location":"packages/phlo-core-plugins/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Quality Checks Auto-registered via <code>phlo.plugins.quality</code> entry points Source Connectors Auto-registered via <code>phlo.plugins.sources</code> entry points"},{"location":"packages/phlo-core-plugins/#quality-check-plugins","title":"Quality Check Plugins","text":"Check Entry Point Description <code>null_check</code> <code>phlo.plugins.quality</code> Validates no NULL values <code>uniqueness_check</code> <code>phlo.plugins.quality</code> Validates unique values <code>range_check</code> <code>phlo.plugins.quality</code> Validates value ranges <code>regex_check</code> <code>phlo.plugins.quality</code> Validates regex patterns <code>freshness_check</code> <code>phlo.plugins.quality</code> Validates data freshness"},{"location":"packages/phlo-core-plugins/#quality-check-usage","title":"Quality Check Usage","text":"<pre><code>from phlo import phlo_quality\nfrom phlo_quality.checks import null_check, uniqueness_check, range_check\n\n@phlo_quality(\n    asset=\"bronze.users\",\n    checks=[\n        null_check(column=\"id\"),\n        uniqueness_check(column=\"email\"),\n        range_check(column=\"age\", min_value=0, max_value=150),\n    ]\n)\ndef validate_users():\n    pass\n</code></pre>"},{"location":"packages/phlo-core-plugins/#source-connector-plugins","title":"Source Connector Plugins","text":"Connector Entry Point Description <code>rest_api</code> <code>phlo.plugins.sources</code> Generic REST API connector"},{"location":"packages/phlo-core-plugins/#source-connector-usage","title":"Source Connector Usage","text":"<pre><code>from phlo import phlo_ingestion\n\n@phlo_ingestion(\n    name=\"api_data\",\n    source=\"rest_api\",  # Uses rest_api connector\n    destination=\"bronze.api_data\"\n)\ndef ingest_api():\n    return {\n        \"client\": {\"base_url\": \"https://api.example.com\"},\n        \"resources\": [\"users\", \"events\"]\n    }\n</code></pre>"},{"location":"packages/phlo-core-plugins/#detailed-plugin-reference","title":"Detailed Plugin Reference","text":""},{"location":"packages/phlo-core-plugins/#null_check","title":"null_check","text":"<p>Validates that a column contains no NULL values.</p> <pre><code>null_check(\n    column=\"id\",           # Column to check\n    tolerance=0.0          # Allowed fraction of nulls (0.0-1.0)\n)\n</code></pre>"},{"location":"packages/phlo-core-plugins/#uniqueness_check","title":"uniqueness_check","text":"<p>Validates that values in a column are unique.</p> <pre><code>uniqueness_check(\n    column=\"email\",        # Column to check\n    tolerance=0.0          # Allowed fraction of duplicates\n)\n</code></pre>"},{"location":"packages/phlo-core-plugins/#range_check","title":"range_check","text":"<p>Validates that numeric values fall within a range.</p> <pre><code>range_check(\n    column=\"age\",\n    min_value=0,           # Minimum allowed value\n    max_value=150,         # Maximum allowed value\n    tolerance=0.0          # Allowed fraction of violations\n)\n</code></pre>"},{"location":"packages/phlo-core-plugins/#regex_check","title":"regex_check","text":"<p>Validates that string values match a pattern.</p> <pre><code>regex_check(\n    column=\"email\",\n    pattern=r\"^[\\w.-]+@[\\w.-]+\\.\\w+$\",  # Regex pattern\n    tolerance=0.0\n)\n</code></pre>"},{"location":"packages/phlo-core-plugins/#freshness_check","title":"freshness_check","text":"<p>Validates that data is recent.</p> <pre><code>freshness_check(\n    column=\"updated_at\",   # Timestamp column\n    max_age_hours=24       # Maximum age in hours\n)\n</code></pre>"},{"location":"packages/phlo-core-plugins/#entry-points","title":"Entry Points","text":"Entry Point Plugins <code>phlo.plugins.quality</code> Quality check plugins <code>phlo.plugins.sources</code> Source connector plugins"},{"location":"packages/phlo-core-plugins/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-quality - Quality framework</li> <li>phlo-dlt - Ingestion framework</li> </ul>"},{"location":"packages/phlo-core-plugins/#next-steps","title":"Next Steps","text":"<ul> <li>Plugin Development Guide - Create custom plugins</li> <li>Testing Strategy - Test quality checks</li> </ul>"},{"location":"packages/phlo-dagster/","title":"phlo-dagster","text":"<p>Data orchestration platform for Phlo.</p>"},{"location":"packages/phlo-dagster/#overview","title":"Overview","text":"<p><code>phlo-dagster</code> provides the core data orchestration platform for scheduling, monitoring, and managing data pipelines. It runs ingestion, transformation, and quality workflows as Dagster assets.</p>"},{"location":"packages/phlo-dagster/#installation","title":"Installation","text":"<pre><code>pip install phlo-dagster\n# or\nphlo plugin install dagster\n</code></pre>"},{"location":"packages/phlo-dagster/#configuration","title":"Configuration","text":"Variable Default Description <code>DAGSTER_PORT</code> <code>3000</code> Dagster webserver port <code>WORKFLOWS_PATH</code> <code>workflows</code> Path to workflow files"},{"location":"packages/phlo-dagster/#features","title":"Features","text":""},{"location":"packages/phlo-dagster/#auto-configuration","title":"Auto-Configuration","text":"<p>This package is fully auto-configured:</p> Feature How It Works Plugin Discovery Auto-discovers Dagster extensions via <code>phlo.plugins.dagster</code> entry points dbt Compilation Auto-compiles dbt on startup via post_start hook Workflow Discovery Auto-discovers workflows in <code>workflows/</code> directory Metrics Labels Exposes Dagster metrics for Prometheus"},{"location":"packages/phlo-dagster/#post-start-hook","title":"Post-Start Hook","text":"<p>The Dagster service automatically runs dbt compilation on startup:</p> <pre><code>hooks:\n  post_start:\n    - name: dbt-compile\n      command: dbt compile\n</code></pre>"},{"location":"packages/phlo-dagster/#plugin-discovery","title":"Plugin Discovery","text":"<p>Dagster extensions are auto-loaded through the plugin system:</p> <ul> <li><code>@phlo_ingestion</code> assets from phlo-dlt</li> <li><code>IcebergResource</code> from phlo-iceberg</li> <li>dbt assets from phlo-dbt</li> <li>Quality checks from phlo-quality</li> </ul>"},{"location":"packages/phlo-dagster/#usage","title":"Usage","text":""},{"location":"packages/phlo-dagster/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start Dagster\nphlo services start --service dagster\n\n# Start with dev mode (editable install from source)\nphlo services init --dev --phlo-source /path/to/phlo\nphlo services start\n</code></pre>"},{"location":"packages/phlo-dagster/#development-server","title":"Development Server","text":"<p>For local development without Docker:</p> <pre><code>phlo dev                    # Start on localhost:3000\nphlo dev --port 8080        # Use custom port\n</code></pre>"},{"location":"packages/phlo-dagster/#materializing-assets","title":"Materializing Assets","text":"<pre><code># Via Dagster CLI\ndagster asset materialize --select my_asset\n\n# Via Phlo CLI\nphlo materialize my_asset\n</code></pre>"},{"location":"packages/phlo-dagster/#endpoints-docker-mode","title":"Endpoints (Docker Mode)","text":"Endpoint URL Web UI <code>http://localhost:10006</code> GraphQL <code>http://localhost:10006/graphql</code> Metrics <code>http://localhost:10006/metrics</code> <p>Note: When using <code>phlo dev</code> for local development, these are available at port 3000 instead.</p>"},{"location":"packages/phlo-dagster/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Dagster Webserver              \u2502\n\u2502  - Asset UI                              \u2502\n\u2502  - Run monitoring                        \u2502\n\u2502  - Schedule management                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Dagster Daemon                 \u2502\n\u2502  - Schedule execution                    \u2502\n\u2502  - Sensor polling                        \u2502\n\u2502  - Auto-materialization                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Phlo Plugin Extensions           \u2502\n\u2502  - DLT Ingestion                         \u2502\n\u2502  - dbt Transformations                   \u2502\n\u2502  - Quality Checks                        \u2502\n\u2502  - Iceberg Resources                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"packages/phlo-dagster/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>DagsterServicePlugin</code>, <code>DagsterDaemonServicePlugin</code> <code>phlo.plugins.cli</code> Dagster CLI commands"},{"location":"packages/phlo-dagster/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-dlt - Data ingestion</li> <li>phlo-dbt - SQL transformations</li> <li>phlo-quality - Data quality checks</li> <li>phlo-iceberg - Iceberg table access</li> </ul>"},{"location":"packages/phlo-dagster/#next-steps","title":"Next Steps","text":"<ul> <li>Workflow Development Guide - Build data pipelines</li> <li>Dagster Assets Guide - Asset patterns</li> <li>Developer Guide - Decorator usage</li> </ul>"},{"location":"packages/phlo-dbt/","title":"phlo-dbt","text":"<p>dbt transformation integration for Phlo.</p>"},{"location":"packages/phlo-dbt/#overview","title":"Overview","text":"<p><code>phlo-dbt</code> integrates dbt (data build tool) with the Phlo lakehouse. It provides Dagster assets from dbt models, CLI commands, and automatic project discovery.</p>"},{"location":"packages/phlo-dbt/#installation","title":"Installation","text":"<pre><code>pip install phlo-dbt\n# or\nphlo plugin install dbt\n</code></pre>"},{"location":"packages/phlo-dbt/#configuration","title":"Configuration","text":"Variable Default Description <code>DBT_PROJECT_DIR</code> <code>workflows/transforms/dbt</code> Path to dbt project directory <code>DBT_PROFILES_DIR</code> <code>workflows/transforms/dbt/profiles</code> Path to dbt profiles <code>DBT_MANIFEST_PATH</code> <code>workflows/transforms/dbt/target/manifest.json</code> Path to dbt manifest <code>DBT_CATALOG_PATH</code> <code>workflows/transforms/dbt/target/catalog.json</code> Path to dbt catalog"},{"location":"packages/phlo-dbt/#features","title":"Features","text":""},{"location":"packages/phlo-dbt/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Project Discovery Auto-discovers <code>dbt_project.yml</code> in workspace via <code>find_dbt_projects()</code> Dagster Assets Automatically creates Dagster assets from dbt models Lineage Events Emits lineage events during model execution Auto-Compile Compiles dbt on Dagster startup via post_start hook"},{"location":"packages/phlo-dbt/#discovery-locations","title":"Discovery Locations","text":"<p>If <code>DBT_PROJECT_DIR</code> is set, it is used before discovery.</p> <p>The discovery module searches these paths in order:</p> <ol> <li><code>workflows/transforms/dbt/</code></li> </ol>"},{"location":"packages/phlo-dbt/#usage","title":"Usage","text":""},{"location":"packages/phlo-dbt/#cli-commands","title":"CLI Commands","text":"<pre><code># Compile dbt project\nphlo dbt compile\n\n# Run dbt models\nphlo dbt run\n\n# Run specific models\nphlo dbt run --select silver.*\n\n# Generate dbt docs\nphlo dbt docs generate\n\n# Serve dbt docs\nphlo dbt docs serve\n</code></pre>"},{"location":"packages/phlo-dbt/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from phlo_dbt.discovery import find_dbt_projects, get_dbt_project_dir\n\n# Find all dbt projects in workspace\nprojects = find_dbt_projects()\n\n# Get the active dbt project directory\nproject_dir = get_dbt_project_dir()\n</code></pre>"},{"location":"packages/phlo-dbt/#project-structure","title":"Project Structure","text":"<p>Standard dbt project layout for Phlo:</p> <pre><code>workflows/transforms/dbt/\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 profiles.yml\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 bronze/           # Staging models\n\u2502   \u2502   \u2514\u2500\u2500 stg_*.sql\n\u2502   \u251c\u2500\u2500 silver/           # Cleaned models\n\u2502   \u2502   \u2514\u2500\u2500 *.sql\n\u2502   \u2514\u2500\u2500 gold/             # Mart models\n\u2502       \u2514\u2500\u2500 mrt_*.sql\n\u251c\u2500\u2500 macros/\n\u251c\u2500\u2500 seeds/\n\u2514\u2500\u2500 target/               # Compiled artifacts\n    \u251c\u2500\u2500 manifest.json\n    \u2514\u2500\u2500 catalog.json\n</code></pre>"},{"location":"packages/phlo-dbt/#bronzesilvergold-pattern","title":"Bronze/Silver/Gold Pattern","text":""},{"location":"packages/phlo-dbt/#bronze-staging","title":"Bronze (Staging)","text":"<pre><code>-- models/bronze/stg_events.sql\n{{ config(materialized='incremental', unique_key='id') }}\n\nSELECT\n    id,\n    timestamp,\n    user_id,\n    event_type\nFROM {{ source('raw', 'events') }}\n{% if is_incremental() %}\nWHERE timestamp &gt; (SELECT MAX(timestamp) FROM {{ this }})\n{% endif %}\n</code></pre>"},{"location":"packages/phlo-dbt/#silver-cleaned","title":"Silver (Cleaned)","text":"<pre><code>-- models/silver/events_cleaned.sql\n{{ config(materialized='table') }}\n\nSELECT\n    id,\n    timestamp,\n    user_id,\n    UPPER(event_type) as event_type,\n    DATE(timestamp) as event_date\nFROM {{ ref('stg_events') }}\nWHERE user_id IS NOT NULL\n</code></pre>"},{"location":"packages/phlo-dbt/#gold-marts","title":"Gold (Marts)","text":"<pre><code>-- models/gold/mrt_daily_events.sql\n{{ config(materialized='table') }}\n\nSELECT\n    event_date,\n    event_type,\n    COUNT(*) as event_count,\n    COUNT(DISTINCT user_id) as unique_users\nFROM {{ ref('events_cleaned') }}\nGROUP BY 1, 2\n</code></pre>"},{"location":"packages/phlo-dbt/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.dagster</code> <code>DbtDagsterPlugin</code> for asset definitions <code>phlo.plugins.cli</code> <code>dbt</code> CLI commands"},{"location":"packages/phlo-dbt/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-dagster - Orchestration platform</li> <li>phlo-trino - Query engine</li> <li>phlo-iceberg - Table format</li> </ul>"},{"location":"packages/phlo-dbt/#next-steps","title":"Next Steps","text":"<ul> <li>dbt Development Guide - Build transformations</li> <li>Data Modeling Guide - Bronze/Silver/Gold patterns</li> <li>Workflow Development - Complete pipelines</li> </ul>"},{"location":"packages/phlo-dlt/","title":"phlo-dlt","text":"<p>DLT (Data Load Tool) ingestion engine for Phlo.</p>"},{"location":"packages/phlo-dlt/#overview","title":"Overview","text":"<p><code>phlo-dlt</code> provides the <code>@phlo_ingestion</code> decorator for defining data ingestion pipelines using DLT. It automatically materializes data into Iceberg tables with schema evolution and full lineage tracking.</p>"},{"location":"packages/phlo-dlt/#installation","title":"Installation","text":"<pre><code>pip install phlo-dlt\n# or\nphlo plugin install dlt\n</code></pre>"},{"location":"packages/phlo-dlt/#configuration","title":"Configuration","text":"Variable Default Description <code>ICEBERG_WAREHOUSE_PATH</code> <code>s3://lake/warehouse</code> Iceberg warehouse S3 path <code>ICEBERG_STAGING_PATH</code> <code>s3://lake/stage</code> Staging path for ingestion <code>NESSIE_HOST</code> <code>nessie</code> Nessie catalog host <code>NESSIE_PORT</code> <code>19120</code> Nessie catalog port"},{"location":"packages/phlo-dlt/#features","title":"Features","text":""},{"location":"packages/phlo-dlt/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Asset Registration Ingestion assets auto-registered with Dagster via entry points Lineage Events Emits <code>ingestion.start</code>, <code>ingestion.end</code> events for lineage tracking Schema Evolution Automatically handles schema changes during ingestion Hook Integration Events captured by alerting, metrics, and OpenMetadata plugins"},{"location":"packages/phlo-dlt/#event-flow","title":"Event Flow","text":"<pre><code>@phlo_ingestion \u2192 IngestionEventEmitter \u2192 HookBus \u2192 [Alerting, Metrics, Lineage plugins]\n</code></pre>"},{"location":"packages/phlo-dlt/#usage","title":"Usage","text":""},{"location":"packages/phlo-dlt/#basic-ingestion","title":"Basic Ingestion","text":"<pre><code>from phlo import phlo_ingestion\nfrom workflows.schemas.events import EventSchema\n\n@phlo_ingestion(\n    table_name=\"events\",\n    unique_key=\"id\",\n    validation_schema=EventSchema,\n    group=\"api\",\n    cron=\"0 */1 * * *\",\n    freshness_hours=(1, 24),\n)\ndef api_events(partition_date: str):\n    \"\"\"Ingest events from REST API.\"\"\"\n    from dlt.sources.rest_api import rest_api\n\n    return rest_api({\n        \"client\": {\"base_url\": \"https://api.example.com\"},\n        \"resources\": [{\"name\": \"events\", \"endpoint\": \"/events\"}]\n    })\n</code></pre>"},{"location":"packages/phlo-dlt/#decorator-options","title":"Decorator Options","text":"Option Type Description <code>table_name</code> <code>str</code> Target Iceberg table name <code>unique_key</code> <code>str</code> Column for deduplication <code>validation_schema</code> <code>DataFrameModel</code> Pandera schema for validation <code>group</code> <code>str</code> Asset group name <code>cron</code> <code>str</code> Schedule expression <code>freshness_hours</code> <code>tuple[int, int]</code> (warn, fail) freshness thresholds <code>merge_strategy</code> <code>str</code> <code>merge</code> (default) or <code>append</code> <code>merge_config</code> <code>dict</code> Advanced merge configuration"},{"location":"packages/phlo-dlt/#merge-strategies","title":"Merge Strategies","text":"<pre><code># Default merge with deduplication\n@phlo_ingestion(\n    table_name=\"events\",\n    unique_key=\"id\",\n    merge_strategy=\"merge\",\n    merge_config={\"deduplication_method\": \"last\"}  # or \"first\", \"hash\"\n)\n\n# Append-only (no deduplication)\n@phlo_ingestion(\n    table_name=\"events\",\n    merge_strategy=\"append\"\n)\n</code></pre>"},{"location":"packages/phlo-dlt/#running-ingestion","title":"Running Ingestion","text":"<pre><code># Via Dagster CLI\ndagster asset materialize --select api_events\n\n# Via Phlo CLI\nphlo materialize api_events --partition 2025-01-15\n</code></pre>"},{"location":"packages/phlo-dlt/#data-flow","title":"Data Flow","text":"<pre><code>External API\n     \u2193\nDLT Pipeline (extract + normalize)\n     \u2193\nParquet Staging (S3)\n     \u2193\nPandera Validation\n     \u2193\nPyIceberg Merge\n     \u2193\nIceberg Table (on Nessie branch)\n</code></pre>"},{"location":"packages/phlo-dlt/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.dagster</code> <code>DltDagsterPlugin</code> for ingestion assets"},{"location":"packages/phlo-dlt/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-dagster - Orchestration platform</li> <li>phlo-iceberg - Iceberg table format</li> <li>phlo-quality - Data validation</li> <li>phlo-nessie - Branch management</li> </ul>"},{"location":"packages/phlo-dlt/#next-steps","title":"Next Steps","text":"<ul> <li>Developer Guide - Master decorators</li> <li>Workflow Development - Build pipelines</li> <li>Core Concepts - Understand patterns</li> </ul>"},{"location":"packages/phlo-grafana/","title":"phlo-grafana","text":"<p>Grafana visualization service for Phlo.</p>"},{"location":"packages/phlo-grafana/#overview","title":"Overview","text":"<p><code>phlo-grafana</code> provides metrics visualization and dashboards for observability. It comes pre-configured with datasources for Prometheus, Loki, Trino, and PostgreSQL.</p>"},{"location":"packages/phlo-grafana/#installation","title":"Installation","text":"<pre><code>pip install phlo-grafana\n# or\nphlo plugin install grafana\n</code></pre>"},{"location":"packages/phlo-grafana/#profile","title":"Profile","text":"<p>Part of the <code>observability</code> profile.</p>"},{"location":"packages/phlo-grafana/#configuration","title":"Configuration","text":"Variable Default Description <code>GRAFANA_PORT</code> <code>3003</code> Grafana web UI port <code>GRAFANA_VERSION</code> <code>11.3.1</code> Grafana version <code>GRAFANA_ADMIN_USER</code> <code>admin</code> Admin username <code>GRAFANA_ADMIN_PASSWORD</code> <code>admin</code> Admin password"},{"location":"packages/phlo-grafana/#features","title":"Features","text":""},{"location":"packages/phlo-grafana/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Datasources Pre-provisioned: Prometheus, Loki, Trino, PostgreSQL Dashboards Pre-provisioned dashboards in <code>grafana/dashboards/</code> Metrics Labels Exposes Grafana metrics for Prometheus"},{"location":"packages/phlo-grafana/#pre-configured-datasources","title":"Pre-Configured Datasources","text":"Datasource Type URL Prometheus prometheus http://prometheus:9090 Loki loki http://loki:3100 Trino trino http://trino:8080 PostgreSQL postgres postgres:5432"},{"location":"packages/phlo-grafana/#pre-built-dashboards","title":"Pre-Built Dashboards","text":"Dashboard Description Phlo Overview High-level system health Dagster Pipelines Pipeline execution metrics Data Quality Quality check results Trino Queries Query performance MinIO Storage Storage utilization"},{"location":"packages/phlo-grafana/#usage","title":"Usage","text":""},{"location":"packages/phlo-grafana/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start with observability profile\nphlo services start --profile observability\n\n# Or start individually\nphlo services start --service grafana\n</code></pre>"},{"location":"packages/phlo-grafana/#access","title":"Access","text":"<ul> <li>URL: <code>http://localhost:3003</code></li> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul>"},{"location":"packages/phlo-grafana/#creating-dashboards","title":"Creating Dashboards","text":"<ol> <li>Login to Grafana</li> <li>Navigate to Dashboards \u2192 New Dashboard</li> <li>Add panels with queries to your datasources</li> <li>Save and export JSON</li> </ol>"},{"location":"packages/phlo-grafana/#alerting","title":"Alerting","text":"<p>Configure alerts in Grafana:</p> <ol> <li>Edit a panel</li> <li>Go to Alert tab</li> <li>Configure conditions</li> <li>Add notification channels</li> </ol>"},{"location":"packages/phlo-grafana/#endpoints","title":"Endpoints","text":"Endpoint URL Web UI <code>http://localhost:3003</code> API <code>http://localhost:3003/api</code> Metrics <code>http://localhost:3003/metrics</code>"},{"location":"packages/phlo-grafana/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>GrafanaServicePlugin</code>"},{"location":"packages/phlo-grafana/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-prometheus - Metrics collection</li> <li>phlo-loki - Log aggregation</li> <li>phlo-alerting - Alert management</li> </ul>"},{"location":"packages/phlo-grafana/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Setup - Complete monitoring setup</li> <li>Operations Guide - Monitoring best practices</li> <li>Troubleshooting - Debug issues</li> </ul>"},{"location":"packages/phlo-hasura/","title":"phlo-hasura","text":"<p>Hasura GraphQL engine for Phlo.</p>"},{"location":"packages/phlo-hasura/#overview","title":"Overview","text":"<p><code>phlo-hasura</code> exposes PostgreSQL tables as a GraphQL API with real-time subscriptions.</p>"},{"location":"packages/phlo-hasura/#installation","title":"Installation","text":"<pre><code>pip install phlo-hasura\n# or\nphlo plugin install hasura\n</code></pre>"},{"location":"packages/phlo-hasura/#profile","title":"Profile","text":"<p>Part of the <code>api</code> profile.</p>"},{"location":"packages/phlo-hasura/#configuration","title":"Configuration","text":"Variable Default Description <code>HASURA_PORT</code> <code>8082</code> Hasura console/API port <code>HASURA_VERSION</code> <code>v2.46.0</code> Hasura version <code>HASURA_ADMIN_SECRET</code> <code>phlo-hasura-admin-secret</code> Admin secret"},{"location":"packages/phlo-hasura/#features","title":"Features","text":""},{"location":"packages/phlo-hasura/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Table Tracking Auto-tracks tables in <code>api</code> schema via post_start hook Metrics Labels Exposes Hasura metrics at <code>/v1/metrics</code> Anonymous Role Configured with <code>anonymous</code> role for public access"},{"location":"packages/phlo-hasura/#post-start-hook","title":"Post-Start Hook","text":"<pre><code>hooks:\n  post_start:\n    - name: track-tables\n      command: python -m phlo_hasura.hooks track-tables api\n</code></pre>"},{"location":"packages/phlo-hasura/#usage","title":"Usage","text":""},{"location":"packages/phlo-hasura/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start with API profile\nphlo services start --profile api\n\n# Or start individually\nphlo services start --service hasura\n</code></pre>"},{"location":"packages/phlo-hasura/#graphql-examples","title":"GraphQL Examples","text":"<pre><code># Query marts data\nquery GetDailySummary {\n  mrt_daily_summary(order_by: { date: desc }, limit: 10) {\n    date\n    total_count\n    average_value\n  }\n}\n\n# Filter data\nquery GetHighValues {\n  mrt_daily_summary(where: { average_value: { _gt: 100 } }) {\n    date\n    average_value\n  }\n}\n\n# Aggregation\nquery GetStats {\n  mrt_daily_summary_aggregate {\n    aggregate {\n      count\n      avg {\n        average_value\n      }\n      max {\n        average_value\n      }\n    }\n  }\n}\n\n# Real-time subscription\nsubscription WatchData {\n  mrt_daily_summary(order_by: { date: desc }, limit: 5) {\n    date\n    total_count\n  }\n}\n</code></pre>"},{"location":"packages/phlo-hasura/#curl-examples","title":"cURL Examples","text":"<pre><code># Query with admin secret\ncurl -X POST http://localhost:8082/v1/graphql \\\n  -H \"X-Hasura-Admin-Secret: phlo-hasura-admin-secret\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"{ mrt_daily_summary(limit: 5) { date total_count } }\"}'\n</code></pre>"},{"location":"packages/phlo-hasura/#endpoints","title":"Endpoints","text":"Endpoint URL GraphQL <code>http://localhost:8082/v1/graphql</code> Console <code>http://localhost:8082/console</code> Metrics <code>http://localhost:8082/v1/metrics</code>"},{"location":"packages/phlo-hasura/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>HasuraServicePlugin</code> <code>phlo.plugins.cli</code> Hasura CLI commands"},{"location":"packages/phlo-hasura/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-postgres - Database service</li> <li>phlo-postgrest - REST alternative</li> <li>phlo-api - Custom API endpoints</li> </ul>"},{"location":"packages/phlo-hasura/#next-steps","title":"Next Steps","text":"<ul> <li>Hasura Setup - Complete configuration</li> <li>API Reference - API documentation</li> </ul>"},{"location":"packages/phlo-iceberg/","title":"phlo-iceberg","text":"<p>Apache Iceberg catalog integration for Phlo.</p>"},{"location":"packages/phlo-iceberg/#overview","title":"Overview","text":"<p><code>phlo-iceberg</code> provides PyIceberg resources for Dagster and Trino catalog configuration. It enables ACID transactions, schema evolution, and time travel on the data lakehouse.</p>"},{"location":"packages/phlo-iceberg/#installation","title":"Installation","text":"<pre><code>pip install phlo-iceberg\n# or\nphlo plugin install iceberg\n</code></pre>"},{"location":"packages/phlo-iceberg/#configuration","title":"Configuration","text":"Variable Required Default Description <code>ICEBERG_WAREHOUSE_PATH</code> Yes <code>s3://lake/warehouse</code> S3 path for Iceberg warehouse <code>ICEBERG_STAGING_PATH</code> No <code>s3://lake/stage</code> S3 path for staging <code>ICEBERG_DEFAULT_NAMESPACE</code> No <code>raw</code> Default namespace/schema <code>ICEBERG_NESSIE_REF</code> No <code>main</code> Default Nessie branch/tag <code>NESSIE_HOST</code> No <code>nessie</code> Nessie catalog host <code>NESSIE_PORT</code> No <code>19120</code> Nessie REST API port <p>S3 Access: Configure AWS credentials via <code>~/.aws/credentials</code> or <code>AWS_ACCESS_KEY_ID</code>/<code>AWS_SECRET_ACCESS_KEY</code> env vars. When using MinIO, these are set automatically.</p>"},{"location":"packages/phlo-iceberg/#features","title":"Features","text":""},{"location":"packages/phlo-iceberg/#auto-configuration","title":"Auto-Configuration","text":"<p>Works out-of-the-box when MinIO and Nessie are running:</p> Feature How It Works Dagster Resource <code>IcebergResource</code> auto-registered via entry points Trino Catalogs Registers <code>iceberg</code> and <code>iceberg_dev</code> catalogs via <code>phlo.plugins.trino_catalogs</code> Catalog Generation Catalog <code>.properties</code> files auto-generated at Trino startup"},{"location":"packages/phlo-iceberg/#trino-catalog-entry-points","title":"Trino Catalog Entry Points","text":"<pre><code>[project.entry-points.\"phlo.plugins.trino_catalogs\"]\niceberg = \"phlo_iceberg.catalog_plugin:IcebergCatalogPlugin\"\niceberg_dev = \"phlo_iceberg.catalog_plugin:IcebergDevCatalogPlugin\"\n</code></pre>"},{"location":"packages/phlo-iceberg/#usage","title":"Usage","text":""},{"location":"packages/phlo-iceberg/#dagster-resource","title":"Dagster Resource","text":"<pre><code>from dagster import asset\nfrom phlo_iceberg.resource import IcebergResource\n\n@asset\ndef my_asset(iceberg: IcebergResource):\n    catalog = iceberg.load_catalog()\n    table = catalog.load_table(\"bronze.users\")\n    return table.scan().to_pandas()\n</code></pre>"},{"location":"packages/phlo-iceberg/#direct-usage","title":"Direct Usage","text":"<pre><code>from phlo.config import get_settings\n\n# Get PyIceberg catalog configuration\nconfig = get_settings().get_pyiceberg_catalog_config(\"main\")\n\n# Use with PyIceberg\nfrom pyiceberg.catalog import load_catalog\ncatalog = load_catalog(\"nessie\", **config)\n</code></pre>"},{"location":"packages/phlo-iceberg/#time-travel","title":"Time Travel","text":"<pre><code># Query specific snapshot\ntable = catalog.load_table(\"bronze.users\")\nsnapshots = table.snapshots()\n\n# Read from specific snapshot\ndf = table.scan().using(snapshot_id=snapshot_id).to_pandas()\n</code></pre>"},{"location":"packages/phlo-iceberg/#branch-aware-operations","title":"Branch-Aware Operations","text":"<pre><code># Load catalog for specific branch\nconfig = get_settings().get_pyiceberg_catalog_config(\"dev\")\ncatalog = load_catalog(\"nessie\", **config)\n\n# All operations now target the 'dev' branch\ntable = catalog.load_table(\"bronze.users\")\n</code></pre>"},{"location":"packages/phlo-iceberg/#trino-integration","title":"Trino Integration","text":"<p>Once running, query Iceberg tables via Trino:</p> <pre><code>-- Query from main branch\nSELECT * FROM iceberg.bronze.users LIMIT 10;\n\n-- Query from dev branch (using iceberg_dev catalog)\nSELECT * FROM iceberg_dev.bronze.users LIMIT 10;\n\n-- Time travel\nSELECT * FROM iceberg.bronze.users FOR VERSION AS OF 123456789;\n</code></pre>"},{"location":"packages/phlo-iceberg/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.dagster</code> <code>IcebergDagsterPlugin</code> with <code>IcebergResource</code> <code>phlo.plugins.trino_catalogs</code> Iceberg catalog configurations"},{"location":"packages/phlo-iceberg/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-nessie - Git-like catalog</li> <li>phlo-trino - Query engine</li> <li>phlo-minio - Object storage</li> <li>phlo-dlt - Data ingestion</li> </ul>"},{"location":"packages/phlo-iceberg/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Reference - System design</li> <li>DuckDB Queries - Ad-hoc analysis</li> <li>Core Concepts - Understand patterns</li> </ul>"},{"location":"packages/phlo-lineage/","title":"phlo-lineage","text":"<p>Row-level data lineage tracking for Phlo.</p>"},{"location":"packages/phlo-lineage/#overview","title":"Overview","text":"<p><code>phlo-lineage</code> tracks data lineage at the row level, enabling you to trace individual records back through transformations to their source. It integrates with DLT and dbt pipelines.</p>"},{"location":"packages/phlo-lineage/#installation","title":"Installation","text":"<pre><code>pip install phlo-lineage\n# or\nphlo plugin install lineage\n</code></pre>"},{"location":"packages/phlo-lineage/#configuration","title":"Configuration","text":"Variable Default Description <code>LINEAGE_DB_URL</code> - PostgreSQL DSN for lineage store"},{"location":"packages/phlo-lineage/#features","title":"Features","text":""},{"location":"packages/phlo-lineage/#auto-configuration","title":"Auto-Configuration","text":"<p>Auto-wired when <code>LINEAGE_DB_URL</code> is set:</p> Feature How It Works Hook Registration Receives <code>lineage.edges</code> events via HookBus Event Capture Automatically captures lineage from <code>@phlo_ingestion</code> and dbt Row ID Injection <code>_phlo_row_id</code> column auto-injected during ingestion <p>Note: If <code>LINEAGE_DB_URL</code> is not configured, lineage events are logged but not persisted.</p>"},{"location":"packages/phlo-lineage/#event-flow","title":"Event Flow","text":"<pre><code>Ingestion/Transform \u2192 emit lineage.edges \u2192 LineageHookPlugin \u2192 PostgreSQL\n</code></pre>"},{"location":"packages/phlo-lineage/#row-level-tracking","title":"Row-Level Tracking","text":"<p>Each row gets a unique <code>_phlo_row_id</code> that allows tracing:</p> <pre><code>-- Find the source of a specific row\nSELECT * FROM lineage.edges\nWHERE target_row_id = 'abc123';\n\n-- Trace full lineage chain\nWITH RECURSIVE chain AS (\n    SELECT * FROM lineage.edges WHERE target_row_id = 'abc123'\n    UNION ALL\n    SELECT e.* FROM lineage.edges e\n    JOIN chain c ON e.target_row_id = c.source_row_id\n)\nSELECT * FROM chain;\n</code></pre>"},{"location":"packages/phlo-lineage/#usage","title":"Usage","text":""},{"location":"packages/phlo-lineage/#cli-commands","title":"CLI Commands","text":"<pre><code># Query lineage for a table\nphlo lineage show bronze.users\n\n# Show upstream dependencies\nphlo lineage show bronze.users --upstream\n\n# Show downstream consumers\nphlo lineage show bronze.users --downstream\n\n# Export lineage graph\nphlo lineage export --format json &gt; lineage.json\nphlo lineage export --format dot &gt; lineage.dot\n</code></pre>"},{"location":"packages/phlo-lineage/#programmatic","title":"Programmatic","text":"<pre><code>from phlo_lineage import get_lineage\n\n# Get upstream lineage for a table\nupstream = get_lineage(\"gold.fct_orders\", direction=\"upstream\")\n\n# Get downstream lineage\ndownstream = get_lineage(\"bronze.raw_events\", direction=\"downstream\")\n\n# Get full lineage graph\ngraph = get_lineage(\"silver.users\", direction=\"both\", depth=3)\n</code></pre>"},{"location":"packages/phlo-lineage/#viewing-in-observatory","title":"Viewing in Observatory","text":"<p>The Observatory UI provides visual lineage graphs:</p> <ol> <li>Navigate to a table in Data Explorer</li> <li>Click the \"Lineage\" tab</li> <li>Use the graph to explore dependencies</li> </ol>"},{"location":"packages/phlo-lineage/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.cli</code> <code>lineage</code> CLI commands <code>phlo.plugins.hooks</code> <code>LineageHookPlugin</code> for event capture"},{"location":"packages/phlo-lineage/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-observatory - Lineage visualization</li> <li>phlo-dlt - Ingestion lineage</li> <li>phlo-dbt - Transformation lineage</li> <li>phlo-openmetadata - Catalog integration</li> </ul>"},{"location":"packages/phlo-lineage/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Reference - System design</li> <li>Developer Guide - Building pipelines</li> </ul>"},{"location":"packages/phlo-loki/","title":"phlo-loki","text":"<p>Loki log aggregation for Phlo.</p>"},{"location":"packages/phlo-loki/#overview","title":"Overview","text":"<p><code>phlo-loki</code> provides centralized log aggregation for the Phlo observability stack. It collects logs from all services and makes them queryable via Grafana.</p>"},{"location":"packages/phlo-loki/#installation","title":"Installation","text":"<pre><code>pip install phlo-loki\n# or\nphlo plugin install loki\n</code></pre>"},{"location":"packages/phlo-loki/#profile","title":"Profile","text":"<p>Part of the <code>observability</code> profile.</p>"},{"location":"packages/phlo-loki/#configuration","title":"Configuration","text":"Variable Default Description <code>LOKI_PORT</code> <code>3100</code> Loki API port <code>LOKI_RETENTION_DAYS</code> <code>7</code> Log retention period"},{"location":"packages/phlo-loki/#features","title":"Features","text":""},{"location":"packages/phlo-loki/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Log Collection All Docker container logs collected via Alloy Label Enrichment Auto-labels with service name, container, etc. Grafana Integration Pre-configured as Grafana datasource"},{"location":"packages/phlo-loki/#usage","title":"Usage","text":""},{"location":"packages/phlo-loki/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start with observability profile\nphlo services start --profile observability\n\n# Or start individually\nphlo services start --service loki\n</code></pre>"},{"location":"packages/phlo-loki/#querying-logs","title":"Querying Logs","text":"<p>Access logs via Grafana's Explore view:</p> <pre><code># All Dagster logs\n{service=\"dagster\"}\n\n# Error logs from any service\n{} |= \"error\"\n\n# Pipeline-specific logs\n{service=\"dagster\"} |~ \"pipeline.*run\"\n\n# Logs with JSON parsing\n{service=\"dagster\"} | json | level=\"ERROR\"\n</code></pre>"},{"location":"packages/phlo-loki/#log-labels","title":"Log Labels","text":"<p>Logs are labeled with:</p> Label Description <code>service</code> Service name (dagster, trino, etc.) <code>container</code> Container name <code>level</code> Log level (INFO, ERROR, etc.) <code>job</code> Job/pipeline name"},{"location":"packages/phlo-loki/#endpoints","title":"Endpoints","text":"Endpoint URL API <code>http://localhost:3100</code> Push <code>http://localhost:3100/loki/api/v1/push</code> Query <code>http://localhost:3100/loki/api/v1/query</code>"},{"location":"packages/phlo-loki/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>LokiServicePlugin</code>"},{"location":"packages/phlo-loki/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-grafana - Visualization</li> <li>phlo-alloy - Log shipping</li> <li>phlo-prometheus - Metrics</li> </ul>"},{"location":"packages/phlo-loki/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Setup - Complete monitoring setup</li> <li>Troubleshooting Guide - Debug with logs</li> </ul>"},{"location":"packages/phlo-metrics/","title":"phlo-metrics","text":"<p>Metrics collection and export for Phlo pipelines.</p>"},{"location":"packages/phlo-metrics/#overview","title":"Overview","text":"<p><code>phlo-metrics</code> collects metrics from pipeline executions, quality checks, and system events. It exports to Prometheus format for scraping.</p>"},{"location":"packages/phlo-metrics/#installation","title":"Installation","text":"<pre><code>pip install phlo-metrics\n# or\nphlo plugin install metrics\n</code></pre>"},{"location":"packages/phlo-metrics/#configuration","title":"Configuration","text":"Variable Default Description <code>PROMETHEUS_PUSHGATEWAY_URL</code> - Optional pushgateway URL"},{"location":"packages/phlo-metrics/#features","title":"Features","text":""},{"location":"packages/phlo-metrics/#auto-configuration","title":"Auto-Configuration","text":"<p>Auto-wires with HookBus for event collection:</p> Feature How It Works Hook Registration Receives all events via HookBus Metric Collection Auto-increments counters and gauges from events Prometheus Format Exports in Prometheus text format"},{"location":"packages/phlo-metrics/#exposure","title":"Exposure","text":"<ul> <li>Default: Metrics available via CLI (<code>phlo metrics show</code>) or export</li> <li>With Pushgateway: Set <code>PROMETHEUS_PUSHGATEWAY_URL</code> to push metrics to a gateway</li> </ul>"},{"location":"packages/phlo-metrics/#collected-metrics","title":"Collected Metrics","text":"Metric Type Description <code>phlo_ingestion_total</code> Counter Total ingestion runs <code>phlo_ingestion_rows</code> Counter Rows ingested <code>phlo_ingestion_duration</code> Histogram Ingestion duration <code>phlo_quality_checks_total</code> Counter Quality checks executed <code>phlo_quality_failures</code> Counter Failed quality checks <code>phlo_transform_total</code> Counter Transform executions <code>phlo_transform_duration</code> Histogram Transform duration"},{"location":"packages/phlo-metrics/#usage","title":"Usage","text":""},{"location":"packages/phlo-metrics/#cli-commands","title":"CLI Commands","text":"<pre><code># View current metrics\nphlo metrics show\n\n# Export metrics to file\nphlo metrics export --format prometheus &gt; metrics.prom\n\n# Reset metrics\nphlo metrics reset\n</code></pre>"},{"location":"packages/phlo-metrics/#programmatic","title":"Programmatic","text":"<pre><code>from phlo_metrics.collector import MetricsCollector\n\ncollector = MetricsCollector()\n\n# Increment a counter\ncollector.increment(\"custom_metric\", labels={\"source\": \"api\"})\n\n# Set a gauge\ncollector.set_gauge(\"active_pipelines\", 5)\n\n# Record a histogram observation\ncollector.observe(\"request_duration\", 0.5, labels={\"endpoint\": \"/api/data\"})\n</code></pre>"},{"location":"packages/phlo-metrics/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.cli</code> <code>metrics</code> CLI commands <code>phlo.plugins.hooks</code> <code>MetricsHookPlugin</code> for event handling"},{"location":"packages/phlo-metrics/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-prometheus - Metrics storage</li> <li>phlo-grafana - Metrics visualization</li> <li>phlo-alerting - Alert routing</li> </ul>"},{"location":"packages/phlo-metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Setup - Monitoring setup</li> <li>Operations Guide - Metrics best practices</li> </ul>"},{"location":"packages/phlo-minio/","title":"phlo-minio","text":"<p>MinIO S3-compatible object storage for Phlo.</p>"},{"location":"packages/phlo-minio/#overview","title":"Overview","text":"<p><code>phlo-minio</code> provides S3-compatible object storage for the data lake. It stores Iceberg table data, staging files, and backups.</p>"},{"location":"packages/phlo-minio/#installation","title":"Installation","text":"<pre><code>pip install phlo-minio\n# or\nphlo plugin install minio\n</code></pre>"},{"location":"packages/phlo-minio/#configuration","title":"Configuration","text":"Variable Default Description <code>MINIO_ROOT_USER</code> <code>minio</code> Root username <code>MINIO_ROOT_PASSWORD</code> <code>minio123</code> Root password <code>MINIO_API_PORT</code> <code>9000</code> S3 API port <code>MINIO_CONSOLE_PORT</code> <code>9001</code> Web console port <code>MINIO_SERVER_URL</code> - TLS server URL <code>MINIO_OIDC_CONFIG_URL</code> - OIDC provider config URL <code>MINIO_AUTO_ENCRYPTION</code> <code>off</code> Auto-encryption mode <code>MINIO_AUDIT_ENABLED</code> <code>off</code> Audit logging"},{"location":"packages/phlo-minio/#features","title":"Features","text":""},{"location":"packages/phlo-minio/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Metrics Labels Exposes MinIO metrics at <code>/minio/v2/metrics/cluster</code> Prometheus Scraping Auto-scraped by Prometheus via Docker labels Volume Mounting Persists data to <code>./volumes/minio</code>"},{"location":"packages/phlo-minio/#default-buckets","title":"Default Buckets","text":"Bucket Purpose <code>lake</code> Main data lake storage <code>lake/warehouse</code> Iceberg table data <code>lake/stage</code> Ingestion staging area"},{"location":"packages/phlo-minio/#usage","title":"Usage","text":""},{"location":"packages/phlo-minio/#starting-the-service","title":"Starting the Service","text":"<pre><code>phlo services start --service minio\n</code></pre>"},{"location":"packages/phlo-minio/#web-console","title":"Web Console","text":"<p>Access the MinIO console at <code>http://localhost:9001</code>:</p> <ul> <li>Username: <code>minio</code> (or <code>MINIO_ROOT_USER</code>)</li> <li>Password: <code>minio123</code> (or <code>MINIO_ROOT_PASSWORD</code>)</li> </ul>"},{"location":"packages/phlo-minio/#aws-cli","title":"AWS CLI","text":"<pre><code># Configure AWS CLI for MinIO\naws configure set aws_access_key_id minio\naws configure set aws_secret_access_key minio123\n\n# List buckets\naws --endpoint-url http://localhost:9000 s3 ls\n\n# List warehouse contents\naws --endpoint-url http://localhost:9000 s3 ls s3://lake/warehouse/\n\n# Copy file to staging\naws --endpoint-url http://localhost:9000 s3 cp data.parquet s3://lake/stage/\n</code></pre>"},{"location":"packages/phlo-minio/#python-boto3","title":"Python (boto3)","text":"<pre><code>import boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='http://localhost:9000',\n    aws_access_key_id='minio',\n    aws_secret_access_key='minio123'\n)\n\n# List objects\nresponse = s3.list_objects_v2(Bucket='lake', Prefix='warehouse/')\nfor obj in response.get('Contents', []):\n    print(obj['Key'])\n</code></pre>"},{"location":"packages/phlo-minio/#endpoints","title":"Endpoints","text":"Endpoint URL S3 API <code>http://localhost:9000</code> Console <code>http://localhost:9001</code> Metrics <code>http://localhost:9000/minio/v2/metrics/cluster</code>"},{"location":"packages/phlo-minio/#metrics-integration","title":"Metrics Integration","text":"<p>MinIO metrics are automatically scraped by Prometheus:</p> <pre><code>compose:\n  labels:\n    phlo.metrics.enabled: \"true\"\n    phlo.metrics.port: \"minio:9000\"\n    phlo.metrics.path: \"/minio/v2/metrics/cluster\"\n</code></pre>"},{"location":"packages/phlo-minio/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>MinioServicePlugin</code>"},{"location":"packages/phlo-minio/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-iceberg - Table format</li> <li>phlo-nessie - Catalog service</li> <li>phlo-prometheus - Metrics collection</li> </ul>"},{"location":"packages/phlo-minio/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Complete setup</li> <li>Architecture Reference - System design</li> <li>Operations Guide - Backup and maintenance</li> </ul>"},{"location":"packages/phlo-nessie/","title":"phlo-nessie","text":"<p>Nessie Git-like catalog for Phlo.</p>"},{"location":"packages/phlo-nessie/#overview","title":"Overview","text":"<p><code>phlo-nessie</code> provides Git-like version control for Iceberg tables. It enables branching, merging, and time travel for the data lakehouse, allowing teams to work on data in isolation before merging to production.</p>"},{"location":"packages/phlo-nessie/#installation","title":"Installation","text":"<pre><code>pip install phlo-nessie\n# or\nphlo plugin install nessie\n</code></pre>"},{"location":"packages/phlo-nessie/#configuration","title":"Configuration","text":"Variable Default Description <code>NESSIE_PORT</code> <code>19120</code> Nessie API port <code>NESSIE_VERSION</code> <code>0.106.0</code> Nessie version <code>NESSIE_OIDC_ENABLED</code> <code>false</code> Enable OIDC authentication <code>NESSIE_AUTHZ_ENABLED</code> <code>false</code> Enable authorization"},{"location":"packages/phlo-nessie/#features","title":"Features","text":""},{"location":"packages/phlo-nessie/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Branch Init Auto-creates <code>main</code> and <code>dev</code> branches via post_start hook Metrics Labels Exposes Quarkus metrics at <code>/q/metrics</code> Postgres Storage Uses PostgreSQL for version store (default backend)"},{"location":"packages/phlo-nessie/#post-start-hook","title":"Post-Start Hook","text":"<pre><code>hooks:\n  post_start:\n    - name: init-branches\n      command: python -m phlo_nessie.hooks init-branches\n</code></pre>"},{"location":"packages/phlo-nessie/#usage","title":"Usage","text":""},{"location":"packages/phlo-nessie/#cli-commands","title":"CLI Commands","text":"<pre><code># Start Nessie\nphlo services start --service nessie\n\n# List branches\nphlo branch list\n\n# Create a new branch\nphlo branch create feature/my-feature\n\n# Delete a branch\nphlo branch delete feature/my-feature\n\n# Merge branches\nphlo branch merge dev main\n</code></pre>"},{"location":"packages/phlo-nessie/#write-audit-publish-pattern","title":"Write-Audit-Publish Pattern","text":"<p>Phlo uses Nessie branches for the Write-Audit-Publish (WAP) pattern:</p> <pre><code>1. Write Phase\n   \u2514\u2500\u2500 Data lands on isolated branch: pipeline/run-{run_id}\n\n2. Audit Phase\n   \u2514\u2500\u2500 Quality checks validate data on the branch\n\n3. Publish Phase\n   \u2514\u2500\u2500 Auto-promotion sensor merges to main when checks pass\n</code></pre>"},{"location":"packages/phlo-nessie/#branch-lifecycle","title":"Branch Lifecycle","text":"<pre><code># Create feature branch\nfrom phlo_nessie.client import NessieClient\n\nclient = NessieClient()\nclient.create_branch(\"feature/new-data\", from_ref=\"main\")\n\n# Work on the branch...\n\n# Merge when ready\nclient.merge(\"feature/new-data\", \"main\")\n\n# Clean up\nclient.delete_branch(\"feature/new-data\")\n</code></pre>"},{"location":"packages/phlo-nessie/#branching-strategy","title":"Branching Strategy","text":"Branch Pattern Purpose <code>main</code> Production data (read-only for most) <code>dev</code> Development workspace <code>pipeline/run-{id}</code> Isolated pipeline execution <code>feature/*</code> Feature development"},{"location":"packages/phlo-nessie/#endpoints","title":"Endpoints","text":"Endpoint URL API v1 <code>http://localhost:19120/api/v1</code> API v2 <code>http://localhost:19120/api/v2</code> Iceberg REST <code>http://localhost:19120/iceberg</code> Metrics <code>http://localhost:19120/q/metrics</code>"},{"location":"packages/phlo-nessie/#api-examples","title":"API Examples","text":"<pre><code># Get server config\ncurl http://localhost:19120/api/v2/config\n\n# List branches\ncurl http://localhost:19120/api/v2/trees\n\n# Get branch details\ncurl http://localhost:19120/api/v2/trees/main\n</code></pre>"},{"location":"packages/phlo-nessie/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>NessieServicePlugin</code> <code>phlo.plugins.cli</code> Nessie CLI commands"},{"location":"packages/phlo-nessie/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-iceberg - Iceberg table format</li> <li>phlo-trino - Query engine</li> <li>phlo-postgres - Backend storage</li> </ul>"},{"location":"packages/phlo-nessie/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Understand WAP pattern</li> <li>Architecture Reference - System design</li> <li>Workflow Development - Build pipelines</li> </ul>"},{"location":"packages/phlo-observatory/","title":"phlo-observatory","text":"<p>Phlo Observatory UI for data platform visibility.</p>"},{"location":"packages/phlo-observatory/#overview","title":"Overview","text":"<p><code>phlo-observatory</code> is a web-based UI for exploring the data lakehouse. It enables viewing lineage, browsing tables, running queries, and monitoring pipeline health.</p>"},{"location":"packages/phlo-observatory/#installation","title":"Installation","text":"<pre><code>pip install phlo-observatory\n# or\nphlo plugin install observatory\n</code></pre>"},{"location":"packages/phlo-observatory/#configuration","title":"Configuration","text":"Variable Default Description <code>OBSERVATORY_PORT</code> <code>3001</code> Observatory web UI port <code>DAGSTER_GRAPHQL_URL</code> <code>http://dagster:3000/graphql</code> Dagster GraphQL endpoint <code>NESSIE_URL</code> <code>http://nessie:19120/api/v2</code> Nessie API URL <code>TRINO_URL</code> <code>http://trino:8080</code> Trino HTTP URL <code>PHLO_API_URL</code> <code>http://phlo-api:4000</code> Phlo API URL"},{"location":"packages/phlo-observatory/#features","title":"Features","text":""},{"location":"packages/phlo-observatory/#core-capabilities","title":"Core Capabilities","text":"Feature Description Data Explorer Browse tables, view schemas, preview data Lineage Graph Interactive visualization of data flow Asset Browser View Dagster assets and materialization status Quality Dashboard Monitor quality check results Branch Management Create, view, and merge Nessie branches SQL Workbench Execute ad-hoc queries against Trino"},{"location":"packages/phlo-observatory/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works API Connection Connects to phlo-api for backend data Service URLs Auto-configured from environment variables Dev Mode Hot-reloading in <code>--dev</code> mode"},{"location":"packages/phlo-observatory/#usage","title":"Usage","text":""},{"location":"packages/phlo-observatory/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start Observatory\nphlo services start --service observatory\n\n# Start with native mode (better for ARM Macs)\nphlo services start --native\n\n# Start with dev mode (hot-reload)\nphlo services start --dev\n</code></pre>"},{"location":"packages/phlo-observatory/#accessing-the-ui","title":"Accessing the UI","text":"<p>Open <code>http://localhost:3001</code> in your browser.</p>"},{"location":"packages/phlo-observatory/#ui-sections","title":"UI Sections","text":""},{"location":"packages/phlo-observatory/#data-explorer","title":"Data Explorer","text":"<p>Browse and explore your data lakehouse:</p> <ul> <li>View all schemas and tables</li> <li>Inspect table schemas and statistics</li> <li>Preview data with pagination</li> <li>Export query results</li> </ul>"},{"location":"packages/phlo-observatory/#lineage-graph","title":"Lineage Graph","text":"<p>Visualize data dependencies:</p> <ul> <li>Interactive node-based graph</li> <li>Click tables to see details</li> <li>Filter by upstream/downstream</li> <li>Highlight specific paths</li> </ul>"},{"location":"packages/phlo-observatory/#asset-browser","title":"Asset Browser","text":"<p>Monitor Dagster assets:</p> <ul> <li>View materialization status</li> <li>See last run timestamps</li> <li>Check freshness policies</li> <li>Trigger materializations</li> </ul>"},{"location":"packages/phlo-observatory/#quality-dashboard","title":"Quality Dashboard","text":"<p>Track data quality:</p> <ul> <li>View check results over time</li> <li>Filter by status (pass/fail)</li> <li>Drill into failure details</li> <li>See violation samples</li> </ul>"},{"location":"packages/phlo-observatory/#branch-manager","title":"Branch Manager","text":"<p>Work with Nessie branches:</p> <ul> <li>List all branches</li> <li>Create new branches</li> <li>Compare branches</li> <li>Merge branches</li> </ul>"},{"location":"packages/phlo-observatory/#sql-workbench","title":"SQL Workbench","text":"<p>Run ad-hoc queries:</p> <ul> <li>Syntax highlighting</li> <li>Auto-complete for tables</li> <li>Result pagination</li> <li>Export to CSV</li> </ul>"},{"location":"packages/phlo-observatory/#endpoints","title":"Endpoints","text":"Endpoint URL Web UI <code>http://localhost:3001</code>"},{"location":"packages/phlo-observatory/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>ObservatoryServicePlugin</code>"},{"location":"packages/phlo-observatory/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-api - Backend API</li> <li>phlo-lineage - Lineage data</li> <li>phlo-quality - Quality checks</li> <li>phlo-nessie - Branch management</li> </ul>"},{"location":"packages/phlo-observatory/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Complete setup</li> <li>Quickstart - First steps</li> <li>Troubleshooting - Debug issues</li> </ul>"},{"location":"packages/phlo-openmetadata/","title":"phlo-openmetadata","text":"<p>OpenMetadata integration for Phlo.</p>"},{"location":"packages/phlo-openmetadata/#overview","title":"Overview","text":"<p><code>phlo-openmetadata</code> syncs table metadata, lineage, and quality check results to OpenMetadata for data governance and discovery.</p>"},{"location":"packages/phlo-openmetadata/#installation","title":"Installation","text":"<pre><code>pip install phlo-openmetadata\n# or\nphlo plugin install openmetadata\n</code></pre>"},{"location":"packages/phlo-openmetadata/#configuration","title":"Configuration","text":"Variable Default Description <code>OPENMETADATA_HOST</code> <code>openmetadata-server</code> OpenMetadata server host <code>OPENMETADATA_PORT</code> <code>8585</code> OpenMetadata API port <code>OPENMETADATA_USERNAME</code> <code>admin</code> Admin username <code>OPENMETADATA_PASSWORD</code> <code>admin</code> Admin password <code>OPENMETADATA_VERIFY_SSL</code> <code>false</code> Verify SSL certificates <code>OPENMETADATA_SYNC_ENABLED</code> <code>true</code> Enable automatic sync <code>OPENMETADATA_SYNC_INTERVAL_SECONDS</code> <code>300</code> Min interval between syncs"},{"location":"packages/phlo-openmetadata/#features","title":"Features","text":""},{"location":"packages/phlo-openmetadata/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Hook Registration Receives <code>lineage.edges</code>, <code>quality.result</code>, <code>publish.end</code> events Lineage Sync Automatically syncs lineage edges to OpenMetadata Quality Results Syncs quality check results as test cases Table Metadata Syncs published tables with documentation"},{"location":"packages/phlo-openmetadata/#event-flow","title":"Event Flow","text":"<pre><code>Pipeline Events \u2192 HookBus \u2192 OpenMetadataHookPlugin \u2192 OpenMetadata API\n</code></pre>"},{"location":"packages/phlo-openmetadata/#synced-data","title":"Synced Data","text":"Data Type Description Tables Schema, columns, descriptions Lineage Table-to-table and column-level lineage Quality Test case results and scores Tags Domain and classification tags"},{"location":"packages/phlo-openmetadata/#usage","title":"Usage","text":""},{"location":"packages/phlo-openmetadata/#cli-commands","title":"CLI Commands","text":"<pre><code># Manually sync all tables\nphlo openmetadata sync\n\n# Sync specific table\nphlo openmetadata sync --table bronze.users\n\n# Check sync status\nphlo openmetadata status\n</code></pre>"},{"location":"packages/phlo-openmetadata/#programmatic","title":"Programmatic","text":"<pre><code>from phlo_openmetadata.client import OpenMetadataClient\n\nclient = OpenMetadataClient()\n\n# Sync table metadata\nclient.sync_table_metadata(\"bronze.users\")\n\n# Sync lineage\nclient.sync_lineage_edge(\"bronze.raw_events\", \"silver.events\")\n\n# Add quality result\nclient.add_quality_result(\n    table=\"bronze.users\",\n    check_name=\"null_check\",\n    passed=True\n)\n</code></pre>"},{"location":"packages/phlo-openmetadata/#accessing-openmetadata-ui","title":"Accessing OpenMetadata UI","text":"<p>Open <code>http://localhost:8585</code> in your browser:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul>"},{"location":"packages/phlo-openmetadata/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.cli</code> <code>openmetadata</code> CLI commands <code>phlo.plugins.hooks</code> <code>OpenMetadataHookPlugin</code> for sync"},{"location":"packages/phlo-openmetadata/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-lineage - Lineage tracking</li> <li>phlo-quality - Quality checks</li> <li>phlo-dagster - Asset metadata</li> </ul>"},{"location":"packages/phlo-openmetadata/#next-steps","title":"Next Steps","text":"<ul> <li>OpenMetadata Setup - Complete configuration</li> <li>Architecture Reference - System design</li> </ul>"},{"location":"packages/phlo-pgweb/","title":"phlo-pgweb","text":"<p>pgweb database browser for Phlo.</p>"},{"location":"packages/phlo-pgweb/#overview","title":"Overview","text":"<p><code>phlo-pgweb</code> provides a web-based PostgreSQL database browser for exploring metadata, lineage store, and operational data.</p>"},{"location":"packages/phlo-pgweb/#installation","title":"Installation","text":"<pre><code>pip install phlo-pgweb\n# or\nphlo plugin install pgweb\n</code></pre>"},{"location":"packages/phlo-pgweb/#configuration","title":"Configuration","text":"Variable Default Description <code>PGWEB_PORT</code> <code>8081</code> Web UI port <code>POSTGRES_USER</code> <code>phlo</code> Database user <code>POSTGRES_PASSWORD</code> <code>phlo</code> Database password <code>POSTGRES_DB</code> <code>phlo</code> Database name"},{"location":"packages/phlo-pgweb/#features","title":"Features","text":""},{"location":"packages/phlo-pgweb/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Database Connection Auto-connects to Phlo's PostgreSQL using <code>DATABASE_URL</code> Service Dependencies Depends on <code>postgres</code> service"},{"location":"packages/phlo-pgweb/#usage","title":"Usage","text":""},{"location":"packages/phlo-pgweb/#starting-the-service","title":"Starting the Service","text":"<pre><code>phlo services start --service pgweb\n</code></pre>"},{"location":"packages/phlo-pgweb/#accessing-pgweb","title":"Accessing pgweb","text":"<p>Open <code>http://localhost:8081</code> in your browser.</p>"},{"location":"packages/phlo-pgweb/#features_1","title":"Features","text":"<ul> <li>Query Editor: Write and execute SQL queries</li> <li>Table Browser: Explore tables and columns</li> <li>Data Export: Export query results to CSV</li> <li>Connection Info: View database connection details</li> <li>Table Statistics: View row counts and sizes</li> </ul>"},{"location":"packages/phlo-pgweb/#dependencies","title":"Dependencies","text":"<ul> <li>postgres</li> </ul>"},{"location":"packages/phlo-pgweb/#endpoints","title":"Endpoints","text":"Endpoint URL Web UI <code>http://localhost:8081</code>"},{"location":"packages/phlo-pgweb/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>PgwebServicePlugin</code>"},{"location":"packages/phlo-pgweb/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-postgres - Database service</li> <li>phlo-postgrest - REST API</li> </ul>"},{"location":"packages/phlo-pgweb/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Complete setup</li> </ul>"},{"location":"packages/phlo-postgres/","title":"phlo-postgres","text":"<p>PostgreSQL database service for Phlo.</p>"},{"location":"packages/phlo-postgres/#overview","title":"Overview","text":"<p><code>phlo-postgres</code> provides the core PostgreSQL database for metadata storage, lineage tracking, and operational data. It includes an optional Prometheus exporter for database metrics.</p>"},{"location":"packages/phlo-postgres/#installation","title":"Installation","text":"<pre><code>pip install phlo-postgres\n# or\nphlo plugin install postgres\n</code></pre>"},{"location":"packages/phlo-postgres/#configuration","title":"Configuration","text":"Variable Default Description <code>POSTGRES_PORT</code> <code>5432</code> PostgreSQL port <code>POSTGRES_USER</code> <code>phlo</code> Database username <code>POSTGRES_PASSWORD</code> <code>phlo</code> Database password <code>POSTGRES_DB</code> <code>phlo</code> Database name <code>POSTGRES_SSL_MODE</code> <code>prefer</code> SSL mode <code>POSTGRES_EXPORTER_PORT</code> <code>9187</code> Postgres exporter port"},{"location":"packages/phlo-postgres/#features","title":"Features","text":""},{"location":"packages/phlo-postgres/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Grafana Datasource Auto-registers as Grafana datasource via labels postgres-exporter Optional Prometheus exporter for native PostgreSQL metrics Service Discovery Exporter auto-scraped by Prometheus"},{"location":"packages/phlo-postgres/#databases-created","title":"Databases Created","text":"Database Purpose <code>phlo</code> Main application database <code>dagster</code> Dagster metadata storage <code>nessie</code> Nessie version store <code>openmetadata</code> OpenMetadata catalog (if enabled)"},{"location":"packages/phlo-postgres/#usage","title":"Usage","text":""},{"location":"packages/phlo-postgres/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start PostgreSQL\nphlo services start --service postgres\n\n# Start with exporter (for observability)\nphlo services start --service postgres,postgres-exporter\n</code></pre>"},{"location":"packages/phlo-postgres/#connecting","title":"Connecting","text":"<pre><code># Via psql\npsql -h localhost -p 5432 -U phlo -d phlo\n\n# Via Docker\ndocker exec -it phlo-postgres-1 psql -U phlo\n</code></pre>"},{"location":"packages/phlo-postgres/#sqlalchemy-connection","title":"SQLAlchemy Connection","text":"<pre><code>from sqlalchemy import create_engine\n\nengine = create_engine(\n    \"postgresql://phlo:phlo@localhost:5432/phlo\"\n)\n\nwith engine.connect() as conn:\n    result = conn.execute(\"SELECT * FROM marts.daily_summary\")\n</code></pre>"},{"location":"packages/phlo-postgres/#using-with-phlo-config","title":"Using with Phlo Config","text":"<pre><code>from phlo.config import get_settings\n\nsettings = get_settings()\nconn_string = settings.get_postgres_connection_string()\n\n# Use connection string with SQLAlchemy, psycopg2, etc.\n</code></pre>"},{"location":"packages/phlo-postgres/#marts-schema","title":"Marts Schema","text":"<p>Gold layer data is published to the <code>marts</code> schema:</p> <pre><code>-- Query marts\nSELECT * FROM marts.mrt_daily_summary;\nSELECT * FROM marts.mrt_user_metrics;\n</code></pre>"},{"location":"packages/phlo-postgres/#endpoints","title":"Endpoints","text":"Endpoint URL PostgreSQL <code>localhost:5432</code> Exporter Metrics <code>http://localhost:9187/metrics</code>"},{"location":"packages/phlo-postgres/#grafana-integration","title":"Grafana Integration","text":"<p>PostgreSQL is automatically registered as a Grafana datasource:</p> <pre><code>compose:\n  labels:\n    phlo.grafana.datasource: \"true\"\n    phlo.grafana.datasource.type: \"postgres\"\n    phlo.grafana.datasource.name: \"PostgreSQL\"\n</code></pre>"},{"location":"packages/phlo-postgres/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>PostgresServicePlugin</code>, <code>PostgresExporterServicePlugin</code>"},{"location":"packages/phlo-postgres/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-postgrest - REST API</li> <li>phlo-hasura - GraphQL API</li> <li>phlo-grafana - Visualization</li> <li>phlo-dagster - Orchestration</li> </ul>"},{"location":"packages/phlo-postgres/#next-steps","title":"Next Steps","text":"<ul> <li>PostgREST Setup - REST API generation</li> <li>Hasura Setup - GraphQL API</li> <li>API Reference - Data access</li> </ul>"},{"location":"packages/phlo-postgrest/","title":"phlo-postgrest","text":"<p>PostgREST API service for Phlo.</p>"},{"location":"packages/phlo-postgrest/#overview","title":"Overview","text":"<p><code>phlo-postgrest</code> automatically generates a RESTful API from PostgreSQL schemas. It exposes published tables via REST endpoints.</p>"},{"location":"packages/phlo-postgrest/#installation","title":"Installation","text":"<pre><code>pip install phlo-postgrest\n# or\nphlo plugin install postgrest\n</code></pre>"},{"location":"packages/phlo-postgrest/#profile","title":"Profile","text":"<p>Part of the <code>api</code> profile.</p>"},{"location":"packages/phlo-postgrest/#configuration","title":"Configuration","text":"Variable Default Description <code>POSTGREST_PORT</code> <code>3002</code> PostgREST API port <code>POSTGREST_VERSION</code> <code>v12.2.3</code> PostgREST version <code>POSTGRES_USER</code> <code>phlo</code> Database user <code>POSTGRES_PASSWORD</code> <code>phlo</code> Database password <code>POSTGRES_DB</code> <code>phlo</code> Database name"},{"location":"packages/phlo-postgrest/#features","title":"Features","text":""},{"location":"packages/phlo-postgrest/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Database Connection Auto-connects to Phlo's PostgreSQL Schema Exposure Exposes <code>api</code> and <code>public</code> schemas Anonymous Role Uses <code>POSTGRES_USER</code> as anonymous role OpenAPI Docs Auto-generates OpenAPI spec"},{"location":"packages/phlo-postgrest/#usage","title":"Usage","text":""},{"location":"packages/phlo-postgrest/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start with API profile\nphlo services start --profile api\n\n# Or start individually\nphlo services start --service postgrest\n</code></pre>"},{"location":"packages/phlo-postgrest/#api-examples","title":"API Examples","text":"<pre><code># Get OpenAPI spec\ncurl http://localhost:3002/\n\n# List all rows from a table\ncurl http://localhost:3002/mrt_daily_summary\n\n# Filter rows\ncurl \"http://localhost:3002/mrt_daily_summary?date=gte.2024-01-01\"\n\n# Pagination\ncurl \"http://localhost:3002/mrt_daily_summary?limit=10&amp;offset=20\"\n\n# Sorting\ncurl \"http://localhost:3002/mrt_daily_summary?order=date.desc\"\n\n# Select specific columns\ncurl \"http://localhost:3002/mrt_daily_summary?select=date,total_count\"\n</code></pre>"},{"location":"packages/phlo-postgrest/#filter-operators","title":"Filter Operators","text":"Operator Description Example <code>eq</code> Equal <code>?column=eq.value</code> <code>neq</code> Not equal <code>?column=neq.value</code> <code>gt</code> Greater than <code>?column=gt.100</code> <code>gte</code> Greater or equal <code>?column=gte.100</code> <code>lt</code> Less than <code>?column=lt.100</code> <code>lte</code> Less or equal <code>?column=lte.100</code> <code>like</code> Pattern match <code>?column=like.*pattern*</code> <code>ilike</code> Case-insensitive pattern <code>?column=ilike.*pattern*</code> <code>in</code> In list <code>?column=in.(a,b,c)</code> <code>is</code> Is null/true/false <code>?column=is.null</code>"},{"location":"packages/phlo-postgrest/#endpoints","title":"Endpoints","text":"Endpoint URL API Base <code>http://localhost:3002</code> OpenAPI Spec <code>http://localhost:3002/</code>"},{"location":"packages/phlo-postgrest/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>PostgrestServicePlugin</code> <code>phlo.plugins.cli</code> PostgREST CLI commands"},{"location":"packages/phlo-postgrest/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-postgres - Database service</li> <li>phlo-hasura - GraphQL alternative</li> <li>phlo-api - Custom API endpoints</li> </ul>"},{"location":"packages/phlo-postgrest/#next-steps","title":"Next Steps","text":"<ul> <li>PostgREST Setup - Complete configuration</li> <li>API Reference - API documentation</li> </ul>"},{"location":"packages/phlo-prometheus/","title":"phlo-prometheus","text":"<p>Prometheus metrics collection for Phlo.</p>"},{"location":"packages/phlo-prometheus/#overview","title":"Overview","text":"<p><code>phlo-prometheus</code> provides metrics collection and alerting for the Phlo observability stack. It auto-discovers and scrapes metrics from all Phlo services.</p>"},{"location":"packages/phlo-prometheus/#installation","title":"Installation","text":"<pre><code>pip install phlo-prometheus\n# or\nphlo plugin install prometheus\n</code></pre>"},{"location":"packages/phlo-prometheus/#profile","title":"Profile","text":"<p>Part of the <code>observability</code> profile.</p>"},{"location":"packages/phlo-prometheus/#configuration","title":"Configuration","text":"Variable Default Description <code>PROMETHEUS_PORT</code> <code>9090</code> Prometheus web UI port <code>PROMETHEUS_RETENTION_TIME</code> <code>15d</code> Metrics retention period <code>PROMETHEUS_SCRAPE_INTERVAL</code> <code>15s</code> Default scrape interval"},{"location":"packages/phlo-prometheus/#features","title":"Features","text":""},{"location":"packages/phlo-prometheus/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Service Discovery Auto-discovers metrics endpoints via Docker labels Scrape Configs Pre-configured for all Phlo services Alerting Rules Pre-configured alerting rules for common issues"},{"location":"packages/phlo-prometheus/#auto-discovered-services","title":"Auto-Discovered Services","text":"<p>Services with the following labels are automatically scraped:</p> <pre><code>labels:\n  phlo.metrics.enabled: \"true\"\n  phlo.metrics.port: \"8080\"\n  phlo.metrics.path: \"/metrics\"\n</code></pre>"},{"location":"packages/phlo-prometheus/#default-scrape-targets","title":"Default Scrape Targets","text":"Target Endpoint Dagster <code>dagster:3000/metrics</code> Trino <code>trino:8080/v1/info</code> MinIO <code>minio:9000/minio/v2/metrics/cluster</code> Nessie <code>nessie:19120/q/metrics</code> PostgreSQL Exporter <code>postgres-exporter:9187/metrics</code>"},{"location":"packages/phlo-prometheus/#usage","title":"Usage","text":""},{"location":"packages/phlo-prometheus/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start with observability profile\nphlo services start --profile observability\n\n# Or start individually\nphlo services start --service prometheus\n</code></pre>"},{"location":"packages/phlo-prometheus/#querying-metrics","title":"Querying Metrics","text":"<p>Access the Prometheus UI at <code>http://localhost:9090</code>:</p> <pre><code># Dagster run count\ndagster_runs_total\n\n# Trino query count\ntrino_running_queries\n\n# MinIO storage used\nminio_bucket_usage_total_bytes\n\n# Pipeline latency\nhistogram_quantile(0.95, dagster_run_duration_seconds_bucket)\n</code></pre>"},{"location":"packages/phlo-prometheus/#recording-rules","title":"Recording Rules","text":"<p>Common aggregations are pre-computed:</p> <pre><code>groups:\n  - name: phlo_aggregations\n    rules:\n      - record: phlo:pipeline_success_rate:5m\n        expr: rate(dagster_runs_total{status=\"success\"}[5m]) / rate(dagster_runs_total[5m])\n</code></pre>"},{"location":"packages/phlo-prometheus/#alerting-rules","title":"Alerting Rules","text":"<p>Pre-configured alerts:</p> Alert Condition <code>PhloServiceDown</code> Service unreachable for &gt; 5m <code>PipelineFailures</code> &gt; 3 pipeline failures in 1h <code>HighQueryLatency</code> P95 query latency &gt; 30s <code>StorageNearFull</code> Storage usage &gt; 80%"},{"location":"packages/phlo-prometheus/#endpoints","title":"Endpoints","text":"Endpoint URL Web UI <code>http://localhost:9090</code> API <code>http://localhost:9090/api/v1</code> Metrics <code>http://localhost:9090/metrics</code>"},{"location":"packages/phlo-prometheus/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>PrometheusServicePlugin</code>"},{"location":"packages/phlo-prometheus/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-grafana - Visualization</li> <li>phlo-alerting - Alert routing</li> <li>phlo-loki - Log aggregation</li> </ul>"},{"location":"packages/phlo-prometheus/#next-steps","title":"Next Steps","text":"<ul> <li>Observability Setup - Complete monitoring setup</li> <li>Operations Guide - Monitoring best practices</li> </ul>"},{"location":"packages/phlo-quality/","title":"phlo-quality","text":"<p>Data quality checks and validation for Phlo.</p>"},{"location":"packages/phlo-quality/#overview","title":"Overview","text":"<p><code>phlo-quality</code> enables defining and executing data quality checks using the <code>@phlo_quality</code> decorator. Checks run as Dagster asset checks and results are emitted to alerting, metrics, and data catalog systems.</p>"},{"location":"packages/phlo-quality/#installation","title":"Installation","text":"<pre><code>pip install phlo-quality\n# or\nphlo plugin install quality\n</code></pre>"},{"location":"packages/phlo-quality/#configuration","title":"Configuration","text":"Variable Default Description <code>PANDERA_CRITICAL_LEVEL</code> <code>error</code> Severity that blocks promotion"},{"location":"packages/phlo-quality/#features","title":"Features","text":""},{"location":"packages/phlo-quality/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Check Discovery Quality workflows auto-discovered in <code>workflows/quality/</code> Event Emission Emits <code>quality.result</code> events to HookBus Dagster Integration Checks run as Dagster asset checks Alerting Failed checks auto-routed to alerting destinations"},{"location":"packages/phlo-quality/#event-flow","title":"Event Flow","text":"<pre><code>@phlo_quality \u2192 QualityEventEmitter \u2192 quality.result \u2192 [Alerting, Metrics, OpenMetadata]\n</code></pre>"},{"location":"packages/phlo-quality/#usage","title":"Usage","text":""},{"location":"packages/phlo-quality/#defining-checks","title":"Defining Checks","text":"<pre><code>from phlo import phlo_quality\nfrom phlo_quality import NullCheck, UniqueCheck, RangeCheck\n\n@phlo_quality(\n    asset=\"bronze.users\",\n    checks=[\n        NullCheck(columns=[\"id\"]),\n        UniqueCheck(columns=[\"email\"]),\n        RangeCheck(column=\"age\", min_value=0, max_value=150),\n    ]\n)\ndef validate_users():\n    pass\n</code></pre>"},{"location":"packages/phlo-quality/#using-pandera-schemas","title":"Using Pandera Schemas","text":"<pre><code>import pandera as pa\nfrom pandera.typing import Series\n\nclass UserSchema(pa.DataFrameModel):\n    id: Series[str] = pa.Field(nullable=False, unique=True)\n    email: Series[str] = pa.Field(nullable=False)\n    age: Series[int] = pa.Field(ge=0, le=150)\n\n    class Config:\n        strict = True\n        coerce = True\n\n# Use with @phlo_ingestion for automatic validation\n@phlo_ingestion(\n    table_name=\"users\",\n    validation_schema=UserSchema,\n    # ...\n)\ndef ingest_users():\n    pass\n</code></pre>"},{"location":"packages/phlo-quality/#cli-commands","title":"CLI Commands","text":"<pre><code># Run quality checks\nphlo quality run --asset bronze.users\n\n# List available checks\nphlo quality list\n\n# Run all quality checks\nphlo quality run --all\n</code></pre>"},{"location":"packages/phlo-quality/#built-in-checks","title":"Built-in Checks","text":"Check Description Example <code>NullCheck</code> Validates column has no NULL values <code>NullCheck(columns=[\"id\"])</code> <code>UniqueCheck</code> Validates column values are unique <code>UniqueCheck(columns=[\"email\"])</code> <code>RangeCheck</code> Validates values are within range <code>RangeCheck(column=\"age\", min_value=0, max_value=150)</code> <code>PatternCheck</code> Validates values match pattern <code>PatternCheck(column=\"email\", pattern=r\".*@.*\\..*\")</code> <code>FreshnessCheck</code> Validates data is recent <code>FreshnessCheck(column=\"updated_at\", max_age_hours=24)</code> <code>CountCheck</code> Validates row count <code>CountCheck(min_rows=100, max_rows=10000)</code> <code>CustomSQLCheck</code> Arbitrary SQL validation <code>CustomSQLCheck(sql=\"SELECT COUNT(*) FROM ... WHERE ...\")</code>"},{"location":"packages/phlo-quality/#quality-check-contract","title":"Quality Check Contract","text":"<p>For integration with Observatory and monitoring:</p> <ul> <li>Pandera schema checks use the name <code>pandera_contract</code></li> <li>dbt test checks use the name <code>dbt__&lt;test_type&gt;__&lt;target&gt;</code></li> <li>Checks emit metadata: <code>source</code>, <code>partition_key</code>, <code>failed_count</code>, <code>total_count</code>, <code>query_or_sql</code>, <code>sample</code> (\u2264 20 rows)</li> <li>Checks may emit <code>repro_sql</code> for Trino reproduction</li> <li>Partitioned runs scope checks to the partition by default</li> </ul>"},{"location":"packages/phlo-quality/#custom-checks","title":"Custom Checks","text":"<p>Create custom quality check plugins:</p> <pre><code>from phlo.plugins.base import QualityCheckPlugin, PluginMetadata\n\nclass MyCustomCheck(QualityCheckPlugin):\n    @property\n    def metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"my_custom_check\",\n            version=\"1.0.0\",\n            description=\"Custom validation logic\"\n        )\n\n    def create_check(self, **kwargs):\n        return CustomCheckInstance(**kwargs)\n</code></pre> <p>See Plugin Development Guide for details.</p>"},{"location":"packages/phlo-quality/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.cli</code> <code>quality</code> CLI commands <code>phlo.plugins.quality</code> Built-in check plugins"},{"location":"packages/phlo-quality/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-dagster - Orchestration platform</li> <li>phlo-alerting - Alert routing</li> <li>phlo-openmetadata - Data catalog</li> </ul>"},{"location":"packages/phlo-quality/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Strategy Guide - Testing approaches</li> <li>Best Practices - Production patterns</li> <li>Developer Guide - Decorator usage</li> </ul>"},{"location":"packages/phlo-superset/","title":"phlo-superset","text":"<p>Apache Superset BI service for Phlo.</p>"},{"location":"packages/phlo-superset/#overview","title":"Overview","text":"<p><code>phlo-superset</code> provides a business intelligence and data visualization platform. It connects to Trino to query the lakehouse data.</p>"},{"location":"packages/phlo-superset/#installation","title":"Installation","text":"<pre><code>pip install phlo-superset\n# or\nphlo plugin install superset\n</code></pre>"},{"location":"packages/phlo-superset/#configuration","title":"Configuration","text":"Variable Default Description <code>SUPERSET_PORT</code> <code>8088</code> Superset web UI port <code>SUPERSET_VERSION</code> <code>4.0.0</code> Superset version <code>SUPERSET_SECRET_KEY</code> auto-generated Session encryption key <code>SUPERSET_ADMIN_USER</code> <code>admin</code> Admin username <code>SUPERSET_ADMIN_PASSWORD</code> <code>admin</code> Admin password <code>SUPERSET_ADMIN_EMAIL</code> <code>admin@example.com</code> Admin email"},{"location":"packages/phlo-superset/#features","title":"Features","text":""},{"location":"packages/phlo-superset/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Trino Database Auto-registered on startup via post_start hook Metrics Labels Exposes health endpoint for Prometheus Admin User Auto-created on first startup"},{"location":"packages/phlo-superset/#post-start-hook","title":"Post-Start Hook","text":"<pre><code>hooks:\n  post_start:\n    - name: add-trino-database\n      command: python -m phlo_superset.hooks add-database\n</code></pre>"},{"location":"packages/phlo-superset/#pre-configured-databases","title":"Pre-Configured Databases","text":"Database Connection Trino (Iceberg) <code>trino://trino:8080/iceberg</code> PostgreSQL (Marts) <code>postgresql://phlo:phlo@postgres:5432/phlo</code>"},{"location":"packages/phlo-superset/#usage","title":"Usage","text":""},{"location":"packages/phlo-superset/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start Superset\nphlo services start --service superset\n</code></pre>"},{"location":"packages/phlo-superset/#accessing-superset","title":"Accessing Superset","text":"<p>Open <code>http://localhost:8088</code> in your browser:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul>"},{"location":"packages/phlo-superset/#creating-dashboards","title":"Creating Dashboards","text":"<ol> <li> <p>Add a Dataset</p> </li> <li> <p>Navigate to Data \u2192 Datasets</p> </li> <li>Select the Trino database</li> <li> <p>Choose a table</p> </li> <li> <p>Create a Chart</p> </li> <li> <p>Navigate to Charts \u2192 New Chart</p> </li> <li>Select your dataset</li> <li> <p>Build your visualization</p> </li> <li> <p>Build a Dashboard</p> </li> <li>Navigate to Dashboards \u2192 New Dashboard</li> <li>Add your charts</li> <li>Arrange and save</li> </ol>"},{"location":"packages/phlo-superset/#sql-lab","title":"SQL Lab","text":"<p>Use SQL Lab for ad-hoc queries:</p> <ol> <li>Navigate to SQL \u2192 SQL Lab</li> <li>Select a database (Trino or PostgreSQL)</li> <li>Write and execute SQL queries</li> <li>Save queries and export results</li> </ol>"},{"location":"packages/phlo-superset/#endpoints","title":"Endpoints","text":"Endpoint URL Web UI <code>http://localhost:8088</code> Login admin / admin"},{"location":"packages/phlo-superset/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>SupersetServicePlugin</code>"},{"location":"packages/phlo-superset/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-trino - Query engine</li> <li>phlo-postgres - Marts storage</li> <li>phlo-grafana - Metrics visualization</li> </ul>"},{"location":"packages/phlo-superset/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Complete setup</li> <li>dbt Development - Create mart tables</li> </ul>"},{"location":"packages/phlo-testing/","title":"phlo-testing","text":"<p>Testing utilities for Phlo packages.</p>"},{"location":"packages/phlo-testing/#overview","title":"Overview","text":"<p><code>phlo-testing</code> provides shared testing utilities, fixtures, and mocks for developing and testing Phlo packages. This is a development utility library, not a runtime plugin.</p>"},{"location":"packages/phlo-testing/#installation","title":"Installation","text":"<pre><code>pip install phlo-testing\n</code></pre>"},{"location":"packages/phlo-testing/#features","title":"Features","text":"<p>This package is a utility library with no auto-configuration:</p> Feature Status Entry Points None - utility library Runtime Hooks None - testing only"},{"location":"packages/phlo-testing/#usage","title":"Usage","text":""},{"location":"packages/phlo-testing/#fixtures","title":"Fixtures","text":"<pre><code>import pytest\nfrom phlo_testing.fixtures import (\n    mock_iceberg_catalog,\n    mock_nessie_client,\n    mock_trino_connection,\n    test_dataframe\n)\n\ndef test_my_ingestion(mock_iceberg_catalog):\n    # Test with mocked catalog\n    catalog = mock_iceberg_catalog\n    # ... test code\n    pass\n\ndef test_with_nessie(mock_nessie_client):\n    # Test with mocked Nessie client\n    client = mock_nessie_client\n    client.create_branch(\"test-branch\")\n    # ... test code\n    pass\n</code></pre>"},{"location":"packages/phlo-testing/#test-utilities","title":"Test Utilities","text":"<pre><code>from phlo_testing.utils import (\n    create_test_dataframe,\n    assert_table_exists,\n    assert_schema_matches,\n    wait_for_service\n)\n\n# Create test data\ndf = create_test_dataframe(\n    columns=[\"id\", \"name\", \"value\"],\n    rows=10\n)\n\n# Assert table exists\nassert_table_exists(\"bronze.test_table\")\n\n# Assert schema matches expected\nassert_schema_matches(\n    table=\"bronze.users\",\n    expected_columns=[\"id\", \"name\", \"email\"]\n)\n\n# Wait for service to be ready\nwait_for_service(\"http://localhost:8080/health\", timeout=30)\n</code></pre>"},{"location":"packages/phlo-testing/#mock-factories","title":"Mock Factories","text":"<pre><code>from phlo_testing.mocks import (\n    MockDagsterContext,\n    MockIcebergTable,\n    MockTrinoResult\n)\n\n# Create mock Dagster context\ncontext = MockDagsterContext(\n    partition_key=\"2024-01-15\"\n)\n\n# Create mock Iceberg table\ntable = MockIcebergTable(\n    name=\"bronze.users\",\n    schema={\"id\": \"string\", \"name\": \"string\"}\n)\n\n# Create mock Trino result\nresult = MockTrinoResult(\n    columns=[\"id\", \"name\"],\n    rows=[(\"1\", \"Alice\"), (\"2\", \"Bob\")]\n)\n</code></pre>"},{"location":"packages/phlo-testing/#integration-test-markers","title":"Integration Test Markers","text":"<pre><code>import pytest\n\n@pytest.mark.integration\ndef test_full_pipeline():\n    \"\"\"Requires Docker services to be running.\"\"\"\n    pass\n\n@pytest.mark.slow\ndef test_large_dataset():\n    \"\"\"Takes a long time to run.\"\"\"\n    pass\n\n# Run specific markers\n# pytest -m integration\n# pytest -m \"not slow\"\n</code></pre>"},{"location":"packages/phlo-testing/#common-test-patterns","title":"Common Test Patterns","text":""},{"location":"packages/phlo-testing/#testing-ingestion","title":"Testing Ingestion","text":"<pre><code>from phlo_testing.fixtures import mock_iceberg_catalog, test_dataframe\n\ndef test_ingestion_creates_table(mock_iceberg_catalog, test_dataframe):\n    # Arrange\n    catalog = mock_iceberg_catalog\n\n    # Act\n    # ... run ingestion\n\n    # Assert\n    assert catalog.table_exists(\"bronze.my_table\")\n</code></pre>"},{"location":"packages/phlo-testing/#testing-quality-checks","title":"Testing Quality Checks","text":"<pre><code>from phlo_testing.utils import create_test_dataframe\nfrom phlo_quality.checks import null_check\n\ndef test_null_check_fails_on_nulls():\n    # Create data with nulls\n    df = create_test_dataframe(\n        columns=[\"id\", \"name\"],\n        rows=10,\n        null_columns=[\"name\"]\n    )\n\n    # Run check\n    check = null_check(column=\"name\")\n    result = check.execute(df)\n\n    # Assert\n    assert not result[\"passed\"]\n</code></pre>"},{"location":"packages/phlo-testing/#note","title":"Note","text":"<p>This package does not register any entry points as it is intended for development and testing only.</p>"},{"location":"packages/phlo-testing/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-quality - Quality checks</li> <li>phlo-dlt - Data ingestion</li> </ul>"},{"location":"packages/phlo-testing/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Strategy Guide - Testing approaches</li> <li>Operations Testing - Integration tests</li> </ul>"},{"location":"packages/phlo-trino/","title":"phlo-trino","text":"<p>Trino distributed SQL query engine for Phlo.</p>"},{"location":"packages/phlo-trino/#overview","title":"Overview","text":"<p><code>phlo-trino</code> provides the Trino query engine for SQL access to the Iceberg lakehouse. It enables fast analytics queries across all data layers.</p>"},{"location":"packages/phlo-trino/#installation","title":"Installation","text":"<pre><code>pip install phlo-trino\n# or\nphlo plugin install trino\n</code></pre>"},{"location":"packages/phlo-trino/#configuration","title":"Configuration","text":"Variable Default Description <code>TRINO_PORT</code> <code>8080</code> Trino HTTP port <code>TRINO_VERSION</code> <code>467</code> Trino version <code>TRINO_HOST</code> <code>trino</code> Trino hostname <code>TRINO_CATALOG</code> <code>iceberg</code> Default catalog"},{"location":"packages/phlo-trino/#features","title":"Features","text":""},{"location":"packages/phlo-trino/#auto-configuration","title":"Auto-Configuration","text":"Feature How It Works Catalog Discovery Auto-generates catalog files from <code>phlo.plugins.trino_catalogs</code> entry points Metrics Labels Exposes Trino metrics for Prometheus Grafana Datasource Auto-registers as Grafana datasource via labels Superset Connection Auto-registered in Superset via Superset hook"},{"location":"packages/phlo-trino/#catalog-generation","title":"Catalog Generation","text":"<p>Trino catalog <code>.properties</code> files are generated from installed catalog plugins:</p> <pre><code>from phlo_trino.catalog_generator import generate_catalog_files\n\n# Discovers all TrinoCatalogPlugin instances and generates files\ngenerate_catalog_files(\"/path/to/trino/catalog/\")\n</code></pre>"},{"location":"packages/phlo-trino/#default-catalogs","title":"Default Catalogs","text":"Catalog Description <code>iceberg</code> Main Iceberg catalog (main branch) <code>iceberg_dev</code> Development Iceberg catalog (dev branch) <code>postgres</code> PostgreSQL connection for marts"},{"location":"packages/phlo-trino/#usage","title":"Usage","text":""},{"location":"packages/phlo-trino/#starting-the-service","title":"Starting the Service","text":"<pre><code># Start Trino\nphlo services start --service trino\n\n# Start with all dependencies\nphlo services start\n</code></pre>"},{"location":"packages/phlo-trino/#cli-queries","title":"CLI Queries","text":"<pre><code># Run SQL query\nphlo trino query \"SELECT * FROM iceberg.bronze.users LIMIT 10\"\n\n# Interactive shell\ndocker exec -it phlo-trino-1 trino\n</code></pre>"},{"location":"packages/phlo-trino/#sql-examples","title":"SQL Examples","text":"<pre><code>-- Query bronze layer\nSELECT * FROM iceberg.bronze.users LIMIT 10;\n\n-- Query from dev branch\nSELECT * FROM iceberg_dev.bronze.users LIMIT 10;\n\n-- Time travel query\nSELECT * FROM iceberg.bronze.users FOR VERSION AS OF 123456789;\n\n-- Cross-schema join\nSELECT\n    u.id,\n    u.name,\n    e.event_count\nFROM iceberg.silver.users u\nJOIN iceberg.gold.user_events e ON u.id = e.user_id;\n\n-- Query PostgreSQL marts\nSELECT * FROM postgres.public.mrt_daily_summary;\n</code></pre>"},{"location":"packages/phlo-trino/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from trino.dbapi import connect\n\nconn = connect(\n    host=\"localhost\",\n    port=8080,\n    user=\"phlo\",\n    catalog=\"iceberg\",\n    schema=\"bronze\"\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM users LIMIT 10\")\nresults = cursor.fetchall()\n</code></pre>"},{"location":"packages/phlo-trino/#endpoints","title":"Endpoints","text":"Endpoint URL HTTP API <code>http://localhost:8080</code> Web UI <code>http://localhost:8080/ui</code> Metrics <code>http://localhost:8080/v1/info</code>"},{"location":"packages/phlo-trino/#grafana-integration","title":"Grafana Integration","text":"<p>Trino is automatically registered as a Grafana datasource:</p> <pre><code>compose:\n  labels:\n    phlo.grafana.datasource: \"true\"\n    phlo.grafana.datasource.type: \"trino\"\n    phlo.grafana.datasource.name: \"Trino\"\n</code></pre>"},{"location":"packages/phlo-trino/#entry-points","title":"Entry Points","text":"Entry Point Plugin <code>phlo.plugins.services</code> <code>TrinoServicePlugin</code>"},{"location":"packages/phlo-trino/#related-packages","title":"Related Packages","text":"<ul> <li>phlo-iceberg - Table format</li> <li>phlo-nessie - Catalog service</li> <li>phlo-postgres - Marts storage</li> <li>phlo-grafana - Visualization</li> </ul>"},{"location":"packages/phlo-trino/#next-steps","title":"Next Steps","text":"<ul> <li>DuckDB Queries - Ad-hoc analysis</li> <li>API Reference - REST/GraphQL access</li> <li>dbt Development - SQL transformations</li> </ul>"},{"location":"reference/architecture/","title":"Architecture Overview","text":"<p>Phlo is built on modern lakehouse principles using Apache Iceberg as the table format and Project Nessie as a Git-like catalog. This architecture provides ACID transactions, schema evolution, time travel, and branch isolation for data engineering workflows.</p>"},{"location":"reference/architecture/#design-principles","title":"Design Principles","text":""},{"location":"reference/architecture/#1-open-standards","title":"1. Open Standards","text":"<ul> <li>Apache Iceberg for table format (open spec, multi-engine support)</li> <li>S3-compatible storage (portable across cloud providers)</li> <li>Standard SQL via Trino</li> <li>Open-source components throughout</li> </ul>"},{"location":"reference/architecture/#2-stateless-services","title":"2. Stateless Services","text":"<ul> <li>All state stored in volumes (MinIO, PostgreSQL)</li> <li>Services can be restarted without data loss</li> <li>Configuration via environment variables</li> <li>12-factor app compliance</li> </ul>"},{"location":"reference/architecture/#3-git-like-workflows","title":"3. Git-Like Workflows","text":"<ul> <li>Branch isolation (dev/staging/prod)</li> <li>Atomic multi-table commits</li> <li>Time travel to any point in history</li> <li>Tag releases for reproducibility</li> </ul>"},{"location":"reference/architecture/#4-asset-based-orchestration","title":"4. Asset-Based Orchestration","text":"<ul> <li>Declarative data assets in Dagster</li> <li>Automatic lineage tracking</li> <li>Partition-aware dependencies</li> <li>Freshness policies for monitoring</li> </ul>"},{"location":"reference/architecture/#system-architecture","title":"System Architecture","text":""},{"location":"reference/architecture/#high-level-component-diagram","title":"High-Level Component Diagram","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        NS[Nightscout API]\n        FILES[CSV/Excel Files]\n        EXTERNAL[External APIs]\n    end\n\n    subgraph \"User Interface\"\n        OBS[Observatory UI]\n        DAGUI[Dagster Web UI]\n    end\n\n    subgraph \"API Layer\"\n        PAPI[phlo-api]\n        POSTGREST[PostgREST]\n        HASURA[Hasura GraphQL]\n    end\n\n    subgraph \"Ingestion Layer\"\n        DLT[DLT Python]\n        STAGE[S3 Staging Area]\n        PYI[PyIceberg Writer]\n    end\n\n    subgraph \"Storage Layer\"\n        MINIO[MinIO S3 Storage]\n        NESSIE[Nessie Catalog]\n        PG[PostgreSQL]\n    end\n\n    subgraph \"Compute Layer\"\n        TRINO[Trino Query Engine]\n        DBT[dbt Transformations]\n        DUCK[DuckDB Ad-hoc]\n    end\n\n    subgraph \"Orchestration\"\n        DAGSTER[Dagster Assets]\n        SCHEDULES[Schedules &amp; Sensors]\n    end\n\n    subgraph \"Analytics Layer\"\n        MARTS[PostgreSQL Marts]\n        SUPERSET[Superset Dashboards]\n    end\n\n    NS --&gt; DLT\n    FILES --&gt; DLT\n    EXTERNAL --&gt; DLT\n    DLT --&gt; STAGE\n    STAGE --&gt; PYI\n    PYI --&gt; MINIO\n    PYI --&gt; NESSIE\n\n    NESSIE --&gt; TRINO\n    MINIO --&gt; TRINO\n    TRINO --&gt; DBT\n    MINIO --&gt; DUCK\n\n    DAGSTER --&gt; DLT\n    DAGSTER --&gt; DBT\n    DAGSTER --&gt; TRINO\n    SCHEDULES --&gt; DAGSTER\n\n    DBT --&gt; MARTS\n    TRINO --&gt; MARTS\n    MARTS --&gt; SUPERSET\n    TRINO --&gt; SUPERSET\n\n    PG --&gt; NESSIE\n    PG --&gt; DAGSTER\n    PG --&gt; SUPERSET\n    PG --&gt; POSTGREST\n    PG --&gt; HASURA\n\n    OBS --&gt; PAPI\n    PAPI --&gt; TRINO\n    PAPI --&gt; NESSIE\n    PAPI --&gt; DAGSTER\n    PAPI --&gt; MINIO\n    DAGUI --&gt; DAGSTER\n</code></pre>"},{"location":"reference/architecture/#key-components","title":"Key Components","text":""},{"location":"reference/architecture/#user-interface-layer","title":"User Interface Layer","text":"<ul> <li>Observatory: Web-based UI for exploring data, viewing lineage, managing branches, and monitoring data quality. Provides a unified interface for all Phlo operations.</li> <li>Dagster Web UI: Native Dagster interface for asset materialization, run monitoring, and workflow debugging.</li> </ul>"},{"location":"reference/architecture/#api-layer","title":"API Layer","text":"<ul> <li>phlo-api: Python FastAPI service exposing Phlo internals (plugins, services, configs) and providing data access endpoints for Observatory.</li> <li>PostgREST (optional): Auto-generated REST API from PostgreSQL schemas.</li> <li>Hasura (optional): GraphQL API with real-time subscriptions.</li> </ul>"},{"location":"reference/architecture/#storage-layer","title":"Storage Layer","text":"<ul> <li>MinIO: S3-compatible object storage for data files and Iceberg metadata</li> <li>Nessie: Git-like catalog for table metadata and branching</li> <li>PostgreSQL: Relational database for operational metadata and analytics marts</li> </ul>"},{"location":"reference/architecture/#compute-layer","title":"Compute Layer","text":"<ul> <li>Trino: Distributed SQL query engine for analytics</li> <li>dbt: Data transformation framework with SQL</li> <li>DuckDB: Embedded analytical database for ad-hoc queries</li> </ul>"},{"location":"reference/architecture/#orchestration","title":"Orchestration","text":"<ul> <li>Dagster: Asset-based data orchestration platform</li> <li>Schedules &amp; Sensors: Automated pipeline triggers</li> </ul>"},{"location":"reference/architecture/#analytics-layer","title":"Analytics Layer","text":"<ul> <li>PostgreSQL Marts: Curated datasets for BI tools</li> <li>Superset (optional): Business intelligence and visualization</li> </ul>"},{"location":"reference/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Ingestion: Data sources \u2192 DLT \u2192 S3 staging \u2192 PyIceberg \u2192 Iceberg tables</li> <li>Transformation: Iceberg tables \u2192 dbt \u2192 PostgreSQL marts</li> <li>Querying: Multiple engines can query Iceberg tables directly</li> <li>Visualization: BI tools connect to PostgreSQL marts or query engines</li> </ol>"},{"location":"reference/architecture/#plugin-system-architecture","title":"Plugin System Architecture","text":"<p>Phlo uses a unified plugin system for extending functionality:</p>"},{"location":"reference/architecture/#plugin-types","title":"Plugin Types","text":"<ol> <li>Service Plugins: Infrastructure services (Dagster, Postgres, Trino, etc.)</li> <li>Distributed as Python packages (e.g., <code>phlo-dagster</code>, <code>phlo-trino</code>)</li> <li>Contain Docker service definitions via <code>service.yaml</code></li> <li> <p>Auto-discovered via <code>phlo.plugins.services</code> entry points</p> </li> <li> <p>Data Plugins: Data processing extensions</p> </li> <li>Source Connectors: Fetch data from external systems</li> <li>Quality Checks: Custom validation rules</li> <li> <p>Transformations: Reusable transformation logic</p> </li> <li> <p>Dagster Extension Plugins: Extend Dagster functionality</p> </li> <li>Custom resources, sensors, schedules</li> <li> <p>Integration with external systems</p> </li> <li> <p>CLI Plugins: Add custom CLI commands</p> </li> </ol>"},{"location":"reference/architecture/#plugin-discovery","title":"Plugin Discovery","text":"<p>Plugins are automatically discovered using Python entry points:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Python Environment               \u2502\n\u2502  - phlo (core framework)             \u2502\n\u2502  - phlo-dagster (service plugin)     \u2502\n\u2502  - phlo-custom-source (data plugin)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Entry Point Discovery              \u2502\n\u2502   (importlib.metadata)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Plugin Registry                    \u2502\n\u2502   - Installed plugins                \u2502\n\u2502   - Available plugins (remote)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/architecture/#plugin-registry","title":"Plugin Registry","text":"<ul> <li>registry/plugins.json: Authoritative catalog of available plugins</li> <li>Remote + Bundled: Fetches from remote, falls back to bundled version offline</li> <li>CLI Management: <code>phlo plugin search/install/list</code> commands</li> </ul> <p>For more details, see: - Plugin System Blog Post - ADR 0030: Unified Plugin System</p>"},{"location":"reference/architecture/#branching-strategy","title":"Branching Strategy","text":"<p>Phlo uses a Git-like branching model for data:</p> <ul> <li>main: Production data (read-only for most users)</li> <li>dev: Development workspace (read-write)</li> <li>feature branches: Isolated development environments</li> </ul> <p>See NESSIE_WORKFLOW.md for detailed branching workflows.</p>"},{"location":"reference/architecture/#for-more-details","title":"For More Details","text":"<p>Read the full Architecture Guide for complete technical specifications.</p>"},{"location":"reference/cli-reference/","title":"CLI Reference","text":"<p>Complete reference for the Phlo command-line interface.</p>"},{"location":"reference/cli-reference/#installation","title":"Installation","text":"<pre><code># Using pip\npip install -e .\n\n# Using uv (recommended)\nuv pip install -e .\n</code></pre> <p>Verify:</p> <pre><code>phlo --version\n</code></pre>"},{"location":"reference/cli-reference/#global-options","title":"Global Options","text":"<pre><code>phlo --help              # Show help\nphlo --version           # Show version\n</code></pre>"},{"location":"reference/cli-reference/#command-overview","title":"Command Overview","text":"<p>Core Commands:</p> <pre><code>phlo init                # Initialize new project\nphlo services            # Manage infrastructure services\nphlo plugin              # Manage plugins\nphlo dev                 # Start development server\nphlo test                # Run tests\nphlo config              # Configuration management\nphlo env                 # Environment exports\n</code></pre> <p>Workflow Commands:</p> <pre><code>phlo create-workflow     # Create new workflow\nphlo validate-workflow   # Validate workflow configuration\n</code></pre> <p>Asset Commands:</p> <pre><code>phlo materialize         # Materialize assets\nphlo backfill            # Backfill partitioned assets\nphlo status              # Show asset status\nphlo logs                # Query structured logs\n</code></pre> <p>Data Catalog Commands:</p> <pre><code>phlo branch              # Manage Nessie branches\nphlo catalog             # Catalog operations (tables, describe, history)\nphlo lineage             # Asset lineage (show, export)\n</code></pre> <p>Quality Commands:</p> <pre><code>phlo schema              # Manage Pandera schemas\nphlo validate-schema     # Validate Pandera schemas\n</code></pre> <p>Optional Plugin Commands (requires installation):</p> <pre><code>phlo postgrest           # PostgREST management\nphlo hasura              # Hasura GraphQL management\nphlo publishing          # dbt publishing layer\nphlo metrics             # Prometheus metrics\nphlo alerts              # Alerting rules\nphlo openmetadata        # OpenMetadata catalog\n</code></pre>"},{"location":"reference/cli-reference/#services-commands","title":"Services Commands","text":"<p>Manage Docker infrastructure services.</p> <p>Services are provided by installed service packages (e.g., <code>phlo-dagster</code>, <code>phlo-trino</code>). Install core services with:</p> <pre><code>uv pip install -e \".[core-services]\"\n</code></pre> <p>Optional services can be added later:</p> <pre><code>phlo plugin install phlo-observatory\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-init","title":"phlo services init","text":"<p>Initialize infrastructure directory and configuration.</p> <pre><code>phlo services init [OPTIONS]\n</code></pre> <p>What it does:</p> <ul> <li>Creates <code>.phlo/</code> directory</li> <li>Generates Docker Compose configurations</li> <li>Sets up network and volume definitions</li> <li>Creates <code>phlo.yaml</code> config file</li> </ul> <p>Options:</p> <pre><code>--force              # Overwrite existing configuration\n--name NAME          # Project name (default: directory name)\n--dev                # Development mode: mount local phlo source\n--no-dev             # Explicitly disable dev mode\n--phlo-source PATH   # Path to phlo repo or src/phlo for dev mode\n</code></pre> <p>Examples:</p> <pre><code># Basic initialization\nphlo services init\n\n# Force overwrite existing\nphlo services init --force\n\n# With custom project name\nphlo services init --name my-lakehouse\n\n# Development mode with local source\nphlo services init --dev --phlo-source /path/to/phlo\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-start","title":"phlo services start","text":"<p>Start all infrastructure services.</p> <pre><code>phlo services start [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--native                 # Run native dev services (phlo-api, Observatory) as subprocesses\n--profile PROFILE        # Additional service profiles\n--detach, -d             # Run in background\n--build                  # Rebuild containers before starting\n</code></pre> <p>Profiles:</p> <ul> <li><code>observability</code>: Prometheus, Grafana, Loki</li> <li><code>api</code>: PostgREST, Hasura</li> <li><code>catalog</code>: OpenMetadata</li> </ul> <p>Examples:</p> <pre><code># Start core services\nphlo services start\n\n# Start with observability\nphlo services start --profile observability\n\n# Run Observatory/phlo-api without Docker (useful on ARM Macs)\nphlo services start --native\n\n# Develop the phlo framework from a local monorepo (Dagster container installs editable from source)\nphlo services init --force --dev --phlo-source /path/to/phlo\nphlo services start\n\n# Multiple profiles\nphlo services start --profile observability --profile api\n\n# Rebuild and start\nphlo services start --build\n</code></pre> <p>Services started:</p> <ul> <li>PostgreSQL (port 10000)</li> <li>MinIO (ports 10001-10002)</li> <li>Nessie (port 10003)</li> <li>Trino (port 10005)</li> <li>Dagster webserver (port 10006)</li> <li>Dagster daemon</li> </ul>"},{"location":"reference/cli-reference/#phlo-services-stop","title":"phlo services stop","text":"<p>Stop all running services.</p> <pre><code>phlo services stop [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--volumes, -v        # Remove volumes (deletes all data)\n--profile PROFILE    # Stop only services in specified profile\n--service SERVICE    # Stop specific service(s)\n--stop-native        # Also stop native subprocess services\n</code></pre> <p>Examples:</p> <pre><code># Stop all services (preserve data)\nphlo services stop\n\n# Stop and delete all data\nphlo services stop --volumes\n\n# Stop specific profile\nphlo services stop --profile observability\n\n# Stop specific services\nphlo services stop --service postgres,minio\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-list","title":"phlo services list","text":"<p>List available services with status.</p> <pre><code>phlo services list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--all                # Show all services including optional\n--json               # Output as JSON\n</code></pre> <p>Examples:</p> <pre><code>phlo services list\nphlo services list --all\nphlo services list --json\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-add","title":"phlo services add","text":"<p>Add an optional service to the project.</p> <pre><code>phlo services add SERVICE_NAME [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--no-start           # Don't start the service after adding\n</code></pre> <p>Examples:</p> <pre><code>phlo services add prometheus\nphlo services add grafana --no-start\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-remove","title":"phlo services remove","text":"<p>Remove a service from the project.</p> <pre><code>phlo services remove SERVICE_NAME [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--keep-running       # Don't stop the service\n</code></pre> <p>Examples:</p> <pre><code>phlo services remove prometheus\nphlo services remove grafana --keep-running\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-reset","title":"phlo services reset","text":"<p>Reset infrastructure by stopping services and deleting volumes.</p> <pre><code>phlo services reset [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--service SERVICE    # Reset only specific service(s)\n-y, --yes            # Skip confirmation\n</code></pre> <p>Examples:</p> <pre><code>phlo services reset\nphlo services reset --service postgres\nphlo services reset -y\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-restart","title":"phlo services restart","text":"<p>Restart services (stop + start).</p> <pre><code>phlo services restart [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--build              # Rebuild containers before starting\n--profile PROFILE    # Restart services in profile\n--service SERVICE    # Restart specific service(s)\n--dev                # Enable dev mode when restarting\n</code></pre> <p>Examples:</p> <pre><code>phlo services restart\nphlo services restart --build\nphlo services restart --service dagster\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-status","title":"phlo services status","text":"<p>Show status of all services.</p> <pre><code>phlo services status\n</code></pre> <p>Output:</p> <pre><code>SERVICE              STATUS    PORTS\npostgres             running   10000\nminio                running   10001-10002\nnessie               running   10003\ntrino                running   10005\ndagster-webserver    running   10006\ndagster-daemon       running\n</code></pre>"},{"location":"reference/cli-reference/#phlo-services-logs","title":"phlo services logs","text":"<p>View service logs.</p> <pre><code>phlo services logs [OPTIONS] [SERVICE]\n</code></pre> <p>Options:</p> <pre><code>--follow, -f         # Follow log output\n--tail N             # Show last N lines\n--timestamps         # Show timestamps\n</code></pre> <p>Examples:</p> <pre><code># All logs\nphlo services logs\n\n# Follow specific service\nphlo services logs -f dagster-webserver\n\n# Last 100 lines\nphlo services logs --tail 100 trino\n</code></pre>"},{"location":"reference/cli-reference/#plugin-commands","title":"Plugin Commands","text":"<p>Manage Phlo plugins for extending functionality.</p>"},{"location":"reference/cli-reference/#phlo-plugin-list","title":"phlo plugin list","text":"<p>List all installed plugins.</p> <pre><code>phlo plugin list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--type TYPE          # Filter by plugin type (sources, quality, transforms, services, hooks, dagster, cli)\n--json               # Output as JSON\n</code></pre> <p>Examples:</p> <pre><code># List all plugins\nphlo plugin list\n\n# List only source connectors\nphlo plugin list --type sources\n\n# List hook plugins\nphlo plugin list --type hooks\n\n# JSON output\nphlo plugin list --json\n</code></pre> <p>Output:</p> <pre><code>Services:\n  NAME              VERSION    DESCRIPTION\n  dagster           0.1.0      Dagster orchestration engine\n  postgres          0.1.0      PostgreSQL database\n  trino             0.1.0      Distributed SQL query engine\n\nSources:\n  NAME              VERSION    DESCRIPTION\n  rest_api          1.0.0      REST API connector\n  jsonplaceholder   1.0.0      JSONPlaceholder example source\n\nQuality Checks:\n  NAME              VERSION    DESCRIPTION\n  null_check        1.0.0      Null value validation\n  threshold_check   1.0.0      Threshold validation\n</code></pre>"},{"location":"reference/cli-reference/#phlo-plugin-search","title":"phlo plugin search","text":"<p>Search available plugins in the registry.</p> <pre><code>phlo plugin search [QUERY] [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--type TYPE          # Filter by plugin type\n--json               # Output as JSON\n--tag TAG            # Filter by tag\n</code></pre> <p>Examples:</p> <pre><code># Search for PostgreSQL-related plugins\nphlo plugin search postgres\n\n# Search for quality check plugins\nphlo plugin search --type quality\n\n# Search for hook plugins\nphlo plugin search --type hooks\n\n# Search by tag\nphlo plugin search --tag observability\n</code></pre>"},{"location":"reference/cli-reference/#phlo-plugin-install","title":"phlo plugin install","text":"<p>Install a plugin from the registry.</p> <pre><code>phlo plugin install PLUGIN_NAME [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--version VERSION    # Specific version to install\n--upgrade            # Upgrade if already installed\n</code></pre> <p>Examples:</p> <pre><code># Install a plugin\nphlo plugin install phlo-superset\n\n# Install specific version\nphlo plugin install phlo-superset --version 0.2.0\n\n# Upgrade existing\nphlo plugin install phlo-superset --upgrade\n</code></pre>"},{"location":"reference/cli-reference/#phlo-plugin-info","title":"phlo plugin info","text":"<p>Show detailed information about a plugin.</p> <pre><code>phlo plugin info PLUGIN_NAME [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--type TYPE          # Plugin type (auto-detected if omitted)\n--json               # Output as JSON\n</code></pre> <p>Examples:</p> <pre><code># Get plugin info (auto-detect type)\nphlo plugin info dagster\n\n# Specify type\nphlo plugin info rest_api --type sources\n\n# JSON output\nphlo plugin info dagster --json\n</code></pre> <p>Output:</p> <pre><code>dagster\nType: services\nVersion: 0.1.0\nAuthor: Phlo Team\nDescription: Dagster orchestration engine for workflow management\nLicense: MIT\nHomepage: https://github.com/iamgp/phlo\nTags: orchestration, core, service\n\nService Details:\n  Container: dagster-webserver\n  Ports: 10006\n  Dependencies: postgres\n</code></pre>"},{"location":"reference/cli-reference/#phlo-plugin-update","title":"phlo plugin update","text":"<p>Update plugins to latest versions.</p> <pre><code>phlo plugin update [PLUGIN_NAME] [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--all                # Update all plugins\n--check              # Check for updates without installing\n</code></pre> <p>Examples:</p> <pre><code># Update specific plugin\nphlo plugin update phlo-superset\n\n# Update all plugins\nphlo plugin update --all\n\n# Check for updates\nphlo plugin update --check\n</code></pre>"},{"location":"reference/cli-reference/#phlo-plugin-create","title":"phlo plugin create","text":"<p>Create a new plugin scaffold (for plugin development).</p> <pre><code>phlo plugin create PLUGIN_NAME --type TYPE [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--type TYPE          # Plugin type: source, quality, transform, service\n--path PATH          # Custom output path\n--author AUTHOR      # Author name\n--description DESC   # Plugin description\n</code></pre> <p>Examples:</p> <pre><code># Create source connector plugin\nphlo plugin create my-api-source --type source\n\n# Create quality check plugin\nphlo plugin create my-validation --type quality\n\n# Create hook plugin\nphlo plugin create my-hooks --type hook\n\n# Create with custom path\nphlo plugin create my-plugin --type source --path ./plugins/my-plugin\n</code></pre> <p>Creates:</p> <pre><code>phlo-plugin-my-api-source/\n\u251c\u2500\u2500 pyproject.toml           # Package config with entry points\n\u251c\u2500\u2500 README.md                # Documentation\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 phlo_my_api_source/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 plugin.py        # Plugin implementation\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_plugin.py       # Test suite\n</code></pre>"},{"location":"reference/cli-reference/#phlo-plugin-check","title":"phlo plugin check","text":"<p>Validate installed plugins.</p> <pre><code>phlo plugin check [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--json               # Output as JSON\n</code></pre> <p>Examples:</p> <pre><code># Validate all plugins\nphlo plugin check\n\n# JSON output\nphlo plugin check --json\n</code></pre>"},{"location":"reference/cli-reference/#project-commands","title":"Project Commands","text":""},{"location":"reference/cli-reference/#phlo-init","title":"phlo init","text":"<p>Initialize a new Phlo project.</p> <pre><code>phlo init [PROJECT_NAME] [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--template TEMPLATE      # Project template (default: basic)\n--directory PATH         # Target directory\n--no-git                 # Don't initialize git repository\n</code></pre> <p>Templates:</p> <ul> <li><code>basic</code>: Minimal project structure</li> <li><code>complete</code>: Full example with ingestion and transformations</li> </ul> <p>Example:</p> <pre><code>phlo init my-lakehouse --template complete\ncd my-lakehouse\n</code></pre> <p>Creates:</p> <pre><code>my-lakehouse/\n\u251c\u2500\u2500 .env.example          # Local secrets template (.phlo/.env.local)\n\u251c\u2500\u2500 workflows/\n\u2502   \u251c\u2500\u2500 ingestion/\n\u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2514\u2500\u2500 transforms/\n\u2502       \u2514\u2500\u2500 dbt/\n\u251c\u2500\u2500 tests/\n\u2514\u2500\u2500 phlo.yaml\n</code></pre>"},{"location":"reference/cli-reference/#phlo-dev","title":"phlo dev","text":"<p>Start Dagster development server.</p> <pre><code>phlo dev [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--port PORT          # Port for webserver (default: 3000)\n--host HOST          # Host to bind (default: 127.0.0.1)\n--workspace PATH     # Path to workspace.yaml\n</code></pre> <p>Example:</p> <pre><code>phlo dev --port 3000\n</code></pre> <p>Opens Dagster UI at http://localhost:3000</p>"},{"location":"reference/cli-reference/#workflow-commands","title":"Workflow Commands","text":""},{"location":"reference/cli-reference/#phlo-create-workflow","title":"phlo create-workflow","text":"<p>Interactive workflow creation wizard.</p> <pre><code>phlo create-workflow [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--type TYPE          # Workflow type: ingestion, quality, transform\n--domain DOMAIN      # Domain/namespace (e.g., api, files)\n--table TABLE        # Table name\n--unique-key KEY     # Unique key column\n--non-interactive    # Non-interactive mode (requires all options)\n</code></pre> <p>Interactive prompts:</p> <ol> <li>Workflow type (ingestion/quality/transform)</li> <li>Domain name</li> <li>Table name</li> <li>Unique key column</li> <li>Validation schema (optional)</li> <li>Schedule (cron expression)</li> </ol> <p>Example (interactive):</p> <pre><code>phlo create-workflow\n</code></pre> <p>Example (non-interactive):</p> <pre><code>phlo create-workflow \\\n  --type ingestion \\\n  --domain github \\\n  --table events \\\n  --unique-key id\n</code></pre> <p>Creates:</p> <pre><code>workflows/\n\u251c\u2500\u2500 ingestion/\n\u2502   \u2514\u2500\u2500 github/\n\u2502       \u2514\u2500\u2500 events.py\n\u2514\u2500\u2500 schemas/\n    \u2514\u2500\u2500 github.py\n</code></pre>"},{"location":"reference/cli-reference/#asset-commands","title":"Asset Commands","text":""},{"location":"reference/cli-reference/#phlo-materialize","title":"phlo materialize","text":"<p>Materialize Dagster assets.</p> <pre><code>phlo materialize [ASSET_KEYS...] [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--select SELECTOR        # Asset selection query\n--partition PARTITION    # Specific partition to materialize\n--tags TAG=VALUE         # Filter by tags\n--all                    # Materialize all assets\n</code></pre> <p>Selection Syntax:</p> <pre><code>asset_name               # Single asset\nasset_name+              # Asset and downstream\n+asset_name              # Asset and upstream\nasset_name*              # Asset and all dependencies\ntag:group_name           # All assets with tag\n*                        # All assets\n</code></pre> <p>Examples:</p> <pre><code># Single asset\nphlo materialize dlt_glucose_entries\n\n# Asset and downstream\nphlo materialize dlt_glucose_entries+\n\n# Specific partition\nphlo materialize dlt_glucose_entries --partition 2025-01-15\n\n# By tag\nphlo materialize --select \"tag:nightscout\"\n\n# Multiple assets\nphlo materialize asset1 asset2 asset3\n\n# All assets\nphlo materialize --all\n</code></pre>"},{"location":"reference/cli-reference/#testing-commands","title":"Testing Commands","text":""},{"location":"reference/cli-reference/#phlo-test","title":"phlo test","text":"<p>Run tests.</p> <pre><code>phlo test [TEST_PATH] [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--local              # Skip Docker integration tests\n--verbose, -v        # Verbose output\n--marker, -m MARKER  # Run tests with marker\n--keyword, -k EXPR   # Run tests matching keyword\n--coverage           # Generate coverage report\n</code></pre> <p>Markers:</p> <ul> <li><code>integration</code>: Integration tests requiring Docker</li> <li><code>unit</code>: Fast unit tests</li> <li><code>slow</code>: Slow-running tests</li> </ul> <p>Examples:</p> <pre><code># All tests\nphlo test\n\n# Specific test file\nphlo test tests/test_ingestion.py\n\n# Unit tests only\nphlo test -m unit\n\n# Skip integration tests\nphlo test --local\n\n# Specific test\nphlo test -k test_glucose_ingestion\n\n# With coverage\nphlo test --coverage\n</code></pre>"},{"location":"reference/cli-reference/#logging-monitoring-commands","title":"Logging &amp; Monitoring Commands","text":""},{"location":"reference/cli-reference/#phlo-logs","title":"phlo logs","text":"<p>Query and filter structured logs from Dagster runs.</p> <pre><code>phlo logs [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--asset NAME         # Filter by asset name\n--job NAME           # Filter by job name\n--level LEVEL        # Filter by log level: DEBUG, INFO, WARNING, ERROR\n--since TIME         # Filter by time (e.g., 1h, 30m, 2d)\n--run-id ID          # Get logs for specific run\n--follow, -f         # Tail mode - follow new logs in real-time\n--full               # Don't truncate long messages\n--limit N            # Number of logs to retrieve (default: 100)\n--json               # JSON output for scripting\n</code></pre> <p>Examples:</p> <pre><code># View recent logs\nphlo logs\n\n# Filter by asset\nphlo logs --asset dlt_glucose_entries\n\n# Filter by log level\nphlo logs --level ERROR\n\n# Logs from last hour\nphlo logs --since 1h\n\n# Follow logs in real-time\nphlo logs --follow\n\n# Logs for specific run\nphlo logs --run-id abc-123-def\n\n# JSON output for processing\nphlo logs --json --limit 500\n</code></pre>"},{"location":"reference/cli-reference/#phlo-backfill","title":"phlo backfill","text":"<p>Backfill partitioned assets over a date range.</p> <pre><code>phlo backfill [ASSET_NAME] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>ASSET_NAME</code> (optional): Asset to backfill</li> </ul> <p>Options:</p> <pre><code>--start-date DATE    # Start date (YYYY-MM-DD)\n--end-date DATE      # End date (YYYY-MM-DD)\n--partitions DATES   # Comma-separated partition dates (YYYY-MM-DD,...)\n--parallel N         # Number of concurrent partitions (default: 1)\n--resume             # Resume last backfill, skipping completed partitions\n--dry-run            # Show what would be executed without running\n--delay SECS         # Delay between parallel executions in seconds (default: 0.0)\n</code></pre> <p>Examples:</p> <pre><code># Backfill for date range\nphlo backfill dlt_glucose_entries --start-date 2025-01-01 --end-date 2025-01-31\n\n# Specific partitions\nphlo backfill dlt_glucose_entries --partitions 2025-01-15,2025-01-16,2025-01-17\n\n# Parallel backfill (5 concurrent)\nphlo backfill dlt_glucose_entries --start-date 2025-01-01 --end-date 2025-01-31 --parallel 5\n\n# Resume interrupted backfill\nphlo backfill dlt_glucose_entries --resume\n\n# Dry run to see what would execute\nphlo backfill dlt_glucose_entries --start-date 2025-01-01 --end-date 2025-01-07 --dry-run\n\n# With delay between executions\nphlo backfill dlt_glucose_entries --start-date 2025-01-01 --end-date 2025-01-31 --parallel 3 --delay 2.5\n</code></pre>"},{"location":"reference/cli-reference/#branch-commands","title":"Branch Commands","text":"<p>Manage Nessie catalog branches.</p>"},{"location":"reference/cli-reference/#phlo-branch-create","title":"phlo branch create","text":"<p>Create a new branch.</p> <pre><code>phlo branch create BRANCH_NAME [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--from REF           # Create from reference (default: main)\n--description DESC   # Branch description\n</code></pre> <p>Examples:</p> <pre><code># Create from main\nphlo branch create dev\n\n# Create from specific commit\nphlo branch create feature --from abc123\n\n# With description\nphlo branch create dev --description \"Development branch\"\n</code></pre>"},{"location":"reference/cli-reference/#phlo-branch-list","title":"phlo branch list","text":"<p>List all branches.</p> <pre><code>phlo branch list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--pattern PATTERN    # Filter by pattern\n--show-hashes        # Show commit hashes\n</code></pre> <p>Example:</p> <pre><code>phlo branch list\nphlo branch list --pattern \"pipeline/*\"\n</code></pre>"},{"location":"reference/cli-reference/#phlo-branch-merge","title":"phlo branch merge","text":"<p>Merge branches.</p> <pre><code>phlo branch merge SOURCE TARGET [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--strategy STRATEGY  # Merge strategy (default: normal)\n--no-ff              # Create merge commit even if fast-forward\n</code></pre> <p>Examples:</p> <pre><code># Merge dev to main\nphlo branch merge dev main\n\n# Force merge commit\nphlo branch merge dev main --no-ff\n</code></pre>"},{"location":"reference/cli-reference/#phlo-branch-delete","title":"phlo branch delete","text":"<p>Delete a branch.</p> <pre><code>phlo branch delete BRANCH_NAME [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--force, -f          # Force delete even if not merged\n</code></pre> <p>Examples:</p> <pre><code>phlo branch delete old-feature\nphlo branch delete old-feature --force\n</code></pre>"},{"location":"reference/cli-reference/#phlo-branch-diff","title":"phlo branch diff","text":"<p>Show differences between two branches.</p> <pre><code>phlo branch diff SOURCE_BRANCH [TARGET_BRANCH] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>SOURCE_BRANCH</code>: Source branch to compare</li> <li><code>TARGET_BRANCH</code>: Target branch (default: main)</li> </ul> <p>Options:</p> <pre><code>--format FORMAT      # Output format: table, json (default: table)\n</code></pre> <p>Examples:</p> <pre><code># Compare dev to main\nphlo branch diff dev\n\n# Compare two branches\nphlo branch diff feature-a feature-b\n\n# JSON output\nphlo branch diff dev main --format json\n</code></pre>"},{"location":"reference/cli-reference/#catalog-commands","title":"Catalog Commands","text":"<p>Manage the Iceberg catalog (Nessie-backed).</p>"},{"location":"reference/cli-reference/#phlo-catalog-tables","title":"phlo catalog tables","text":"<p>List all Iceberg tables in the catalog.</p> <pre><code>phlo catalog tables [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--namespace NS       # Filter by namespace (e.g., bronze, silver)\n--ref REF            # Nessie branch/tag reference (default: main)\n--format FORMAT      # Output format: table, json (default: table)\n</code></pre> <p>Examples:</p> <pre><code># List all tables\nphlo catalog tables\n\n# List tables in specific namespace\nphlo catalog tables --namespace bronze\n\n# List tables on specific branch\nphlo catalog tables --ref feature-branch\n\n# JSON output\nphlo catalog tables --format json\n</code></pre>"},{"location":"reference/cli-reference/#phlo-catalog-describe","title":"phlo catalog describe","text":"<p>Show detailed table metadata.</p> <pre><code>phlo catalog describe TABLE_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Table to describe (e.g., <code>bronze.events</code>)</li> </ul> <p>Options:</p> <pre><code>--ref REF            # Nessie branch/tag reference (default: main)\n</code></pre> <p>Examples:</p> <pre><code># Describe table\nphlo catalog describe bronze.events\n\n# Describe on specific branch\nphlo catalog describe bronze.events --ref dev\n</code></pre> <p>Output:</p> <p>Shows table metadata including:</p> <ul> <li>Location</li> <li>Current snapshot ID</li> <li>Format version</li> <li>Schema (columns, types, constraints)</li> <li>Partitioning</li> <li>Properties</li> </ul>"},{"location":"reference/cli-reference/#phlo-catalog-history","title":"phlo catalog history","text":"<p>Show table snapshot history.</p> <pre><code>phlo catalog history TABLE_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Table name</li> </ul> <p>Options:</p> <pre><code>--limit N            # Number of snapshots to show (default: 10)\n--ref REF            # Nessie branch/tag reference (default: main)\n</code></pre> <p>Examples:</p> <pre><code># Show recent snapshots\nphlo catalog history bronze.events\n\n# Show last 20 snapshots\nphlo catalog history bronze.events --limit 20\n</code></pre>"},{"location":"reference/cli-reference/#phlo-lineage-show","title":"phlo lineage show","text":"<p>Display asset lineage in ASCII tree format.</p> <pre><code>phlo lineage show ASSET_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>ASSET_NAME</code>: Asset name to show lineage for</li> </ul> <p>Options:</p> <pre><code>--direction DIR      # Direction: upstream, downstream, both (default: both)\n--depth N            # Maximum depth to traverse\n</code></pre> <p>Examples:</p> <pre><code># Show full lineage\nphlo lineage show dlt_glucose_entries\n\n# Show only upstream dependencies\nphlo lineage show dlt_glucose_entries --direction upstream\n\n# Limit depth\nphlo lineage show dlt_glucose_entries --depth 2\n</code></pre>"},{"location":"reference/cli-reference/#phlo-lineage-export","title":"phlo lineage export","text":"<p>Export lineage to external formats.</p> <pre><code>phlo lineage export ASSET_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>ASSET_NAME</code>: Asset name to export lineage for</li> </ul> <p>Options:</p> <pre><code>--format FORMAT      # Export format: dot, mermaid, json (default: dot)\n--output PATH        # Output file path (required)\n</code></pre> <p>Examples:</p> <pre><code># Export to Graphviz DOT\nphlo lineage export dlt_glucose_entries --format dot --output lineage.dot\n\n# Export to Mermaid diagram\nphlo lineage export dlt_glucose_entries --format mermaid --output lineage.md\n\n# Export to JSON\nphlo lineage export dlt_glucose_entries --format json --output lineage.json\n</code></pre>"},{"location":"reference/cli-reference/#configuration-commands","title":"Configuration Commands","text":""},{"location":"reference/cli-reference/#phlo-config-show","title":"phlo config show","text":"<p>Display current configuration.</p> <pre><code>phlo config show [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--format FORMAT      # Output format: yaml, json, env\n--secrets            # Show secrets (masked by default)\n</code></pre> <p>Examples:</p> <pre><code>phlo config show\nphlo config show --format json\nphlo config show --secrets\n</code></pre>"},{"location":"reference/cli-reference/#phlo-config-validate","title":"phlo config validate","text":"<p>Validate configuration files.</p> <pre><code>phlo config validate [FILE]\n</code></pre> <p>Examples:</p> <pre><code># Validate phlo.yaml\nphlo config validate phlo.yaml\n</code></pre>"},{"location":"reference/cli-reference/#phlo-env-export","title":"phlo env export","text":"<p>Export the generated environment configuration.</p> <pre><code>phlo env export [OPTIONS]\n</code></pre> <p>Examples:</p> <pre><code># Export non-secret defaults\nphlo env export\n\n# Export with secrets (from .phlo/.env.local)\nphlo env export --include-secrets\n\n# Write to a file\nphlo env export --include-secrets --output env.full\n</code></pre>"},{"location":"reference/cli-reference/#utility-commands","title":"Utility Commands","text":""},{"location":"reference/cli-reference/#phlo-status","title":"phlo status","text":"<p>Show asset status and freshness.</p> <pre><code>phlo status [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--stale              # Show only stale assets\n--failed             # Show only failed assets\n--group GROUP        # Filter by group\n</code></pre> <p>Example:</p> <pre><code>phlo status\nphlo status --stale\nphlo status --group nightscout\n</code></pre>"},{"location":"reference/cli-reference/#phlo-validate-schema","title":"phlo validate-schema","text":"<p>Validate Pandera schemas.</p> <pre><code>phlo validate-schema SCHEMA_PATH [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--data DATA_PATH     # Validate against sample data\n</code></pre> <p>Example:</p> <pre><code>phlo validate-schema workflows/schemas/events.py\nphlo validate-schema workflows/schemas/events.py --data sample.parquet\n</code></pre>"},{"location":"reference/cli-reference/#phlo-validate-workflow","title":"phlo validate-workflow","text":"<p>Validate workflow configuration.</p> <pre><code>phlo validate-workflow WORKFLOW_PATH [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>WORKFLOW_PATH</code>: Path to workflow file or directory</li> </ul> <p>Options:</p> <pre><code>--fix                # Auto-fix issues where possible\n</code></pre> <p>Examples:</p> <pre><code># Validate single workflow\nphlo validate-workflow workflows/ingestion/api/events.py\n\n# Validate directory\nphlo validate-workflow workflows/ingestion/\n\n# Auto-fix where possible\nphlo validate-workflow workflows/ingestion/weather.py --fix\n</code></pre>"},{"location":"reference/cli-reference/#phlo-schema","title":"phlo schema","text":"<p>Manage Pandera schemas.</p> <pre><code>phlo schema COMMAND [OPTIONS]\n</code></pre> <p>Subcommands:</p>"},{"location":"reference/cli-reference/#list","title":"list","text":"<p>List all available Pandera schemas.</p> <pre><code>phlo schema list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>--domain DOMAIN      # Filter by domain\n--format FORMAT      # Output format: table, json (default: table)\n</code></pre> <p>Examples:</p> <pre><code># List all schemas\nphlo schema list\n\n# Filter by domain\nphlo schema list --domain api\n\n# JSON output\nphlo schema list --format json\n</code></pre>"},{"location":"reference/cli-reference/#optional-plugin-commands","title":"Optional Plugin Commands","text":"<p>The following commands are provided by optional packages. Install the corresponding package to use these commands.</p>"},{"location":"reference/cli-reference/#phlo-postgrest","title":"phlo postgrest","text":"<p>PostgREST configuration and management.</p> <p>Installation: <code>phlo plugin install phlo-postgrest</code></p>"},{"location":"reference/cli-reference/#phlo-hasura","title":"phlo hasura","text":"<p>Hasura GraphQL engine management.</p> <p>Installation: <code>phlo plugin install phlo-hasura</code></p>"},{"location":"reference/cli-reference/#phlo-publishing","title":"phlo publishing","text":"<p>dbt publishing layer management.</p> <p>Installation: <code>phlo plugin install phlo-dbt</code></p>"},{"location":"reference/cli-reference/#phlo-metrics","title":"phlo metrics","text":"<p>Prometheus metrics configuration.</p> <p>Installation: <code>phlo plugin install phlo-metrics</code></p>"},{"location":"reference/cli-reference/#phlo-alerts","title":"phlo alerts","text":"<p>Alerting rules and notifications.</p> <p>Installation: <code>phlo plugin install phlo-alerting</code></p>"},{"location":"reference/cli-reference/#phlo-openmetadata","title":"phlo openmetadata","text":"<p>OpenMetadata catalog integration.</p> <p>Installation: <code>phlo plugin install phlo-openmetadata</code></p> <p>Run <code>phlo &lt;command&gt; --help</code> after installation for command-specific documentation.</p>"},{"location":"reference/cli-reference/#environment-variables","title":"Environment Variables","text":"<p>CLI behavior can be customized with environment variables:</p> <pre><code># Dagster home directory\nexport DAGSTER_HOME=~/.dagster\n\n# Workspace YAML location\nexport DAGSTER_WORKSPACE=/path/to/workspace.yaml\n\n# Phlo configuration\nexport PHLO_CONFIG=/path/to/phlo.yaml\n\n# Log level\nexport PHLO_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"reference/cli-reference/#exit-codes","title":"Exit Codes","text":"<pre><code>0    # Success\n1    # General error\n2    # Command not found\n3    # Invalid arguments\n4    # Configuration error\n5    # Service error\n</code></pre>"},{"location":"reference/cli-reference/#examples-cookbook","title":"Examples Cookbook","text":""},{"location":"reference/cli-reference/#complete-workflow-setup","title":"Complete Workflow Setup","text":"<pre><code># 1. Create project\nphlo init my-project\ncd my-project\n\n# 2. Initialize infrastructure\nphlo services init\n\n# 3. Start services\nphlo services start\n\n# 4. Create workflow\nphlo create-workflow\n\n# 5. Run tests\nphlo test\n\n# 6. Materialize\nphlo materialize --all\n</code></pre>"},{"location":"reference/cli-reference/#development-workflow","title":"Development Workflow","text":"<pre><code># Start Observatory/phlo-api natively (no Docker)\nphlo services start --native\n\n# Create feature branch\nphlo branch create feature-new-workflow\n\n# Create workflow\nphlo create-workflow\n\n# Test workflow\nphlo test workflows/ingestion/api/events.py\n\n# Materialize to test\nphlo materialize dlt_events --partition 2025-01-15\n\n# Merge to main\nphlo branch merge feature-new-workflow main\n</code></pre>"},{"location":"reference/cli-reference/#troubleshooting-workflow","title":"Troubleshooting Workflow","text":"<pre><code># Check service status\nphlo services status\n\n# View logs\nphlo services logs -f dagster-webserver\n\n# Check asset status\nphlo status --failed\n\n# Validate configuration\nphlo config validate\n\n# Re-materialize failed asset\nphlo materialize failed_asset\n</code></pre>"},{"location":"reference/cli-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference - Detailed configuration options</li> <li>Developer Guide - Building workflows</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"reference/common-errors/","title":"Common Errors and Solutions","text":"<p>Quick reference for resolving common Phlo errors.</p>"},{"location":"reference/common-errors/#top-10-errors","title":"Top 10 Errors","text":""},{"location":"reference/common-errors/#1-asset-not-found-in-dagster-ui","title":"1. Asset Not Found in Dagster UI","text":"<p>Error Message:</p> <pre><code>DagsterInvalidDefinitionError: Asset 'my_asset' not found\n</code></pre> <p>Cause: Asset not in the workflows path or workflow import failed</p> <p>Solution: Ensure the asset lives under <code>workflows/</code> and imports cleanly.</p> <p>Then restart Dagster:</p> <pre><code>docker restart dagster-webserver\n</code></pre> <p>Details: Troubleshooting Guide - Asset Not Found</p>"},{"location":"reference/common-errors/#2-unique_key-field-not-in-schema","title":"2. unique_key Field Not in Schema","text":"<p>Error Message:</p> <pre><code>KeyError: 'observation_id'\n</code></pre> <p>Cause: <code>unique_key</code> in decorator doesn't match schema field name</p> <p>Solution:</p> <pre><code># Check your decorator\n@phlo_ingestion(\n    unique_key=\"id\",  # Must match a field in validation_schema\n    validation_schema=MySchema,\n    ...\n)\n\n# Check your schema has this field\nclass MySchema(pa.DataFrameModel):\n    id: Series[str]  # Field must exist\n    ...\n</code></pre> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#3-invalid-cron-expression","title":"3. Invalid Cron Expression","text":"<p>Error Message:</p> <pre><code>ValueError: Invalid cron expression: 'every hour'\n</code></pre> <p>Cause: Cron string is not in standard format</p> <p>Solution:</p> <pre><code># Use standard cron format: minute hour day month weekday\n@phlo_ingestion(\n    cron=\"0 */1 * * *\",  # Every hour at minute 0\n    # NOT: cron=\"every hour\"\n    ...\n)\n</code></pre> <p>Common cron patterns:</p> <ul> <li><code>\"0 */1 * * *\"</code> - Every hour</li> <li><code>\"*/15 * * * *\"</code> - Every 15 minutes</li> <li><code>\"0 0 * * *\"</code> - Daily at midnight</li> <li><code>\"0 9 * * MON\"</code> - Every Monday at 9am</li> </ul> <p>Test your cron: https://crontab.guru/</p> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#4-schema-validation-failed","title":"4. Schema Validation Failed","text":"<p>Error Message:</p> <pre><code>pandera.errors.SchemaError: Column 'temperature' failed validation\nfailure cases:\n   index  failure_case\n0     42         -150.5\n</code></pre> <p>Cause: Data violates Pandera schema constraints</p> <p>Solution:</p> <pre><code># Check constraint in schema\nclass MySchema(pa.DataFrameModel):\n    temperature: Series[float] = pa.Field(\n        ge=-100,  # -150.5 violates this constraint\n        le=100,\n    )\n</code></pre> <p>Fix options:</p> <ol> <li>Clean data before validation</li> <li>Adjust schema constraint if value is valid</li> <li>Add data filtering in asset function</li> </ol> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#5-missing-required-schema-parameter","title":"5. Missing Required Schema Parameter","text":"<p>Error Message:</p> <pre><code>ValueError: Either 'validation_schema' or 'iceberg_schema' must be provided\n</code></pre> <p>Cause: Decorator missing schema parameter</p> <p>Solution:</p> <pre><code># Add validation_schema (recommended)\n@phlo_ingestion(\n    table_name=\"my_table\",\n    unique_key=\"id\",\n    validation_schema=MySchema,  # Add this\n    group=\"my_group\",\n)\n</code></pre> <p>Best practice: Always use <code>validation_schema</code> - Iceberg schema is auto-generated.</p> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#6-dlt-pipeline-failed","title":"6. DLT Pipeline Failed","text":"<p>Error Message:</p> <pre><code>dlt.pipeline.exceptions.PipelineStepFailed: Pipeline step 'extract' failed\nCaused by: requests.exceptions.ConnectionError: Connection refused\n</code></pre> <p>Cause: Usually API connection issue, credentials, or network problem</p> <p>Common causes:</p> <ol> <li>API endpoint is down</li> <li>Invalid API credentials</li> <li>Network connectivity issue</li> <li>Rate limit exceeded</li> </ol> <p>Solution:</p> <pre><code># 1. Check API status page\n# 2. Verify credentials in .phlo/.env.local\n# 3. Test connection manually\ncurl https://api.example.com/endpoint\n\n# 4. Check Dagster logs\ndocker logs dagster-webserver\n\n# 5. Retry materialization\ndocker exec dagster-webserver dagster asset materialize \\\n  --select my_asset --partition 2024-01-15\n</code></pre> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#7-iceberg-table-doesnt-exist","title":"7. Iceberg Table Doesn't Exist","text":"<p>Error Message:</p> <pre><code>pyiceberg.exceptions.NoSuchTableError: Table does not exist: raw.my_table\n</code></pre> <p>Cause: Asset not yet materialized</p> <p>Solution:</p> <pre><code># Materialize the asset first\ndocker exec dagster-webserver dagster asset materialize \\\n  --select my_asset --partition 2024-01-15\n\n# Or via Dagster UI\n# http://localhost:3000 \u2192 Assets \u2192 my_asset \u2192 Materialize\n</code></pre> <p>Note: Tables are created on first materialization, not when asset is defined.</p> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#8-docker-service-not-running","title":"8. Docker Service Not Running","text":"<p>Error Message:</p> <pre><code>requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionRefusedError(111, 'Connection refused'))\n</code></pre> <p>Cause: Nessie, MinIO, Trino, or Dagster not running</p> <p>Solution:</p> <pre><code># Check service status\ndocker compose ps\n\n# Start services\nmake up-core up-query\n\n# Check logs\ndocker logs nessie\ndocker logs minio\ndocker logs trino\ndocker logs dagster-webserver\n\n# Verify connection\ncurl http://localhost:19120/api/v2/config  # Nessie\ncurl http://localhost:9001  # MinIO console\n</code></pre> <p>Details: Error Message Audit</p>"},{"location":"reference/common-errors/#9-import-error-in-python-code","title":"9. Import Error in Python Code","text":"<p>Error Message:</p> <pre><code>ModuleNotFoundError: No module named 'phlo'\n</code></pre> <p>Cause: Phlo not installed or PYTHONPATH not set</p> <p>Solution:</p> <pre><code># Inside Docker container (preferred)\ndocker exec -it dagster-webserver bash\npip install -e /app\n\n# Or set PYTHONPATH\nexport PYTHONPATH=/home/user/phlo/src:$PYTHONPATH\n</code></pre>"},{"location":"reference/common-errors/#10-asset-shows-in-ui-but-wont-materialize","title":"10. Asset Shows in UI But Won't Materialize","text":"<p>Error Message: No clear error, just \"Failed\" status</p> <p>Cause: Various - check logs for details</p> <p>Solution:</p> <pre><code># 1. Check Dagster logs\ndocker logs dagster-webserver | tail -100\n\n# 2. Check asset run logs in UI\n# Click on failed run \u2192 View logs\n\n# 3. Common issues:\n# - API credentials missing/invalid\n# - Schema validation failures\n# - Network timeouts\n# - Iceberg catalog connection issues\n\n# 4. Restart services if needed\ndocker restart dagster-webserver\ndocker restart nessie\n</code></pre>"},{"location":"reference/common-errors/#error-categories","title":"Error Categories","text":""},{"location":"reference/common-errors/#configuration-errors","title":"Configuration Errors","text":"Error Cause Quick Fix Asset not found Workflow not discovered Ensure asset lives under <code>workflows/</code> and imports cleanly unique_key not in schema Field name mismatch Match field names exactly Invalid cron Wrong format Use standard cron format Missing schema No validation_schema Add Pandera schema"},{"location":"reference/common-errors/#runtime-errors","title":"Runtime Errors","text":"Error Cause Quick Fix Schema validation failed Data violates constraints Check Pandera schema constraints DLT pipeline failed API/network issue Check credentials, connection Table doesn't exist Asset not materialized Materialize asset first Connection refused Service not running Start Docker services"},{"location":"reference/common-errors/#development-errors","title":"Development Errors","text":"Error Cause Quick Fix ModuleNotFoundError Import issues Install phlo in editable mode Permission denied File permissions Check Docker volume mounts Port already in use Port conflict Stop conflicting service or change port"},{"location":"reference/common-errors/#debugging-workflow","title":"Debugging Workflow","text":"<p>When you encounter an error:</p> <ol> <li>Read the error message carefully</li> <li> <p>Look for field names, file paths, line numbers</p> </li> <li> <p>Check this guide</p> </li> <li>Search for similar error message</li> <li> <p>Try suggested solution</p> </li> <li> <p>Check logs</p> </li> </ol> <p>```bash    # Dagster logs    docker logs dagster-webserver | tail -100</p> <p># All service logs    make logs    ```</p> <ol> <li>Verify configuration</li> <li>Schema field names match unique_key</li> <li>Asset file is under <code>workflows/</code> and imports without errors</li> <li>Cron expression is valid</li> <li> <p>Docker services are running</p> </li> <li> <p>Try minimal reproduction</p> </li> <li>Test with simple data</li> <li>Test schema validation separately</li> <li> <p>Test API connection manually</p> </li> <li> <p>Search documentation</p> </li> <li>Troubleshooting Guide</li> <li> <p>Workflow Development Guide</p> </li> <li> <p>Ask for help</p> </li> <li>GitHub Discussions</li> <li>GitHub Issues</li> </ol>"},{"location":"reference/common-errors/#prevention-best-practices","title":"Prevention Best Practices","text":""},{"location":"reference/common-errors/#1-validate-schema-early","title":"1. Validate Schema Early","text":"<pre><code># Test schema validation before full pipeline\nimport pandas as pd\n\ntest_data = pd.DataFrame([{...}])\nMySchema.validate(test_data)  # Fails fast if schema is wrong\n</code></pre>"},{"location":"reference/common-errors/#2-match-field-names-exactly","title":"2. Match Field Names Exactly","text":"<pre><code># unique_key must match schema field exactly\n@phlo_ingestion(\n    unique_key=\"id\",  # Must match field name below\n    validation_schema=MySchema,\n)\n\nclass MySchema(pa.DataFrameModel):\n    id: Series[str]  # Exact match\n</code></pre>"},{"location":"reference/common-errors/#3-test-cron-expressions","title":"3. Test Cron Expressions","text":"<p>Use https://crontab.guru/ to validate cron expressions before using them.</p>"},{"location":"reference/common-errors/#4-use-environment-variables-for-credentials","title":"4. Use Environment Variables for Credentials","text":"<pre><code># .phlo/.env.local file\nAPI_KEY=your_key_here\n\n# In code\nimport os\napi_key = os.getenv(\"API_KEY\")\n</code></pre>"},{"location":"reference/common-errors/#5-check-docker-services-before-materializing","title":"5. Check Docker Services Before Materializing","text":"<pre><code># Quick health check\ndocker compose ps\n\n# All services should show \"Up\" or \"healthy\"\n</code></pre>"},{"location":"reference/common-errors/#6-use-descriptive-asset-names","title":"6. Use Descriptive Asset Names","text":"<pre><code># Good\ndef weather_observations(partition_date: str):\n    pass\n\n# Bad\ndef data(partition_date: str):  # Too generic\n    pass\n</code></pre>"},{"location":"reference/common-errors/#7-add-logging","title":"7. Add Logging","text":"<pre><code>def my_asset(partition_date: str):\n    print(f\"Processing partition: {partition_date}\")\n    # ... your code ...\n    print(f\"Fetched {len(data)} rows\")\n    return source\n</code></pre>"},{"location":"reference/common-errors/#getting-more-help","title":"Getting More Help","text":""},{"location":"reference/common-errors/#documentation","title":"Documentation","text":"<ul> <li>Full Troubleshooting: Troubleshooting Guide</li> <li>Workflow Development: Workflow Development Guide</li> <li>Testing: Testing Guide</li> <li>Architecture: Architecture</li> </ul>"},{"location":"reference/common-errors/#community","title":"Community","text":"<ul> <li>GitHub Discussions: https://github.com/iamgp/phlo/discussions</li> <li>GitHub Issues: https://github.com/iamgp/phlo/issues</li> </ul>"},{"location":"reference/common-errors/#tips-for-asking-questions","title":"Tips for Asking Questions","text":"<ol> <li>Include error message: Full stack trace</li> <li>Include context: What were you trying to do?</li> <li>Include code: Schema, decorator config, asset function</li> <li>Include environment: Docker logs, service status</li> <li>Include what you tried: Debugging steps taken</li> </ol> <p>Found a solution not listed here? Contribute to this guide via pull request!</p>"},{"location":"reference/configuration-reference/","title":"Configuration Reference","text":"<p>Complete reference for configuring Phlo.</p>"},{"location":"reference/configuration-reference/#configuration-system","title":"Configuration System","text":"<p>Phlo uses multiple configuration sources:</p> <ol> <li>Infrastructure defaults (<code>phlo.yaml</code>, <code>env:</code>)</li> <li>Local secrets/overrides (<code>.phlo/.env.local</code>)</li> <li>Runtime environment (process environment variables)</li> <li>Python settings (<code>phlo.config</code>)</li> <li>Runtime configuration (Dagster run config)</li> </ol>"},{"location":"reference/configuration-reference/#environment-variables","title":"Environment Variables","text":"<p>Environment variables are materialized into <code>.phlo/.env</code> (generated, non-secret defaults) and <code>.phlo/.env.local</code> (local secrets). Edit <code>phlo.yaml</code> for committed defaults and <code>.phlo/.env.local</code> for secrets.</p>"},{"location":"reference/configuration-reference/#database-configuration","title":"Database Configuration","text":"<p>PostgreSQL database settings:</p> <pre><code># Host and port\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=10000\n\n# Credentials\nPOSTGRES_USER=lake\nPOSTGRES_PASSWORD=phlo\n\n# Database\nPOSTGRES_DB=lakehouse\nPOSTGRES_MART_SCHEMA=marts\n\n# Lineage tracking database (optional, defaults to Dagster Postgres connection)\nPHLO_LINEAGE_DB_URL=postgresql://lake:phlo@postgres:10000/lakehouse\n# Alternative: DAGSTER_PG_DB_CONNECTION_STRING (alias for lineage_db_url)\n</code></pre> <p>Connection string format:</p> <pre><code>postgresql://lake:phlo@postgres:10000/lakehouse\n</code></pre>"},{"location":"reference/configuration-reference/#storage-configuration","title":"Storage Configuration","text":"<p>MinIO S3-compatible object storage:</p> <pre><code># Host and ports\nMINIO_HOST=minio\nMINIO_API_PORT=10001\nMINIO_CONSOLE_PORT=10002\n\n# Credentials\nMINIO_ROOT_USER=minioadmin\nMINIO_ROOT_PASSWORD=minioadmin\n</code></pre> <p>MinIO endpoint:</p> <pre><code>http://minio:10001\n</code></pre> <p>Console UI: http://localhost:10002</p>"},{"location":"reference/configuration-reference/#catalog-configuration","title":"Catalog Configuration","text":"<p>Nessie Git-like catalog:</p> <pre><code># Version and connectivity\nNESSIE_VERSION=0.106.0\nNESSIE_PORT=19120\nNESSIE_HOST=nessie\nNESSIE_API_VERSION=v1\n</code></pre> <p>API endpoints:</p> <ul> <li>v1 API: <code>http://nessie:10003/api/v1</code></li> <li>v2 API: <code>http://nessie:10003/api/v2</code></li> <li>Iceberg REST: <code>http://nessie:10003/iceberg</code></li> </ul>"},{"location":"reference/configuration-reference/#query-engine-configuration","title":"Query Engine Configuration","text":"<p>Trino distributed SQL engine:</p> <pre><code># Version and connectivity\nTRINO_VERSION=477\nTRINO_PORT=10005\nTRINO_HOST=trino\n\n# Catalog\nTRINO_CATALOG=iceberg\n</code></pre> <p>Connection string:</p> <pre><code>trino://trino:10005/iceberg_dev\n</code></pre>"},{"location":"reference/configuration-reference/#data-lake-configuration","title":"Data Lake Configuration","text":"<p>Apache Iceberg table format:</p> <pre><code># Storage paths\nICEBERG_WAREHOUSE_PATH=s3://lake/warehouse\nICEBERG_STAGING_PATH=s3://lake/stage\n\n# Default namespace\nICEBERG_DEFAULT_NAMESPACE=raw\n\n# Default branch reference\nICEBERG_NESSIE_REF=main\n</code></pre> <p>Warehouse paths by branch:</p> <pre><code># main branch\ns3://lake/warehouse\n\n# Custom branch\ns3://lake/warehouse@feature-branch\n</code></pre>"},{"location":"reference/configuration-reference/#branch-management","title":"Branch Management","text":"<p>Nessie branch lifecycle configuration:</p> <pre><code># Retention periods (days)\nBRANCH_RETENTION_DAYS=7\nBRANCH_RETENTION_DAYS_FAILED=2\n\n# Automation\nAUTO_PROMOTE_ENABLED=true\nBRANCH_CLEANUP_ENABLED=false\n</code></pre> <p>Behavior:</p> <ul> <li><code>BRANCH_RETENTION_DAYS</code>: Days to keep successful pipeline branches</li> <li><code>BRANCH_RETENTION_DAYS_FAILED</code>: Days to keep failed pipeline branches</li> <li><code>AUTO_PROMOTE_ENABLED</code>: Auto-merge to main when quality checks pass</li> <li><code>BRANCH_CLEANUP_ENABLED</code>: Automatically delete old branches</li> </ul>"},{"location":"reference/configuration-reference/#validation-configuration","title":"Validation Configuration","text":"<p>Data quality validation settings:</p> <pre><code># Freshness blocking\nFRESHNESS_BLOCKS_PROMOTION=false\n\n# Pandera validation level\nPANDERA_CRITICAL_LEVEL=error  # error, warning, or skip\n\n# Validation retry\nVALIDATION_RETRY_ENABLED=true\nVALIDATION_RETRY_MAX_ATTEMPTS=3\nVALIDATION_RETRY_DELAY_SECONDS=300  # seconds\n</code></pre> <p>Pandera levels:</p> <ul> <li><code>error</code>: Validation failures block pipeline</li> <li><code>warning</code>: Log warnings but continue</li> <li><code>skip</code>: Skip validation entirely (not recommended)</li> </ul>"},{"location":"reference/configuration-reference/#service-configuration","title":"Service Configuration","text":""},{"location":"reference/configuration-reference/#superset","title":"Superset","text":"<p>Business intelligence and visualization:</p> <pre><code>SUPERSET_PORT=10007\nSUPERSET_ADMIN_USER=admin\nSUPERSET_ADMIN_PASSWORD=admin\nSUPERSET_ADMIN_EMAIL=admin@superset.com\n</code></pre> <p>Access: http://localhost:10007</p>"},{"location":"reference/configuration-reference/#dagster","title":"Dagster","text":"<p>Orchestration platform:</p> <pre><code>DAGSTER_PORT=10006\n\n# Executor configuration (set only one)\nPHLO_FORCE_IN_PROCESS_EXECUTOR=false   # Force in-process executor\nPHLO_FORCE_MULTIPROCESS_EXECUTOR=false # Force multiprocess executor\n\n# Host platform (auto-detected, but can be set explicitly for daemon/webserver on macOS)\nPHLO_HOST_PLATFORM=  # Darwin, Linux, or Windows\n</code></pre> <p>Access: http://localhost:10006</p>"},{"location":"reference/configuration-reference/#hubflask","title":"Hub/Flask","text":"<p>Internal API server:</p> <pre><code>HUB_APP_PORT=10009\nHUB_DEBUG=false\n</code></pre>"},{"location":"reference/configuration-reference/#integration-services","title":"Integration Services","text":""},{"location":"reference/configuration-reference/#api-layer","title":"API Layer","text":"<p>JWT authentication:</p> <pre><code>JWT_SECRET_KEY=your-secret-key-change-this-in-production\nJWT_ALGORITHM=HS256\nJWT_EXPIRATION_HOURS=24\n</code></pre>"},{"location":"reference/configuration-reference/#hasura-graphql","title":"Hasura GraphQL","text":"<pre><code>HASURA_GRAPHQL_PORT=10012\nHASURA_GRAPHQL_ADMIN_SECRET=hasura-admin-secret\nHASURA_GRAPHQL_ENABLE_CONSOLE=true\n</code></pre> <p>Access: http://localhost:10012</p>"},{"location":"reference/configuration-reference/#postgrest","title":"PostgREST","text":"<pre><code>POSTGREST_PORT=10011\nPOSTGREST_DB_SCHEMA=marts\nPOSTGREST_DB_ANON_ROLE=web_anon\n</code></pre> <p>Access: http://localhost:10011</p>"},{"location":"reference/configuration-reference/#openmetadata","title":"OpenMetadata","text":"<p>Data catalog and governance:</p> <pre><code>OPENMETADATA_HOST=openmetadata-server\nOPENMETADATA_PORT=8585\nOPENMETADATA_USERNAME=admin\nOPENMETADATA_PASSWORD=admin\nOPENMETADATA_VERIFY_SSL=false\nOPENMETADATA_SYNC_ENABLED=true\nOPENMETADATA_SYNC_INTERVAL_SECONDS=300  # Minimum interval between syncs\n</code></pre> <p>Access: http://localhost:8585</p>"},{"location":"reference/configuration-reference/#observability-stack","title":"Observability Stack","text":""},{"location":"reference/configuration-reference/#prometheus","title":"Prometheus","text":"<p>Metrics collection:</p> <pre><code>PROMETHEUS_PORT=9090\n</code></pre> <p>Access: http://localhost:9090</p>"},{"location":"reference/configuration-reference/#loki","title":"Loki","text":"<p>Log aggregation:</p> <pre><code>LOKI_PORT=3100\n</code></pre>"},{"location":"reference/configuration-reference/#grafana","title":"Grafana","text":"<p>Dashboards and visualization:</p> <pre><code>GRAFANA_PORT=3000\nGRAFANA_ADMIN_USER=admin\nGRAFANA_ADMIN_PASSWORD=admin\n</code></pre> <p>Access: http://localhost:3000</p>"},{"location":"reference/configuration-reference/#alerting-configuration","title":"Alerting Configuration","text":""},{"location":"reference/configuration-reference/#slack","title":"Slack","text":"<pre><code>PHLO_ALERT_SLACK_WEBHOOK=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\nPHLO_ALERT_SLACK_CHANNEL=#data-alerts\n</code></pre>"},{"location":"reference/configuration-reference/#pagerduty","title":"PagerDuty","text":"<pre><code>PHLO_ALERT_PAGERDUTY_KEY=your-integration-key\n</code></pre>"},{"location":"reference/configuration-reference/#email-smtp","title":"Email (SMTP)","text":"<pre><code>PHLO_ALERT_EMAIL_SMTP_HOST=smtp.gmail.com\nPHLO_ALERT_EMAIL_SMTP_PORT=587\nPHLO_ALERT_EMAIL_SMTP_USER=your-email@gmail.com\nPHLO_ALERT_EMAIL_SMTP_PASSWORD=your-app-password\nPHLO_ALERT_EMAIL_RECIPIENTS=team@yourdomain.com,alerts@yourdomain.com  # Comma-separated list\n</code></pre>"},{"location":"reference/configuration-reference/#security-configuration","title":"Security Configuration","text":"<p>See Security Setup Guide for detailed setup instructions.</p>"},{"location":"reference/configuration-reference/#trino-authentication","title":"Trino Authentication","text":"<pre><code># Authentication type (PASSWORD, OAUTH2, JWT, CERTIFICATE, KERBEROS, or empty)\nTRINO_AUTH_TYPE=\n\n# LDAP Authentication (when TRINO_AUTH_TYPE=PASSWORD)\nTRINO_LDAP_URL=ldaps://ldap.example.com:636\nTRINO_LDAP_USER_BIND_PATTERN=${USER}@example.com\n\n# OAuth2/OIDC Authentication (when TRINO_AUTH_TYPE=OAUTH2)\nTRINO_OAUTH2_ISSUER=https://auth.example.com\nTRINO_OAUTH2_CLIENT_ID=trino\nTRINO_OAUTH2_CLIENT_SECRET=your-client-secret\n\n# HTTPS/TLS\nTRINO_HTTPS_ENABLED=false\nTRINO_HTTPS_KEYSTORE_PATH=/etc/trino/keystore.jks\nTRINO_HTTPS_KEYSTORE_PASSWORD=keystore-password\n\n# Access Control\nTRINO_ACCESS_CONTROL_TYPE=file\nTRINO_ACCESS_CONTROL_CONFIG_FILE=/etc/trino/access-control.json\n</code></pre>"},{"location":"reference/configuration-reference/#nessie-authentication","title":"Nessie Authentication","text":"<pre><code># OIDC/OAuth2 Authentication\nNESSIE_OIDC_ENABLED=false\nNESSIE_OIDC_SERVER_URL=https://auth.example.com/realms/phlo\nNESSIE_OIDC_CLIENT_ID=nessie\nNESSIE_OIDC_CLIENT_SECRET=your-client-secret\nNESSIE_OIDC_ISSUER=https://auth.example.com\n\n# Authorization\nNESSIE_AUTHZ_ENABLED=false\n</code></pre>"},{"location":"reference/configuration-reference/#minio-security","title":"MinIO Security","text":"<pre><code># TLS (set server URL to enable HTTPS)\nMINIO_SERVER_URL=https://minio.example.com\n\n# OIDC Authentication\nMINIO_OIDC_CONFIG_URL=https://auth.example.com/.well-known/openid-configuration\nMINIO_OIDC_CLIENT_ID=minio\nMINIO_OIDC_CLIENT_SECRET=your-client-secret\nMINIO_OIDC_CLAIM_NAME=policy\nMINIO_OIDC_SCOPES=openid\n\n# LDAP Authentication\nMINIO_LDAP_SERVER=ldap.example.com:636\nMINIO_LDAP_BIND_DN=cn=admin,dc=example,dc=com\nMINIO_LDAP_BIND_PASSWORD=ldap-password\nMINIO_LDAP_USER_BASE_DN=ou=users,dc=example,dc=com\nMINIO_LDAP_USER_FILTER=(uid=%s)\n\n# Encryption at Rest\nMINIO_AUTO_ENCRYPTION=off\n\n# Audit Logging\nMINIO_AUDIT_ENABLED=off\nMINIO_AUDIT_ENDPOINT=http://audit-service:8080/logs\n</code></pre>"},{"location":"reference/configuration-reference/#postgresql-ssl","title":"PostgreSQL SSL","text":"<pre><code># SSL Mode (disable, allow, prefer, require, verify-ca, verify-full)\nPOSTGRES_SSL_MODE=prefer\nPOSTGRES_SSL_CERT_FILE=/path/to/cert.pem\nPOSTGRES_SSL_KEY_FILE=/path/to/key.pem\nPOSTGRES_SSL_CA_FILE=/path/to/ca.pem\n</code></pre>"},{"location":"reference/configuration-reference/#dbt-configuration","title":"dbt Configuration","text":"<pre><code># dbt artifact paths (defaults to &lt;DBT_PROJECT_DIR&gt;/target when unset)\nDBT_MANIFEST_PATH=workflows/transforms/dbt/target/manifest.json\nDBT_CATALOG_PATH=workflows/transforms/dbt/target/catalog.json\n\n# dbt project directory\nDBT_PROJECT_DIR=workflows/transforms/dbt\n\n# Workflows path (for external projects)\nWORKFLOWS_PATH=workflows\n</code></pre>"},{"location":"reference/configuration-reference/#plugin-configuration","title":"Plugin Configuration","text":"<pre><code># Plugin system\nPLUGINS_ENABLED=true\nPLUGINS_AUTO_DISCOVER=true\n\n# Whitelist/blacklist (comma-separated)\nPLUGINS_WHITELIST=plugin1,plugin2\nPLUGINS_BLACKLIST=deprecated_plugin\n\n# Plugin registry\nPLUGIN_REGISTRY_URL=https://registry.phlo.dev/plugins.json\nPLUGIN_REGISTRY_CACHE_TTL_SECONDS=3600\nPLUGIN_REGISTRY_TIMEOUT_SECONDS=10\n</code></pre>"},{"location":"reference/configuration-reference/#infrastructure-configuration-phloyaml","title":"Infrastructure Configuration (phlo.yaml)","text":"<p>Project-level configuration in <code>phlo.yaml</code>:</p> <pre><code>name: my-project\ndescription: My data lakehouse project\n\ninfrastructure:\n  # Container naming pattern\n  container_naming_pattern: \"{{project}}-{{service}}-1\"\n\n  # Service-specific configuration\n  services:\n    dagster_webserver:\n      container_name: null # Use pattern\n      service_name: dagster-webserver\n      host: localhost\n      internal_host: dagster-webserver\n      port: 10006\n\n    postgres:\n      container_name: null\n      service_name: postgres\n      host: localhost\n      internal_host: postgres\n      port: 10000\n      credentials:\n        user: postgres\n        password: postgres\n        database: cascade\n\n    minio:\n      container_name: null\n      service_name: minio\n      host: localhost\n      internal_host: minio\n      api_port: 10001\n      console_port: 10002\n\n    nessie:\n      container_name: null\n      service_name: nessie\n      host: localhost\n      internal_host: nessie\n      port: 10003\n\n    trino:\n      container_name: null\n      service_name: trino\n      host: localhost\n      internal_host: trino\n      port: 10005\n</code></pre>"},{"location":"reference/configuration-reference/#loading-infrastructure-config","title":"Loading Infrastructure Config","text":"<pre><code>from phlo.infrastructure.config import (\n    load_infrastructure_config,\n    get_container_name,\n    get_service_config\n)\n\n# Load config\nconfig = load_infrastructure_config()\n\n# Get container name\ncontainer = get_container_name(\"dagster-webserver\")\n# Returns: \"my-project-dagster-webserver-1\"\n\n# Get service config\nservice = get_service_config(\"postgres\")\n# Returns: dict with host, port, credentials, etc.\n</code></pre>"},{"location":"reference/configuration-reference/#python-configuration-configpy","title":"Python Configuration (config.py)","text":"<p>Programmatic access to configuration:</p> <pre><code>from phlo.config import settings\n\n# Database\nsettings.postgres_host\nsettings.postgres_port\nsettings.get_postgres_connection_string()\n\n# MinIO\nsettings.minio_endpoint\n# Returns: \"http://minio:10001\"\n\n# Nessie\nsettings.nessie_uri\nsettings.nessie_api_v1_uri\nsettings.nessie_iceberg_rest_uri\n\n# Trino\nsettings.trino_connection_string\n# Returns: \"trino://trino:10005/iceberg_dev\"\n\n# Iceberg\nsettings.iceberg_warehouse_path\nsettings.get_iceberg_warehouse_for_branch(\"main\")\n# Returns: \"s3://lake/warehouse\"\n\nsettings.get_iceberg_warehouse_for_branch(\"feature\")\n# Returns: \"s3://lake/warehouse@feature\"\n</code></pre>"},{"location":"reference/configuration-reference/#runtime-configuration","title":"Runtime Configuration","text":"<p>Dagster run configuration for asset execution:</p> <pre><code># Example run config\n{\n    \"ops\": {\n        \"my_asset\": {\n            \"config\": {\n                \"partition_date\": \"2025-01-15\",\n                \"full_refresh\": false\n            }\n        }\n    },\n    \"resources\": {\n        \"iceberg\": {\n            \"config\": {\n                \"ref\": \"pipeline/run-abc123\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"reference/configuration-reference/#port-reference","title":"Port Reference","text":"<p>Standard port assignments:</p> <pre><code>10000  PostgreSQL\n10001  MinIO API\n10002  MinIO Console\n10003  Nessie\n10005  Trino\n10006  Dagster\n10007  Superset\n10009  Hub/Flask\n10011  PostgREST\n10012  Hasura GraphQL\n8585   OpenMetadata\n3000   Grafana\n9090   Prometheus\n3100   Loki\n</code></pre>"},{"location":"reference/configuration-reference/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"reference/configuration-reference/#development","title":"Development","text":"<pre><code># phlo.yaml (development)\nenv:\n  POSTGRES_HOST: localhost\n  MINIO_HOST: localhost\n  DAGSTER_HOST_PLATFORM: local\n  HUB_DEBUG: true\n  AUTO_PROMOTE_ENABLED: true\n  BRANCH_CLEANUP_ENABLED: false\n</code></pre>"},{"location":"reference/configuration-reference/#staging","title":"Staging","text":"<pre><code># phlo.staging.yaml\nenv:\n  POSTGRES_HOST: postgres-staging\n  MINIO_HOST: minio-staging\n  DAGSTER_HOST_PLATFORM: docker\n  HUB_DEBUG: false\n  AUTO_PROMOTE_ENABLED: true\n  BRANCH_CLEANUP_ENABLED: true\n  BRANCH_RETENTION_DAYS: 3\n</code></pre>"},{"location":"reference/configuration-reference/#production","title":"Production","text":"<pre><code># phlo.production.yaml\nenv:\n  POSTGRES_HOST: postgres-prod.internal\n  POSTGRES_PORT: 5432\n  MINIO_HOST: minio-prod.internal\n  NESSIE_HOST: nessie-prod.internal\n  TRINO_HOST: trino-prod.internal\n  DAGSTER_HOST_PLATFORM: k8s\n  DAGSTER_EXECUTOR: multiprocess\n  HUB_DEBUG: false\n  AUTO_PROMOTE_ENABLED: true\n  BRANCH_CLEANUP_ENABLED: true\n  BRANCH_RETENTION_DAYS: 7\n  BRANCH_RETENTION_DAYS_FAILED: 2\n  FRESHNESS_BLOCKS_PROMOTION: true\n  PANDERA_CRITICAL_LEVEL: error\n  VALIDATION_RETRY_ENABLED: true\n  OPENMETADATA_SYNC_ENABLED: true\n</code></pre>"},{"location":"reference/configuration-reference/#security-best-practices","title":"Security Best Practices","text":""},{"location":"reference/configuration-reference/#secrets-management","title":"Secrets Management","text":"<p>Do not commit secrets to version control:</p> <pre><code># .gitignore\n.phlo/.env.local\n</code></pre> <p>Use environment-specific files:</p> <pre><code>.env.example      # Secrets template (committed)\n.phlo/.env.local  # Local secrets (ignored)\n</code></pre>"},{"location":"reference/configuration-reference/#strong-passwords","title":"Strong Passwords","text":"<p>Generate secure passwords:</p> <pre><code># Generate random password\nopenssl rand -base64 32\n\n# Use in .phlo/.env.local\nPOSTGRES_PASSWORD=&lt;generated-password&gt;\nMINIO_ROOT_PASSWORD=&lt;generated-password&gt;\nJWT_SECRET_KEY=&lt;generated-password&gt;\n</code></pre>"},{"location":"reference/configuration-reference/#minimal-permissions","title":"Minimal Permissions","text":"<p>Use least-privilege principle:</p> <pre><code># Read-only user for BI tools\nPOSTGRES_BI_USER=bi_readonly\nPOSTGRES_BI_PASSWORD=&lt;password&gt;\n\n# Grant only SELECT on marts\nGRANT SELECT ON SCHEMA marts TO bi_readonly;\n</code></pre>"},{"location":"reference/configuration-reference/#configuration-validation","title":"Configuration Validation","text":""},{"location":"reference/configuration-reference/#validate-with-cli","title":"Validate with CLI","text":"<pre><code># Validate phlo.yaml\nphlo config validate phlo.yaml\n\n# Show current config\nphlo config show\n\n# Show with secrets (masked by default)\nphlo config show --secrets\n</code></pre>"},{"location":"reference/configuration-reference/#validation-in-python","title":"Validation in Python","text":"<pre><code>from phlo.config import settings\nfrom pydantic import ValidationError\n\ntry:\n    # Access settings (validates on load)\n    conn_str = settings.get_postgres_connection_string()\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"reference/configuration-reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/configuration-reference/#connection-issues","title":"Connection Issues","text":"<pre><code># Test PostgreSQL\npsql postgresql://postgres:password@localhost:10000/cascade\n\n# Test MinIO\nmc alias set local http://localhost:10001 minioadmin minioadmin\nmc ls local\n\n# Test Nessie\ncurl http://localhost:10003/api/v2/config\n\n# Test Trino\ndocker exec -it phlo-trino-1 trino\n</code></pre>"},{"location":"reference/configuration-reference/#port-conflicts","title":"Port Conflicts","text":"<p>Check if ports are in use:</p> <pre><code># macOS/Linux\nlsof -i :10000\nlsof -i :10006\n\n# Windows\nnetstat -ano | findstr :10000\n</code></pre> <p>Change ports in <code>phlo.yaml</code> (env:):</p> <pre><code>POSTGRES_PORT=15432\nDAGSTER_PORT=13000\n</code></pre>"},{"location":"reference/configuration-reference/#permission-errors","title":"Permission Errors","text":"<p>Fix Docker volume permissions:</p> <pre><code>sudo chown -R $USER:$USER .phlo/\nchmod -R 755 .phlo/\n</code></pre>"},{"location":"reference/configuration-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Setup instructions</li> <li>CLI Reference - Command-line tools</li> <li>Developer Guide - Building workflows</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"reference/duckdb-queries/","title":"DuckDB Iceberg Extension for Ad-hoc Analysis","text":"<p>This guide explains how to query Phlo's Iceberg tables directly using DuckDB's Iceberg extension, enabling fast ad-hoc analysis without going through Trino.</p>"},{"location":"reference/duckdb-queries/#why-duckdb-iceberg","title":"Why DuckDB + Iceberg?","text":"<ul> <li>Fast ad-hoc queries: DuckDB is optimized for analytical queries on local machines</li> <li>Direct Iceberg access: Read Iceberg tables directly from MinIO/S3</li> <li>No server required: Runs entirely on your laptop</li> <li>Perfect for exploration: Jupyter notebooks, scripts, and interactive analysis</li> <li>Compatible: Same data as Trino/dbt pipeline, different query engine</li> </ul>"},{"location":"reference/duckdb-queries/#prerequisites","title":"Prerequisites","text":"<ul> <li>DuckDB v1.1.0 or later (supports Iceberg extension)</li> <li>Access to MinIO endpoint (default: <code>localhost:10001</code>)</li> <li>MinIO credentials from <code>.phlo/.env.local</code></li> </ul>"},{"location":"reference/duckdb-queries/#installation","title":"Installation","text":""},{"location":"reference/duckdb-queries/#1-install-duckdb","title":"1. Install DuckDB","text":"<pre><code># macOS (Homebrew)\nbrew install duckdb\n\n# or download from https://duckdb.org/docs/installation/\n\n# Verify installation\nduckdb --version\n</code></pre>"},{"location":"reference/duckdb-queries/#2-install-iceberg-extension","title":"2. Install Iceberg Extension","text":"<pre><code>-- Start DuckDB CLI\n$ duckdb\n\n-- Install and load the iceberg extension\nD INSTALL iceberg;\nD LOAD iceberg;\n</code></pre>"},{"location":"reference/duckdb-queries/#configuration","title":"Configuration","text":""},{"location":"reference/duckdb-queries/#option-1-interactive-configuration-duckdb-cli","title":"Option 1: Interactive Configuration (DuckDB CLI)","text":"<pre><code>-- Start DuckDB CLI\n$ duckdb\n\n-- Load extension\nD LOAD iceberg;\n\n-- Configure S3/MinIO connection\nD SET s3_endpoint = 'localhost:10001';\nD SET s3_use_ssl = false;\nD SET s3_url_style = 'path';\nD SET s3_access_key_id = 'minio';\nD SET s3_secret_access_key = 'minio123';\n</code></pre>"},{"location":"reference/duckdb-queries/#option-2-python-script-configuration","title":"Option 2: Python Script Configuration","text":"<pre><code>import duckdb\n\n# Create connection\nconn = duckdb.connect()\n\n# Install and load extension\nconn.execute(\"INSTALL iceberg\")\nconn.execute(\"LOAD iceberg\")\n\n# Configure S3/MinIO\nconn.execute(\"SET s3_endpoint = 'localhost:10001'\")\nconn.execute(\"SET s3_use_ssl = false\")\nconn.execute(\"SET s3_url_style = 'path'\")\nconn.execute(\"SET s3_access_key_id = 'minio'\")\nconn.execute(\"SET s3_secret_access_key = 'minio123'\")\n</code></pre>"},{"location":"reference/duckdb-queries/#option-3-environment-variables-recommended-for-scripts","title":"Option 3: Environment Variables (Recommended for Scripts)","text":"<pre><code># Set environment variables\nexport S3_ENDPOINT=localhost:10001\nexport S3_USE_SSL=false\nexport S3_URL_STYLE=path\nexport AWS_ACCESS_KEY_ID=minio\nexport AWS_SECRET_ACCESS_KEY=minio123\n</code></pre> <pre><code>import duckdb\n\nconn = duckdb.connect()\nconn.execute(\"INSTALL iceberg\")\nconn.execute(\"LOAD iceberg\")\n\n# S3 config automatically picked up from environment variables\n</code></pre>"},{"location":"reference/duckdb-queries/#querying-iceberg-tables","title":"Querying Iceberg Tables","text":""},{"location":"reference/duckdb-queries/#method-1-direct-s3-path-recommended","title":"Method 1: Direct S3 Path (Recommended)","text":"<p>Query Iceberg tables using their S3/MinIO paths:</p> <pre><code>-- Query raw entries table\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nLIMIT 10;\n\n-- Query bronze staging table\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/bronze/stg_entries')\nWHERE date_partition &gt;= CURRENT_DATE - INTERVAL 7 DAY;\n\n-- Query silver fact table\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/silver/fct_glucose_readings')\nWHERE date &gt;= CURRENT_DATE - INTERVAL 30 DAY;\n\n-- Query gold dimension table\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/gold/dim_date')\nORDER BY date DESC\nLIMIT 100;\n</code></pre>"},{"location":"reference/duckdb-queries/#method-2-nessie-catalog-integration-advanced","title":"Method 2: Nessie Catalog Integration (Advanced)","text":"<p>For branch-aware queries, you can query specific Nessie branches:</p> <pre><code>-- Query from main branch (production)\nSELECT *\nFROM iceberg_scan(\n    's3://lake/warehouse/raw/entries',\n    metadata_location='s3://lake/warehouse/raw/entries/metadata/v1.metadata.json'\n);\n\n-- Note: Nessie catalog integration requires knowing the metadata file location\n-- This is more complex and typically not needed for ad-hoc analysis\n</code></pre>"},{"location":"reference/duckdb-queries/#example-use-cases","title":"Example Use Cases","text":""},{"location":"reference/duckdb-queries/#1-quick-data-exploration","title":"1. Quick Data Exploration","text":"<pre><code>-- See latest glucose readings\nSELECT\n    date_string,\n    sgv,\n    direction,\n    device\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nORDER BY date DESC\nLIMIT 20;\n</code></pre>"},{"location":"reference/duckdb-queries/#2-data-quality-checks","title":"2. Data Quality Checks","text":"<pre><code>-- Check for null values in critical fields\nSELECT\n    COUNT(*) as total_rows,\n    COUNT(sgv) as non_null_sgv,\n    COUNT(date) as non_null_date,\n    COUNT(*) - COUNT(sgv) as null_sgv_count\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nWHERE date_partition &gt;= CURRENT_DATE - INTERVAL 7 DAY;\n</code></pre>"},{"location":"reference/duckdb-queries/#3-aggregations-and-analysis","title":"3. Aggregations and Analysis","text":"<pre><code>-- Daily glucose statistics\nSELECT\n    date_trunc('day', timestamp) as day,\n    COUNT(*) as reading_count,\n    AVG(sgv) as avg_glucose,\n    MIN(sgv) as min_glucose,\n    MAX(sgv) as max_glucose,\n    STDDEV(sgv) as std_glucose\nFROM iceberg_scan('s3://lake/warehouse/silver/fct_glucose_readings')\nWHERE date &gt;= CURRENT_DATE - INTERVAL 30 DAY\nGROUP BY day\nORDER BY day DESC;\n</code></pre>"},{"location":"reference/duckdb-queries/#4-time-based-filtering","title":"4. Time-Based Filtering","text":"<pre><code>-- Readings from last hour\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nWHERE timestamp &gt;= CURRENT_TIMESTAMP - INTERVAL 1 HOUR\nORDER BY timestamp DESC;\n</code></pre>"},{"location":"reference/duckdb-queries/#5-export-to-csv","title":"5. Export to CSV","text":"<pre><code>-- Export analysis results to CSV\nCOPY (\n    SELECT\n        date_partition,\n        AVG(sgv) as avg_glucose,\n        COUNT(*) as reading_count\n    FROM iceberg_scan('s3://lake/warehouse/silver/fct_glucose_readings')\n    WHERE date &gt;= CURRENT_DATE - INTERVAL 90 DAY\n    GROUP BY date_partition\n) TO 'glucose_analysis.csv' (HEADER, DELIMITER ',');\n</code></pre>"},{"location":"reference/duckdb-queries/#python-integration-examples","title":"Python Integration Examples","text":""},{"location":"reference/duckdb-queries/#pandas-integration","title":"Pandas Integration","text":"<pre><code>import duckdb\nimport pandas as pd\n\n# Configure connection\nconn = duckdb.connect()\nconn.execute(\"INSTALL iceberg\")\nconn.execute(\"LOAD iceberg\")\nconn.execute(\"SET s3_endpoint = 'localhost:10001'\")\nconn.execute(\"SET s3_use_ssl = false\")\nconn.execute(\"SET s3_url_style = 'path'\")\nconn.execute(\"SET s3_access_key_id = 'minio'\")\nconn.execute(\"SET s3_secret_access_key = 'minio123'\")\n\n# Query to DataFrame\ndf = conn.execute(\"\"\"\n    SELECT *\n    FROM iceberg_scan('s3://lake/warehouse/silver/fct_glucose_readings')\n    WHERE date &gt;= CURRENT_DATE - INTERVAL 7 DAY\n\"\"\").df()\n\nprint(df.head())\nprint(df.describe())\n</code></pre>"},{"location":"reference/duckdb-queries/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code># In Jupyter notebook\nimport duckdb\nimport plotly.express as px\n\n# Setup\nconn = duckdb.connect()\nconn.execute(\"INSTALL iceberg\")\nconn.execute(\"LOAD iceberg\")\nconn.execute(\"SET s3_endpoint = 'localhost:10001'\")\nconn.execute(\"SET s3_use_ssl = false\")\nconn.execute(\"SET s3_url_style = 'path'\")\nconn.execute(\"SET s3_access_key_id = 'minio'\")\nconn.execute(\"SET s3_secret_access_key = 'minio123'\")\n\n# Query and visualize\ndf = conn.execute(\"\"\"\n    SELECT\n        timestamp,\n        sgv\n    FROM iceberg_scan('s3://lake/warehouse/silver/fct_glucose_readings')\n    WHERE date &gt;= CURRENT_DATE - INTERVAL 7 DAY\n    ORDER BY timestamp\n\"\"\").df()\n\n# Plot with Plotly\nfig = px.line(df, x='timestamp', y='sgv', title='Glucose Readings (Last 7 Days)')\nfig.show()\n</code></pre>"},{"location":"reference/duckdb-queries/#performance-tips","title":"Performance Tips","text":""},{"location":"reference/duckdb-queries/#1-use-partition-filters","title":"1. Use Partition Filters","text":"<p>Iceberg tables are partitioned by date. Always filter by partition when possible:</p> <pre><code>-- GOOD: Uses partition filtering\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nWHERE date_partition &gt;= '2025-10-01';\n\n-- LESS EFFICIENT: Full table scan\nSELECT *\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nWHERE timestamp &gt;= '2025-10-01'::TIMESTAMP;\n</code></pre>"},{"location":"reference/duckdb-queries/#2-limit-result-sets","title":"2. Limit Result Sets","text":"<p>For exploration, use LIMIT to avoid pulling large datasets:</p> <pre><code>SELECT *\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nLIMIT 1000;\n</code></pre>"},{"location":"reference/duckdb-queries/#3-use-aggregations","title":"3. Use Aggregations","text":"<p>Push aggregations down to DuckDB instead of pulling all data:</p> <pre><code>-- GOOD: Aggregation in DuckDB\nSELECT date_partition, COUNT(*)\nFROM iceberg_scan('s3://lake/warehouse/raw/entries')\nGROUP BY date_partition;\n\n-- LESS EFFICIENT: Pull all data then aggregate in Python\n</code></pre>"},{"location":"reference/duckdb-queries/#available-tables","title":"Available Tables","text":"<p>Query these Iceberg tables based on your pipeline layer:</p>"},{"location":"reference/duckdb-queries/#raw-layer","title":"Raw Layer","text":"<ul> <li><code>s3://lake/warehouse/raw/entries</code> - Nightscout CGM entries (raw ingestion)</li> </ul>"},{"location":"reference/duckdb-queries/#bronze-layer-via-dbt","title":"Bronze Layer (via dbt)","text":"<ul> <li><code>s3://lake/warehouse/bronze/stg_entries</code> - Staged entries with basic transformations</li> </ul>"},{"location":"reference/duckdb-queries/#silver-layer-via-dbt","title":"Silver Layer (via dbt)","text":"<ul> <li><code>s3://lake/warehouse/silver/fct_glucose_readings</code> - Cleaned glucose facts</li> </ul>"},{"location":"reference/duckdb-queries/#gold-layer-via-dbt","title":"Gold Layer (via dbt)","text":"<ul> <li><code>s3://lake/warehouse/gold/dim_date</code> - Date dimension table</li> <li><code>s3://lake/warehouse/gold/mrt_glucose_readings</code> - Glucose mart (materialized)</li> </ul>"},{"location":"reference/duckdb-queries/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/duckdb-queries/#issue-cannot-connect-to-minio","title":"Issue: Cannot connect to MinIO","text":"<pre><code>Error: Connection failed to localhost:10001\n</code></pre> <p>Solution: Ensure MinIO is running and accessible:</p> <pre><code># Check if MinIO is running\ndocker compose ps minio\n\n# Verify MinIO endpoint\ncurl http://localhost:10001/minio/health/ready\n</code></pre>"},{"location":"reference/duckdb-queries/#issue-authentication-failed","title":"Issue: Authentication failed","text":"<pre><code>Error: Access Denied\n</code></pre> <p>Solution: Verify credentials match your <code>.phlo/.env.local</code> file:</p> <pre><code># Check .phlo/.env.local\ngrep MINIO .phlo/.env.local\n\n# Use matching credentials in DuckDB\nSET s3_access_key_id = 'minio';\nSET s3_secret_access_key = 'minio123';\n</code></pre>"},{"location":"reference/duckdb-queries/#issue-table-not-found","title":"Issue: Table not found","text":"<pre><code>Error: Failed to read Iceberg table metadata\n</code></pre> <p>Solution:</p> <ol> <li>Verify the table path exists in MinIO</li> <li>Check that ingestion pipeline has run successfully</li> <li>Ensure you're using the correct warehouse path</li> </ol> <pre><code>-- List files in MinIO using AWS CLI or MinIO console\n-- Correct path should be: s3://lake/warehouse/&lt;schema&gt;/&lt;table&gt;/\n</code></pre>"},{"location":"reference/duckdb-queries/#issue-ssltls-errors","title":"Issue: SSL/TLS errors","text":"<pre><code>Error: SSL peer certificate validation failed\n</code></pre> <p>Solution: Ensure SSL is disabled for MinIO:</p> <pre><code>SET s3_use_ssl = false;\n</code></pre>"},{"location":"reference/duckdb-queries/#comparison-duckdb-vs-trino","title":"Comparison: DuckDB vs Trino","text":"Feature DuckDB + Iceberg Trino (via dbt/Dagster) Use Case Ad-hoc analysis, exploration Production pipelines, transformations Setup Local CLI/Python Requires Trino service Performance Fast for local queries Distributed queries Branching Manual metadata paths Full Nessie integration Write Support Read-only Read and write Best For Analysts, data scientists Data engineers, pipelines"},{"location":"reference/duckdb-queries/#best-practices","title":"Best Practices","text":"<ol> <li>Use DuckDB for:</li> <li>Quick data exploration</li> <li>Ad-hoc analysis</li> <li>Jupyter notebook workflows</li> <li>Export to CSV/Parquet</li> <li> <p>Local development/testing</p> </li> <li> <p>Use Trino for:</p> </li> <li>Production pipelines</li> <li>Branch-aware workflows (dev/prod)</li> <li>Writing to Iceberg tables</li> <li>Complex multi-table joins</li> <li> <p>dbt transformations</p> </li> <li> <p>Security:</p> </li> <li>Never commit credentials to git</li> <li>Use environment variables for automation</li> <li> <p>DuckDB is read-only for Iceberg by default (safe for exploration)</p> </li> <li> <p>Performance:</p> </li> <li>Always use partition filters when possible</li> <li>Limit result sets during exploration</li> <li>Use aggregations to reduce data transfer</li> </ol>"},{"location":"reference/duckdb-queries/#further-reading","title":"Further Reading","text":"<ul> <li>DuckDB Iceberg Extension Docs</li> <li>Apache Iceberg Documentation</li> <li>DuckDB S3 Configuration</li> <li>Project Nessie Documentation</li> </ul>"},{"location":"reference/phlo-api/","title":"phlo-api","text":"<p>Observatory backend API for Phlo infrastructure management.</p>"},{"location":"reference/phlo-api/#overview","title":"Overview","text":"<p>The <code>phlo-api</code> is a FastAPI-based backend service that provides the Observatory UI with access to:</p> <ul> <li>Plugin &amp; Service Management: Discover and manage plugins and services</li> <li>Data Querying: Execute queries against Trino and Iceberg tables</li> <li>Orchestration: Interact with Dagster assets and runs</li> <li>Data Catalog: Manage Nessie branches and catalog metadata</li> <li>Quality Monitoring: Query data quality check results</li> <li>Logging: Search and correlate logs via Loki</li> <li>Lineage Tracking: Row-level lineage queries</li> <li>Maintenance: Iceberg maintenance operation status</li> <li>Search: Unified search across assets, tables, and columns</li> </ul>"},{"location":"reference/phlo-api/#quick-start","title":"Quick Start","text":"<pre><code># Start the API service\nphlo services start --service phlo-api\n\n# Or run natively in dev mode (no Docker)\nphlo services start --native\n</code></pre> <p>Access the API at:</p> <ul> <li>Base URL: http://localhost:4000</li> <li>Health Check: http://localhost:4000/health</li> <li>OpenAPI Docs: http://localhost:4000/docs</li> <li>Metrics: http://localhost:4000/metrics</li> </ul>"},{"location":"reference/phlo-api/#complete-api-reference","title":"Complete API Reference","text":"<p>See the full API reference documentation in the phlo-api package:</p> <p>phlo-api API Reference</p> <p>This includes detailed documentation for all endpoints:</p> <ul> <li>Core endpoints (health, config, plugins, services, registry)</li> <li>Trino query engine (connection, preview, profiling, metrics, query execution)</li> <li>Iceberg tables (list, schema, metadata)</li> <li>Dagster assets (health, assets, history)</li> <li>Nessie catalog (branches, commits, merge, diff)</li> <li>Data quality (overview, checks, history)</li> <li>Logging (Loki integration, run logs, asset logs)</li> <li>Row lineage (ancestors, descendants, journey)</li> <li>Maintenance (status, metrics)</li> <li>Search index (assets, tables, columns)</li> </ul>"},{"location":"reference/phlo-api/#key-features","title":"Key Features","text":""},{"location":"reference/phlo-api/#1-read-only-query-guardrails","title":"1. Read-Only Query Guardrails","text":"<p>The API enforces read-only mode by default for ad-hoc queries:</p> <ul> <li>Blocks <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>DROP</code>, <code>CREATE</code>, <code>ALTER</code>, <code>TRUNCATE</code>, <code>MERGE</code></li> <li>Prevents multiple statements (SQL injection protection)</li> <li>30-second query timeout</li> <li>10,000 row limit</li> </ul>"},{"location":"reference/phlo-api/#2-caching","title":"2. Caching","text":"<p>Intelligent caching for frequently accessed data:</p> <ul> <li>Tables list: 1 minute TTL</li> <li>Table schema: 5 minutes TTL</li> <li>In-memory cache (can be extended with Redis)</li> </ul>"},{"location":"reference/phlo-api/#3-connection-flexibility","title":"3. Connection Flexibility","text":"<p>All endpoints support URL overrides via query parameters for testing:</p> <pre><code># Use different Trino instance\ncurl \"http://localhost:4000/api/trino/preview/my_table?trino_url=http://trino-prod:8080\"\n\n# Use different Dagster instance\ncurl \"http://localhost:4000/api/dagster/assets?dagster_url=http://localhost:3000/graphql\"\n</code></pre>"},{"location":"reference/phlo-api/#4-log-correlation","title":"4. Log Correlation","text":"<p>Loki endpoints support correlation by:</p> <ul> <li><code>run_id</code>: Dagster run identifier</li> <li><code>asset_key</code>: Asset name</li> <li><code>job_name</code>: Dagster job name</li> <li><code>partition_key</code>: Partition identifier</li> <li><code>check_name</code>: Quality check name</li> </ul>"},{"location":"reference/phlo-api/#5-row-level-lineage","title":"5. Row-Level Lineage","text":"<p>Track individual row provenance through transformations:</p> <pre><code># Get full lineage journey for a row\ncurl http://localhost:4000/api/lineage/rows/abc-123/journey\n</code></pre>"},{"location":"reference/phlo-api/#configuration","title":"Configuration","text":"<p>Configure via environment variables:</p> <pre><code># API Server\nPHLO_API_PORT=4000\nHOST=0.0.0.0\n\n# Backend Services\nTRINO_URL=http://trino:10005\nDAGSTER_GRAPHQL_URL=http://dagster:3000/graphql\nNESSIE_URL=http://nessie:19120/api/v2\nLOKI_URL=http://loki:3100\n</code></pre>"},{"location":"reference/phlo-api/#architecture","title":"Architecture","text":"<p>The API is structured as:</p> <pre><code>phlo-api/\n\u251c\u2500\u2500 main.py                      # Core endpoints (config, plugins, services)\n\u2514\u2500\u2500 observatory_api/\n    \u251c\u2500\u2500 trino.py                 # Query execution\n    \u251c\u2500\u2500 iceberg.py               # Table catalog\n    \u251c\u2500\u2500 dagster.py               # Asset management\n    \u251c\u2500\u2500 nessie.py                # Version control\n    \u251c\u2500\u2500 quality.py               # Quality checks\n    \u251c\u2500\u2500 loki.py                  # Log queries\n    \u251c\u2500\u2500 lineage.py               # Row lineage\n    \u251c\u2500\u2500 maintenance.py           # Maintenance metrics\n    \u2514\u2500\u2500 search.py                # Unified search\n</code></pre> <p>Each router is independently importable and can be disabled by removing the import in <code>main.py</code>.</p>"},{"location":"reference/phlo-api/#development","title":"Development","text":""},{"location":"reference/phlo-api/#running-locally","title":"Running Locally","text":"<pre><code># Install in editable mode\ncd packages/phlo-api\npip install -e .\n\n# Run directly\npython -m phlo_api.main\n\n# Or via uvicorn\nuvicorn phlo_api.main:app --reload --port 4000\n</code></pre>"},{"location":"reference/phlo-api/#adding-new-endpoints","title":"Adding New Endpoints","text":"<ol> <li>Create a new router in <code>observatory_api/</code>:</li> </ol> <pre><code># observatory_api/my_feature.py\nfrom fastapi import APIRouter\nrouter = APIRouter(tags=[\"my_feature\"])\n\n@router.get(\"/status\")\nasync def get_status():\n    return {\"status\": \"ok\"}\n</code></pre> <ol> <li>Register in <code>main.py</code>:</li> </ol> <pre><code>from phlo_api.observatory_api.my_feature import router as my_feature_router\napp.include_router(my_feature_router, prefix=\"/api/my-feature\")\n</code></pre>"},{"location":"reference/phlo-api/#monitoring","title":"Monitoring","text":"<p>The API automatically exposes Prometheus metrics at <code>/metrics</code>:</p> <pre><code>curl http://localhost:4000/metrics\n</code></pre> <p>Metrics include:</p> <ul> <li>HTTP request counts and duration</li> <li>Active requests</li> <li>Error rates</li> </ul> <p>These metrics are automatically scraped by Prometheus when the observability stack is running.</p>"},{"location":"reference/phlo-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/phlo-api/#api-wont-start","title":"API Won't Start","text":"<pre><code># Check logs\ndocker logs phlo-api-1\n\n# Or if running natively\ndocker logs dagster-webserver  # Check native service logs\n</code></pre>"},{"location":"reference/phlo-api/#backend-connection-errors","title":"Backend Connection Errors","text":"<p>Test each backend service:</p> <pre><code># Trino\ncurl http://localhost:10005/v1/info\n\n# Dagster\ncurl http://localhost:3000/graphql\n\n# Nessie\ncurl http://localhost:19120/api/v2/config\n\n# Loki\ncurl http://localhost:3100/ready\n</code></pre>"},{"location":"reference/phlo-api/#slow-queries","title":"Slow Queries","text":"<ul> <li>Check Trino query history at http://localhost:10005/ui</li> <li>Use smaller <code>limit</code> values for data preview</li> <li>Check table partitioning and use partition filters</li> <li>Monitor query execution times in response metadata</li> </ul>"},{"location":"reference/phlo-api/#next-steps","title":"Next Steps","text":"<ul> <li>phlo-api Package README</li> <li>phlo-api Complete API Reference</li> <li>Observatory Package</li> <li>CLI Reference</li> </ul>"},{"location":"setup/hasura/","title":"Hasura GraphQL Setup Guide","text":"<p>Step-by-step guide to configuring Hasura for the Phlo lakehouse.</p>"},{"location":"setup/hasura/#overview","title":"Overview","text":"<p>Hasura auto-generates a GraphQL API from your Postgres <code>marts</code> schema, providing:</p> <ul> <li>Instant GraphQL queries and mutations</li> <li>Real-time subscriptions</li> <li>Role-based permissions</li> <li>GraphQL playground/explorer</li> </ul>"},{"location":"setup/hasura/#quick-start","title":"Quick Start","text":"<pre><code># Start Hasura (requires Postgres running)\nmake up-api\n\n# Open Hasura console\nmake hasura\n# Or: http://localhost:8081/console\n</code></pre> <p>Admin Secret: <code>phlo-admin-secret-change-me</code> (set in <code>.phlo/.env.local</code>)</p>"},{"location":"setup/hasura/#initial-setup","title":"Initial Setup","text":""},{"location":"setup/hasura/#1-track-postgres-tables","title":"1. Track Postgres Tables","text":"<p>Hasura needs to \"track\" tables to generate GraphQL schema.</p> <ol> <li>Open Hasura console: http://localhost:8081/console</li> <li>Navigate to Data tab</li> <li>Click public schema (or marts if you have it)</li> <li>You'll see available tables:</li> <li><code>mrt_glucose_overview</code></li> <li><code>mrt_glucose_hourly_patterns</code></li> <li>Click Track for each table you want to expose</li> <li>Click Track All to track all foreign keys/relationships</li> </ol> <p>Result: GraphQL types are now generated for each table.</p>"},{"location":"setup/hasura/#2-configure-permissions","title":"2. Configure Permissions","text":"<p>By default, tracked tables have NO permissions. You must configure access.</p>"},{"location":"setup/hasura/#configure-admin-role","title":"Configure <code>admin</code> Role","text":"<ol> <li>Click on <code>mrt_glucose_overview</code> table</li> <li>Go to Permissions tab</li> <li>Click Enter new role \u2192 type <code>admin</code></li> <li>Click select column</li> <li>Configure permissions:</li> <li>Row select permissions: <code>Without any checks</code> (full access)</li> <li>Column select permissions: Check all columns</li> <li>Aggregation queries permissions: Enable</li> <li>Click Save Permissions</li> <li>Repeat for insert, update, delete if needed (usually not for marts)</li> </ol>"},{"location":"setup/hasura/#configure-analyst-role","title":"Configure <code>analyst</code> Role","text":"<ol> <li>Click Enter new role \u2192 type <code>analyst</code></li> <li>Click select column</li> <li>Configure permissions:</li> <li>Row select permissions: <code>Without any checks</code> (read-only, full access)</li> <li>Column select permissions: Check all columns</li> <li>Aggregation queries permissions: Enable</li> <li>Click Save Permissions</li> <li>DO NOT enable insert/update/delete (analyst is read-only)</li> </ol>"},{"location":"setup/hasura/#repeat-for-all-tables","title":"Repeat for All Tables","text":"<p>Repeat the above for:</p> <ul> <li><code>mrt_glucose_hourly_patterns</code></li> <li>Any other marts tables</li> </ul>"},{"location":"setup/hasura/#3-test-graphql-queries","title":"3. Test GraphQL Queries","text":"<ol> <li>Navigate to API tab (GraphiQL explorer)</li> <li>You'll see GraphQL schema in the right sidebar</li> <li>Try a query:</li> </ol> <pre><code>query GetGlucoseReadings {\n  mrt_glucose_overview(limit: 10, order_by: { date: desc }) {\n    date\n    avg_glucose\n    min_glucose\n    max_glucose\n    readings_count\n  }\n}\n</code></pre> <ol> <li>Click Play button</li> <li>You should see results from your Postgres <code>marts</code> schema</li> </ol>"},{"location":"setup/hasura/#4-set-request-headers","title":"4. Set Request Headers","text":"<p>For authenticated requests:</p> <ol> <li>In GraphiQL, click Request Headers at bottom</li> <li>Add JWT token:</li> </ol> <pre><code>{\n  \"Authorization\": \"Bearer YOUR_JWT_TOKEN_HERE\"\n}\n</code></pre> <ol> <li>Get token from FastAPI login:</li> </ol> <pre><code>curl -X POST http://localhost:8000/api/v1/auth/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\":\"admin\",\"password\":\"admin123\"}'\n</code></pre> <ol> <li>Copy <code>access_token</code> from response</li> <li>Paste into GraphiQL header</li> <li>Run authenticated queries</li> </ol>"},{"location":"setup/hasura/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"setup/hasura/#custom-views","title":"Custom Views","text":"<p>Create views in Postgres for common queries, then track them in Hasura.</p> <p>Example:</p> <pre><code>CREATE VIEW marts.v_recent_readings AS\nSELECT *\nFROM marts.mrt_glucose_overview\nWHERE date &gt;= CURRENT_DATE - INTERVAL '30 days';\n</code></pre> <p>Then track <code>v_recent_readings</code> in Hasura console.</p>"},{"location":"setup/hasura/#relationships","title":"Relationships","text":"<p>If you have foreign keys between tables, Hasura can auto-track relationships.</p> <p>Example: If you add a <code>user_id</code> column to marts tables:</p> <ol> <li>Create foreign key in Postgres:</li> </ol> <pre><code>ALTER TABLE marts.mrt_glucose_overview\nADD COLUMN user_id TEXT REFERENCES users(id);\n</code></pre> <ol> <li>In Hasura console \u2192 Data \u2192 Track relationship</li> <li>Hasura generates nested queries:</li> </ol> <pre><code>query GetUserWithReadings {\n  users {\n    id\n    name\n    glucose_readings {\n      # Auto-generated relationship\n      date\n      avg_glucose\n    }\n  }\n}\n</code></pre>"},{"location":"setup/hasura/#computed-fields","title":"Computed Fields","text":"<p>Add server-side computed values to GraphQL responses.</p> <p>Example: Create Postgres function:</p> <pre><code>CREATE FUNCTION marts.glucose_status(mrt_glucose_overview_row marts.mrt_glucose_overview)\nRETURNS TEXT AS $$\n  SELECT CASE\n    WHEN mrt_glucose_overview_row.avg_glucose &lt; 70 THEN 'low'\n    WHEN mrt_glucose_overview_row.avg_glucose &gt; 180 THEN 'high'\n    ELSE 'normal'\n  END;\n$$ LANGUAGE SQL STABLE;\n</code></pre> <p>Add computed field in Hasura:</p> <ol> <li>Data \u2192 <code>mrt_glucose_overview</code> \u2192 Modify tab</li> <li>Add a computed field</li> <li>Function: <code>glucose_status</code></li> <li>Save</li> </ol> <p>Query with computed field:</p> <pre><code>query {\n  mrt_glucose_overview {\n    date\n    avg_glucose\n    status # Computed field\n  }\n}\n</code></pre>"},{"location":"setup/hasura/#row-level-security","title":"Row-Level Security","text":"<p>Restrict data access based on JWT claims.</p> <p>Example: Only show data for authenticated user:</p> <ol> <li>Add <code>user_id</code> column to tables</li> <li>In permissions for <code>analyst</code> role:</li> <li>Row select permissions: Custom check</li> <li>Enter condition:    <code>json    {      \"user_id\": {        \"_eq\": \"X-Hasura-User-Id\"      }    }</code></li> <li>Save</li> </ol> <p>Now analysts only see rows where <code>user_id</code> matches their JWT token's <code>user_id</code> claim.</p>"},{"location":"setup/hasura/#actions-custom-business-logic","title":"Actions (Custom Business Logic)","text":"<p>Call external REST endpoints from GraphQL.</p> <p>Example: Trigger data refresh:</p> <ol> <li>Actions tab \u2192 Create</li> <li>Define action:</li> </ol> <pre><code>type Mutation {\n  refreshGlucoseData: RefreshResponse\n}\n\ntype RefreshResponse {\n  success: Boolean!\n  message: String!\n}\n</code></pre> <ol> <li>Handler: <code>http://api:8000/api/v1/admin/refresh</code></li> <li>Add custom headers (JWT token forwarding)</li> <li>Save</li> </ol> <p>Query:</p> <pre><code>mutation {\n  refreshGlucoseData {\n    success\n    message\n  }\n}\n</code></pre>"},{"location":"setup/hasura/#event-triggers","title":"Event Triggers","text":"<p>Run webhooks when data changes.</p> <p>Example: Send alert when glucose reading is high:</p> <ol> <li>Events tab \u2192 Create Event Trigger</li> <li>Table: <code>mrt_glucose_overview</code></li> <li>Operations: Insert, Update</li> <li>Webhook: <code>http://api:8000/api/v1/webhooks/glucose-alert</code></li> <li>Add condition:</li> </ol> <pre><code>{\n  \"avg_glucose\": {\n    \"_gt\": 180\n  }\n}\n</code></pre> <ol> <li>Save</li> </ol> <p>Hasura will POST to webhook when condition matches.</p>"},{"location":"setup/hasura/#subscriptions","title":"Subscriptions","text":"<p>Real-time data via WebSockets.</p> <p>Example subscription:</p> <pre><code>subscription WatchGlucose {\n  mrt_glucose_overview(limit: 1, order_by: { date: desc }) {\n    date\n    avg_glucose\n  }\n}\n</code></pre> <p>Client will receive updates when data changes.</p> <p>JavaScript client:</p> <pre><code>import { createClient } from \"graphql-ws\";\n\nconst client = createClient({\n  url: \"ws://localhost:8081/v1/graphql\",\n  connectionParams: {\n    headers: {\n      Authorization: \"Bearer YOUR_TOKEN\",\n    },\n  },\n});\n\nclient.subscribe(\n  {\n    query: `subscription { mrt_glucose_overview(limit: 1, order_by: {date: desc}) { date avg_glucose } }`,\n  },\n  {\n    next: (data) =&gt; console.log(\"Update:\", data),\n    error: (error) =&gt; console.error(\"Error:\", error),\n    complete: () =&gt; console.log(\"Done\"),\n  }\n);\n</code></pre>"},{"location":"setup/hasura/#metadata-management","title":"Metadata Management","text":""},{"location":"setup/hasura/#export-metadata","title":"Export Metadata","text":"<p>Save Hasura configuration to version control:</p> <pre><code># Install Hasura CLI\nnpm install --global hasura-cli\n\n# Export metadata\ncd /path/to/phlo\nhasura metadata export \\\n  --endpoint http://localhost:8081 \\\n  --admin-secret phlo-admin-secret-change-me\n</code></pre> <p>Creates <code>metadata/</code> directory with:</p> <ul> <li>Tables tracking</li> <li>Permissions</li> <li>Relationships</li> <li>Actions</li> <li>Event triggers</li> </ul> <p>Commit to git for reproducible setup.</p>"},{"location":"setup/hasura/#apply-metadata","title":"Apply Metadata","text":"<p>Restore configuration to new Hasura instance:</p> <pre><code>hasura metadata apply \\\n  --endpoint http://localhost:8081 \\\n  --admin-secret phlo-admin-secret-change-me\n</code></pre> <p>Useful for:</p> <ul> <li>Development \u2192 Production deployment</li> <li>Team collaboration</li> <li>Disaster recovery</li> </ul>"},{"location":"setup/hasura/#migrations","title":"Migrations","text":"<p>Track database schema changes:</p> <pre><code># Initialize\nhasura init hasura-project --endpoint http://localhost:8081\n\n# Create migration\nhasura migrate create add_user_column \\\n  --sql-from-server \\\n  --database-name default\n\n# Apply migration\nhasura migrate apply\n</code></pre> <p>Keeps database schema and Hasura metadata in sync.</p>"},{"location":"setup/hasura/#security-hardening","title":"Security Hardening","text":""},{"location":"setup/hasura/#production-checklist","title":"Production Checklist","text":"<ul> <li>[ ] Change admin secret - Update <code>HASURA_ADMIN_SECRET</code> in <code>.phlo/.env.local</code></li> <li>[ ] Disable dev mode - Set <code>HASURA_GRAPHQL_DEV_MODE=false</code></li> <li>[ ] Disable console - Set <code>HASURA_GRAPHQL_ENABLE_CONSOLE=false</code> (use CLI)</li> <li>[ ] Restrict CORS - Set specific origins in <code>HASURA_GRAPHQL_CORS_DOMAIN</code></li> <li>[ ] Enable HTTPS - Use reverse proxy (nginx/Caddy)</li> <li>[ ] Add rate limiting - Configure at reverse proxy level</li> <li>[ ] Audit permissions - Review all role permissions regularly</li> <li>[ ] Monitor logs - Enable all log types for audit trail</li> </ul>"},{"location":"setup/hasura/#jwt-configuration","title":"JWT Configuration","text":"<p>Hasura validates JWT tokens using shared secret with FastAPI.</p> <p>Current config (in docker-compose.yml):</p> <pre><code>HASURA_GRAPHQL_JWT_SECRET: '{\"type\":\"HS256\",\"key\":\"${JWT_SECRET}\"}'\n</code></pre> <p>JWT payload structure:</p> <pre><code>{\n  \"sub\": \"admin\",\n  \"user_id\": \"admin_001\",\n  \"email\": \"admin@phlo.local\",\n  \"role\": \"admin\",\n  \"exp\": 1234567890,\n  \"https://hasura.io/jwt/claims\": {\n    \"x-hasura-allowed-roles\": [\"admin\"],\n    \"x-hasura-default-role\": \"admin\",\n    \"x-hasura-user-id\": \"admin_001\"\n  }\n}\n</code></pre> <p>Hasura reads <code>x-hasura-*</code> claims for permission checks.</p>"},{"location":"setup/hasura/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/hasura/#tables-not-showing","title":"Tables Not Showing","text":"<p>Issue: Tables not appearing in Hasura console</p> <p>Solutions:</p> <ol> <li>Verify Postgres connection: Check <code>HASURA_GRAPHQL_DATABASE_URL</code> in docker-compose.yml</li> <li>Check schema: Ensure tables are in correct schema (<code>marts</code> or <code>public</code>)</li> <li>Reload metadata: Data tab \u2192 gear icon \u2192 Reload</li> <li>Check Postgres logs: <code>docker compose logs postgres</code></li> </ol>"},{"location":"setup/hasura/#permission-denied-errors","title":"Permission Denied Errors","text":"<p>Issue: GraphQL query returns permission denied</p> <p>Solutions:</p> <ol> <li>Check role in JWT token matches configured role (<code>admin</code> or <code>analyst</code>)</li> <li>Verify permissions set for that role in Hasura console</li> <li>Check row-level security conditions</li> <li>Test with admin secret header instead of JWT to verify table access</li> </ol>"},{"location":"setup/hasura/#slow-queries","title":"Slow Queries","text":"<p>Issue: GraphQL queries taking too long</p> <p>Solutions:</p> <ol> <li>Add indexes to Postgres tables:</li> </ol> <pre><code>CREATE INDEX idx_glucose_date ON marts.mrt_glucose_overview(date);\n</code></pre> <ol> <li>Limit query depth in Hasura settings</li> <li>Use pagination with <code>limit</code> and <code>offset</code></li> <li>Enable query caching (requires Hasura Pro)</li> <li>Use FastAPI for heavy analytics queries instead</li> </ol>"},{"location":"setup/hasura/#jwt-validation-failures","title":"JWT Validation Failures","text":"<p>Issue: <code>JWTInvalid</code> or <code>JWTExpired</code> errors</p> <p>Solutions:</p> <ol> <li>Verify JWT secret matches between FastAPI and Hasura:</li> </ol> <pre><code>docker compose exec api env | grep JWT_SECRET\ndocker compose exec hasura env | grep JWT_SECRET\n</code></pre> <ol> <li>Check token expiration (default 60 minutes)</li> <li>Verify token format in request header: <code>Bearer YOUR_TOKEN</code></li> <li>Test token at https://jwt.io (paste token, verify signature with secret)</li> </ol>"},{"location":"setup/hasura/#graphql-best-practices","title":"GraphQL Best Practices","text":""},{"location":"setup/hasura/#pagination","title":"Pagination","text":"<p>Always paginate large result sets:</p> <pre><code>query GetReadingsPaginated($limit: Int!, $offset: Int!) {\n  mrt_glucose_overview(\n    limit: $limit\n    offset: $offset\n    order_by: { date: desc }\n  ) {\n    date\n    avg_glucose\n  }\n}\n</code></pre>"},{"location":"setup/hasura/#aggregations","title":"Aggregations","text":"<p>Use aggregation queries for summaries:</p> <pre><code>query GetStats {\n  mrt_glucose_overview_aggregate {\n    aggregate {\n      count\n      avg {\n        avg_glucose\n      }\n      max {\n        max_glucose\n      }\n      min {\n        min_glucose\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"setup/hasura/#query-naming","title":"Query Naming","text":"<p>Always name your queries:</p> <pre><code># Good\nquery GetGlucoseReadings { ... }\n\n# Bad (anonymous)\nquery { ... }\n</code></pre> <p>Helps with debugging and monitoring.</p>"},{"location":"setup/hasura/#resources","title":"Resources","text":"<ul> <li>Hasura Docs</li> <li>GraphQL Spec</li> <li>Hasura CLI</li> <li>GraphQL Best Practices</li> </ul>"},{"location":"setup/hasura/#next-steps","title":"Next Steps","text":"<ol> <li>Track all marts tables in Hasura console</li> <li>Configure permissions for admin and analyst roles</li> <li>Test queries in GraphiQL playground</li> <li>Export metadata for version control</li> <li>Integrate with frontend using Apollo Client or similar</li> <li>Set up monitoring via Hasura logs</li> </ol> <p>For API usage examples, see API Reference.</p>"},{"location":"setup/observability/","title":"Observability Guide","text":"<p>Complete guide to monitoring, logging, and observability in the Phlo lakehouse platform.</p>"},{"location":"setup/observability/#overview","title":"Overview","text":"<p>Phlo includes a production-ready observability stack based on industry-standard tools:</p> <ul> <li>Prometheus - Metrics collection and storage</li> <li>Loki - Log aggregation and querying</li> <li>Grafana Alloy - Unified telemetry collection agent</li> <li>Grafana - Visualization and dashboarding</li> <li>Postgres Exporter - Database metrics</li> </ul> <p>All observability services are optional and run under the <code>observability</code> docker-compose profile.</p>"},{"location":"setup/observability/#quick-start","title":"Quick Start","text":"<pre><code># Start core services\nmake up-core\n\n# Start observability stack\nmake up-observability\n\n# Check health\nmake health-observability\n\n# Open Grafana\nmake grafana\n</code></pre> <p>Default credentials:</p> <ul> <li>Grafana: admin / admin123 (change in <code>.phlo/.env.local</code>)</li> <li>Prometheus: No authentication (localhost only)</li> </ul>"},{"location":"setup/observability/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Grafana Dashboards                      \u2502\n\u2502         (Visualization + Alerting + Exploration)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                           \u2502\n             \u2502 Queries                   \u2502 Queries\n             \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Prometheus         \u2502   \u2502         Loki           \u2502\n\u2502   (Metrics Storage)     \u2502   \u2502   (Log Aggregation)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                             \u2502\n             \u2502 Scrape                      \u2502 Push\n             \u2502                             \u2502\n             \u25bc                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Grafana Alloy                          \u2502\n\u2502        (Unified Collection Agent + Label Processing)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502        \u2502        \u2502        \u2502        \u2502        \u2502\n     \u25bc        \u25bc        \u25bc        \u25bc        \u25bc        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502Trino \u2502 \u2502Nessie\u2502 \u2502MinIO \u2502 \u2502Dagster\u2502 \u2502Postgres\u2502 \u2502Containers\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"setup/observability/#component-details","title":"Component Details","text":""},{"location":"setup/observability/#prometheus-port-9090","title":"Prometheus (Port 9090)","text":"<ul> <li>Scrapes metrics from all services every 15 seconds</li> <li>30-day retention period</li> <li>Collects:</li> <li>Trino JMX metrics (query performance, memory, workers)</li> <li>Nessie Quarkus metrics (HTTP requests, catalog operations)</li> <li>MinIO S3 metrics (storage, bandwidth, operations)</li> <li>Postgres metrics (connections, transactions, database size)</li> <li>Container metrics (CPU, memory, network)</li> </ul>"},{"location":"setup/observability/#loki-port-3100","title":"Loki (Port 3100)","text":"<ul> <li>Aggregates logs from all Docker containers</li> <li>30-day retention period</li> <li>Automatically extracts log levels (ERROR, WARN, INFO, DEBUG)</li> <li>Supports LogQL for powerful log queries</li> <li>Integrated with Grafana for log exploration</li> </ul>"},{"location":"setup/observability/#grafana-alloy-port-12345","title":"Grafana Alloy (Port 12345)","text":"<ul> <li>Modern replacement for Prometheus exporters + Promtail</li> <li>Discovers Docker containers automatically</li> <li>Adds phlo-specific labels (component, service, job)</li> <li>Parses JSON logs and extracts fields</li> <li>Forwards metrics to Prometheus, logs to Loki</li> </ul>"},{"location":"setup/observability/#grafana-port-3003","title":"Grafana (Port 3003)","text":"<ul> <li>Pre-configured datasources (Prometheus + Loki)</li> <li>Pre-built dashboards:</li> <li>Phlo Overview - Service health, errors, key metrics</li> <li>Infrastructure - Detailed per-service metrics and logs</li> <li>Custom dashboards stored in <code>.phlo/grafana/dashboards/</code></li> </ul>"},{"location":"setup/observability/#metrics","title":"Metrics","text":""},{"location":"setup/observability/#available-metrics","title":"Available Metrics","text":""},{"location":"setup/observability/#trino","title":"Trino","text":"<pre><code># Query execution rate\nrate(trino_execution_QueuedQueries[5m])\n\n# Active workers\ntrino_cluster_ActiveWorkers\n\n# Memory usage\ntrino_memory_ClusterMemoryPool_general_ReservedBytes\n</code></pre>"},{"location":"setup/observability/#nessie","title":"Nessie","text":"<pre><code># HTTP request rate by endpoint\nrate(http_server_requests_seconds_count{job=\"nessie\"}[5m])\n\n# Request duration (p95)\nhistogram_quantile(0.95, rate(http_server_requests_seconds_bucket[5m]))\n\n# Catalog operations\nnessie_catalog_operations_total\n</code></pre>"},{"location":"setup/observability/#minio","title":"MinIO","text":"<pre><code># Bucket usage\nminio_bucket_usage_total_bytes\n\n# S3 traffic\nrate(minio_s3_requests_incoming_bytes[5m])\nrate(minio_s3_requests_outgoing_bytes[5m])\n\n# Object count\nminio_bucket_usage_object_total\n</code></pre>"},{"location":"setup/observability/#postgres","title":"Postgres","text":"<pre><code># Active connections\npg_stat_database_numbackends{datname=\"lakehouse\"}\n\n# Transaction rate\nrate(pg_stat_database_xact_commit{datname=\"lakehouse\"}[5m])\n\n# Database size\npg_database_size_bytes{datname=\"lakehouse\"}\n\n# Cache hit ratio\npg_stat_database_blks_hit / (pg_stat_database_blks_hit + pg_stat_database_blks_read)\n</code></pre>"},{"location":"setup/observability/#creating-alerts","title":"Creating Alerts","text":"<p>Add alert rules to <code>.phlo/prometheus/alerts/</code> (create directory):</p> <pre><code># .phlo/prometheus/alerts/phlo.yml\ngroups:\n  - name: cascade_lakehouse\n    interval: 30s\n    rules:\n      - alert: TrinoDown\n        expr: up{service=\"trino\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Trino query engine is down\"\n          description: \"Trino has been unreachable for 1 minute\"\n\n      - alert: HighErrorRate\n        expr: rate(http_server_requests_seconds_count{status=~\"5..\"}[5m]) &gt; 0.1\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High HTTP error rate detected\"\n</code></pre> <p>Then update <code>.phlo/prometheus/prometheus.yml</code>:</p> <pre><code>rule_files:\n  - \"alerts/*.yml\"\n</code></pre>"},{"location":"setup/observability/#logs","title":"Logs","text":""},{"location":"setup/observability/#viewing-logs-in-grafana","title":"Viewing Logs in Grafana","text":"<ol> <li>Open Grafana: <code>make grafana</code></li> <li>Navigate to Explore (compass icon)</li> <li>Select Loki datasource</li> <li>Use LogQL queries:</li> </ol> <pre><code># All errors across services\n{cascade_component=~\"trino|nessie|dagster-.*\"} |~ \"(?i)(error|exception|fatal)\"\n\n# Dagster pipeline logs\n{cascade_component=~\"dagster-.*\"}\n\n# Nessie catalog operations\n{cascade_component=\"nessie\"} |= \"catalog\"\n\n# Trino query logs\n{cascade_component=\"trino\"} |~ \"query.*completed\"\n\n# Filter by log level\n{job=\"dagster-webserver\"} | json | level=\"ERROR\"\n</code></pre>"},{"location":"setup/observability/#structured-logging","title":"Structured Logging","text":"<p>Dagster sensors emit structured logs that Loki automatically indexes:</p> <pre><code># From workflows/sensors/failure_monitoring.py\ncontext.log.error(\n    \"Pipeline failure detected\",\n    extra={\n        \"event_type\": \"pipeline_failure\",\n        \"run_id\": run_id,\n        \"job_name\": job_name,\n        \"failure_message\": failure_message,\n    }\n)\n</code></pre> <p>Query in LogQL:</p> <pre><code>{cascade_component=~\"dagster-.*\"} | json | event_type=\"pipeline_failure\"\n</code></pre>"},{"location":"setup/observability/#dashboards","title":"Dashboards","text":""},{"location":"setup/observability/#pre-built-dashboards","title":"Pre-built Dashboards","text":""},{"location":"setup/observability/#phlo-overview","title":"Phlo Overview","text":"<ul> <li>Service status indicators (up/down)</li> <li>Recent errors from all services</li> <li>MinIO bucket usage</li> <li>Postgres connection count</li> </ul>"},{"location":"setup/observability/#infrastructure","title":"Infrastructure","text":"<p>Organized by service layer:</p> <ul> <li>Service Health: All service status in one view</li> <li>Trino: Query logs and JMX metrics</li> <li>Nessie: HTTP request rate, catalog operations, logs</li> <li>MinIO: S3 traffic, object counts, bucket usage</li> <li>Postgres: Transaction rate, database size, connection pool</li> <li>Dagster: Pipeline logs and execution history</li> </ul>"},{"location":"setup/observability/#creating-custom-dashboards","title":"Creating Custom Dashboards","text":"<ol> <li>Open Grafana (<code>make grafana</code>)</li> <li>Create Dashboard \u2192 Add Visualization</li> <li>Select datasource (Prometheus or Loki)</li> <li>Build query using visual builder or code</li> <li>Save dashboard to <code>.phlo/grafana/dashboards/my-dashboard.json</code></li> </ol> <p>Example query panel (Prometheus):</p> <pre><code>{\n  \"targets\": [\n    {\n      \"expr\": \"rate(minio_s3_requests_total[5m])\",\n      \"legendFormat\": \"{{api}} - {{bucket}}\"\n    }\n  ]\n}\n</code></pre>"},{"location":"setup/observability/#dagster-integration","title":"Dagster Integration","text":""},{"location":"setup/observability/#failure-sensors","title":"Failure Sensors","text":"<p>Phlo includes three monitoring sensors:</p>"},{"location":"setup/observability/#pipeline_failure_sensor","title":"pipeline_failure_sensor","text":"<ul> <li>Triggers on any pipeline failure</li> <li>Logs structured error information</li> <li>Picked up by Loki for alerting</li> <li>Extensible for Slack/PagerDuty integration</li> </ul> <pre><code># Future: Add Slack alerting\n@dg.run_failure_sensor(...)\ndef pipeline_failure_sensor(context):\n    # ... existing logging ...\n    slack_client.send_alert(\n        channel=\"#data-alerts\",\n        text=f\"Pipeline {job_name} failed: {failure_message}\"\n    )\n</code></pre>"},{"location":"setup/observability/#pipeline_success_sensor","title":"pipeline_success_sensor","text":"<ul> <li>Logs successful completions</li> <li>Provides SLO tracking data</li> <li>Useful for measuring pipeline duration</li> </ul>"},{"location":"setup/observability/#iceberg_freshness_sensor","title":"iceberg_freshness_sensor","text":"<ul> <li>Monitors critical Iceberg table updates</li> <li>Complements Dagster FreshnessPolicy</li> <li>Asset-level monitoring for data quality</li> </ul>"},{"location":"setup/observability/#viewing-sensor-events","title":"Viewing Sensor Events","text":"<pre><code># All sensor events\n{cascade_component=~\"dagster-.*\"} | json | event_type=~\"pipeline_.*|iceberg_.*\"\n\n# Pipeline failures only\n{cascade_component=~\"dagster-.*\"} | json | event_type=\"pipeline_failure\"\n\n# Iceberg table updates\n{cascade_component=~\"dagster-.*\"} | json | event_type=\"iceberg_table_updated\"\n</code></pre>"},{"location":"setup/observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/observability/#service-wont-start","title":"Service Won't Start","text":"<p>Check logs:</p> <pre><code>docker compose logs prometheus\ndocker compose logs loki\ndocker compose logs alloy\ndocker compose logs grafana\n</code></pre> <p>Common issues:</p> <ul> <li>Permission errors: Check volume permissions in <code>volumes/</code></li> <li>Port conflicts: Ensure ports 9090, 3100, 3003, 12345 are free</li> <li>Config errors: Validate YAML syntax in <code>.phlo/prometheus/</code>, <code>.phlo/loki/</code>, <code>.phlo/alloy/</code></li> </ul>"},{"location":"setup/observability/#no-metrics-appearing","title":"No Metrics Appearing","text":"<ol> <li>Check Prometheus targets: http://localhost:9090/targets</li> <li>All targets should show \"UP\"</li> <li> <p>If \"DOWN\", check service health: <code>make health</code></p> </li> <li> <p>Check Alloy status: http://localhost:12345</p> </li> <li>Verify components are running</li> <li> <p>Check for collection errors</p> </li> <li> <p>Verify Grafana datasources:</p> </li> <li>Navigate to Configuration \u2192 Data Sources</li> <li>Click \"Test\" on Prometheus and Loki</li> <li>Both should return \"Data source is working\"</li> </ol>"},{"location":"setup/observability/#no-logs-in-loki","title":"No Logs in Loki","text":"<ol> <li>Check Alloy is running: <code>docker ps | grep alloy</code></li> <li>Verify Docker socket mount: <code>docker inspect alloy | grep docker.sock</code></li> <li>Check Alloy logs: <code>docker compose logs alloy</code></li> <li>Test Loki endpoint: <code>curl http://localhost:3100/ready</code></li> </ol>"},{"location":"setup/observability/#high-resource-usage","title":"High Resource Usage","text":"<p>Observability stack resource profile (typical):</p> <ul> <li>Prometheus: ~200-500 MB RAM</li> <li>Loki: ~100-300 MB RAM</li> <li>Alloy: ~50-100 MB RAM</li> <li>Grafana: ~100-200 MB RAM</li> </ul> <p>Total: ~450 MB - 1.1 GB additional overhead</p> <p>To reduce:</p> <ul> <li>Decrease scrape intervals in <code>prometheus.yml</code></li> <li>Reduce retention periods (30d \u2192 7d)</li> <li>Limit log volume via Alloy filters</li> </ul>"},{"location":"setup/observability/#production-hardening","title":"Production Hardening","text":""},{"location":"setup/observability/#security","title":"Security","text":"<pre><code># Change default passwords in .phlo/.env.local\nGRAFANA_ADMIN_PASSWORD=&lt;strong-password&gt;\n\n# Enable authentication in prometheus.yml (add later)\n# Enable TLS for Grafana (production only)\n# Restrict network access (firewall rules)\n</code></pre>"},{"location":"setup/observability/#retention-tuning","title":"Retention Tuning","text":"<pre><code># .phlo/prometheus/prometheus.yml\nglobal:\n  # Reduce for less storage\n  scrape_interval: 30s\n\n# Add storage retention flags\ncommand:\n  - \"--storage.tsdb.retention.time=7d\" # Reduce from 30d\n</code></pre> <pre><code># .phlo/loki/loki-config.yml\nlimits_config:\n  retention_period: 168h # 7 days instead of 30\n</code></pre>"},{"location":"setup/observability/#alerting","title":"Alerting","text":"<p>For production, add Alertmanager:</p> <pre><code># docker-compose.yml (add to observability profile)\nalertmanager:\n  image: prom/alertmanager:v0.27.0\n  ports:\n    - \"9093:9093\"\n  volumes:\n    - ./.phlo/alertmanager:/etc/alertmanager\n</code></pre> <p>Configure routes for Slack, PagerDuty, email, etc.</p>"},{"location":"setup/observability/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"setup/observability/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"Metric Threshold Action Trino query duration p95 &gt; 30s Investigate slow queries MinIO S3 operations/sec Sudden spike Check ingestion jobs Postgres connections &gt; 80% max Scale or pool tuning Nessie HTTP 5xx rate &gt; 1% Check catalog health Dagster failure rate &gt; 5% Review pipeline logs"},{"location":"setup/observability/#creating-slos","title":"Creating SLOs","text":"<p>Example: 99% of queries complete in &lt; 10s</p> <pre><code># Query success rate\nsum(rate(trino_execution_CompletedQueries[5m]))\n/\nsum(rate(trino_execution_TotalQueries[5m]))\n\n# Query duration SLI\nhistogram_quantile(0.99, rate(trino_execution_ExecutionTime_bucket[5m])) &lt; 10\n</code></pre>"},{"location":"setup/observability/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"setup/observability/#custom-metrics","title":"Custom Metrics","text":"<p>Expose custom metrics from Python code:</p> <pre><code>from prometheus_client import Counter, Histogram\n\npipeline_runs = Counter('cascade_pipeline_runs_total', 'Total pipeline runs')\npipeline_duration = Histogram('cascade_pipeline_duration_seconds', 'Pipeline duration')\n\n@asset\ndef my_asset(context):\n    with pipeline_duration.time():\n        # ... work ...\n        pass\n    pipeline_runs.inc()\n</code></pre> <p>Then scrape via Dagster's metrics endpoint (if exposed).</p>"},{"location":"setup/observability/#distributed-tracing-future","title":"Distributed Tracing (Future)","text":"<p>Add Tempo for distributed tracing:</p> <pre><code># docker-compose.yml\ntempo:\n  image: grafana/tempo:latest\n  profiles: [\"observability\"]\n</code></pre> <p>Integrate with Dagster, Trino, and dbt for end-to-end trace visibility.</p>"},{"location":"setup/observability/#references","title":"References","text":"<ul> <li>Prometheus Documentation</li> <li>Loki Documentation</li> <li>Grafana Alloy Documentation</li> <li>Grafana Dashboards</li> <li>LogQL Syntax</li> <li>PromQL Syntax</li> </ul>"},{"location":"setup/observability/#support","title":"Support","text":"<p>For issues specific to Phlo observability:</p> <ol> <li>Check logs: <code>docker compose logs &lt;service&gt;</code></li> <li>Verify health: <code>make health-observability</code></li> <li>Review configurations in <code>.phlo/prometheus/</code>, <code>.phlo/loki/</code>, <code>.phlo/alloy/</code></li> <li>Consult upstream documentation for Prometheus, Loki, Grafana</li> </ol>"},{"location":"setup/openmetadata/","title":"OpenMetadata Data Catalog Setup Guide","text":""},{"location":"setup/openmetadata/#overview","title":"Overview","text":"<p>OpenMetadata is an open-source data catalog that provides a unified platform for data discovery, governance, and collaboration. This guide explains how to set up and use OpenMetadata with Phlo to enable self-service data discovery.</p>"},{"location":"setup/openmetadata/#what-is-a-data-catalog","title":"What is a Data Catalog?","text":"<p>A data catalog is a searchable inventory of your data assets that helps users:</p> <ul> <li>Discover datasets through search and browsing</li> <li>Understand data through metadata, descriptions, and lineage</li> <li>Access data through multiple interfaces (SQL, APIs, dashboards)</li> <li>Govern data with ownership, tags, and quality metrics</li> </ul>"},{"location":"setup/openmetadata/#why-openmetadata-for-phlo","title":"Why OpenMetadata for Phlo?","text":"<p>OpenMetadata integrates seamlessly with Phlo's tech stack:</p> <ul> <li>\u2705 Trino connector - Auto-discovers Iceberg tables</li> <li>\u2705 Modern UI - Intuitive search and browsing experience</li> <li>\u2705 Active development - Regular updates and improvements</li> <li>\u2705 Simple architecture - MySQL + Elasticsearch (6GB RAM required)</li> <li>\u2705 Open source - No licensing costs</li> </ul>"},{"location":"setup/openmetadata/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         OpenMetadata Server (UI)           \u2502\n\u2502         http://localhost:10020              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    MySQL    \u2502 \u2502 Elasticsearch  \u2502\n\u2502  (metadata) \u2502 \u2502   (search)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 Ingests metadata from:\n       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Trino \u2192 Iceberg Tables (Nessie)   \u2502\n\u2502  - bronze.entries_cleaned          \u2502\n\u2502  - silver.glucose_daily_stats      \u2502\n\u2502  - gold.dim_date                   \u2502\n\u2502  - marts.glucose_analytics_mart    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"setup/openmetadata/#quick-start","title":"Quick Start","text":""},{"location":"setup/openmetadata/#1-start-openmetadata-services","title":"1. Start OpenMetadata Services","text":"<pre><code># Start the data catalog stack\nmake up-catalog\n\n# Check health status\nmake health-catalog\n</code></pre> <p>Expected output:</p> <pre><code>=== Data Catalog Health Check ===\nOpenMetadata:\n  Ready\n  UI: http://localhost:10020\n  Default credentials: admin / admin\nMySQL:\n  Ready\nElasticsearch:\n  Ready\n</code></pre>"},{"location":"setup/openmetadata/#2-access-openmetadata-ui","title":"2. Access OpenMetadata UI","text":"<pre><code># Open in browser\nmake catalog\n# Or manually visit: http://localhost:10020\n</code></pre> <p>Default credentials:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul> <p>\u26a0\ufe0f Security Note: Change the default password in production by updating <code>OPENMETADATA_ADMIN_PASSWORD</code> in <code>.phlo/.env.local</code></p>"},{"location":"setup/openmetadata/#3-complete-setup-checklist","title":"3. Complete Setup Checklist","text":"<p>After first-time setup, you MUST complete these steps in order:</p> <ol> <li>\u2705 Configure Trino data source (see \"Setting Up Trino Source\" section)</li> <li>\u2705 Create and run metadata ingestion pipeline</li> <li>\u2705 Enable search indices (Settings \u2192 OpenMetadata \u2192 Search \u2192 Run with \"Recreate Indexes\")</li> <li>\u2705 Verify search works on Explore page</li> </ol> <p>Skip step 3 at your own peril - without it, search and the Explore page will be completely broken.</p>"},{"location":"setup/openmetadata/#3-first-login","title":"3. First Login","text":"<ol> <li>Navigate to http://localhost:10020</li> <li>Login with <code>admin</code> / <code>admin</code></li> <li>Complete the welcome tour (optional)</li> <li>You'll see the empty catalog dashboard</li> </ol>"},{"location":"setup/openmetadata/#connecting-phlo-data-sources","title":"Connecting Phlo Data Sources","text":""},{"location":"setup/openmetadata/#step-1-add-trino-database-service","title":"Step 1: Add Trino Database Service","text":"<ol> <li>Click Settings (gear icon) in the top-right corner</li> <li>Navigate to Integrations \u2192 Databases</li> <li>Click Add New Service</li> <li>Select Trino from the list of database types</li> <li>Click Next</li> </ol>"},{"location":"setup/openmetadata/#step-2-configure-trino-connection","title":"Step 2: Configure Trino Connection","text":"<p>Service Name:</p> <pre><code>trino\n</code></pre> <p>Description (optional):</p> <pre><code>Phlo lakehouse Trino query engine with Iceberg catalog\n</code></pre> <p>Connection Configuration:</p> <p>Click on Basic authentication type, then configure:</p> Field Value Notes Host <code>trino</code> Docker service name (internal network) Port <code>8080</code> Internal container port (NOT 10005!) Username <code>phlo</code> Any username (no auth in dev) Catalog Leave empty We'll filter by catalog in ingestion Database Schema Leave empty - <p>Port Note: Trino runs on port <code>8080</code> inside the Docker network. The external host port <code>10005</code> is only for accessing Trino from your laptop. OpenMetadata uses the internal port <code>8080</code>.</p> <p>Advanced Options (leave defaults):</p> <ul> <li>Connection Options: (empty)</li> <li>Connection Arguments: (empty)</li> </ul> <p>Click Test Connection - you should see:</p> <pre><code>Connection test was successful\n</code></pre> <p>If the test fails, verify Trino is running:</p> <pre><code>docker ps --filter name=trino\ncurl http://localhost:8080/v1/info\n</code></pre> <p>Click Submit to save the service.</p>"},{"location":"setup/openmetadata/#step-3-configure-metadata-ingestion-pipeline","title":"Step 3: Configure Metadata Ingestion Pipeline","text":"<p>After creating the service, you'll be prompted to set up metadata ingestion.</p> <ol> <li>Pipeline Name: <code>trino-metadata</code></li> <li>Pipeline Type: Select Metadata Ingestion</li> <li>Click Next</li> </ol> <p>Metadata Configuration:</p> <p>Filter Patterns (CRITICAL - prevents crashes):</p> <pre><code>Database Filter Pattern:\n  Include: iceberg\n  Exclude: system\n\nSchema Filter Pattern:\n  Include: raw, bronze, silver, gold\n  Exclude: information_schema\n\nTable Filter Pattern:\n  Include: .*\n  Exclude: (leave empty)\n</code></pre> <p>Advanced Configuration:</p> <p>Enable/disable these options:</p> Option Enable? Reason Include Tables \u2705 Yes Core metadata Include Views \u2705 Yes Include views Include Tags \u2705 Yes Catalog tags Include Owners \u274c No Not used in dev Include Stored Procedures \u274c NO Causes crashes Mark Deleted Stored Procedures \u274c NO Causes crashes Include DDL \u274c No Not needed Override Metadata \u274c No - <p>Ingestion Settings:</p> <ul> <li>Thread Count: <code>1</code> (default)</li> <li>Timeout: <code>300</code> seconds (default)</li> </ul> <p>Query Log Ingestion: Skip for now (can add later for lineage)</p> <p>Click Next.</p>"},{"location":"setup/openmetadata/#step-4-configure-scheduling","title":"Step 4: Configure Scheduling","text":"<p>Schedule Type: Choose one:</p> <p>Option A: Manual (Recommended for Development)</p> <ul> <li>Select Manual</li> <li>Run ingestion on-demand when you need to refresh metadata</li> <li>Good for: Development, testing</li> </ul> <p>Option B: Scheduled (Recommended for Production)</p> <ul> <li>Select Scheduled</li> <li>Choose Cron Expression</li> <li>Enter: <code>0 3 * * *</code> (runs daily at 3 AM, after Dagster pipelines complete)</li> <li>Timezone: <code>UTC</code></li> </ul> <p>Click Next.</p>"},{"location":"setup/openmetadata/#step-5-review-and-deploy","title":"Step 5: Review and Deploy","text":"<ol> <li>Review your configuration</li> <li>Click Deploy</li> <li>The pipeline will be created</li> </ol> <p>You'll see a success message with the pipeline ID.</p>"},{"location":"setup/openmetadata/#step-6-run-initial-ingestion","title":"Step 6: Run Initial Ingestion","text":"<p>Via OpenMetadata UI:</p> <ol> <li>Go to Settings \u2192 Integrations \u2192 Databases</li> <li>Click on trino service</li> <li>Click Ingestions tab</li> <li>Find <code>trino-metadata</code> pipeline</li> <li>Click Run (play button)</li> <li>Monitor progress in real-time</li> </ol> <p>Expected Output:</p> <pre><code>INFO - Starting metadata ingestion\nINFO - Connecting to Trino at trino:8080\nINFO - Processing catalog: iceberg\nINFO - Processing schema: raw\nINFO - Discovered 1 tables in schema raw\nINFO - Processing table: glucose_entries\nINFO - Successfully ingested table: trino.iceberg.raw.glucose_entries\nINFO - Processing schema: bronze\nINFO - Processing schema: silver\nINFO - Processing schema: gold\nINFO - Metadata ingestion completed\nINFO - Total tables ingested: 15\nINFO - Total schemas ingested: 4\nINFO - Total errors: 0\n</code></pre>"},{"location":"setup/openmetadata/#step-7-enable-search-critical","title":"Step 7: Enable Search (CRITICAL)","text":"<p>After initial ingestion, search will NOT work until you populate the search index. This is a required step.</p> <p>Navigate to Search Settings:</p> <ol> <li>Go to Settings (gear icon) \u2192 OpenMetadata \u2192 Search</li> <li>Click on SearchIndexingApplication</li> <li>Click Run Now button</li> </ol> <p>Configure the Reindex Job:</p> <ol> <li>Enable \"Recreate Indexes\" toggle (IMPORTANT)</li> <li>Select \"All\" entities (or leave default)</li> <li>Click Submit</li> </ol> <p>Monitor Progress:</p> <ul> <li>The job will run for 1-2 minutes</li> <li>You'll see \"Success\" when complete</li> <li>Or check logs:   <code>bash   docker logs openmetadata-server --tail 100 | grep -i \"reindex\"</code></li> </ul> <p>What This Does:</p> <ul> <li>Creates the <code>all</code> search alias</li> <li>Populates search indices from metadata</li> <li>Enables Explore page and search functionality</li> </ul> <p>Without this step:</p> <ul> <li>Explore page will show error: \"Search failed due to Elasticsearch exception\"</li> <li>Global search will not work</li> <li>You can only navigate by direct URLs</li> </ul>"},{"location":"setup/openmetadata/#step-8-verify-everything-works","title":"Step 8: Verify Everything Works","text":"<p>Check Search Works:</p> <ol> <li>Go to Explore page</li> <li>Should see databases/tables listed (no errors)</li> <li>Type <code>glucose</code> in search bar</li> <li>Should find <code>glucose_entries</code> table</li> </ol> <p>Browse via Navigation:</p> <ol> <li>Go to Settings \u2192 Services \u2192 Database Services</li> <li>Click on trino</li> <li>Navigate: iceberg \u2192 raw \u2192 glucose_entries</li> <li>You should see:</li> <li>Table schema with all columns</li> <li>Column descriptions</li> <li>Sample data (if profiling enabled)</li> </ol> <p>Direct URL Access:</p> <pre><code>http://localhost:10020/table/trino.iceberg.raw.glucose_entries\n</code></pre> <p>Check Elasticsearch Indices:</p> <pre><code># Verify tables are indexed\ndocker exec openmetadata-elasticsearch curl -s \\\n  \"http://localhost:9200/table_search_index/_count\"\n\n# Should show: {\"count\": 6, ...}\n</code></pre>"},{"location":"setup/openmetadata/#configuration-reference","title":"Configuration Reference","text":""},{"location":"setup/openmetadata/#complete-ingestion-configuration","title":"Complete Ingestion Configuration","text":"<p>This is what your pipeline configuration looks like:</p> <pre><code>{\n  \"source\": {\n    \"type\": \"trino\",\n    \"serviceName\": \"trino\",\n    \"sourceConfig\": {\n      \"config\": {\n        \"type\": \"DatabaseMetadata\",\n        \"markDeletedTables\": true,\n        \"markDeletedStoredProcedures\": false,\n        \"includeTables\": true,\n        \"includeViews\": true,\n        \"includeTags\": true,\n        \"includeOwners\": false,\n        \"includeStoredProcedures\": false,\n        \"includeDDL\": false,\n        \"overrideMetadata\": false,\n        \"databaseFilterPattern\": {\n          \"includes\": [\"iceberg\"],\n          \"excludes\": [\"system\"]\n        },\n        \"schemaFilterPattern\": {\n          \"includes\": [\"raw\", \"bronze\", \"silver\", \"gold\"],\n          \"excludes\": [\"information_schema\"]\n        },\n        \"tableFilterPattern\": {\n          \"includes\": [\".*\"]\n        },\n        \"threads\": 1,\n        \"queryLogDuration\": 1\n      }\n    }\n  },\n  \"sink\": {\n    \"type\": \"metadata-rest\",\n    \"config\": {}\n  },\n  \"workflowConfig\": {\n    \"loggerLevel\": \"INFO\",\n    \"openMetadataServerConfig\": {\n      \"hostPort\": \"http://openmetadata-server:8585/api\",\n      \"authProvider\": \"openmetadata\",\n      \"securityConfig\": {\n        \"jwtToken\": \"&lt;auto-generated&gt;\"\n      }\n    }\n  },\n  \"airflowConfig\": {\n    \"pausePipeline\": false,\n    \"concurrency\": 1,\n    \"pipelineCatchup\": false,\n    \"pipelineTimezone\": \"UTC\",\n    \"retries\": 0,\n    \"retryDelay\": 300,\n    \"maxActiveRuns\": 1\n  }\n}\n</code></pre>"},{"location":"setup/openmetadata/#editing-configuration-later","title":"Editing Configuration Later","text":"<p>Via UI:</p> <ol> <li>Go to Settings \u2192 Databases \u2192 trino</li> <li>Click Ingestions tab</li> <li>Click Edit (pencil icon) on your pipeline</li> <li>Modify configuration</li> <li>Click Save</li> <li>Re-run the pipeline</li> </ol>"},{"location":"setup/openmetadata/#discovered-data-assets","title":"Discovered Data Assets","text":"<p>After ingestion, you'll see:</p>"},{"location":"setup/openmetadata/#bronze-layer-staging","title":"Bronze Layer (Staging)","text":"<ul> <li><code>bronze.entries_cleaned</code> - CGM entries with type conversions</li> <li><code>bronze.device_status_cleaned</code> - Device status events</li> </ul>"},{"location":"setup/openmetadata/#silver-layer-facts","title":"Silver Layer (Facts)","text":"<ul> <li><code>silver.glucose_daily_stats</code> - Daily glucose aggregations</li> <li><code>silver.glucose_weekly_stats</code> - Weekly glucose aggregations</li> </ul>"},{"location":"setup/openmetadata/#gold-layer-dimensions","title":"Gold Layer (Dimensions)","text":"<ul> <li><code>gold.dim_date</code> - Date dimension table</li> </ul>"},{"location":"setup/openmetadata/#marts-bi-ready","title":"Marts (BI-Ready)","text":"<ul> <li><code>marts.glucose_analytics_mart</code> - Published to Postgres for Superset</li> </ul>"},{"location":"setup/openmetadata/#using-the-data-catalog","title":"Using the Data Catalog","text":""},{"location":"setup/openmetadata/#search-for-data","title":"Search for Data","text":"<ol> <li>Use the search bar at the top</li> <li>Search by:</li> <li>Table name: <code>glucose_daily_stats</code></li> <li>Column name: <code>mean_glucose</code></li> <li>Description keywords: <code>\"blood sugar\"</code></li> <li>Tags: <code>#glucose</code> (after adding tags)</li> </ol>"},{"location":"setup/openmetadata/#view-table-details","title":"View Table Details","text":"<p>Click on any table to see:</p> <ul> <li>Schema: Column names, types, descriptions</li> <li>Sample Data: Preview of actual data</li> <li>Lineage: Visual graph showing upstream/downstream tables</li> <li>Queries: Recent SQL queries (if query log enabled)</li> <li>Usage: Access patterns and popularity</li> </ul>"},{"location":"setup/openmetadata/#add-documentation","title":"Add Documentation","text":"<ol> <li>Click on a table (e.g., <code>silver.glucose_daily_stats</code>)</li> <li>Click Edit (pencil icon)</li> <li>Add description:</li> </ol> <pre><code>## Description\n\nDaily aggregated glucose statistics including mean, standard deviation,\ntime in range, and estimated A1C.\n\n## Update Schedule\n\nUpdated daily at 2:00 AM UTC via Dagster pipeline.\n\n## Business Logic\n\n- `time_in_range_pct`: Percentage of readings between 70-180 mg/dL\n- `estimated_a1c`: Calculated using formula: (mean_glucose + 46.7) / 28.7\n</code></pre> <ol> <li> <p>Add column descriptions:</p> </li> <li> <p><code>date</code>: Measurement date (partition key)</p> </li> <li><code>mean_glucose</code>: Daily average glucose in mg/dL</li> <li><code>std_glucose</code>: Standard deviation of glucose readings</li> <li> <p><code>time_in_range_pct</code>: % of time in target range (70-180 mg/dL)</p> </li> <li> <p>Click Save</p> </li> </ol>"},{"location":"setup/openmetadata/#add-tags","title":"Add Tags","text":"<ol> <li>Click on a table</li> <li>Click Add Tag</li> <li>Use built-in tags or create custom:</li> <li><code>PII.None</code> - No personal information</li> <li><code>Tier.Bronze</code> / <code>Tier.Silver</code> / <code>Tier.Gold</code></li> <li>Create custom: <code>Healthcare</code>, <code>CGM</code>, <code>Analytics</code></li> </ol>"},{"location":"setup/openmetadata/#set-ownership","title":"Set Ownership","text":"<ol> <li>Click on a table</li> <li>Click Add Owner</li> <li>Select user or team (create teams in Settings)</li> </ol>"},{"location":"setup/openmetadata/#data-lineage","title":"Data Lineage","text":"<p>OpenMetadata can show visual lineage graphs:</p> <pre><code>entries_raw (raw)\n    \u2193\nentries_cleaned (bronze) \u2190 dbt model\n    \u2193\nglucose_daily_stats (silver) \u2190 dbt model\n    \u2193\nglucose_analytics_mart (mart) \u2190 Trino publish\n    \u2193\nSuperset Dashboard: \"CGM Overview\"\n</code></pre>"},{"location":"setup/openmetadata/#enable-lineage-tracking-with-dbt","title":"Enable Lineage Tracking with dbt","text":"<p>Lineage is automatically extracted from:</p> <ul> <li>dbt models - Shows dependencies between models and tables</li> <li>SQL queries - Enable query log ingestion (advanced)</li> </ul> <p>Step 1: Add dbt Service</p> <ol> <li>Go to Settings \u2192 Services \u2192 Pipeline Services</li> <li>Click Add Service</li> <li>Select dbt</li> <li>Configure:</li> </ol> Field Value Name <code>phlo-dbt</code> dbt Cloud API URL Leave empty (we use local files) dbt Cloud Account ID Leave empty <p>Click Next.</p> <p>Step 2: Configure dbt Metadata Ingestion</p> <ol> <li>Source Configuration:</li> </ol> Field Value Notes dbt Configuration Source <code>Local</code> We're using local files, not dbt Cloud dbt Catalog File Path <code>/dbt/target/catalog.json</code> Contains column-level metadata dbt Manifest File Path <code>/dbt/target/manifest.json</code> Contains lineage and dependencies dbt Run Results File Path <code>/dbt/target/run_results.json</code> Optional: test results <ol> <li> <p>Database Service Name: <code>trino</code></p> </li> <li> <p>This links dbt models to your Trino tables</p> </li> <li> <p>Must match the name of your Trino service</p> </li> <li> <p>Include Tags: <code>Yes</code> (Enable)</p> </li> <li>Imports dbt model tags as OpenMetadata tags</li> </ol> <p>Click Next.</p> <p>Step 3: Schedule dbt Ingestion</p> <p>For Development:</p> <ul> <li>Select Manual</li> <li>Run after <code>dbt run</code> or <code>dbt build</code> completes</li> </ul> <p>For Production:</p> <ul> <li>Select Scheduled</li> <li>Cron: <code>0 4 * * *</code> (4 AM, after Dagster + Trino ingestion)</li> </ul> <p>Click Next \u2192 Deploy.</p> <p>Step 4: Run dbt Ingestion</p> <ol> <li>Ensure dbt artifacts are fresh:</li> </ol> <p>```bash    # From your local machine    cd workflows/transforms/dbt    dbt compile --profiles-dir ./profiles</p> <p># Or run the full build    dbt build --profiles-dir ./profiles    ```</p> <ol> <li>Go to Settings \u2192 Integrations \u2192 Pipeline \u2192 phlo-dbt</li> <li>Click Ingestions tab</li> <li>Find <code>phlo-dbt-metadata</code> pipeline</li> <li>Click Run (play button)</li> </ol> <p>Expected Output:</p> <pre><code>INFO - Starting dbt metadata ingestion\nINFO - Reading manifest from /dbt/target/manifest.json\nINFO - Found 12 dbt models\nINFO - Processing model: glucose_daily_stats\nINFO - Linking model to table: trino.iceberg.silver.glucose_daily_stats\nINFO - Extracted lineage: bronze.entries_cleaned \u2192 silver.glucose_daily_stats\nINFO - Successfully ingested dbt metadata\n</code></pre> <p>What You'll See:</p> <p>After ingestion:</p> <ol> <li>Enhanced Table Descriptions: dbt model descriptions appear on tables</li> <li>Column Descriptions: From dbt schema YAML files</li> <li>Lineage Graphs: Visual connections between models</li> <li>dbt Tags: As OpenMetadata tags on tables</li> <li>Test Results: dbt tests show as data quality metrics</li> </ol> <p>Verify dbt Integration:</p> <p>Navigate to a table created by dbt (e.g., <code>silver.glucose_daily_stats</code>):</p> <ul> <li>Lineage tab: Shows upstream dependencies</li> <li>Schema tab: Has column descriptions from dbt</li> <li>Data Quality tab: Shows dbt test results</li> </ul>"},{"location":"setup/openmetadata/#advanced-features","title":"Advanced Features","text":""},{"location":"setup/openmetadata/#quality-checks","title":"Quality Checks","text":"<p>Add data quality tests in OpenMetadata UI:</p> <ol> <li>Navigate to table</li> <li>Click Profiler &amp; Data Quality</li> <li>Add tests:</li> <li>Column null checks</li> <li>Value range validations</li> <li>Uniqueness constraints</li> </ol>"},{"location":"setup/openmetadata/#glossary-terms","title":"Glossary Terms","text":"<p>Create a business glossary:</p> <ol> <li>Settings \u2192 Glossary</li> <li>Add terms:</li> <li>Time in Range (TIR): Percentage of glucose readings within target range (70-180 mg/dL)</li> <li>A1C: Hemoglobin A1C estimated from mean glucose</li> <li>Link terms to table columns</li> </ol>"},{"location":"setup/openmetadata/#api-access","title":"API Access","text":"<p>OpenMetadata provides a REST API:</p> <pre><code># Get all tables\ncurl http://localhost:10020/api/v1/tables\n\n# Get specific table\ncurl http://localhost:10020/api/v1/tables/name/iceberg.silver.glucose_daily_stats\n\n# Search\ncurl \"http://localhost:10020/api/v1/search/query?q=glucose\"\n</code></pre>"},{"location":"setup/openmetadata/#integration-with-phlo-workflows","title":"Integration with Phlo Workflows","text":""},{"location":"setup/openmetadata/#update-ingestion-schedule","title":"Update Ingestion Schedule","text":"<p>Match OpenMetadata ingestion with your Dagster pipelines:</p> <pre><code>Dagster Pipeline: Daily at 2:00 AM\nOpenMetadata Ingestion: Daily at 3:00 AM (1 hour after data refresh)\n</code></pre>"},{"location":"setup/openmetadata/#document-in-dbt-models","title":"Document in dbt Models","text":"<p>Add descriptions to dbt models that will appear in OpenMetadata:</p> <pre><code># workflows/transforms/dbt/models/silver/glucose_daily_stats.yml\nversion: 2\n\nmodels:\n  - name: glucose_daily_stats\n    description: |\n      Daily aggregated glucose statistics with A1C estimates.\n      Source: bronze.entries_cleaned\n      Refresh: Daily at 2 AM\n    columns:\n      - name: date\n        description: Measurement date (partition key)\n      - name: mean_glucose\n        description: Daily average glucose in mg/dL\n        tests:\n          - not_null\n          - dbt_utils.accepted_range:\n              min_value: 40\n              max_value: 400\n</code></pre>"},{"location":"setup/openmetadata/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/openmetadata/#openmetadata-ui-not-loading","title":"OpenMetadata UI Not Loading","text":"<pre><code># Check service health\nmake health-catalog\n\n# Check logs\ndocker logs openmetadata-server\ndocker logs openmetadata-mysql\ndocker logs openmetadata-elasticsearch\n</code></pre> <p>Common causes:</p> <ol> <li>Database migrations haven't run yet</li> <li>Server still initializing (wait 2-3 minutes)</li> <li>Dependency services not healthy</li> </ol> <p>Check migration status:</p> <pre><code># Verify migration completed successfully\ndocker logs openmetadata-migrate\n\n# Check exit code (should be 0)\ndocker inspect openmetadata-migrate --format='{{.State.ExitCode}}'\n</code></pre>"},{"location":"setup/openmetadata/#search-not-working-all-shards-failed-error","title":"Search Not Working / \"All Shards Failed\" Error","text":"<p>Symptom:</p> <ul> <li>Explore page shows: \"Search failed due to Elasticsearch exception [type=search_phase_execution_exception, reason=all shards failed]\"</li> <li>Global search returns no results</li> <li>Direct table URLs work fine</li> </ul> <p>Cause: The <code>all</code> search index is not populated after initial setup.</p> <p>Solution:</p> <ol> <li>Go to Settings \u2192 OpenMetadata \u2192 Search</li> <li>Click on SearchIndexingApplication</li> <li>Click Run Now</li> <li>IMPORTANT: Enable \"Recreate Indexes\" toggle</li> <li>Click Submit</li> <li>Wait 1-2 minutes for completion</li> <li>Refresh browser and test search</li> </ol> <p>If reindex fails with \"Invalid alias name [all]\" error:</p> <pre><code># Delete the incorrect index\ndocker exec openmetadata-elasticsearch curl -s -X DELETE \"http://localhost:9200/all\"\n\n# Run reindex again from UI with \"Recreate Indexes\" enabled\n</code></pre> <p>Verify search is working:</p> <pre><code># Check all alias exists and points to indices\ndocker exec openmetadata-elasticsearch curl -s \"http://localhost:9200/_cat/aliases?v\" | grep all\n\n# Test search query\ndocker exec openmetadata-elasticsearch curl -s \\\n  \"http://localhost:9200/all/_search?q=glucose&amp;size=1\" | grep -o '\"total\":{\"value\":[0-9]*'\n</code></pre>"},{"location":"setup/openmetadata/#missing-elasticsearch-indices","title":"Missing Elasticsearch Indices","text":"<p>Error: <code>Failed to find index table_search_index</code> or similar</p> <p>Root cause: OpenMetadata should create Elasticsearch indices automatically on startup. If you see this error, it indicates the initialization failed.</p> <p>Verification:</p> <pre><code># Check if indices exist\ndocker exec openmetadata-elasticsearch curl -s http://localhost:9200/_cat/indices | grep search_index\n</code></pre> <p>Solution: Indices should be created automatically by OpenMetadata. If they're missing after a fresh install:</p> <ol> <li>Check OpenMetadata server logs for initialization errors:</li> </ol> <p><code>bash    docker logs openmetadata-server | grep -i \"elastic\\|index\"</code></p> <ol> <li>Restart OpenMetadata server to trigger re-initialization:</li> </ol> <p><code>bash    docker restart openmetadata-server</code></p> <ol> <li>If problem persists, this is a bug in OpenMetadata 1.6.1 - report to the OpenMetadata team</li> </ol>"},{"location":"setup/openmetadata/#server-crashes-during-ingestion","title":"Server Crashes During Ingestion","text":"<p>Symptoms:</p> <ul> <li>OpenMetadata server keeps restarting</li> <li>Container shows \"Restarting (137)\" or \"Killed\"</li> <li>Logs show signal 9 or OOM errors</li> </ul> <p>Root cause: Memory exhaustion, typically from stored procedure queries</p> <p>Solution:</p> <ol> <li> <p>Disable stored procedures in ingestion configuration:</p> </li> <li> <p>Edit your Trino service ingestion pipeline</p> </li> <li>Advanced Options \u2192 Uncheck \"Include Stored Procedures\"</li> <li> <p>Advanced Options \u2192 Uncheck \"Mark Deleted Stored Procedures\"</p> </li> <li> <p>Add schema filters to reduce scope:</p> </li> </ol> <p><code>yaml    Include Schemas: raw,bronze,silver,gold    Exclude Schemas: information_schema,system    Include Databases: iceberg</code></p> <ol> <li>Increase server memory if crashes continue:    <code>yaml    # In docker-compose.yml    openmetadata-server:      environment:        OPENMETADATA_HEAP_OPTS: \"-Xmx4G -Xms4G\" # Increase from default</code></li> </ol>"},{"location":"setup/openmetadata/#elasticsearch-out-of-memory","title":"Elasticsearch Out of Memory","text":"<p>If you see OOM errors, increase memory:</p> <pre><code># In docker-compose.yml\nopenmetadata-elasticsearch:\n  environment:\n    ES_JAVA_OPTS: \"-Xms1g -Xmx1g\" # Increase from 512m\n</code></pre>"},{"location":"setup/openmetadata/#trino-connection-failed","title":"Trino Connection Failed","text":"<p>Ensure Trino is running:</p> <pre><code>make health\n\n# Start Trino if not running\nmake up-query\n</code></pre> <p>Check connection from OpenMetadata container:</p> <pre><code>docker exec -it openmetadata-server curl http://trino:8080/v1/info\n</code></pre>"},{"location":"setup/openmetadata/#ingestion-pipeline-failing","title":"Ingestion Pipeline Failing","text":"<ol> <li>Check logs in OpenMetadata UI \u2192 Settings \u2192 Services \u2192 Ingestion Logs</li> <li>Verify schemas exist in Trino:    <code>bash    make trino-shell    SHOW SCHEMAS FROM iceberg;</code></li> </ol>"},{"location":"setup/openmetadata/#tables-not-appearing-after-ingestion","title":"Tables Not Appearing After Ingestion","text":"<p>Problem: Ingestion runs successfully but tables don't show in UI</p> <p>Solutions:</p> <ol> <li> <p>Verify schema filters aren't too restrictive:</p> </li> <li> <p>Go to Settings \u2192 Database Services \u2192 trino \u2192 Ingestion</p> </li> <li>Check Include/Exclude Schemas configuration</li> <li> <p>Ensure your target schemas are included</p> </li> <li> <p>Search by fully qualified name:</p> </li> <li> <p>In UI search bar: <code>trino.iceberg.raw.glucose_entries</code></p> </li> <li> <p>Navigate to Explore \u2192 Tables and browse database hierarchy</p> </li> <li> <p>Check Elasticsearch index: <code>bash    # Verify table is in Elasticsearch    docker exec openmetadata-elasticsearch curl -s \\      \"http://localhost:9200/table_search_index/_search?q=glucose_entries&amp;pretty\"</code></p> </li> </ol>"},{"location":"setup/openmetadata/#resource-requirements","title":"Resource Requirements","text":"<p>Minimum:</p> <ul> <li>6 GB RAM</li> <li>4 vCPUs</li> <li>10 GB disk space</li> </ul> <p>Recommended:</p> <ul> <li>8 GB RAM</li> <li>6 vCPUs</li> <li>20 GB disk space</li> </ul>"},{"location":"setup/openmetadata/#best-practices","title":"Best Practices","text":"<ol> <li>Document Everything: Add descriptions to all tables and columns</li> <li>Use Tags: Create a consistent tagging strategy (layers, domains, sensitivity)</li> <li>Set Ownership: Assign owners to all datasets</li> <li>Regular Updates: Run ingestion daily to keep metadata fresh</li> <li>Quality Checks: Add data quality tests to critical tables</li> <li>Glossary: Maintain business terms for domain-specific language</li> </ol>"},{"location":"setup/openmetadata/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature OpenMetadata Amundsen DataHub Setup Ease \u2b50\u2b50 Moderate \u2b50 Easy \u2b50\u2b50\u2b50 Complex Active Development \u2705 Active \u26a0\ufe0f Slowed \u2705 Very Active UI/UX Excellent Good Very Good Resource Usage Medium (6GB) Low High (Kafka) Iceberg Support \u2705 Yes \u274c No \u2705 Yes"},{"location":"setup/openmetadata/#next-steps","title":"Next Steps","text":"<ol> <li>Enrich Metadata: Add descriptions and tags to all tables</li> <li>Set Up dbt Ingestion: Enable lineage tracking from dbt models</li> <li>Create Glossary: Define business terms</li> <li>Add Quality Tests: Monitor data quality</li> <li>Enable Alerts: Get notified about schema changes</li> </ol>"},{"location":"setup/openmetadata/#additional-resources","title":"Additional Resources","text":"<ul> <li>OpenMetadata Documentation</li> <li>Trino Connector Guide</li> <li>Data Quality Guide</li> <li>API Documentation</li> </ul>"},{"location":"setup/openmetadata/#related-phlo-documentation","title":"Related Phlo Documentation","text":"<ul> <li>Quick Start Guide - Get Phlo running</li> <li>API Documentation - FastAPI and Hasura setup</li> <li>dbt Development Guide - Creating dbt models</li> <li>Workflow Development Guide - Dagster pipelines</li> </ul>"},{"location":"setup/postgrest/","title":"PostgREST Deployment Guide","text":"<p>This guide covers deploying PostgREST alongside the existing FastAPI service in the Phlo Lakehouse platform.</p>"},{"location":"setup/postgrest/#overview","title":"Overview","text":"<p>PostgREST is a standalone web server that automatically generates a RESTful API from your PostgreSQL database schema. This deployment implements Phase 1 of the PostgREST Migration PRD.</p> <p>Key Benefits:</p> <ul> <li>Zero-code API generation from database schema</li> <li>Auto-generated OpenAPI documentation</li> <li>Database-native authentication with JWT</li> <li>High performance (written in Haskell)</li> <li>Reduced code complexity</li> </ul>"},{"location":"setup/postgrest/#prerequisites","title":"Prerequisites","text":"<ol> <li>PostgreSQL 12+ with:</li> <li><code>pgcrypto</code> extension (for password hashing)</li> <li> <p>Superuser access (for creating roles and schemas)</p> </li> <li> <p>Docker Compose (for containerized deployment)</p> </li> <li> <p>Phlo services running:</p> </li> <li>PostgreSQL database</li> <li>(Optional) FastAPI for comparison</li> </ol>"},{"location":"setup/postgrest/#deployment-steps","title":"Deployment Steps","text":""},{"location":"setup/postgrest/#step-1-update-environment-variables","title":"Step 1: Update Environment Variables","text":"<p>Initialize infra and copy the secrets template:</p> <pre><code>phlo services init\ncp .env.example .phlo/.env.local\n</code></pre> <p>Ensure these variables are set in <code>phlo.yaml</code> and <code>.phlo/.env.local</code>:</p> <pre><code># phlo.yaml (env:)\nenv:\n  POSTGREST_VERSION: v12.2.3\n  POSTGREST_PORT: 10018\n  POSTGREST_ADMIN_PORT: 10019\n</code></pre> <pre><code># .phlo/.env.local\nPOSTGREST_AUTHENTICATOR_PASSWORD=your_secure_password_here\nJWT_SECRET=your_jwt_secret_min_32_chars\n</code></pre> <p>Security Note: Change <code>POSTGREST_AUTHENTICATOR_PASSWORD</code> and <code>JWT_SECRET</code> in production!</p>"},{"location":"setup/postgrest/#step-2-apply-database-migrations","title":"Step 2: Apply Database Migrations","text":"<p>Run the migration script to create schemas, tables, and functions:</p> <pre><code>cd migrations/postgrest\n./apply_migrations.sh\n</code></pre> <p>This will:</p> <ol> <li>Install PostgreSQL extensions (<code>pgcrypto</code>)</li> <li>Create <code>auth</code> schema with users table</li> <li>Create JWT signing/verification functions</li> <li>Create <code>api</code> schema with glucose views</li> <li>Implement API functions (<code>login</code>, <code>glucose_statistics</code>, <code>user_info</code>)</li> <li>Create PostgreSQL roles and RLS policies</li> </ol> <p>Verify migrations:</p> <pre><code>psql -h localhost -p 10000 -U lake -d lakehouse -c \"\nSELECT schemaname, viewname\nFROM pg_views\nWHERE schemaname = 'api';\n\"\n</code></pre> <p>Expected output:</p> <pre><code> schemaname |        viewname\n------------+--------------------------\n api        | glucose_readings\n api        | glucose_daily_summary\n api        | glucose_hourly_patterns\n</code></pre>"},{"location":"setup/postgrest/#step-3-start-postgrest-service","title":"Step 3: Start PostgREST Service","text":"<p>Start PostgREST using Docker Compose:</p> <pre><code># Start with api profile (includes PostgREST, FastAPI, Hasura)\ndocker-compose --profile api up -d postgrest\n\n# Or start all services\ndocker-compose --profile all up -d\n</code></pre> <p>Check service status:</p> <pre><code>docker-compose ps postgrest\n</code></pre> <p>Expected output:</p> <pre><code>NAME        IMAGE                       STATUS\npostgrest   postgrest/postgrest:v12.2.3 Up (healthy)\n</code></pre> <p>View logs:</p> <pre><code>docker-compose logs -f postgrest\n</code></pre> <p>Expected logs:</p> <pre><code>postgrest | Listening on port 3000\npostgrest | Admin server listening on port 3001\npostgrest | Attempting to connect to the database...\npostgrest | Connection successful\n</code></pre>"},{"location":"setup/postgrest/#step-4-verify-health-check","title":"Step 4: Verify Health Check","text":"<p>Test the admin health endpoint:</p> <pre><code>curl http://localhost:10019/live\n</code></pre> <p>Expected response:</p> <pre><code>{ \"status\": \"UP\" }\n</code></pre>"},{"location":"setup/postgrest/#testing-the-api","title":"Testing the API","text":""},{"location":"setup/postgrest/#test-1-login-and-get-jwt-token","title":"Test 1: Login and Get JWT Token","text":"<pre><code>curl -X POST http://localhost:10018/rpc/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"username\": \"analyst\",\n    \"password\": \"analyst123\"\n  }' | jq .\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"user\": {\n    \"user_id\": \"uuid-here\",\n    \"username\": \"analyst\",\n    \"email\": \"analyst@phlo.local\",\n    \"role\": \"analyst\"\n  }\n}\n</code></pre> <p>Save the token for subsequent requests:</p> <pre><code>TOKEN=$(curl -s -X POST http://localhost:10018/rpc/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"analyst\", \"password\": \"analyst123\"}' | jq -r '.access_token')\n\necho \"Token: $TOKEN\"\n</code></pre>"},{"location":"setup/postgrest/#test-2-get-glucose-readings","title":"Test 2: Get Glucose Readings","text":"<pre><code>curl \"http://localhost:10018/glucose_readings?order=reading_date.desc&amp;limit=5\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>Expected response (array of glucose readings):</p> <pre><code>[\n  {\n    \"reading_date\": \"2024-01-15\",\n    \"day_name\": \"Monday\",\n    \"avg_glucose_mg_dl\": 125.5,\n    \"reading_count\": 288,\n    \"time_in_range_pct\": 78.2\n  },\n  ...\n]\n</code></pre>"},{"location":"setup/postgrest/#test-3-filter-glucose-readings","title":"Test 3: Filter Glucose Readings","text":"<p>PostgREST supports powerful filtering via URL parameters:</p> <pre><code># Get readings from a specific date range\ncurl \"http://localhost:10018/glucose_readings?reading_date=gte.2024-01-01&amp;reading_date=lte.2024-01-31\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n\n# Get readings where time in range &gt; 70%\ncurl \"http://localhost:10018/glucose_readings?time_in_range_pct=gt.70&amp;order=time_in_range_pct.desc\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n\n# Get only specific columns\ncurl \"http://localhost:10018/glucose_readings?select=reading_date,avg_glucose_mg_dl,time_in_range_pct&amp;limit=10\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>Filtering Operators:</p> <ul> <li><code>eq</code> - equals</li> <li><code>gt</code> - greater than</li> <li><code>gte</code> - greater than or equal</li> <li><code>lt</code> - less than</li> <li><code>lte</code> - less than or equal</li> <li><code>neq</code> - not equals</li> <li><code>like</code> - LIKE operator (use <code>*</code> for wildcard)</li> <li><code>ilike</code> - case-insensitive LIKE</li> </ul>"},{"location":"setup/postgrest/#test-4-get-daily-summary","title":"Test 4: Get Daily Summary","text":"<pre><code>curl \"http://localhost:10018/glucose_daily_summary?order=reading_date.desc&amp;limit=7\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre>"},{"location":"setup/postgrest/#test-5-get-hourly-patterns","title":"Test 5: Get Hourly Patterns","text":"<pre><code>curl \"http://localhost:10018/glucose_hourly_patterns?day_name=eq.Monday\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre>"},{"location":"setup/postgrest/#test-6-get-statistics-function-call","title":"Test 6: Get Statistics (Function Call)","text":"<pre><code># Default 30-day statistics\ncurl \"http://localhost:10018/rpc/glucose_statistics\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n\n# Custom period (7 days)\ncurl \"http://localhost:10018/rpc/glucose_statistics?period_days=7\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"period_days\": 7,\n  \"start_date\": \"2024-01-08\",\n  \"end_date\": \"2024-01-15\",\n  \"avg_glucose_mg_dl\": 128.5,\n  \"min_glucose_mg_dl\": 65,\n  \"max_glucose_mg_dl\": 210,\n  \"avg_time_in_range_pct\": 75.3,\n  \"total_readings\": 2016,\n  \"days_with_data\": 7\n}\n</code></pre>"},{"location":"setup/postgrest/#test-7-get-current-user-info","title":"Test 7: Get Current User Info","text":"<pre><code>curl \"http://localhost:10018/rpc/user_info\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"user_id\": \"uuid-here\",\n  \"username\": \"analyst\",\n  \"email\": \"analyst@phlo.local\",\n  \"role\": \"analyst\"\n}\n</code></pre>"},{"location":"setup/postgrest/#test-8-test-authentication-errors","title":"Test 8: Test Authentication Errors","text":"<pre><code># Missing token (should return 401)\ncurl -i \"http://localhost:10018/glucose_readings\"\n\n# Invalid token (should return 401)\ncurl -i \"http://localhost:10018/glucose_readings\" \\\n  -H \"Authorization: Bearer invalid.token.here\"\n</code></pre>"},{"location":"setup/postgrest/#test-9-get-openapi-documentation","title":"Test 9: Get OpenAPI Documentation","text":"<pre><code># Get OpenAPI spec as JSON\ncurl \"http://localhost:10018/\" \\\n  -H \"Accept: application/openapi+json\" | jq . &gt; openapi.json\n\n# View in browser (Swagger UI)\n# Open: http://localhost:10018/\n</code></pre>"},{"location":"setup/postgrest/#comparing-with-fastapi","title":"Comparing with FastAPI","text":""},{"location":"setup/postgrest/#response-format-comparison","title":"Response Format Comparison","text":"<p>FastAPI (current):</p> <pre><code>curl \"http://localhost:10010/api/v1/glucose/readings?limit=1\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>PostgREST (new):</p> <pre><code>curl \"http://localhost:10018/glucose_readings?limit=1\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre> <p>Both should return similar data structures. If there are differences, adjust the PostgreSQL views in <code>migrations/postgrest/004_api_schema.sql</code>.</p>"},{"location":"setup/postgrest/#monitoring","title":"Monitoring","text":""},{"location":"setup/postgrest/#health-checks","title":"Health Checks","text":"<p>Liveness probe (is service running?):</p> <pre><code>curl http://localhost:10019/live\n</code></pre> <p>Readiness probe (is service ready to accept requests?):</p> <pre><code>curl http://localhost:10019/ready\n</code></pre>"},{"location":"setup/postgrest/#performance-metrics","title":"Performance Metrics","text":"<p>PostgREST doesn't include built-in Prometheus metrics, but you can monitor via:</p> <ol> <li>PostgreSQL metrics (via <code>postgres-exporter</code>):</li> <li>Query performance: <code>pg_stat_statements</code></li> <li> <p>Connection pool usage: <code>pg_stat_activity</code></p> </li> <li> <p>Database logs:</p> </li> </ol> <p><code>bash    docker-compose logs -f postgres | grep \"duration:\"</code></p> <ol> <li>PostgREST logs:    <code>bash    docker-compose logs -f postgrest</code></li> </ol>"},{"location":"setup/postgrest/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/postgrest/#issue-connection-refused-error","title":"Issue: \"Connection refused\" error","text":"<p>Symptom:</p> <pre><code>psql: error: connection to server at \"localhost\" (127.0.0.1), port 10000 failed: Connection refused\n</code></pre> <p>Solution:</p> <ol> <li>Ensure PostgreSQL is running:</li> </ol> <p><code>bash    docker-compose ps postgres</code></p> <ol> <li>Start PostgreSQL if needed:</li> </ol> <p><code>bash    docker-compose up -d postgres</code></p> <ol> <li>Check logs:    <code>bash    docker-compose logs postgres</code></li> </ol>"},{"location":"setup/postgrest/#issue-relation-does-not-exist-error","title":"Issue: \"relation does not exist\" error","text":"<p>Symptom:</p> <pre><code>ERROR:  relation \"api.glucose_readings\" does not exist\n</code></pre> <p>Solution: Migrations not applied. Run:</p> <pre><code>cd migrations/postgrest\n./apply_migrations.sh\n</code></pre>"},{"location":"setup/postgrest/#issue-jwt-token-invalid-or-expired","title":"Issue: \"JWT token invalid or expired\"","text":"<p>Symptom:</p> <pre><code>HTTP 401 Unauthorized\n</code></pre> <p>Solutions:</p> <ol> <li>Check JWT secret matches:</li> </ol> <p><code>bash    echo $JWT_SECRET    docker-compose exec postgrest env | grep PGRST_JWT_SECRET</code></p> <ol> <li>Regenerate token:</li> </ol> <p><code>bash    TOKEN=$(curl -s -X POST http://localhost:10018/rpc/login \\      -H \"Content-Type: application/json\" \\      -d '{\"username\": \"analyst\", \"password\": \"analyst123\"}' | jq -r '.access_token')</code></p> <ol> <li>Verify token structure: <code>bash    echo $TOKEN | cut -d'.' -f2 | base64 -d | jq .</code></li> </ol>"},{"location":"setup/postgrest/#issue-permission-denied-for-schema-api","title":"Issue: \"permission denied for schema api\"","text":"<p>Symptom:</p> <pre><code>ERROR: permission denied for schema api\n</code></pre> <p>Solution: Role permissions not granted. Re-run migration:</p> <pre><code>psql -h localhost -p 10000 -U lake -d lakehouse -f migrations/postgrest/006_roles_and_rls.sql\n</code></pre>"},{"location":"setup/postgrest/#issue-postgrest-container-keeps-restarting","title":"Issue: PostgREST container keeps restarting","text":"<p>Check logs:</p> <pre><code>docker-compose logs postgrest\n</code></pre> <p>Common causes:</p> <ol> <li>Database connection failed: Verify <code>PGRST_DB_URI</code> in docker-compose.yml</li> <li>Invalid configuration: Check environment variables</li> <li>Port conflict: Ensure ports 10018/10019 are not in use</li> </ol> <p>Verify database connection manually:</p> <pre><code>psql \"postgresql://authenticator:authenticator_password_change_in_production@localhost:10000/lakehouse\" -c \"SELECT 1;\"\n</code></pre>"},{"location":"setup/postgrest/#next-steps","title":"Next Steps","text":""},{"location":"setup/postgrest/#phase-2-nginx-reverse-proxy-optional","title":"Phase 2: Nginx Reverse Proxy (Optional)","text":"<p>To maintain backward compatibility with existing FastAPI clients, configure nginx to route requests:</p> <pre><code># /api/v1/glucose/readings \u2192 /glucose_readings\nlocation /api/v1/glucose/readings {\n  rewrite ^/api/v1/glucose/readings$ /glucose_readings break;\n  proxy_pass http://postgrest:3000;\n  proxy_set_header Authorization $http_authorization;\n}\n</code></pre> <p>See PRD section 7.3 for full nginx configuration.</p>"},{"location":"setup/postgrest/#phase-3-testing","title":"Phase 3: Testing","text":"<p>Run integration tests:</p> <pre><code># Install test dependencies\npip install pytest httpx\n\n# Run tests\npytest tests/integration/test_postgrest.py -v\n</code></pre>"},{"location":"setup/postgrest/#phase-4-migration","title":"Phase 4: Migration","text":"<p>Once validated:</p> <ol> <li>Update client applications to use PostgREST endpoints</li> <li>Monitor error rates and performance</li> <li>Gradually shift traffic from FastAPI to PostgREST</li> <li>Decommission FastAPI service</li> </ol>"},{"location":"setup/postgrest/#api-reference","title":"API Reference","text":""},{"location":"setup/postgrest/#endpoints","title":"Endpoints","text":"Endpoint Method Description Auth Required <code>/rpc/login</code> POST Authenticate and get JWT token No <code>/glucose_readings</code> GET Get glucose readings Yes <code>/glucose_daily_summary</code> GET Get daily summary Yes <code>/glucose_hourly_patterns</code> GET Get hourly patterns Yes <code>/rpc/glucose_statistics</code> GET/POST Get statistics for period Yes <code>/rpc/user_info</code> GET/POST Get current user info Yes <code>/rpc/health</code> GET/POST Health check No <code>/</code> GET OpenAPI spec (with Accept header) No"},{"location":"setup/postgrest/#url-parameters-for-get-requests","title":"URL Parameters (for GET requests)","text":"<p>Filtering:</p> <pre><code>?column=operator.value\n</code></pre> <p>Examples:</p> <ul> <li><code>?reading_date=gte.2024-01-01</code></li> <li><code>?avg_glucose_mg_dl=gt.120&amp;time_in_range_pct=gte.70</code></li> </ul> <p>Ordering:</p> <pre><code>?order=column.asc|desc\n</code></pre> <p>Examples:</p> <ul> <li><code>?order=reading_date.desc</code></li> <li><code>?order=time_in_range_pct.desc,reading_date.asc</code></li> </ul> <p>Limiting:</p> <pre><code>?limit=N&amp;offset=M\n</code></pre> <p>Examples:</p> <ul> <li><code>?limit=10</code> (first 10 rows)</li> <li><code>?limit=10&amp;offset=20</code> (rows 21-30)</li> </ul> <p>Column selection:</p> <pre><code>?select=col1,col2,col3\n</code></pre> <p>Examples:</p> <ul> <li><code>?select=reading_date,avg_glucose_mg_dl</code></li> </ul>"},{"location":"setup/postgrest/#resources","title":"Resources","text":"<ul> <li>PostgREST Documentation: https://postgrest.org</li> <li>PostgREST API Reference: https://postgrest.org/en/stable/api.html</li> <li>PostgreSQL RLS Guide: https://www.postgresql.org/docs/current/ddl-rowsecurity.html</li> <li>Migration PRD: docs/prd-postgrest-migration.md</li> </ul>"},{"location":"setup/postgrest/#support","title":"Support","text":"<p>For issues or questions:</p> <ol> <li>Check logs: <code>docker-compose logs postgrest</code></li> <li>Verify database connection: <code>psql -h localhost -p 10000 -U lake -d lakehouse</code></li> <li>Review PRD risk mitigation strategies</li> <li>Open GitHub issue with logs and reproduction steps</li> </ol> <p>Last Updated: 2025-11-21 Version: 1.0 Status: Ready for Testing</p>"},{"location":"setup/security/","title":"Security Setup Guide","text":"<p>This guide covers enterprise security configuration for Phlo, including authentication, authorization, encryption, and audit logging.</p>"},{"location":"setup/security/#overview","title":"Overview","text":"<p>Phlo's underlying services (Trino, Nessie, MinIO, PostgreSQL) support enterprise security features. This guide explains how to enable and configure them.</p> Feature Trino Nessie MinIO PostgreSQL LDAP Auth Yes No Yes No OAuth2/OIDC Yes Yes Yes No TLS/HTTPS Yes Yes Yes Yes Access Control Yes Yes Yes (IAM) Yes (RLS) Audit Logging Yes Yes Yes Yes"},{"location":"setup/security/#quick-start","title":"Quick Start","text":"<p>For a minimal secure setup, configure these environment variables in your <code>.phlo/.env.local</code>:</p> <pre><code># Strong passwords (required)\nPOSTGRES_PASSWORD=&lt;generate-strong-password&gt;\nMINIO_ROOT_PASSWORD=&lt;generate-strong-password&gt;\nJWT_SECRET=&lt;generate-strong-password&gt;\n\n# Enable TLS (recommended for production)\nPOSTGRES_SSL_MODE=require\nMINIO_SERVER_URL=https://minio.example.com\n\n# Enable authentication (choose based on your IdP)\nNESSIE_OIDC_ENABLED=true\nNESSIE_OIDC_SERVER_URL=https://auth.example.com/realms/phlo\n</code></pre> <p>Generate strong passwords:</p> <pre><code>openssl rand -base64 32\n</code></pre>"},{"location":"setup/security/#authentication","title":"Authentication","text":""},{"location":"setup/security/#option-1-ldap-authentication","title":"Option 1: LDAP Authentication","text":"<p>LDAP works with Trino and MinIO. Configure your LDAP server details:</p>"},{"location":"setup/security/#trino-ldap","title":"Trino LDAP","text":"<pre><code># .phlo/.env.local\nTRINO_AUTH_TYPE=PASSWORD\nTRINO_LDAP_URL=ldaps://ldap.example.com:636\nTRINO_LDAP_USER_BIND_PATTERN=${USER}@example.com\n</code></pre> <p>Users authenticate with their LDAP credentials when connecting to Trino.</p>"},{"location":"setup/security/#minio-ldap","title":"MinIO LDAP","text":"<pre><code># .phlo/.env.local\nMINIO_LDAP_SERVER=ldap.example.com:636\nMINIO_LDAP_BIND_DN=cn=admin,dc=example,dc=com\nMINIO_LDAP_BIND_PASSWORD=ldap-admin-password\nMINIO_LDAP_USER_BASE_DN=ou=users,dc=example,dc=com\nMINIO_LDAP_USER_FILTER=(uid=%s)\n</code></pre>"},{"location":"setup/security/#option-2-oauth2oidc-authentication","title":"Option 2: OAuth2/OIDC Authentication","text":"<p>OIDC works with all services and is recommended for unified SSO.</p>"},{"location":"setup/security/#prerequisites","title":"Prerequisites","text":"<ol> <li>An OIDC provider (Keycloak, Auth0, Okta, Azure AD, etc.)</li> <li>Create clients for each service:</li> <li><code>nessie</code> - for Nessie catalog</li> <li><code>minio</code> - for MinIO storage</li> <li><code>trino</code> - for Trino query engine</li> </ol>"},{"location":"setup/security/#nessie-oidc","title":"Nessie OIDC","text":"<pre><code># .phlo/.env.local\nNESSIE_OIDC_ENABLED=true\nNESSIE_OIDC_SERVER_URL=https://auth.example.com/realms/phlo\nNESSIE_OIDC_CLIENT_ID=nessie\nNESSIE_OIDC_CLIENT_SECRET=your-client-secret\n</code></pre>"},{"location":"setup/security/#minio-oidc","title":"MinIO OIDC","text":"<pre><code># .phlo/.env.local\nMINIO_OIDC_CONFIG_URL=https://auth.example.com/realms/phlo/.well-known/openid-configuration\nMINIO_OIDC_CLIENT_ID=minio\nMINIO_OIDC_CLIENT_SECRET=your-client-secret\nMINIO_OIDC_CLAIM_NAME=policy\n</code></pre> <p>The <code>policy</code> claim in the JWT should contain the MinIO policy name(s) to assign.</p>"},{"location":"setup/security/#trino-oauth2","title":"Trino OAuth2","text":"<pre><code># .phlo/.env.local\nTRINO_AUTH_TYPE=OAUTH2\nTRINO_OAUTH2_ISSUER=https://auth.example.com/realms/phlo\nTRINO_OAUTH2_CLIENT_ID=trino\nTRINO_OAUTH2_CLIENT_SECRET=your-client-secret\n</code></pre>"},{"location":"setup/security/#keycloak-example-setup","title":"Keycloak Example Setup","text":"<p>If using Keycloak as your identity provider:</p> <ol> <li>Create a realm called <code>phlo</code></li> <li>Create clients:</li> </ol> <pre><code>{\n  \"clientId\": \"nessie\",\n  \"protocol\": \"openid-connect\",\n  \"publicClient\": false,\n  \"standardFlowEnabled\": true,\n  \"serviceAccountsEnabled\": true\n}\n</code></pre> <ol> <li>Create users and assign roles</li> <li>Map roles to policies in MinIO</li> </ol>"},{"location":"setup/security/#authorization","title":"Authorization","text":""},{"location":"setup/security/#trino-access-control","title":"Trino Access Control","text":"<p>Create an access control rules file:</p> <pre><code>{\n  \"catalogs\": [\n    {\n      \"catalog\": \"iceberg\",\n      \"allow\": \"all\"\n    }\n  ],\n  \"schemas\": [\n    {\n      \"catalog\": \"iceberg\",\n      \"schema\": \"raw\",\n      \"owner\": false\n    }\n  ],\n  \"tables\": [\n    {\n      \"catalog\": \"iceberg\",\n      \"schema\": \".*\",\n      \"table\": \".*\",\n      \"privileges\": [\"SELECT\"],\n      \"filter\": \"department = current_user_department()\"\n    }\n  ]\n}\n</code></pre> <p>Enable in <code>.phlo/.env.local</code>:</p> <pre><code>TRINO_ACCESS_CONTROL_TYPE=file\nTRINO_ACCESS_CONTROL_CONFIG_FILE=/etc/trino/access-control.json\n</code></pre> <p>Mount the rules file in your service configuration.</p>"},{"location":"setup/security/#nessie-authorization","title":"Nessie Authorization","text":"<p>Enable authorization:</p> <pre><code>NESSIE_AUTHZ_ENABLED=true\n</code></pre> <p>Nessie authorization rules are configured through the Nessie server. See Nessie Authorization for details.</p>"},{"location":"setup/security/#minio-iam-policies","title":"MinIO IAM Policies","text":"<p>Create IAM policies for fine-grained access:</p> <pre><code># Create a read-only policy\nmc admin policy create local readonly-lake - &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n      \"Resource\": [\"arn:aws:s3:::lake/*\", \"arn:aws:s3:::lake\"]\n    }\n  ]\n}\nEOF\n\n# Attach to user\nmc admin policy attach local readonly-lake --user analyst\n</code></pre>"},{"location":"setup/security/#postgresql-row-level-security","title":"PostgreSQL Row-Level Security","text":"<p>Phlo includes a pre-built RLS framework in PostgREST. Enable it:</p> <pre><code>-- Enable RLS on a table\nALTER TABLE marts.customers ENABLE ROW LEVEL SECURITY;\n\n-- Create policy for analysts (see only their region)\nCREATE POLICY analyst_region ON marts.customers\n  FOR SELECT\n  TO analyst\n  USING (region = current_setting('app.user_region'));\n\n-- Create policy for admins (see everything)\nCREATE POLICY admin_all ON marts.customers\n  FOR ALL\n  TO admin\n  USING (true);\n</code></pre> <p>The JWT token should include claims that set the user's role and context.</p>"},{"location":"setup/security/#encryption","title":"Encryption","text":""},{"location":"setup/security/#tlshttps","title":"TLS/HTTPS","text":""},{"location":"setup/security/#postgresql-ssl","title":"PostgreSQL SSL","text":"<pre><code># .phlo/.env.local\nPOSTGRES_SSL_MODE=require\n</code></pre> <p>For certificate verification:</p> <pre><code>POSTGRES_SSL_MODE=verify-full\nPOSTGRES_SSL_CA_FILE=/path/to/ca.pem\n</code></pre>"},{"location":"setup/security/#minio-tls","title":"MinIO TLS","text":"<ol> <li>Place certificates in <code>./volumes/minio-certs/</code>:</li> <li><code>public.crt</code> - Server certificate</li> <li><code>private.key</code> - Private key</li> <li> <p><code>CAs/</code> - CA certificates (optional)</p> </li> <li> <p>Enable TLS:</p> </li> </ol> <pre><code>MINIO_SERVER_URL=https://minio.example.com:9000\n</code></pre>"},{"location":"setup/security/#trino-https","title":"Trino HTTPS","text":"<ol> <li>Create a Java keystore:</li> </ol> <pre><code>keytool -genkeypair -alias trino -keyalg RSA -keysize 2048 \\\n  -keystore keystore.jks -validity 365 \\\n  -dname \"CN=trino.example.com\"\n</code></pre> <ol> <li>Enable HTTPS:</li> </ol> <pre><code>TRINO_HTTPS_ENABLED=true\nTRINO_HTTPS_KEYSTORE_PATH=/etc/trino/keystore.jks\nTRINO_HTTPS_KEYSTORE_PASSWORD=your-keystore-password\n</code></pre>"},{"location":"setup/security/#encryption-at-rest","title":"Encryption at Rest","text":""},{"location":"setup/security/#minio-server-side-encryption","title":"MinIO Server-Side Encryption","text":"<pre><code>MINIO_AUTO_ENCRYPTION=on\n</code></pre> <p>This encrypts all objects automatically using MinIO's built-in encryption.</p> <p>For KMS-based encryption, configure a KMS provider:</p> <pre><code>MINIO_KMS_KES_ENDPOINT=https://kes.example.com:7373\nMINIO_KMS_KES_KEY_FILE=/path/to/kes-key.key\nMINIO_KMS_KES_CERT_FILE=/path/to/kes-cert.crt\nMINIO_KMS_KES_KEY_NAME=my-key\n</code></pre>"},{"location":"setup/security/#audit-logging","title":"Audit Logging","text":""},{"location":"setup/security/#minio-audit-logs","title":"MinIO Audit Logs","text":"<p>Send audit logs to a webhook:</p> <pre><code>MINIO_AUDIT_ENABLED=on\nMINIO_AUDIT_ENDPOINT=http://loki:3100/loki/api/v1/push\n</code></pre> <p>Or configure for Kafka:</p> <pre><code>MINIO_AUDIT_KAFKA_ENABLE=on\nMINIO_AUDIT_KAFKA_BROKERS=kafka:9092\nMINIO_AUDIT_KAFKA_TOPIC=minio-audit\n</code></pre>"},{"location":"setup/security/#postgresql-audit-logging","title":"PostgreSQL Audit Logging","text":"<p>Enable pgaudit extension:</p> <pre><code>-- In PostgreSQL\nCREATE EXTENSION IF NOT EXISTS pgaudit;\nALTER SYSTEM SET pgaudit.log = 'all';\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"setup/security/#trino-query-logging","title":"Trino Query Logging","text":"<p>Trino logs all queries by default. Configure log retention:</p> <pre><code># coordinator config\nquery.max-history=10000\n</code></pre>"},{"location":"setup/security/#existing-security-features","title":"Existing Security Features","text":"<p>Phlo already includes these security features that you can use:</p>"},{"location":"setup/security/#postgrest-jwt-authentication","title":"PostgREST JWT Authentication","text":"<p>Located in <code>packages/phlo-postgrest/src/phlo_postgrest/sql/</code>:</p> <ul> <li><code>003_jwt_functions.sql</code> - JWT signing and verification</li> <li><code>004_roles.sql</code> - Role-based access control</li> </ul> <p>Usage:</p> <pre><code># Get a JWT token\ncurl -X POST http://localhost:10018/rpc/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"admin123\"}'\n\n# Use token for authenticated requests\ncurl http://localhost:10018/customers \\\n  -H \"Authorization: Bearer &lt;token&gt;\"\n</code></pre>"},{"location":"setup/security/#hasura-permissions","title":"Hasura Permissions","text":"<p>Located in <code>packages/phlo-hasura/src/phlo_hasura/permissions.py</code>:</p> <ul> <li>Full RBAC with row and column-level permissions</li> <li>Configure via Hasura console or API</li> </ul>"},{"location":"setup/security/#observatory-token-auth","title":"Observatory Token Auth","text":"<p>Enable UI authentication:</p> <pre><code>OBSERVATORY_AUTH_ENABLED=true\nOBSERVATORY_AUTH_TOKEN=your-secure-token\n</code></pre>"},{"location":"setup/security/#production-checklist","title":"Production Checklist","text":"<p>Before going to production, ensure:</p> <ul> <li>[ ] All default passwords changed</li> <li>[ ] TLS enabled for all services</li> <li>[ ] Authentication configured (LDAP or OIDC)</li> <li>[ ] Access control rules defined</li> <li>[ ] Audit logging enabled</li> <li>[ ] Secrets stored securely (not in git)</li> <li>[ ] Network segmentation (services not exposed publicly)</li> <li>[ ] Regular credential rotation policy</li> </ul>"},{"location":"setup/security/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/security/#oidc-token-issues","title":"OIDC Token Issues","text":"<pre><code># Decode JWT to inspect claims\necho \"&lt;token&gt;\" | cut -d. -f2 | base64 -d | jq\n</code></pre>"},{"location":"setup/security/#ldap-connection-issues","title":"LDAP Connection Issues","text":"<pre><code># Test LDAP connection\nldapsearch -x -H ldaps://ldap.example.com:636 \\\n  -D \"cn=admin,dc=example,dc=com\" \\\n  -W -b \"dc=example,dc=com\" \"(uid=testuser)\"\n</code></pre>"},{"location":"setup/security/#certificate-issues","title":"Certificate Issues","text":"<pre><code># Verify certificate\nopenssl s_client -connect minio.example.com:9000 -showcerts\n\n# Check certificate dates\nopenssl x509 -in cert.pem -noout -dates\n</code></pre>"},{"location":"setup/security/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference - All security variables</li> <li>Production Deployment - Deployment guide</li> <li>Observability Setup - Monitoring and logging</li> </ul>"}]}